<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Time-Bounded Sequential Parameter Optimization</p>
    <p>Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown, Kevin P. Murphy</p>
    <p>Department of Computer Science University of British Columbia</p>
    <p>Canada {hutter, hoos, kevinlb, murphyk}@cs.ubc.ca</p>
  </div>
  <div class="page">
    <p>Automated Parameter Optimization</p>
    <p>Most algorithms have parameters</p>
    <p>I Decisions that are left open during algorithm design</p>
    <p>I Instantiate to optimize empirical performance</p>
    <p>I E.g. local search</p>
    <p>neighbourhoods, restarts, types of perturbations, tabu length (or range for it), etc</p>
    <p>I E.g., tree search</p>
    <p>Branching heuristics, no-good learning, restarts, pre-processing, etc</p>
    <p>Automatically find good instantiation of parameters I Eliminate most tedious part of algorithm design and end use</p>
    <p>I Save development time &amp; improve performance</p>
  </div>
  <div class="page">
    <p>Automated Parameter Optimization</p>
    <p>Most algorithms have parameters</p>
    <p>I Decisions that are left open during algorithm design</p>
    <p>I Instantiate to optimize empirical performance I E.g. local search</p>
    <p>neighbourhoods, restarts, types of perturbations, tabu length (or range for it), etc</p>
    <p>I E.g., tree search</p>
    <p>Branching heuristics, no-good learning, restarts, pre-processing, etc</p>
    <p>Automatically find good instantiation of parameters I Eliminate most tedious part of algorithm design and end use</p>
    <p>I Save development time &amp; improve performance</p>
  </div>
  <div class="page">
    <p>Automated Parameter Optimization</p>
    <p>Most algorithms have parameters</p>
    <p>I Decisions that are left open during algorithm design</p>
    <p>I Instantiate to optimize empirical performance I E.g. local search</p>
    <p>neighbourhoods, restarts, types of perturbations, tabu length (or range for it), etc</p>
    <p>I E.g., tree search</p>
    <p>Branching heuristics, no-good learning, restarts, pre-processing, etc</p>
    <p>Automatically find good instantiation of parameters I Eliminate most tedious part of algorithm design and end use</p>
    <p>I Save development time &amp; improve performance</p>
  </div>
  <div class="page">
    <p>Automated Parameter Optimization</p>
    <p>Most algorithms have parameters</p>
    <p>I Decisions that are left open during algorithm design</p>
    <p>I Instantiate to optimize empirical performance I E.g. local search</p>
    <p>neighbourhoods, restarts, types of perturbations, tabu length (or range for it), etc</p>
    <p>I E.g., tree search</p>
    <p>Branching heuristics, no-good learning, restarts, pre-processing, etc</p>
    <p>Automatically find good instantiation of parameters I Eliminate most tedious part of algorithm design and end use</p>
    <p>I Save development time &amp; improve performance</p>
  </div>
  <div class="page">
    <p>Parameter Optimization Methods</p>
    <p>I Lots of work on numerical parameters, e.g.</p>
    <p>CALIBRA [Adenso-Diaz &amp; Laguna, 06]  Population-based, e.g. CMA-ES [Hansen et al, 95-present]</p>
    <p>I Categorical parameters</p>
    <p>Racing algorithms, F-Race [Birattari et al., 02-present]  Iterated Local Search, ParamILS [Hutter et al., AAAI 07 &amp; JAIR09]</p>
    <p>I Success of parameter optimization</p>
    <p>Many parameters (e.g., CPLEX with 63 parameters)  Large speedups (sometimes orders of magnitude!)  For many problems: SAT, MIP, time-tabling, protein folding, ...</p>
  </div>
  <div class="page">
    <p>Parameter Optimization Methods</p>
    <p>I Lots of work on numerical parameters, e.g.</p>
    <p>CALIBRA [Adenso-Diaz &amp; Laguna, 06]  Population-based, e.g. CMA-ES [Hansen et al, 95-present]</p>
    <p>I Categorical parameters</p>
    <p>Racing algorithms, F-Race [Birattari et al., 02-present]  Iterated Local Search, ParamILS [Hutter et al., AAAI 07 &amp; JAIR09]</p>
    <p>I Success of parameter optimization</p>
    <p>Many parameters (e.g., CPLEX with 63 parameters)  Large speedups (sometimes orders of magnitude!)  For many problems: SAT, MIP, time-tabling, protein folding, ...</p>
  </div>
  <div class="page">
    <p>Parameter Optimization Methods</p>
    <p>I Lots of work on numerical parameters, e.g.</p>
    <p>CALIBRA [Adenso-Diaz &amp; Laguna, 06]  Population-based, e.g. CMA-ES [Hansen et al, 95-present]</p>
    <p>I Categorical parameters</p>
    <p>Racing algorithms, F-Race [Birattari et al., 02-present]  Iterated Local Search, ParamILS [Hutter et al., AAAI 07 &amp; JAIR09]</p>
    <p>I Success of parameter optimization</p>
    <p>Many parameters (e.g., CPLEX with 63 parameters)  Large speedups (sometimes orders of magnitude!)  For many problems: SAT, MIP, time-tabling, protein folding, ...</p>
  </div>
  <div class="page">
    <p>Limitations of Model-Free Parameter Optimization</p>
    <p>Model-free methods only return the best parameter setting I Often that is all you need</p>
    <p>E.g.: end user can customize algorithm</p>
    <p>I But sometimes we would like to know more</p>
    <p>How important is each of the parameters?  Which parameters interact?  For which types of instances is a parameter setting good? Inform algorithm designer</p>
    <p>Response surface models can help I Predictive models of algorithm performance with given</p>
    <p>parameter settings</p>
  </div>
  <div class="page">
    <p>Limitations of Model-Free Parameter Optimization</p>
    <p>Model-free methods only return the best parameter setting I Often that is all you need</p>
    <p>E.g.: end user can customize algorithm</p>
    <p>I But sometimes we would like to know more</p>
    <p>How important is each of the parameters?  Which parameters interact?  For which types of instances is a parameter setting good? Inform algorithm designer</p>
    <p>Response surface models can help I Predictive models of algorithm performance with given</p>
    <p>parameter settings</p>
  </div>
  <div class="page">
    <p>Limitations of Model-Free Parameter Optimization</p>
    <p>Model-free methods only return the best parameter setting I Often that is all you need</p>
    <p>E.g.: end user can customize algorithm</p>
    <p>I But sometimes we would like to know more</p>
    <p>How important is each of the parameters?  Which parameters interact?  For which types of instances is a parameter setting good? Inform algorithm designer</p>
    <p>Response surface models can help I Predictive models of algorithm performance with given</p>
    <p>parameter settings</p>
  </div>
  <div class="page">
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Original SPO [Bartz-Beielstein et al., 05-present] I SPO toolbox I Set of interactive tools for parameter optimization</p>
    <p>I Studied SPO components [Hutter et al, GECCO-09] I Want completely automated tool More robust version: SPO+</p>
    <p>I This work: TB-SPO, reduce computational overheads I Ongoing work: extend TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances  Very promising results for both</p>
  </div>
  <div class="page">
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Original SPO [Bartz-Beielstein et al., 05-present] I SPO toolbox I Set of interactive tools for parameter optimization</p>
    <p>I Studied SPO components [Hutter et al, GECCO-09] I Want completely automated tool More robust version: SPO+</p>
    <p>I This work: TB-SPO, reduce computational overheads I Ongoing work: extend TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances  Very promising results for both</p>
  </div>
  <div class="page">
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Original SPO [Bartz-Beielstein et al., 05-present] I SPO toolbox I Set of interactive tools for parameter optimization</p>
    <p>I Studied SPO components [Hutter et al, GECCO-09] I Want completely automated tool More robust version: SPO+</p>
    <p>I This work: TB-SPO, reduce computational overheads</p>
    <p>I Ongoing work: extend TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances  Very promising results for both</p>
  </div>
  <div class="page">
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Original SPO [Bartz-Beielstein et al., 05-present] I SPO toolbox I Set of interactive tools for parameter optimization</p>
    <p>I Studied SPO components [Hutter et al, GECCO-09] I Want completely automated tool More robust version: SPO+</p>
    <p>I This work: TB-SPO, reduce computational overheads I Ongoing work: extend TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances</p>
    <p>Very promising results for both</p>
  </div>
  <div class="page">
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Original SPO [Bartz-Beielstein et al., 05-present] I SPO toolbox I Set of interactive tools for parameter optimization</p>
    <p>I Studied SPO components [Hutter et al, GECCO-09] I Want completely automated tool More robust version: SPO+</p>
    <p>I This work: TB-SPO, reduce computational overheads I Ongoing work: extend TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances  Very promising results for both</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>.</p>
    <p>.</p>
    <p>True function</p>
    <p>.</p>
    <p>.</p>
    <p>First step</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>..</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>.</p>
    <p>First step</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Function evaluations</p>
    <p>.</p>
    <p>First step</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>.</p>
    <p>Function evaluations</p>
    <p>.</p>
    <p>First step</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>.</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>I Repeat 1-3 until time is up</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
  </div>
  <div class="page">
    <p>Sequential Model-Based Optimization (SMBO) Blackbox function optimization; function = algo. performance</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Second step 8</p>
  </div>
  <div class="page">
    <p>Computational Overhead due to Models: Example Example times</p>
    <p>I Repeat 1-3 until time is up</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Second step 9</p>
  </div>
  <div class="page">
    <p>Computational Overhead due to Models: Example Example times</p>
    <p>I Repeat 1-3 until time is up</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Second step 9</p>
  </div>
  <div class="page">
    <p>Computational Overhead due to Models: Example Example times</p>
    <p>I Repeat 1-3 until time is up</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Second step 9</p>
  </div>
  <div class="page">
    <p>Computational Overhead due to Models: Example Example times</p>
    <p>I Repeat 1-3 until time is up</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Second step 9</p>
  </div>
  <div class="page">
    <p>Computational Overhead due to Models: Example Example times</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>First step</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Second step 9</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Removing the costly initial design (phase 0)</p>
    <p>I How to choose number of param. settings in initial design? I Too large: take too long to evaluate all of the settings I Too small: poor first model, might not recover</p>
    <p>I Our solution: simply drop the initial design I Instead: interleave random settings during the search I Much better anytime performance</p>
  </div>
  <div class="page">
    <p>Removing the costly initial design (phase 0)</p>
    <p>I How to choose number of param. settings in initial design? I Too large: take too long to evaluate all of the settings I Too small: poor first model, might not recover</p>
    <p>I Our solution: simply drop the initial design I Instead: interleave random settings during the search I Much better anytime performance</p>
  </div>
  <div class="page">
    <p>Overhead due to Models</p>
    <p>Central SMBO algorithm loop</p>
    <p>I Repeat: Example times</p>
    <p>Only small fraction of time spent actually running algorithms</p>
    <p>Solution 1</p>
    <p>I Do more algorithm runs to bound model overhead</p>
    <p>Select not one but many promising points (little overhead)  Perform runs for at least as long as phases 1 and 2 took</p>
  </div>
  <div class="page">
    <p>Overhead due to Models</p>
    <p>Central SMBO algorithm loop</p>
    <p>I Repeat: Example times</p>
    <p>Only small fraction of time spent actually running algorithms</p>
    <p>Solution 1</p>
    <p>I Do more algorithm runs to bound model overhead</p>
    <p>Select not one but many promising points (little overhead)  Perform runs for at least as long as phases 1 and 2 took</p>
  </div>
  <div class="page">
    <p>Which Setting to Perform How Many Runs for</p>
    <p>Heuristic Mechanism I Compare one configuration  at a time to the incumbent inc</p>
    <p>Use mechanism from SPO+:  Incrementally perform runs for  until either</p>
    <p>+ Empirical performance for  worse than for inc drop  + Performed as many runs for  as for inc  becomes new inc</p>
    <p>I Stop once time bound is reached</p>
    <p>Algorithms I TB-SPO</p>
    <p>Get ordered list of promising parameter settings using model  Interleave random settings: 2nd, 4th, etc  Compare one param. setting at a time to incumbent  Nice side effect: additional runs on good random settings</p>
    <p>I Strawman algorithm: TB-Random</p>
    <p>Only use random settings  Compare one param. setting at a time to incumbent</p>
  </div>
  <div class="page">
    <p>Which Setting to Perform How Many Runs for</p>
    <p>Heuristic Mechanism I Compare one configuration  at a time to the incumbent inc</p>
    <p>Use mechanism from SPO+:</p>
    <p>Incrementally perform runs for  until either</p>
    <p>+ Empirical performance for  worse than for inc drop  + Performed as many runs for  as for inc  becomes new inc</p>
    <p>I Stop once time bound is reached</p>
    <p>Algorithms I TB-SPO</p>
    <p>Get ordered list of promising parameter settings using model  Interleave random settings: 2nd, 4th, etc  Compare one param. setting at a time to incumbent  Nice side effect: additional runs on good random settings</p>
    <p>I Strawman algorithm: TB-Random</p>
    <p>Only use random settings  Compare one param. setting at a time to incumbent</p>
  </div>
  <div class="page">
    <p>Which Setting to Perform How Many Runs for</p>
    <p>Heuristic Mechanism I Compare one configuration  at a time to the incumbent inc</p>
    <p>Use mechanism from SPO+:  Incrementally perform runs for  until either</p>
    <p>+ Empirical performance for  worse than for inc drop  + Performed as many runs for  as for inc  becomes new inc</p>
    <p>I Stop once time bound is reached</p>
    <p>Algorithms I TB-SPO</p>
    <p>Get ordered list of promising parameter settings using model  Interleave random settings: 2nd, 4th, etc  Compare one param. setting at a time to incumbent  Nice side effect: additional runs on good random settings</p>
    <p>I Strawman algorithm: TB-Random</p>
    <p>Only use random settings  Compare one param. setting at a time to incumbent</p>
  </div>
  <div class="page">
    <p>Which Setting to Perform How Many Runs for</p>
    <p>Heuristic Mechanism I Compare one configuration  at a time to the incumbent inc</p>
    <p>Use mechanism from SPO+:  Incrementally perform runs for  until either</p>
    <p>+ Empirical performance for  worse than for inc drop  + Performed as many runs for  as for inc  becomes new inc</p>
    <p>I Stop once time bound is reached</p>
    <p>Algorithms I TB-SPO</p>
    <p>Get ordered list of promising parameter settings using model  Interleave random settings: 2nd, 4th, etc</p>
    <p>Compare one param. setting at a time to incumbent  Nice side effect: additional runs on good random settings</p>
    <p>I Strawman algorithm: TB-Random</p>
    <p>Only use random settings  Compare one param. setting at a time to incumbent</p>
  </div>
  <div class="page">
    <p>Which Setting to Perform How Many Runs for</p>
    <p>Heuristic Mechanism I Compare one configuration  at a time to the incumbent inc</p>
    <p>Use mechanism from SPO+:  Incrementally perform runs for  until either</p>
    <p>+ Empirical performance for  worse than for inc drop  + Performed as many runs for  as for inc  becomes new inc</p>
    <p>I Stop once time bound is reached</p>
    <p>Algorithms I TB-SPO</p>
    <p>Get ordered list of promising parameter settings using model  Interleave random settings: 2nd, 4th, etc  Compare one param. setting at a time to incumbent  Nice side effect: additional runs on good random settings</p>
    <p>I Strawman algorithm: TB-Random</p>
    <p>Only use random settings  Compare one param. setting at a time to incumbent</p>
  </div>
  <div class="page">
    <p>Which Setting to Perform How Many Runs for</p>
    <p>Heuristic Mechanism I Compare one configuration  at a time to the incumbent inc</p>
    <p>Use mechanism from SPO+:  Incrementally perform runs for  until either</p>
    <p>+ Empirical performance for  worse than for inc drop  + Performed as many runs for  as for inc  becomes new inc</p>
    <p>I Stop once time bound is reached</p>
    <p>Algorithms I TB-SPO</p>
    <p>Get ordered list of promising parameter settings using model  Interleave random settings: 2nd, 4th, etc  Compare one param. setting at a time to incumbent  Nice side effect: additional runs on good random settings</p>
    <p>I Strawman algorithm: TB-Random</p>
    <p>Only use random settings  Compare one param. setting at a time to incumbent</p>
  </div>
  <div class="page">
    <p>Experimental validation: setup</p>
    <p>I Optimizing SLS algorithm SAPS</p>
    <p>Prominent SAT solver with 4 continuous parameters  Previously used to evaluate parameter optimization approaches</p>
    <p>I Seven different SAT instances</p>
    <p>1 Quasigroups with holes (QWH) instance used previously  3 instances from Quasigroup completion (QCP)  3 instances from Graph colouring based on smallworld graphs</p>
    <p>(SWGCP)</p>
  </div>
  <div class="page">
    <p>Experimental validation: setup</p>
    <p>I Optimizing SLS algorithm SAPS</p>
    <p>Prominent SAT solver with 4 continuous parameters  Previously used to evaluate parameter optimization approaches</p>
    <p>I Seven different SAT instances</p>
    <p>1 Quasigroups with holes (QWH) instance used previously  3 instances from Quasigroup completion (QCP)  3 instances from Graph colouring based on smallworld graphs</p>
    <p>(SWGCP)</p>
  </div>
  <div class="page">
    <p>Experimental validation: results</p>
    <p>SAPS-QWH instance</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>.</p>
    <p>Both methods with same LHD</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>TBSPO</p>
    <p>TB-SPO with empty LHD</p>
    <p>Scenario SPO+ TB-SPO TB-Random pval1 pval2</p>
    <p>Saps-QCP-med [102] 4.50  0.31 4.32  0.21 4.23  0.15 4  103 0.17 Saps-QCP-q075 3.77  9.72 0.19  0.02 0.19  0.01 2  106 0.78 Saps-QCP-q095 49.91  0.00 2.20  1.17 2.64  1.24 1  1010 0.12 Saps-QWH [103] 10.7  0.76 10.1  0.58 9.88  0.41 6  103 0.14 Saps-SWGCP-med 49.95  0.00 0.18  0.03 0.17  0.02 1  1010 0.37 Saps-SWGCP-q075 50  0 0.24  0.04 0.22  0.03 1  1010 0.08 Saps-SWGCP-q095 50  0 0.25  0.05 0.28  0.10 1  1010 0.89</p>
  </div>
  <div class="page">
    <p>Experimental validation: results</p>
    <p>SAPS-QWH instance</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>.</p>
    <p>Both methods with same LHD</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>TBSPO</p>
    <p>TB-SPO with empty LHD</p>
    <p>Scenario SPO+ TB-SPO TB-Random pval1 pval2</p>
    <p>Saps-QCP-med [102] 4.50  0.31 4.32  0.21 4.23  0.15 4  103 0.17 Saps-QCP-q075 3.77  9.72 0.19  0.02 0.19  0.01 2  106 0.78 Saps-QCP-q095 49.91  0.00 2.20  1.17 2.64  1.24 1  1010 0.12 Saps-QWH [103] 10.7  0.76 10.1  0.58 9.88  0.41 6  103 0.14 Saps-SWGCP-med 49.95  0.00 0.18  0.03 0.17  0.02 1  1010 0.37 Saps-SWGCP-q075 50  0 0.24  0.04 0.22  0.03 1  1010 0.08 Saps-SWGCP-q095 50  0 0.25  0.05 0.28  0.10 1  1010 0.89</p>
  </div>
  <div class="page">
    <p>Experimental validation: results</p>
    <p>SAPS-QWH instance</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>.</p>
    <p>Both methods with same LHD</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>TBSPO</p>
    <p>TB-SPO with empty LHD</p>
    <p>Scenario SPO+ TB-SPO</p>
    <p>TB-Random</p>
    <p>pval1</p>
    <p>pval2</p>
    <p>Saps-QCP-med [102] 4.50  0.31 4.32  0.21</p>
    <p>Saps-QCP-q075 3.77  9.72 0.19  0.02</p>
    <p>Saps-QCP-q095 49.91  0.00 2.20  1.17</p>
    <p>Saps-QWH [103] 10.7  0.76 10.1  0.58</p>
    <p>Saps-SWGCP-med 49.95  0.00 0.18  0.03</p>
    <p>Saps-SWGCP-q075 50  0 0.24  0.04</p>
    <p>Saps-SWGCP-q095 50  0 0.25  0.05</p>
  </div>
  <div class="page">
    <p>Experimental validation: results</p>
    <p>SAPS-QWH instance</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>.</p>
    <p>Both methods with same LHD</p>
    <p>CPU time t spent for configuration [s]</p>
    <p>p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n c</p>
    <p>e p</p>
    <p>t</p>
    <p>SPO+</p>
    <p>TBSPO (w/ LHD)</p>
    <p>TBSPO</p>
    <p>TB-SPO with empty LHD</p>
    <p>Scenario SPO+ TB-SPO TB-Random pval1 pval2</p>
    <p>Saps-QCP-med [102] 4.50  0.31 4.32  0.21 4.23  0.15 4  103 0.17 Saps-QCP-q075 3.77  9.72 0.19  0.02 0.19  0.01 2  106 0.78 Saps-QCP-q095 49.91  0.00 2.20  1.17 2.64  1.24 1  1010 0.12 Saps-QWH [103] 10.7  0.76 10.1  0.58 9.88  0.41 6  103 0.14 Saps-SWGCP-med 49.95  0.00 0.18  0.03 0.17  0.02 1  1010 0.37 Saps-SWGCP-q075 50  0 0.24  0.04 0.22  0.03 1  1010 0.08 Saps-SWGCP-q095 50  0 0.25  0.05 0.28  0.10 1  1010 0.89</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>I Model I  Fit standard GP assuming Gaussian observation noise</p>
    <p>I Model II (used in SPO, SPO+, and TB-SPO)  Compute empirical mean of responses at each param. setting  Fit noise-free GP to those means  But assumes empirical means are perfect (even when based on</p>
    <p>just 1 run!)  Cheaper (here 11 means vs 110 raw data points)</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>GP mean prediction</p>
    <p>GP mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model I: noisy fit of original response</p>
    <p>parameter x</p>
    <p>re s p o</p>
    <p>n s e</p>
    <p>y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model II: noise-free fit of empir. means</p>
  </div>
  <div class="page">
    <p>I Model I  Fit standard GP assuming Gaussian observation noise</p>
    <p>I Model II (used in SPO, SPO+, and TB-SPO)  Compute empirical mean of responses at each param. setting  Fit noise-free GP to those means</p>
    <p>But assumes empirical means are perfect (even when based on just 1 run!)</p>
    <p>Cheaper (here 11 means vs 110 raw data points)</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>GP mean prediction</p>
    <p>GP mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model I: noisy fit of original response</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model II: noise-free fit of empir. means 18</p>
  </div>
  <div class="page">
    <p>I Model I  Fit standard GP assuming Gaussian observation noise</p>
    <p>I Model II (used in SPO, SPO+, and TB-SPO)  Compute empirical mean of responses at each param. setting  Fit noise-free GP to those means  But assumes empirical means are perfect (even when based on</p>
    <p>just 1 run!)</p>
    <p>Cheaper (here 11 means vs 110 raw data points)</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>GP mean prediction</p>
    <p>GP mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model I: noisy fit of original response</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model II: noise-free fit of empir. means 18</p>
  </div>
  <div class="page">
    <p>I Model I  Fit standard GP assuming Gaussian observation noise</p>
    <p>I Model II (used in SPO, SPO+, and TB-SPO)  Compute empirical mean of responses at each param. setting  Fit noise-free GP to those means  But assumes empirical means are perfect (even when based on</p>
    <p>just 1 run!)  Cheaper (here 11 means vs 110 raw data points)</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>GP mean prediction</p>
    <p>GP mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model I: noisy fit of original response</p>
    <p>parameter x</p>
    <p>re s p o n s e y</p>
    <p>DACE mean prediction</p>
    <p>DACE mean +/ 2*stddev</p>
    <p>True function</p>
    <p>Function evaluations</p>
    <p>EI (scaled)</p>
    <p>Model II: noise-free fit of empir. means 18</p>
  </div>
  <div class="page">
    <p>How much faster is the approximate Gaussian Process?</p>
    <p>Complexity of Gaussian process regression (GPR)</p>
    <p>I n data points</p>
    <p>I Basic GPR equations: inverting n  n matrix I Numerical optimization of hyper-parameters: h steps</p>
    <p>O(h  n3) for model fitting I O(n2) for each model prediction</p>
    <p>Complexity of projected process (PP) approximation</p>
    <p>I Active set of p data points only invert p  p matrix I Throughout: use p = 300</p>
    <p>I O(n  p2 + h  p3) for model fitting I O(p2) for each model prediction</p>
  </div>
  <div class="page">
    <p>How much faster is the approximate Gaussian Process?</p>
    <p>Complexity of Gaussian process regression (GPR)</p>
    <p>I n data points</p>
    <p>I Basic GPR equations: inverting n  n matrix I Numerical optimization of hyper-parameters: h steps</p>
    <p>O(h  n3) for model fitting</p>
    <p>I O(n2) for each model prediction</p>
    <p>Complexity of projected process (PP) approximation</p>
    <p>I Active set of p data points only invert p  p matrix I Throughout: use p = 300</p>
    <p>I O(n  p2 + h  p3) for model fitting I O(p2) for each model prediction</p>
  </div>
  <div class="page">
    <p>How much faster is the approximate Gaussian Process?</p>
    <p>Complexity of Gaussian process regression (GPR)</p>
    <p>I n data points</p>
    <p>I Basic GPR equations: inverting n  n matrix I Numerical optimization of hyper-parameters: h steps</p>
    <p>O(h  n3) for model fitting I O(n2) for each model prediction</p>
    <p>Complexity of projected process (PP) approximation</p>
    <p>I Active set of p data points only invert p  p matrix I Throughout: use p = 300</p>
    <p>I O(n  p2 + h  p3) for model fitting I O(p2) for each model prediction</p>
  </div>
  <div class="page">
    <p>How much faster is the approximate Gaussian Process?</p>
    <p>Complexity of Gaussian process regression (GPR)</p>
    <p>I n data points</p>
    <p>I Basic GPR equations: inverting n  n matrix I Numerical optimization of hyper-parameters: h steps</p>
    <p>O(h  n3) for model fitting I O(n2) for each model prediction</p>
    <p>Complexity of projected process (PP) approximation</p>
    <p>I Active set of p data points only invert p  p matrix I Throughout: use p = 300</p>
    <p>I O(n  p2 + h  p3) for model fitting I O(p2) for each model prediction</p>
  </div>
  <div class="page">
    <p>How much faster is the approximate Gaussian Process?</p>
    <p>Complexity of Gaussian process regression (GPR)</p>
    <p>I n data points</p>
    <p>I Basic GPR equations: inverting n  n matrix I Numerical optimization of hyper-parameters: h steps</p>
    <p>O(h  n3) for model fitting I O(n2) for each model prediction</p>
    <p>Complexity of projected process (PP) approximation</p>
    <p>I Active set of p data points only invert p  p matrix I Throughout: use p = 300</p>
    <p>I O(n  p2 + h  p3) for model fitting I O(p2) for each model prediction</p>
  </div>
  <div class="page">
    <p>Empirical Evaluation of the Model</p>
    <p>Empirical time performance (1 000 data points)</p>
    <p>PP NF 0.5</p>
    <p>QCPmed PP NF</p>
    <p>QCPq075 PP NF</p>
    <p>QCPq095 PP NF</p>
    <p>QWH PP NF</p>
    <p>SWGCPmed PP NF</p>
    <p>SWGCPq075 PP NF</p>
    <p>SWGCPq095</p>
    <p>Log10 of CPU time (in seconds)</p>
    <p>Empirical model quality</p>
    <p>I Measures correlation between</p>
    <p>how promising the model judges a parameter setting to be  true performance of that parameter setting (evaluated offline)</p>
    <p>PP NF</p>
    <p>QCPmed PP NF</p>
    <p>QCPq075 PP NF</p>
    <p>QCPq095 PP NF</p>
    <p>QWH PP NF</p>
    <p>SWGCPmed PP NF</p>
    <p>0.2</p>
    <p>SWGCPq075 PP NF</p>
    <p>0.2</p>
    <p>SWGCPq095</p>
    <p>Correlation (high is good, 1 is optimal)</p>
  </div>
  <div class="page">
    <p>Empirical Evaluation of the Model</p>
    <p>Empirical time performance (1 000 data points)</p>
    <p>PP NF 0.5</p>
    <p>QCPmed PP NF</p>
    <p>QCPq075 PP NF</p>
    <p>QCPq095 PP NF</p>
    <p>QWH PP NF</p>
    <p>SWGCPmed PP NF</p>
    <p>SWGCPq075 PP NF</p>
    <p>SWGCPq095</p>
    <p>Log10 of CPU time (in seconds)</p>
    <p>Empirical model quality</p>
    <p>I Measures correlation between</p>
    <p>how promising the model judges a parameter setting to be  true performance of that parameter setting (evaluated offline)</p>
    <p>PP NF</p>
    <p>QCPmed PP NF</p>
    <p>QCPq075 PP NF</p>
    <p>QCPq095 PP NF</p>
    <p>QWH PP NF</p>
    <p>SWGCPmed PP NF</p>
    <p>0.2</p>
    <p>SWGCPq075 PP NF</p>
    <p>0.2</p>
    <p>SWGCPq095</p>
    <p>Correlation (high is good, 1 is optimal)</p>
  </div>
  <div class="page">
    <p>Empirical Evaluation of the Model</p>
    <p>Empirical time performance (1 000 data points)</p>
    <p>PP NF 0.5</p>
    <p>QCPmed PP NF</p>
    <p>QCPq075 PP NF</p>
    <p>QCPq095 PP NF</p>
    <p>QWH PP NF</p>
    <p>SWGCPmed PP NF</p>
    <p>SWGCPq075 PP NF</p>
    <p>SWGCPq095</p>
    <p>Log10 of CPU time (in seconds)</p>
    <p>Empirical model quality</p>
    <p>I Measures correlation between</p>
    <p>how promising the model judges a parameter setting to be  true performance of that parameter setting (evaluated offline)</p>
    <p>PP NF</p>
    <p>QCPmed PP NF</p>
    <p>QCPq075 PP NF</p>
    <p>QCPq095 PP NF</p>
    <p>QWH PP NF</p>
    <p>SWGCPmed PP NF</p>
    <p>0.2</p>
    <p>SWGCPq075 PP NF</p>
    <p>0.2</p>
    <p>SWGCPq095</p>
    <p>Correlation (high is good, 1 is optimal) 20</p>
  </div>
  <div class="page">
    <p>Final Evaluation</p>
    <p>I Comparing: I R: TB-Random I S: TB-SPO</p>
    <p>I P: TB-SPO(PP) I F: FocusedILS (variant of ParamILS; limited by discretization)</p>
    <p>Scenario TB-Random TB-SPO TB-SPO(PP) FocusedILS</p>
    <p>Saps-QCP-med [102] 4.23  0.15 4.32  0.21 4.13  0.14 5.12  0.41 Saps-QCP-q075 0.19  0.01 0.19  0.02 0.18  0.01 0.24  0.02 Saps-QCP-q095 2.64  1.24 2.20  1.17 1.44  0.53 2.99  3.20 Saps-QWH [103] 9.88  0.41 10.1  0.58 9.42  0.32 10.6  0.49 Saps-SWGCP-med 0.17  0.02 0.18  0.03 0.16  0.02 0.27  0.12 Saps-SWGCP-q075 0.22  0.03 0.24  0.04 0.21  0.02 0.35  0.08 Saps-SWGCP-q095 0.28  0.10 0.25  0.05 0.23  0.05 0.37  0.16</p>
    <p>I TB-SPO(PP) best on all 7 instances I Good models do help</p>
  </div>
  <div class="page">
    <p>Final Evaluation</p>
    <p>I Comparing: I R: TB-Random I S: TB-SPO I P: TB-SPO(PP)</p>
    <p>I F: FocusedILS (variant of ParamILS; limited by discretization)</p>
    <p>Scenario TB-Random TB-SPO TB-SPO(PP) FocusedILS</p>
    <p>Saps-QCP-med [102] 4.23  0.15 4.32  0.21 4.13  0.14 5.12  0.41 Saps-QCP-q075 0.19  0.01 0.19  0.02 0.18  0.01 0.24  0.02 Saps-QCP-q095 2.64  1.24 2.20  1.17 1.44  0.53 2.99  3.20 Saps-QWH [103] 9.88  0.41 10.1  0.58 9.42  0.32 10.6  0.49 Saps-SWGCP-med 0.17  0.02 0.18  0.03 0.16  0.02 0.27  0.12 Saps-SWGCP-q075 0.22  0.03 0.24  0.04 0.21  0.02 0.35  0.08 Saps-SWGCP-q095 0.28  0.10 0.25  0.05 0.23  0.05 0.37  0.16</p>
    <p>I TB-SPO(PP) best on all 7 instances I Good models do help</p>
  </div>
  <div class="page">
    <p>Final Evaluation</p>
    <p>I Comparing: I R: TB-Random I S: TB-SPO I P: TB-SPO(PP) I F: FocusedILS (variant of ParamILS; limited by discretization)</p>
    <p>Scenario TB-Random TB-SPO TB-SPO(PP) FocusedILS</p>
    <p>Saps-QCP-med [102] 4.23  0.15 4.32  0.21 4.13  0.14 5.12  0.41 Saps-QCP-q075 0.19  0.01 0.19  0.02 0.18  0.01 0.24  0.02 Saps-QCP-q095 2.64  1.24 2.20  1.17 1.44  0.53 2.99  3.20 Saps-QWH [103] 9.88  0.41 10.1  0.58 9.42  0.32 10.6  0.49 Saps-SWGCP-med 0.17  0.02 0.18  0.03 0.16  0.02 0.27  0.12 Saps-SWGCP-q075 0.22  0.03 0.24  0.04 0.21  0.02 0.35  0.08 Saps-SWGCP-q095 0.28  0.10 0.25  0.05 0.23  0.05 0.37  0.16</p>
    <p>I TB-SPO(PP) best on all 7 instances I Good models do help</p>
  </div>
  <div class="page">
    <p>Final Evaluation</p>
    <p>I Comparing: I R: TB-Random I S: TB-SPO I P: TB-SPO(PP) I F: FocusedILS (variant of ParamILS; limited by discretization)</p>
    <p>Scenario TB-Random TB-SPO TB-SPO(PP) FocusedILS</p>
    <p>Saps-QCP-med [102] 4.23  0.15 4.32  0.21 4.13  0.14 5.12  0.41 Saps-QCP-q075 0.19  0.01 0.19  0.02 0.18  0.01 0.24  0.02 Saps-QCP-q095 2.64  1.24 2.20  1.17 1.44  0.53 2.99  3.20 Saps-QWH [103] 9.88  0.41 10.1  0.58 9.42  0.32 10.6  0.49 Saps-SWGCP-med 0.17  0.02 0.18  0.03 0.16  0.02 0.27  0.12 Saps-SWGCP-q075 0.22  0.03 0.24  0.04 0.21  0.02 0.35  0.08 Saps-SWGCP-q095 0.28  0.10 0.25  0.05 0.23  0.05 0.37  0.16</p>
    <p>I TB-SPO(PP) best on all 7 instances I Good models do help</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Parameter optimization I Can be performed by automated approaches</p>
    <p>Sometimes much better than by human experts  Automation can cut development time &amp; improve results</p>
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Uses predictive models of algorithm performance</p>
    <p>I Can inform algorithm designer about parameter space</p>
    <p>Time-Bounded SPO I Eliminates Computational Overheads of SPO</p>
    <p>No need for costly initial design  Bounds the time spent building and using the model  Uses efficient approximate Gaussian process model Practical for parameter optimization in a time budget</p>
    <p>I Clearly outperforms previous SPO versions and ParamILS</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Parameter optimization I Can be performed by automated approaches</p>
    <p>Sometimes much better than by human experts  Automation can cut development time &amp; improve results</p>
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Uses predictive models of algorithm performance</p>
    <p>I Can inform algorithm designer about parameter space</p>
    <p>Time-Bounded SPO I Eliminates Computational Overheads of SPO</p>
    <p>No need for costly initial design  Bounds the time spent building and using the model  Uses efficient approximate Gaussian process model Practical for parameter optimization in a time budget</p>
    <p>I Clearly outperforms previous SPO versions and ParamILS</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Parameter optimization I Can be performed by automated approaches</p>
    <p>Sometimes much better than by human experts  Automation can cut development time &amp; improve results</p>
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Uses predictive models of algorithm performance</p>
    <p>I Can inform algorithm designer about parameter space</p>
    <p>Time-Bounded SPO I Eliminates Computational Overheads of SPO</p>
    <p>No need for costly initial design  Bounds the time spent building and using the model  Uses efficient approximate Gaussian process model Practical for parameter optimization in a time budget</p>
    <p>I Clearly outperforms previous SPO versions and ParamILS</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Parameter optimization I Can be performed by automated approaches</p>
    <p>Sometimes much better than by human experts  Automation can cut development time &amp; improve results</p>
    <p>Sequential Parameter Optimization (SPO)</p>
    <p>I Uses predictive models of algorithm performance</p>
    <p>I Can inform algorithm designer about parameter space</p>
    <p>Time-Bounded SPO I Eliminates Computational Overheads of SPO</p>
    <p>No need for costly initial design  Bounds the time spent building and using the model  Uses efficient approximate Gaussian process model Practical for parameter optimization in a time budget</p>
    <p>I Clearly outperforms previous SPO versions and ParamILS 23</p>
  </div>
  <div class="page">
    <p>Current &amp; Future Work</p>
    <p>I Generalizations of TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances</p>
    <p>I Applications of Automated Parameter Optimization</p>
    <p>Optimization of MIP solvers [to be submitted to CP-AI-OR]</p>
    <p>I Use models to gain scientific insights</p>
    <p>Importance of each parameter  Interaction of parameters  Interaction of parameters and instances features</p>
    <p>I Per-instance approaches</p>
    <p>Build joint model of instance features and parameters  Given a new unseen instance:</p>
    <p>+ Compute instance features (fast) + Use parameter setting predicted to be best for those features</p>
  </div>
  <div class="page">
    <p>Current &amp; Future Work</p>
    <p>I Generalizations of TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances</p>
    <p>I Applications of Automated Parameter Optimization</p>
    <p>Optimization of MIP solvers [to be submitted to CP-AI-OR]</p>
    <p>I Use models to gain scientific insights</p>
    <p>Importance of each parameter  Interaction of parameters  Interaction of parameters and instances features</p>
    <p>I Per-instance approaches</p>
    <p>Build joint model of instance features and parameters  Given a new unseen instance:</p>
    <p>+ Compute instance features (fast) + Use parameter setting predicted to be best for those features</p>
  </div>
  <div class="page">
    <p>Current &amp; Future Work</p>
    <p>I Generalizations of TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances</p>
    <p>I Applications of Automated Parameter Optimization</p>
    <p>Optimization of MIP solvers [to be submitted to CP-AI-OR]</p>
    <p>I Use models to gain scientific insights</p>
    <p>Importance of each parameter  Interaction of parameters  Interaction of parameters and instances features</p>
    <p>I Per-instance approaches</p>
    <p>Build joint model of instance features and parameters  Given a new unseen instance:</p>
    <p>+ Compute instance features (fast) + Use parameter setting predicted to be best for those features</p>
  </div>
  <div class="page">
    <p>Current &amp; Future Work</p>
    <p>I Generalizations of TB-SPO to handle</p>
    <p>Categorical parameters  Multiple benchmark instances</p>
    <p>I Applications of Automated Parameter Optimization</p>
    <p>Optimization of MIP solvers [to be submitted to CP-AI-OR]</p>
    <p>I Use models to gain scientific insights</p>
    <p>Importance of each parameter  Interaction of parameters  Interaction of parameters and instances features</p>
    <p>I Per-instance approaches</p>
    <p>Build joint model of instance features and parameters  Given a new unseen instance:</p>
    <p>+ Compute instance features (fast) + Use parameter setting predicted to be best for those features</p>
  </div>
</Presentation>
