<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs</p>
    <p>Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, Joo Seong Jeong, Dong-Jin Shin, Byung-Gon Chun</p>
  </div>
  <div class="page">
    <p>Deep Learning (DL) Frameworks</p>
    <p>ExecuteDefine</p>
    <p>Images From: http://www.mdpi.com/ https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/ Going Deeper with Convolutions, 2014, https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7 Short-Term Load Forecasting Using EMD-LSTM Neural Networks with a Xgboost Algorithm for Feature Importance Evaluation, Energies 2017 https://skymind.ai/wiki/generative-adversarial-network-gan https://en.wikipedia.org/wiki/Reinforcement_learning https://medium.com/@Petuum/intro-to-dynamic-neural-networks-and-dynet-67694b18cb23</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>Build a Symbolic Graph  Execute the Graph</p>
    <p>def build_graph(g): x = g.input(float) linear = g.add(g.mul(W, x), b)</p>
    <p>build_graph(graph) run_graph(graph, x_data)</p>
    <p>Imperative DL Frameworks</p>
    <p>Directly Execute the Computations</p>
    <p>Two Paradigms</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>W</p>
    <p>b</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>imperative</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>Build a Symbolic Graph  Execute the Graph</p>
    <p>def build_graph(g): x = g.input(float) linear = g.add(g.mul(W, x), b)</p>
    <p>build_graph(graph) run_graph(graph, x_data)</p>
    <p>Imperative DL Frameworks</p>
    <p>Directly Execute the Computations</p>
    <p>Two Paradigms</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>W</p>
    <p>b</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>imperative</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>Build a Symbolic Graph  Execute the Graph</p>
    <p>def build_graph(g): x = g.input(float) linear = g.add(g.mul(W, x), b)</p>
    <p>build_graph(graph) run_graph(graph, x_data)</p>
    <p>Imperative DL Frameworks</p>
    <p>Directly Execute the Computations</p>
    <p>Two Paradigms</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>W</p>
    <p>b</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>imperative</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>Build a Symbolic Graph  Execute the Graph</p>
    <p>def build_graph(g): x = g.input(float) linear = g.add(g.mul(W, x), b)</p>
    <p>build_graph(graph) run_graph(graph, x_data)</p>
    <p>Imperative DL Frameworks</p>
    <p>Directly Execute the Computations</p>
    <p>Two Paradigms</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>W</p>
    <p>b</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>imperative</p>
  </div>
  <div class="page">
    <p>Imperative DL Frameworks</p>
    <p>Directly Execute the Computations</p>
    <p>Symbolic DL Frameworks</p>
    <p>Build a Symbolic Graph  Execute the Graph</p>
    <p>def build_graph(g): x = g.input(float) linear = g.add(g.mul(W, x), b)</p>
    <p>build_graph(graph) run_graph(graph, x_data)</p>
    <p>Two Paradigms</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>W</p>
    <p>b</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>imperative</p>
  </div>
  <div class="page">
    <p>Imperative DL Frameworks</p>
    <p>Directly Execute the Computations</p>
    <p>Symbolic DL Frameworks</p>
    <p>Build a Symbolic Graph  Execute the Graph</p>
    <p>def build_graph(g): x = g.input(float) linear = g.add(g.mul(W, x), b)</p>
    <p>build_graph(graph) run_graph(graph, x_data)</p>
    <p>Two Paradigms</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>W</p>
    <p>b</p>
    <p>def linear(x): return W * x + b linear(x_data)</p>
    <p>imperative</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile, ...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>Pros &amp; Cons</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>Performance Programmability</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile,...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>Pros &amp; Cons</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>Performance Programmability</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile,...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>Pros &amp; Cons</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>Performance Programmability</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile,...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>Pros &amp; Cons</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>Performance Programmability</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile, ...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>Pros &amp; Cons</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>Performance Programmability</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile, ...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>Pros &amp; Cons</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>Performance Programmability</p>
  </div>
  <div class="page">
    <p>Symbolic DL Frameworks</p>
    <p>+ Easy to Optimize + Compiler Optimization + Parallel Execution of Operations + Deploy on GPU, Cluster, Mobile,...</p>
    <p>- Decoupled View: Hard to Program &amp; Debug</p>
    <p>Imperative DL Frameworks</p>
    <p>+ Direct Execution: Easy to Program &amp; Debug</p>
    <p>- Hard to Optimize</p>
    <p>What People Want Is...</p>
    <p>Pros</p>
    <p>Cons</p>
    <p>ProgrammabilityPerformance</p>
  </div>
  <div class="page">
    <p>Imperative DL Program</p>
    <p>def foo(x): prod = mul(3, x) return add(prod, 2)</p>
    <p>JANUS: Combining the Best of Both Worlds</p>
    <p>Symbolic DL Graph</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>Easy Programmability High Performance</p>
    <p>Transparent Conversion</p>
  </div>
  <div class="page">
    <p>JANUS: Combining the Best of Both Worlds</p>
    <p>11 models in 5 major neural network categories:  Convolutional Neural Networks (CNN) LeNet, ResNet-50, Inception-v3  Recurrent Neural Networks (RNN) LSTM, LM  Recursive Neural Networks (TreeNN) TreeRNN, TreeLSTM  Generative Adversarial Networks (GAN) GAN, PIX2PIX  Deep Reinforcement Learning (DRL) A3C, PPO</p>
    <p>Up to 47.6x speedup compared to imperative DL framework, comparable performance (within 4%) to symbolic DL framework with unmodified imperative DL programs</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Approach</p>
    <p>Challenges</p>
    <p>JANUS</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Challenges in Graph Conversion</p>
    <p>Imperative DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
    <p>Transparent Conversion</p>
    <p>Symbolic DL Graph</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
  </div>
  <div class="page">
    <p>Challenges in Graph Conversion</p>
    <p>Imperative Python DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
    <p>Transparent Conversion</p>
    <p>Symbolic DL Graph</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
    <p>De-facto Standard Language for DL Programming</p>
  </div>
  <div class="page">
    <p>Challenges in Graph Conversion</p>
    <p>Imperative Python DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
    <p>Transparent Conversion</p>
    <p>Symbolic DL Graph</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
  </div>
  <div class="page">
    <p>Discrepancy between Python Programs and DL Graphs</p>
    <p>Transparent Conversion?</p>
    <p>Dynamic Static</p>
    <p>Imperative Python DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
    <p>Symbolic DL Graph</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
  </div>
  <div class="page">
    <p>Transparent Conversion</p>
    <p>Discrepancy between Python Programs and DL Graphs</p>
    <p>? Characteristics  determined at runtime  change at runtime</p>
    <p>Dynamic Static</p>
    <p>Imperative Python DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
    <p>Symbolic DL Graph</p>
    <p>x</p>
    <p>Mul</p>
    <p>Add</p>
  </div>
  <div class="page">
    <p>Transparent Conversion</p>
    <p>Discrepancy between Python Programs and DL Graphs</p>
    <p>Symbolic DL Graph</p>
    <p>INT, 10x1 x</p>
    <p>INT, 10x1 Mul</p>
    <p>INT, 10x1 Add</p>
    <p>INT 3</p>
    <p>INT 2</p>
    <p>Characteristics  must be given</p>
    <p>when building a graph</p>
    <p>? Characteristics  determined at runtime  change at runtime</p>
    <p>Dynamic Static</p>
    <p>Imperative Python DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
  </div>
  <div class="page">
    <p>Imperative Python DL Program</p>
    <p>def foo(x): tmp = mul(3, x) return add(tmp, 2)</p>
    <p>Transparent Conversion</p>
    <p>Discrepancy between Python Programs and DL Graphs</p>
    <p>Symbolic DL Graph</p>
    <p>INT, 10x1 x</p>
    <p>INT, 10x1 Mul</p>
    <p>INT, 10x1 ADD</p>
    <p>INT 3</p>
    <p>INT 2</p>
    <p>Characteristics  must be given</p>
    <p>when building a graph</p>
    <p>? Characteristics  determined at runtime  change at runtime</p>
    <p>Dynamic Static</p>
    <p>DST: NEED INFO</p>
    <p>SRC: NO INFO</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>Example: Recurrent Neural Network (RNN)</p>
    <p>Correctness &amp; Performance</p>
    <p>of Graph Execution</p>
    <p>Dynamic Features of Python</p>
    <p>Dynamic Control Flow</p>
    <p>Dynamic Types</p>
    <p>Impure Functions</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>sawThey dogsseq[0]:</p>
    <p>sheWas sick?seq[1]:</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>RNN Cell</p>
    <p>sawThey dogs</p>
    <p>state</p>
    <p>out [0]</p>
    <p>seq[0]:</p>
    <p>sheWas sick?seq[1]:</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>sawThey dogs</p>
    <p>state</p>
    <p>out [1]</p>
    <p>out [0]</p>
    <p>seq[0]:</p>
    <p>sheWas sick?seq[1]:</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>sawThey dogs</p>
    <p>state</p>
    <p>out [1]</p>
    <p>out [0]</p>
    <p>out [2]</p>
    <p>seq[0]:</p>
    <p>sheWas sick?seq[1]:</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>RNN Example</p>
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>state</p>
    <p>CellSwitch</p>
    <p>Merge</p>
    <p>i&lt;N</p>
    <p>Next</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>RNN Example</p>
    <p>Correct</p>
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>state</p>
    <p>CellSwitch</p>
    <p>Merge</p>
    <p>i&lt;N</p>
    <p>Next</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>RNN Example</p>
    <p>Correct  Slow</p>
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>state</p>
    <p>CellSwitch</p>
    <p>Merge</p>
    <p>i&lt;N</p>
    <p>Next</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>RNN Example</p>
    <p>Correct  Slow</p>
    <p>Fast</p>
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>state</p>
    <p>CellSwitch</p>
    <p>Merge</p>
    <p>i&lt;N</p>
    <p>Next</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>RNN Example</p>
    <p>Correct  Slow</p>
    <p>Fast  Incorrect  Need Info</p>
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>state</p>
    <p>CellSwitch</p>
    <p>Merge</p>
    <p>i&lt;N</p>
    <p>Next</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>sawThey dogs</p>
    <p>state state</p>
    <p>out [1]</p>
    <p>out [0]</p>
    <p>out [2]</p>
    <p>seq[0]:</p>
    <p>sheWas sick?seq[1]:</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>PlaceHolder type: int shape: ?</p>
    <p>PlaceHolder type: float</p>
    <p>shape: ?...</p>
    <p>Correct  Inefficient</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>PlaceHolder type: int</p>
    <p>shape: (3x128)</p>
    <p>Correct  Inefficient</p>
    <p>Fast  Incorrect  Need Info</p>
    <p>...</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
    <p>PlaceHolder type: int shape: ?</p>
    <p>PlaceHolder type: float</p>
    <p>shape: ?</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>sawThey dogs</p>
    <p>state state</p>
    <p>out [1]</p>
    <p>out [0]</p>
    <p>out [2]</p>
    <p>seq[0]:</p>
    <p>sheWas sick?seq[1]:</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>sawThey dogs</p>
    <p>state state</p>
    <p>out [1]</p>
    <p>out [0]</p>
    <p>out [2]</p>
    <p>seq[0]:</p>
    <p>sheWas sick?</p>
    <p>state</p>
    <p>seq[1]:</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>RNN Cell</p>
    <p>out [1]</p>
    <p>out [0]</p>
    <p>out [2]</p>
    <p>state</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>class RNNModel(object): def __call__(self, sequence):</p>
    <p>state = self.state outputs = [] for item in sequence:</p>
    <p>state = rnn_cell(state, item) outputs += [state]</p>
    <p>self.state = state return compute_loss(outputs)</p>
    <p>for sequence in sequences: optimize(lambda: model(sequence))</p>
    <p>RNN Example</p>
    <p>?</p>
    <p>Dynamic Control Flow Dynamic Types Impure Function</p>
  </div>
  <div class="page">
    <p>Challenge:</p>
    <p>achieving Correctness &amp; Performance at the same time</p>
    <p>Challenge Summary</p>
    <p>Imperative Python DL Program with Dynamic Features</p>
    <p>Correct &amp; Fast Symbolic DL Graph?</p>
  </div>
  <div class="page">
    <p>Challenge:</p>
    <p>achieving Correctness &amp; Performance at the same time</p>
    <p>Challenge Summary</p>
    <p>Imperative Python DL Program with Dynamic Features</p>
    <p>Correct Graph</p>
    <p>Fast Graph</p>
    <p>Slow</p>
    <p>Incorrect</p>
  </div>
  <div class="page">
    <p>Challenge:</p>
    <p>achieving Correctness &amp; Performance at the same time</p>
    <p>Challenge Summary</p>
    <p>Imperative Python DL Program with Dynamic Features</p>
    <p>Correct Graph</p>
    <p>Fast Graph</p>
    <p>Slow</p>
    <p>Incorrect</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Approach</p>
    <p>Challenges</p>
    <p>JANUS</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Solution: Speculative Graph Generation and Execution</p>
    <p>Goal: Correctness &amp; Performance</p>
    <p>[Performance] Speculatively Specialize the Graph  Make reasonable assumptions based on the execution history (Profiling)  Run specialized graph (Common Case)</p>
    <p>[Correctness] Validate Assumptions  Fallback if an assumption is broken (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Imperative Executor</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Overall Workflow on JANUS</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Imperative Executor</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>len:3</p>
    <p>Overall Workflow on JANUS</p>
    <p>Modified Python Interpreter for Transparent Profiling</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Overall Workflow on JANUS</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>Optimize Graph with Profile Information</p>
    <p>Standard Compiler Pass  Reaching Definition Analysis  Type inference  Constant Propagation  ...</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Overall Workflow on JANUS</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>len == 3 ?</p>
    <p>Assert</p>
    <p>Validate Assumption for Correctness</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>len == 3 ?</p>
    <p>Assert</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Symbolic Graph Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>len == 3 ?</p>
    <p>Assert</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>len == 3 ?</p>
    <p>Assert</p>
    <p>Symbolic Graph Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>Assumption Failure</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Symbolic Graph Executor</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>len == 3 ?</p>
    <p>Assert</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Overall Workflow on JANUS</p>
    <p>len:3</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
  </div>
  <div class="page">
    <p>Imperative Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>len:?</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Symbolic DL Graph</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>state</p>
    <p>CellSwitch</p>
    <p>Merge</p>
    <p>i&lt;N</p>
    <p>Next</p>
    <p>len:?</p>
    <p>Fast Path (Common Case)</p>
    <p>Correct Path (Rare Case)</p>
  </div>
  <div class="page">
    <p>Imperative Executor Symbolic Graph Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Graph Cache</p>
    <p>Overall Workflow on JANUS</p>
    <p>Cell</p>
    <p>state</p>
    <p>Cell</p>
    <p>Cell</p>
    <p>len == 3 ?</p>
    <p>Assert</p>
    <p>Symbolic DL Graph len:3</p>
  </div>
  <div class="page">
    <p>Imperative Executor</p>
    <p>Imperative DL Program</p>
    <p>for item in sequence: state = rnn(state, item) outputs += [state]</p>
    <p>Pre-defined DL Operations .Python Interpreter .</p>
    <p>Profiler</p>
    <p>Graph Generator</p>
    <p>Additional System Aspects</p>
    <p>Imperative Execution for Full Python Coverage</p>
    <p>len:3</p>
    <p>See our paper for more details!</p>
    <p>Python Coverage Global State Consistency</p>
  </div>
  <div class="page">
    <p>SetAttr</p>
    <p>value</p>
    <p>Symbolic Graph Executor</p>
    <p>Python Interpreter .</p>
    <p>Additional System Aspects</p>
    <p>Pre-defined DL Operations. .</p>
    <p>Impure Symbolic DL Graph</p>
    <p>Impure Imperative DL Program</p>
    <p>def foo(obj): obj.data = value do_sth if pred else pass</p>
    <p>Python Coverage Global State Consistency</p>
    <p>pred?</p>
    <p>Assert do_sth</p>
    <p>Python Heap</p>
  </div>
  <div class="page">
    <p>Impure Imperative DL Program</p>
    <p>def foo(obj): obj.data = value do_sth if pred else pass</p>
    <p>SetAttr</p>
    <p>value</p>
    <p>pred?</p>
    <p>Assert do_sth</p>
    <p>Symbolic Graph Executor</p>
    <p>Python Interpreter .</p>
    <p>Additional System Aspects</p>
    <p>Pre-defined DL Operations. .</p>
    <p>Assumption Failure</p>
    <p>Unsafe to fallback after heap update</p>
    <p>Impure Symbolic DL Graph</p>
    <p>?</p>
    <p>Python Coverage Global State Consistency</p>
    <p>Modified Python Heap</p>
  </div>
  <div class="page">
    <p>Symbolic Graph Executor</p>
    <p>Python Interpreter .</p>
    <p>Additional System Aspects</p>
    <p>Pre-defined DL Operations. .</p>
    <p>Python Heap Local Copy</p>
    <p>Impure Imperative DL Program</p>
    <p>def foo(obj): obj.data = value do_sth if pred else pass</p>
    <p>SetAttr</p>
    <p>value</p>
    <p>pred?</p>
    <p>Assert do_sth</p>
    <p>Impure Symbolic DL Graph</p>
    <p>Write-back after validating assumptions</p>
    <p>See our paper for more details!</p>
    <p>Python Coverage Global State Consistency</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Approach</p>
    <p>Challenges</p>
    <p>JANUS</p>
    <p>Evaluation</p>
    <p>Correctness and Performance</p>
    <p>Breakdown of Performance Improvement</p>
  </div>
  <div class="page">
    <p>Evaluation Setup: Frameworks &amp; Environments</p>
    <p>Frameworks</p>
    <p>JANUS. Implemented on top of TensorFlow and CPython</p>
    <p>Symbolic. TensorFlow</p>
    <p>Imperative. TensorFlow Eager</p>
    <p>Hardware &amp; Software Setup</p>
    <p>6 machines connected via Mellanox ConnectX-4 cards w/ 100Gbps InfiniBand</p>
    <p>Each machine w/ 2x(Intel Xeon E5-2695)+6x(NVIDIA GeForce Titan Xp)</p>
    <p>Ubuntu 16.04, TensorFlow 1.8.0, CUDA 9.0</p>
    <p>Horovod 0.12.1, NCCL v2.1, OpenMPI v3.0.0 65</p>
    <p>Executors: TensorFlow (TF) + TF Eager (modified) LoC: 4700 LoC / TF diff 771 LoC / CPython diff 1096 LoC</p>
  </div>
  <div class="page">
    <p>Evaluation Setup: Applications</p>
    <p>Convolutional Neural Networks (CNN) LeNet, ResNet-50, Inception-v3</p>
    <p>Recurrent Neural Networks (RNN) LSTM, LM</p>
    <p>Recursive Neural Networks (TreeNN) TreeRNN, TreeLSTM</p>
    <p>Deep Reinforcement Learning (DRL) A3C, PPO</p>
    <p>Generative Adversarial Networks (GAN) AN, PIX2PIX</p>
  </div>
  <div class="page">
    <p>ImageNet Test Error with ResNet50</p>
    <p>ImperativeSymbolic</p>
    <p>Faster</p>
    <p>Time</p>
    <p>Test Error (%)</p>
  </div>
  <div class="page">
    <p>ImageNet Test Error with ResNet50</p>
    <p>ImperativeSymbolic</p>
    <p>Time</p>
    <p>Test Error (%)</p>
    <p>JANUS</p>
    <p>Faster</p>
  </div>
  <div class="page">
    <p>ImageNet Test Error with ResNet50</p>
    <p>ImperativeSymbolic</p>
    <p>Time</p>
    <p>Test Error (%)</p>
    <p>JANUS</p>
    <p>Faster</p>
  </div>
  <div class="page">
    <p>ImageNet Test Error with ResNet50</p>
    <p>ImperativeSymbolic</p>
    <p>Time</p>
    <p>Test Error (%)</p>
    <p>JANUS</p>
    <p>Faster</p>
    <p>Overlapping computation and communication</p>
  </div>
  <div class="page">
    <p>Model Convergence</p>
    <p>RNN TreeNN</p>
    <p>DRL GAN</p>
    <p>Imperative Symbolic</p>
  </div>
  <div class="page">
    <p>Model Convergence</p>
    <p>Faster</p>
    <p>Imperative Symbolic</p>
    <p>JANUS RNN TreeNN</p>
    <p>DRL GAN</p>
  </div>
  <div class="page">
    <p>CNN</p>
    <p>GAN</p>
    <p>DRL</p>
    <p>TreeNN</p>
    <p>RNN</p>
    <p>Normalized Training Throughput</p>
    <p>LeNet</p>
    <p>ResNet-50</p>
    <p>Inception-v3</p>
    <p>LSTM</p>
    <p>LM</p>
    <p>TreeRNN</p>
    <p>TreeLSTM</p>
    <p>A3C</p>
    <p>PPO</p>
    <p>AN</p>
    <p>PIX2PIX</p>
    <p>Single machine w/ Single GPU</p>
    <p>Imp.</p>
    <p>Symbolic</p>
    <p>Imperative JANUS</p>
  </div>
  <div class="page">
    <p>CNN</p>
    <p>GAN</p>
    <p>DRL</p>
    <p>TreeNN</p>
    <p>RNN</p>
    <p>Normalized Training Throughput</p>
    <p>LeNet</p>
    <p>ResNet-50</p>
    <p>Inception-v3</p>
    <p>LSTM</p>
    <p>LM</p>
    <p>TreeRNN</p>
    <p>TreeLSTM</p>
    <p>A3C</p>
    <p>PPO</p>
    <p>AN</p>
    <p>PIX2PIX</p>
    <p>Imp.</p>
    <p>SymbolicImperative JANUS</p>
    <p>} Hand-optimized GPU ops</p>
    <p>dominated execution time;</p>
    <p>Working on applying further graph optimizations!</p>
    <p>Single machine w/ Single GPU</p>
  </div>
  <div class="page">
    <p>CNN</p>
    <p>GAN</p>
    <p>DRL</p>
    <p>TreeNN</p>
    <p>RNN</p>
    <p>JANUS Speedup over Imperative Execution</p>
    <p>LeNet</p>
    <p>ResNet-50</p>
    <p>Inception-v3</p>
    <p>LSTM</p>
    <p>LM</p>
    <p>TreeRNN</p>
    <p>TreeLSTM</p>
    <p>A3C</p>
    <p>PPO</p>
    <p>AN</p>
    <p>PIX2PIX</p>
    <p>Imp.</p>
    <p>JANUS Single machine w/ Single GPU</p>
  </div>
  <div class="page">
    <p>CNN</p>
    <p>GAN</p>
    <p>DRL</p>
    <p>TreeNN</p>
    <p>RNN</p>
    <p>JANUS Speedup over Imperative Execution: Breakdown</p>
    <p>LeNet</p>
    <p>ResNet-50</p>
    <p>Inception-v3</p>
    <p>LSTM</p>
    <p>LM</p>
    <p>TreeRNN</p>
    <p>TreeLSTM</p>
    <p>A3C</p>
    <p>PPO</p>
    <p>AN</p>
    <p>PIX2PIX</p>
    <p>Imp.</p>
    <p>Single machine w/ Single GPU Speedup</p>
    <p>without specialization by runtime profiling</p>
  </div>
  <div class="page">
    <p>CNN</p>
    <p>GAN</p>
    <p>DRL</p>
    <p>TreeNN</p>
    <p>RNN</p>
    <p>JANUS Speedup over Imperative Execution: Breakdown</p>
    <p>LeNet</p>
    <p>ResNet-50</p>
    <p>Inception-v3</p>
    <p>LSTM</p>
    <p>LM</p>
    <p>TreeRNN</p>
    <p>TreeLSTM</p>
    <p>A3C</p>
    <p>PPO</p>
    <p>AN</p>
    <p>PIX2PIX</p>
    <p>Imp.</p>
    <p>Speedup with specialization by runtime profiling</p>
    <p>Single machine w/ Single GPU</p>
  </div>
  <div class="page">
    <p>CNN</p>
    <p>GAN</p>
    <p>DRL</p>
    <p>TreeNN</p>
    <p>RNN</p>
    <p>JANUS Speedup over Imperative Execution: Breakdown</p>
    <p>LeNet</p>
    <p>ResNet-50</p>
    <p>Inception-v3</p>
    <p>LSTM</p>
    <p>LM</p>
    <p>TreeRNN</p>
    <p>TreeLSTM</p>
    <p>A3C</p>
    <p>PPO</p>
    <p>AN</p>
    <p>PIX2PIX</p>
    <p>Imp.</p>
    <p>Bypassing Python Heap</p>
    <p>Control Flow Unrolling</p>
    <p>Type Specialization</p>
    <p>Composed of CNNs</p>
    <p>Single machine w/ Single GPU</p>
  </div>
  <div class="page">
    <p>Related Works</p>
    <p>Imperative to symbolic: one-shot converters</p>
    <p>TensorFlow: defun, AutoGraph, Swift for TensorFlow, JAX, ...  PyTorch JIT trace, script  MXNet Gluon</p>
    <p>Cannot handle the dynamic semantics of Python correctly &amp; efficiently</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Programmability and debuggability of imperative DL frameworks with the performance of symbolic DL frameworks</p>
    <p>Speculative graph generation and execution with runtime profiling</p>
    <p>Up to 47.6x speedup over imperative DL framework, within up to 4% difference compared to symbolic DL framework, while transparently and correctly executing imperative DL programs</p>
  </div>
</Presentation>
