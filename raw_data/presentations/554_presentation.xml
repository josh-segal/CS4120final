<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A study of practical deduplication</p>
    <p>Dutch T. Meyer University of British Columbia</p>
    <p>Microsoft Research Intern William Bolosky</p>
    <p>Microsoft Research</p>
  </div>
  <div class="page">
    <p>A study of practical deduplication</p>
    <p>Dutch T. Meyer University of British Columbia</p>
    <p>Microsoft Research Intern William Bolosky</p>
    <p>Microsoft Research</p>
  </div>
  <div class="page">
    <p>Why study deduplication?</p>
    <p>$0.046 per GB</p>
  </div>
  <div class="page">
    <p>When do we exploit duplicates?</p>
    <p>It Depends.  How much can you get back from deduping?</p>
    <p>How does fragmenting files affect performance?</p>
    <p>How often will you access the data?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Intro</p>
    <p>Methodology</p>
    <p>Theres more here than dedup teaser</p>
    <p>(intermission)</p>
    <p>Deduplication Background</p>
    <p>Deplication Analysis</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>MD5(name) Metadata MD5(data)</p>
    <p>MD5(name) Metadata MD5(data)</p>
    <p>MD5(name) Metadata MD5(data)</p>
    <p>Once per week for 4 weeks. ~875 file systems ~40TB ~200M Files</p>
  </div>
  <div class="page">
    <p>Theres more here than dedup!</p>
    <p>We update and extend filesystem metadata findings from 2000 and 2004</p>
    <p>File system complexity is growing</p>
    <p>Read the paper to answer questions like:</p>
    <p>Are my files bigger now than they used to be?</p>
  </div>
  <div class="page">
    <p>Teaser: Histogram of file size</p>
    <p>File Size (bytes), power-of-two bins</p>
  </div>
  <div class="page">
    <p>Theres more here than dedup!</p>
    <p>How fragmented are my files?</p>
  </div>
  <div class="page">
    <p>Teaser: Layout and Organization</p>
    <p>High linearity: only 4% of files fragmented in practice</p>
    <p>Most windows machines defrag weekly</p>
    <p>One quarter of fragmented files have at least 170 fragments</p>
  </div>
  <div class="page">
    <p>Intermission</p>
    <p>Intro</p>
    <p>Methodology</p>
    <p>Theres more here than dedup teaser</p>
    <p>(intermission)</p>
    <p>Deduplication Background</p>
    <p>Deplication Analysis</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Dedup Background</p>
    <p>foo 01101010.. .110010101</p>
    <p>bar 01101010.. .110010101</p>
    <p>Whole file Deduplication</p>
  </div>
  <div class="page">
    <p>Dedup Background</p>
    <p>foo 01101010.. .110010101</p>
    <p>bar 01101010.. .110010101</p>
    <p>Fixed Chunk Deduplication</p>
    <p>.110010101</p>
    <p>.1100101011</p>
  </div>
  <div class="page">
    <p>Dedup Background</p>
    <p>foo 01101010.. .110010101</p>
    <p>bar 01101010.. .110010101</p>
    <p>Rabin Figerprinting</p>
  </div>
  <div class="page">
    <p>The Deduplication Space</p>
    <p>Algorithm Parameters Cost Deduplication effectiveness</p>
    <p>Whole-file Low Lowest</p>
    <p>Fixed Chunk</p>
    <p>Chunk Size Seeks CPU Complexity</p>
    <p>Middle</p>
    <p>Rabin fingerprints</p>
    <p>Average Chunk Size</p>
    <p>Seeks More CPU More Complexity</p>
    <p>Highest</p>
  </div>
  <div class="page">
    <p>What is the relative deduplication rate of the algorithms?</p>
  </div>
  <div class="page">
    <p>Dedup by method and chunk size</p>
    <p>S p</p>
    <p>a ce</p>
    <p>D e</p>
    <p>d u</p>
    <p>p li</p>
    <p>ca te</p>
    <p>d</p>
    <p>Chunk Size</p>
    <p>Whole File Fixed-Chunk Rabin</p>
  </div>
  <div class="page">
    <p>What if I was doing full weekly backups?</p>
  </div>
  <div class="page">
    <p>Backup dedup over 4 weeks</p>
    <p>Whole File</p>
    <p>Whole File + Sparse</p>
    <p>Deduplicated Space</p>
  </div>
  <div class="page">
    <p>How does the number of filesystems influence deduplication?</p>
  </div>
  <div class="page">
    <p>Dedup by filesystem count</p>
    <p>S p</p>
    <p>a ce</p>
    <p>D e</p>
    <p>d u</p>
    <p>p li</p>
    <p>ca te</p>
    <p>d</p>
    <p>Deduplication Domain Size (file systems)</p>
    <p>Whole File 64 KB Fixed 8KB Fixed 64KB Rabin 8KB Rabin</p>
  </div>
  <div class="page">
    <p>So what is filling up all this space?</p>
  </div>
  <div class="page">
    <p>Bytes by containing file size</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e</p>
    <p>o f</p>
    <p>To ta</p>
    <p>l B</p>
    <p>y te</p>
    <p>s</p>
    <p>Containing File Size (Bytes), Power-of-2 bins</p>
  </div>
  <div class="page">
    <p>What types of files take up disk space?</p>
  </div>
  <div class="page">
    <p>Disk consumption by file type</p>
    <p>dll dll</p>
    <p>pdb vhd</p>
    <p>dll exe</p>
    <p>pdb</p>
    <p>libpst</p>
    <p>exe</p>
    <p>vhd pch</p>
    <p>wma</p>
    <p>pdb</p>
    <p>mp3</p>
    <p>lib</p>
    <p>exe</p>
    <p>lib</p>
    <p>cab</p>
    <p>pch</p>
    <p>chm</p>
    <p>pst</p>
    <p>cab</p>
    <p>cab</p>
    <p>mp3</p>
    <p>wma</p>
    <p>iso</p>
  </div>
  <div class="page">
    <p>Disk consumption by file type</p>
    <p>dll dll</p>
    <p>pdb vhd</p>
    <p>dll exe</p>
    <p>pdb</p>
    <p>libpst</p>
    <p>exe</p>
    <p>vhd pch</p>
    <p>wma</p>
    <p>pdb</p>
    <p>mp3</p>
    <p>lib</p>
    <p>exe</p>
    <p>lib</p>
    <p>cab</p>
    <p>pch</p>
    <p>chm</p>
    <p>pst</p>
    <p>cab</p>
    <p>cab</p>
    <p>mp3</p>
    <p>wma</p>
    <p>iso</p>
  </div>
  <div class="page">
    <p>Which of these types deduplicate well?</p>
  </div>
  <div class="page">
    <p>Whole-file duplicates</p>
    <p>Extension</p>
    <p>% of Duplicate</p>
    <p>Space</p>
    <p>Mean File</p>
    <p>Size (bytes)</p>
    <p>% of</p>
    <p>Total Space</p>
    <p>dll 20% 521K 10%</p>
    <p>lib 11% 1080K 7%</p>
    <p>pdb 11% 2M 7%</p>
    <p>&lt;none&gt; 7% 277K 13%</p>
    <p>exe 6% 572K 4%</p>
    <p>cab 4% 4M 2%</p>
    <p>msp 3% 15M 2%</p>
    <p>msi 3% 5M 1%</p>
    <p>iso 2% 436M 2%</p>
    <p>&lt;a guid&gt; 1% 604K &lt;1%</p>
  </div>
  <div class="page">
    <p>What files make up the 20% difference between whole file dedup and sparse file, as compared to more aggressive deduplication?</p>
  </div>
  <div class="page">
    <p>Where does fine granularity help?</p>
    <p>vhd vhd</p>
    <p>pch</p>
    <p>lib</p>
    <p>dll</p>
    <p>obj</p>
    <p>pdb</p>
    <p>pdb</p>
    <p>lib</p>
    <p>pch</p>
    <p>wma</p>
    <p>iso</p>
    <p>pst</p>
    <p>dll</p>
    <p>avhd</p>
    <p>avhd</p>
    <p>wma</p>
    <p>mo3</p>
    <p>wim</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e</p>
    <p>o f</p>
    <p>d if</p>
    <p>fe re</p>
    <p>n ce</p>
    <p>v s.</p>
    <p>w h</p>
    <p>o le</p>
    <p>f il</p>
    <p>e +</p>
    <p>s p</p>
    <p>a rs</p>
    <p>e</p>
  </div>
  <div class="page">
    <p>Last plea to read the whole paper</p>
    <p>~4x more results in paper!</p>
    <p>Real world filesystem analysis is hard</p>
    <p>Eight machines months in query processing</p>
    <p>Requires careful simplifying assumptions</p>
    <p>Requires heavy optimization</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>The benefit of fine grained dedup is &lt; 20%</p>
    <p>Potentially just a fraction of that.</p>
    <p>Fragmentation is a manageable problem</p>
    <p>Read the paper for more metadata results</p>
    <p>Were releasing this dataset</p>
  </div>
</Presentation>
