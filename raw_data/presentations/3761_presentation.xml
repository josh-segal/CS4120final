<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Evaluating Performance and Energy in File System Server Workloads</p>
    <p>Priya Sehgal, Vasily Tarasov, and Erez Zadok</p>
    <p>File systems and Storage Lab Dept. of Computer Science</p>
    <p>Stony Brook University</p>
    <p>http://green.filesystems.org/</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>For every $1 spent on hardware $0.50 spent on power and cooling [IDC 2007]</p>
    <p>Energy use in U.S. data centers = 12% of total energy in U.S. [EPA 2007]</p>
    <p>Even more outside the data center [Forrester 2008]</p>
    <p>Build performance- and energy-efficient systems</p>
    <p>Evaluate the efficacy of file system in achieving this goal</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation  Related Work  Experimental Methodology  Evaluation Results</p>
    <p>Machine 1 (M1) Results  Machine 2 (M2) Results</p>
    <p>Conclusion and Future Work NEW</p>
  </div>
  <div class="page">
    <p>Techniques</p>
    <p>Right Sizing</p>
    <p>CPU DVFS  Machine ACPI states</p>
    <p>standby, hibernate, off, etc.</p>
    <p>Opportunistic spin-down  DRPM  Virtualization</p>
    <p>Work Reduction</p>
    <p>Aggregation, Localization  Compression, DeDUP  Reconfiguration</p>
    <p>Application/Services</p>
    <p>File Systems</p>
    <p>RAID Levels, etc.</p>
    <p>Complimentary</p>
  </div>
  <div class="page">
    <p>Related Work - 1</p>
    <p>Right Sizing  Redirect the request elsewhere  PDC, MAID, GreenFS, Write-offloading,</p>
    <p>EAVFS, etc.</p>
    <p>Work Reduction  Improve locality  FS2, EEFS, Predictive Data Grouping, etc.</p>
    <p>Others  FAWN  quFiles, etc.</p>
  </div>
  <div class="page">
    <p>Related Work - 2</p>
    <p>Benchmarks  SPECPower</p>
    <p>Metric: operations/second/watt  JouleSort</p>
    <p>Metric: sortedrecs/joule</p>
    <p>Benchmark Studies  Compression evaluation [Kothiyal 2009]</p>
    <p>RAID evaluation [Gurumurthi 2003]</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation  Related Work  Experimental Methodology  Evaluation Results  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Methodology  Workloads (4)</p>
    <p>Web server, Database server,</p>
    <p>File server, Mail server</p>
    <p>FileBench emulated workloads</p>
    <p>File Systems (4)  Type: Ext2, Ext3, ReiserFS, XFS</p>
    <p>Mount Options: noatime, notail, journal=&lt;modes&gt;</p>
    <p>Format Options: inode size, blocksize, allocation/block group count.</p>
    <p>Hardware (2)</p>
    <p>NEW</p>
    <p>We ran a total of 248 benchmarks  414 clock hours!</p>
  </div>
  <div class="page">
    <p>FileBench  Sun Microsystems, 2005</p>
    <p>Used for performance analysis of Solaris OS</p>
    <p>Other studies: [Macko 10, Zhang 10, Gulati 10], etc.</p>
    <p>Rich language to emulate complex workloads  Provide with a few emulated workloads</p>
    <p>Application traces</p>
    <p>Recommend parameters for server workloads</p>
    <p>Superior to few other benchmarks  E.g., Bonnie, Postmark, Andrew Benchmark, etc.</p>
    <p>We ported FileBench to different platforms (FreeBSD, Linux)</p>
  </div>
  <div class="page">
    <p>FileBench Workloads</p>
    <p>Server workload</p>
    <p>Avg. file size</p>
    <p>Avg. directory</p>
    <p>depth</p>
    <p>No. of files</p>
    <p>I/O size (R/W)</p>
    <p>No. of threads</p>
    <p>R/W ratio</p>
    <p>Web 32KB 3.3 20,000 1MB/16KB 100 10:1</p>
    <p>File 256KB 3.6 50,000 1MB/16KB 100 1:2</p>
    <p>Mail 16KB FLAT 50,000 1MB/16KB 100 1:1</p>
    <p>Database 0.5GB FLAT 10 2KB/2KB 200+10 20:1</p>
  </div>
  <div class="page">
    <p>File System Properties Features Ext2 Ext3 ReiserFS XFS</p>
    <p>Disk Layout Linear Linear S+ Tree B+ Tree</p>
    <p>Allocation unit /</p>
    <p>strategy</p>
    <p>Fixed-sized blocks</p>
    <p>Fixed-sized blocks</p>
    <p>Fixed-sized blocks</p>
    <p>Variablesized extents</p>
    <p>(Delayed allocation)</p>
    <p>No. of Files Fixed Fixed Variable Variable</p>
    <p>Journaling modes</p>
    <p>None Ordered, writeback,</p>
    <p>data</p>
    <p>Ordered, writeback, data, none</p>
    <p>Writeback</p>
    <p>Special Feature</p>
    <p>Block groups Block groups Tail Packing Allocation</p>
    <p>groups</p>
    <p>We used CentOS 5.3 Linux 2.6.18-128.1.16.el5.centos.plus</p>
  </div>
  <div class="page">
    <p>Hardware Setup</p>
    <p>Evaluating Performance &amp; Energy in Server Workloads (FAST 2010) 12</p>
    <p>Linux Server</p>
    <p>WattsUP Pro ES (server)</p>
    <p>Server Power Readings (USB)</p>
    <p>A/C Power Supply</p>
  </div>
  <div class="page">
    <p>Machine Configurations</p>
    <p>M1 (Reported in paper) M2</p>
    <p>Machine Age 3 years &lt; 1 year</p>
    <p>CPU Model Intel Xeon Intel Nehalem (E5530)</p>
    <p>CPU Speed 2.8GHz 2.4GHz</p>
    <p># of CPUs 2 dual core 1 quad core</p>
    <p>DVFS No Yes</p>
    <p>L1 cache size 16KB 128KB</p>
    <p>L2 cache size 2MB 1MB</p>
    <p>L3 cache size No 8MB</p>
    <p>FSB speed 800 MHz 1066 MHz</p>
    <p>RAM size 2048 MB 24GB (used 2GB)</p>
    <p>RAM type DIMM DIMM</p>
    <p>Disk RPM 15K RPM 7.2K RPM</p>
    <p>Type of Disk SCSI SATA</p>
    <p>Average Seek Time (ms) 3.2/3.6 ms 10.5/12.5 ms</p>
    <p>Disk Cache 8MB 16MB</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation  Related Work  Experimental Methodology  Evaluation Results</p>
    <p>Machine 1 (M1) Results  Machine 2 (M2) Results</p>
    <p>Conclusion and Future Work NEW</p>
  </div>
  <div class="page">
    <p>Mail Server (M1)</p>
    <p>o p</p>
    <p>s /s</p>
    <p>e c</p>
    <p>Performance</p>
    <p>o p</p>
    <p>s /k</p>
    <p>jo u</p>
    <p>le</p>
    <p>Energy Efficiency</p>
    <p>Higher is better</p>
  </div>
  <div class="page">
    <p>Mail Server (M1)</p>
    <p>Performance</p>
    <p>Energy Efficiency</p>
    <p>ReiserFS-notail best for this configuration</p>
    <p>Linearity between Performance and Energy Efficiency</p>
    <p>Tail packing on by default</p>
    <p>o p</p>
    <p>s /s</p>
    <p>e c</p>
    <p>o p</p>
    <p>s /k</p>
    <p>jo u</p>
    <p>le</p>
  </div>
  <div class="page">
    <p>Database Server (M1)</p>
    <p>Except for Ext2 other default FS perform</p>
    <p>similarly</p>
    <p>efficiency by ~2x</p>
    <p>Performance</p>
    <p>I/O size = Block size</p>
    <p>o p</p>
    <p>s /s</p>
    <p>e c</p>
  </div>
  <div class="page">
    <p>File System Selection Matrix (M1)</p>
    <p>Newer hardware  Different results  Optimal file system often varies with</p>
    <p>changes in  Hardware  Software  Workload</p>
    <p>Workload Best File System (Combination)</p>
    <p>Improvement Range (compared to all default FS)</p>
    <p>Ops/sec Ops/joule</p>
    <p>Web Server XFS</p>
    <p>(inode-size-1K) 8%  9.4x 6%  7.5x</p>
    <p>File Server ReiserFS (default)</p>
    <p>Mail Server ReiserFS (notail)</p>
    <p>Database Server XFS/Ext3 (BLK-2K)</p>
    <p>This recommendation matters but</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation  Related Work  Experimental Methodology  Evaluation Results</p>
    <p>Machine 1 (M1) Results  Machine 2 (M2) Results</p>
    <p>Conclusion and Future Work NEW</p>
  </div>
  <div class="page">
    <p>Mail Server (M1 vs. M2)</p>
    <p>o p</p>
    <p>s /s</p>
    <p>e c</p>
    <p>Performance</p>
    <p>M1</p>
    <p>M2</p>
    <p>Difference from M1: Increasing allocation group decreases performance (~5-10%) but</p>
    <p>M2 vs. M1 35%  3x</p>
    <p>improvement for all defaults</p>
    <p>Trend changes across M1 and M2</p>
    <p>o p</p>
    <p>s /s</p>
    <p>e c</p>
    <p>Better Configs M1: Reiserfs-notail</p>
    <p>M2: Ext3-default</p>
    <p>Fileserver: default agcnt</p>
    <p>suboptimal ~25%</p>
    <p>Same trend as M1</p>
  </div>
  <div class="page">
    <p>Better Configs for M1 and M2</p>
    <p>Ext3 and XFS w/ BLK-2K</p>
    <p>Database Server (M1 vs. M2)</p>
    <p>Performance</p>
    <p>M1</p>
    <p>M2</p>
    <p>p s</p>
    <p>/s e</p>
    <p>c</p>
    <p>o p</p>
    <p>s /s</p>
    <p>e c</p>
    <p>M2 vs. M1 88%  2x</p>
    <p>improvement for all defaults</p>
    <p>Performance trend remains</p>
    <p>the same across M1 and M2</p>
    <p>performance by ~1.5x</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation  Related Work  Experimental Methodology  Evaluation Results  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Conclusions  Workloads drive performance-energy</p>
    <p>Depend also on hardware, software, config</p>
    <p>Significant savings possible</p>
    <p>Recipe to improve work done per dollar</p>
    <p>Evaluating Performance &amp; Energy in Server Workloads (FAST 2010) 23 02/26/2010</p>
    <p>(Re)-calibrate (Re)-configure</p>
    <p>Applicable to entire storage/software stack</p>
    <p>It is expensive and time consuming but  Small savings matter over the long run !</p>
  </div>
  <div class="page">
    <p>Ongoing/Future Work  Study multiple dimensions</p>
    <p>New FS, Disk Scheduler, RAID, LVM, etc.</p>
    <p>Client/Server Systems</p>
    <p>Poster on NFSv4 at Poster Session in FAST 2010</p>
    <p>Disk Types: SAS, SSD, etc.</p>
    <p>Cluster Storage, SANs, OS</p>
    <p>Develop auto-configuration tools  Develop workload specific storage stack</p>
    <p>Evaluating Performance &amp; Energy in Server Workloads (FAST 2010) 24 02/26/2010</p>
  </div>
  <div class="page">
    <p>Q&amp;A</p>
    <p>Evaluating Performance and Energy in File System Server Workloads Priya Sehgal, Vasily Tarasov, and Erez Zadok</p>
    <p>File systems and Storage Lab Dept. of Computer Science</p>
    <p>Stony Brook University</p>
    <p>http://green.filesystems.org/</p>
  </div>
</Presentation>
