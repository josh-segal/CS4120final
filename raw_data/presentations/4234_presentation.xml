<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Language learning and processing in people and machines</p>
    <p>Aida Nematzadeh Richard Futrell Roger Levy</p>
    <p>DeepMind UC Irvine MIT</p>
  </div>
  <div class="page">
    <p>Environmental noise</p>
    <p>Incomplete knowledge of ones interlocutors</p>
    <p>Memory Limitations S</p>
    <p>VP</p>
    <p>NP</p>
    <p>PP</p>
    <p>on the beach</p>
    <p>N</p>
    <p>dogs</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>discussed</p>
    <p>NP</p>
    <p>N</p>
    <p>women</p>
    <p>Det</p>
    <p>The</p>
    <p>Ambiguity S</p>
    <p>VP</p>
    <p>PP</p>
    <p>on the beach</p>
    <p>NP</p>
    <p>N</p>
    <p>dogs</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>discussed</p>
    <p>NP</p>
    <p>N</p>
    <p>women</p>
    <p>Det</p>
    <p>The</p>
    <p>How do humans communicate so well with language?</p>
    <p>How do we acquire the knowledge that enables this?  And how can we get machines to do the same?</p>
  </div>
  <div class="page">
    <p>Overview of tutorial topics  Human language acquisition (Aida)</p>
    <p>Learning mechanisms  Word learning: theory &amp; data  Structure learning: theory &amp; data</p>
    <p>Human language comprehension (Roger)  Doing cognitive science through rational analysis  Revealing cognitive state with psycholinguistic experiments  Theory of human language comprehension</p>
    <p>Cognitive evaluation of NLP systems (Richard)  Language evolution and emergence (Richard)</p>
  </div>
  <div class="page">
    <p>Some things to keep in mind today  NLP and cognitive science offer each other a great deal  NLPcognitive science: formal theory-building for</p>
    <p>understanding human language learning &amp; use  Cognitive scienceNLP: desiderata for human-like</p>
    <p>language processing systems  Weve seen impressive science &amp; engineering progress,</p>
    <p>but many major open questions &amp; problems remain  There are great opportunities for everyone here!!!</p>
  </div>
  <div class="page">
    <p>How Do Children Learn Language?</p>
    <p>Aida Nematzadeh nematzadeh@google.com</p>
  </div>
  <div class="page">
    <p>Language Acquisition in Children</p>
    <p>Children effortlessly learn their language from a noisy and ambiguous input.</p>
  </div>
  <div class="page">
    <p>Language Acquisition in Machines</p>
    <p>Understanding language acquisition might help us build AI systems that understand and produce natural languages.</p>
  </div>
  <div class="page">
    <p>Is Language Learned? How? Is Language Learning Effortless?</p>
    <p>Learning Mechanisms Learning about Words Learning the Structure</p>
  </div>
  <div class="page">
    <p>nativism</p>
    <p>Mind has preexisting structure to interpret experience.</p>
    <p>Language: outcome of nature -an innate endowment (like upright posture).</p>
    <p>Nurture vs Nature</p>
    <p>empiricism</p>
    <p>Knowledge and reason come from experience.</p>
    <p>Language: outcome of how children are nurtured (like table manner).</p>
  </div>
  <div class="page">
    <p>Language learning is not really something that the child does; it is something that happens to the child placed in an appropriate environment, much as the childs body grows and matures in a predetermined way when provided with appropriate nutrition and environmental stimulation.</p>
    <p>Chomsky (1928-)</p>
    <p>Empiricism vs Nativism The human intellect at birth is rather like a tabula rasa, a pure potentiality that is actualized through education and comes to know. Knowledge is attained through empirical familiarity with objects in this world from which one abstracts universal concepts.</p>
    <p>Avicenna (980-1037 AD)</p>
  </div>
  <div class="page">
    <p>cognitivism</p>
    <p>Explaining behavior requires understanding the mind.</p>
    <p>Language ~ mental process</p>
    <p>Cognitive Revolution</p>
    <p>behaviorism</p>
    <p>Can explain behavior in terms of things external to mind.</p>
    <p>Language ~ verbal behavior</p>
  </div>
  <div class="page">
    <p>Domain-General vs Domain-Specific Learning Language is acquired rapidly, effortlessly, and without direct instruction.</p>
    <p>Language is acquired using general cognitive skills like memory, capacity for symbolic representation, and statistical learning.</p>
    <p>[Frank et al, 2019]</p>
  </div>
  <div class="page">
    <p>formalism</p>
    <p>Language form is independent of its function.</p>
    <p>Acquisition of language is not affected by the fact that we use it to communicate.</p>
    <p>Language for Communication</p>
    <p>functionalism</p>
    <p>Language is shaped by its communicative functions.</p>
    <p>Language is acquired through communication (not passive observation).</p>
  </div>
  <div class="page">
    <p>Takeaways: Development vs Learnability</p>
    <p>Modeling language development to shed light on its underlying mechanism.</p>
    <p>Can we learn language (certain linguistic phenomena) from data?</p>
  </div>
  <div class="page">
    <p>Nature of Nature</p>
    <p>Investigate the innateness/learnability of</p>
    <p>knowledge -- inborn linguistic knowledge?</p>
    <p>computational procedure -- domain-general or domain-specific learning mechanism?</p>
  </div>
  <div class="page">
    <p>Is Language Learned? How? Is Language Learning Effortless?</p>
    <p>Learning Mechanisms Learning about Words Learning the Structure</p>
  </div>
  <div class="page">
    <p>Takes children 5 years (14,600h, 8h/day).</p>
    <p>Would take adults 56 years (2920 weeks, 5h/week).</p>
    <p>prelinguistic communication</p>
    <p>single words</p>
    <p>telegraphic speech</p>
    <p>grammatical development</p>
    <p>&quot;bobo&quot; mummy</p>
    <p>doggy</p>
    <p>daddy sleep</p>
    <p>orange juice</p>
    <p>I want some eggs</p>
    <p>Put it table</p>
  </div>
  <div class="page">
    <p>Children make errors but learn to correct them.</p>
    <p>prelinguistic communication</p>
    <p>single words</p>
    <p>telegraphic speech</p>
    <p>grammatical development</p>
    <p>bobo mummy</p>
    <p>doggy</p>
    <p>daddy sleep orange juice</p>
    <p>I want some eggs</p>
    <p>Put it table</p>
  </div>
  <div class="page">
    <p>[Hoff, 2004]</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Should AI models make the same mistakes as children?</p>
    <p>Should we model all the domains at the same time?</p>
  </div>
  <div class="page">
    <p>Is Language Learned? How? Is Language Learning Effortless?</p>
    <p>Learning Mechanisms Learning about Words Learning the Structure</p>
  </div>
  <div class="page">
    <p>Babies as Statistical Learners [Saffran et al, Science 1996]</p>
    <p>Statistical learning in other domains: phonology, syntax, &amp; words.[Gomez et al, 2000; Mintz et al, 2002; Smith &amp; Yu, 2008; Romberg &amp; Saffran, 2010]</p>
    <p>Statistical learning is domain- &amp; species- general. 21</p>
  </div>
  <div class="page">
    <p>Babies as Rule Learners [Marcus et al, Science 1999]</p>
    <p>Seven-month-old infants can learn simple algebra-like rules.  ga ti ti li la la (ABB) or li la li ga la ga (ABA)</p>
    <p>Rule learning is statistical learning? [Christiansen &amp; Curtin, 1999; Seidenberg &amp; Elman, 1999; McClelland &amp; Plaut, 1999]</p>
  </div>
  <div class="page">
    <p>Babies as Social Learners</p>
    <p>Sharing joint attention.</p>
    <p>Understanding and sharing intention. [Tomasello et al, 2005]</p>
    <p>Infants learn about phonetics by listening to native speakers but not their audio/video. [Kuhl et al, 2003]</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>What type of learning does each linguistic domain require?</p>
    <p>What modeling frameworks are suitable for each?</p>
  </div>
  <div class="page">
    <p>Is Language Learned? How? Is Language Learning Effortless?</p>
    <p>Learning Mechanisms Learning about Words Learning the Structure</p>
  </div>
  <div class="page">
    <p>Word Learning Stages</p>
    <p>Segmenting speech to words.</p>
    <p>Mapping a meaning to words.</p>
  </div>
  <div class="page">
    <p>Context-bound Words</p>
    <p>Used only in one context: saying duck only when hitting the toy to the bathtub. [Barrett, 1986]</p>
    <p>Are parts of language games.</p>
    <p>Function-specific understanding -- different from adults mental representations of words.</p>
  </div>
  <div class="page">
    <p>Early Words</p>
    <p>English Turkish</p>
    <p>http://wordbank.stanford.edu/</p>
    <p>nominals modifiers</p>
    <p>action words</p>
    <p>social words</p>
    <p>function words</p>
  </div>
  <div class="page">
    <p>Word Learning Errors</p>
    <p>Underextension: using words in a more restricted fashion; dog to refer to spaniels.</p>
    <p>Overextension: using words more broadly; all four-legged animals as doggie.  cat: cat, cats usual location on the top of TV</p>
    <p>when absent. [Rescorla, 1980]</p>
  </div>
  <div class="page">
    <p>Cross-situational Learning</p>
    <p>People (as young as 12-month-old infants) are sensitive to the statistical regularities across situations. [Pinker 1989; Yu &amp; Smith 2007; Smith &amp; Yu, 2008]</p>
    <p>A zant Look at the zant!</p>
  </div>
  <div class="page">
    <p>Biases that Guide Word Learning</p>
    <p>The input is noisy and ambiguous: many possible mappings/hypotheses for word meanings.</p>
    <p>People learn word meanings from a few exposures.</p>
    <p>Learned/innate biases might facilitate learning.</p>
  </div>
  <div class="page">
    <p>Biases that Guide Word Learning</p>
    <p>mutual exclusivity bias [Markman &amp; Wachtel, 1988]</p>
    <p>taxonomic bias [Markman &amp; Hutchinson, 1984; Markman, 1989]</p>
    <p>basic-level bias [Rosch et al, 1976; Markman, 1991]</p>
    <p>social-pragmatic biases communicative intentions</p>
    <p>[Bloom, 2000; Tomasello, 2001]</p>
    <p>following eye gaze [Baldwin, 1993]</p>
    <p>syntax [Brown, 1957;</p>
    <p>Gelman &amp; Markman, 1985]</p>
    <p>noun bias [Gentner, 1982]</p>
    <p>whole-object bias [Markman, 1991] shape bias [Smith &amp; Jones, 1988]</p>
    <p>attention [Samuelson &amp; smith, 1998;</p>
    <p>Yu et al, 2017]</p>
  </div>
  <div class="page">
    <p>Learn word labels for the whole object.</p>
    <p>The Whole-Object Bias [Markman, 1991]</p>
    <p>What is dax?</p>
  </div>
  <div class="page">
    <p>Limit the number of possible word labels for a familiar object.</p>
    <p>The Mutual Exclusivity Bias [Markman &amp; Wachtel, 1988]</p>
    <p>What is dax?</p>
    <p>familiar object unfamiliar object</p>
  </div>
  <div class="page">
    <p>The Basic-Level Bias</p>
    <p>Golden Retriever?</p>
    <p>Zant dog (any dog breed)?</p>
    <p>animal? Cross-situational statistics are consistent with all.</p>
    <p>Why dog? A bias that focuses generalization to the basic-level (cognitively natural) categories.</p>
  </div>
  <div class="page">
    <p>Syntactic Bootstrapping</p>
    <p>Language structure supports learning new verbs. [Gleitman, 1990; Fisher et al, 1994]</p>
    <p>The rabbit is gorping the duck. or</p>
    <p>The rabbit and the duck are gorping. where is gorping now?</p>
    <p>[Naigles, 1990]</p>
  </div>
  <div class="page">
    <p>Modeling Word Learning</p>
    <p>Solving the translation problem: mapping words to observations. [Siskind, 1996; Yu &amp; Ballard, 2007; Frank et al, 2009; Fazly et al, 2010; Nematzadeh et al, 2015]</p>
    <p>wordsobjects</p>
    <p>the cat is sitting on the sheep</p>
    <p>[Frank et al, 2009]</p>
  </div>
  <div class="page">
    <p>Is Language Learned? How? Is Language Learning Effortless?</p>
    <p>Learning Mechanisms Learning about Words</p>
    <p>Learning the Structure</p>
  </div>
  <div class="page">
    <p>Language is Productive</p>
    <p>We have the capacity to produce and understand an infinite number of new sentences.</p>
    <p>Two productive systems:  Syntax: sentence structure; ordering of words.  Morphology: structure of words &amp; word parts.</p>
  </div>
  <div class="page">
    <p>Syntax: Level of Abstraction</p>
    <p>Rita drinks milk.  Sentence  Rita + drinks + milk (not productive)  Sentence  agent of action + action + theme</p>
    <p>Rita resembles Ray.  Sentence  noun + verb + noun</p>
    <p>What is origin of the variables and the rules?</p>
  </div>
  <div class="page">
    <p>Syntax: Type of Structure</p>
    <p>Sentences have hierarchical structure.</p>
    <p>The (clever) cat cried (a river).  S  NP + VP, NP  (det) + (adj) + N, VP  V + NP</p>
    <p>Is human language use hierarchical? [Frank et al, 2012]</p>
  </div>
  <div class="page">
    <p>Morphology</p>
    <p>Adds grammatical information to words.  Plural s in English</p>
    <p>Children learn morphology earlier when language is morphologically rich. [Peters, 1995]</p>
    <p>Easy morphemes to learn: frequent, fixed form and relative position to stem, clear function.</p>
  </div>
  <div class="page">
    <p>Do Children Know Grammatical Rules?</p>
    <p>Early word combinations are systematic.  my teddy (possessor + possessed)  daddy sit (actor + action)</p>
    <p>Overgeneralization errors:  I am a good boy, amnt I (syntax)  toothes; breaked (morphology)</p>
  </div>
  <div class="page">
    <p>Do Children Know Syntactic Rules?</p>
    <p>The pig is pilking the horse  The horse is being pilked by the pig</p>
  </div>
  <div class="page">
    <p>Do Children Know Morphological Rules? [Berko, 1958]</p>
  </div>
  <div class="page">
    <p>Modeling Structure</p>
    <p>Learning abstractions through hierarchical representations. [Alishahi &amp; Stevenson, 2008; Perfors et al, 2009; Barak et al, 2013]</p>
    <p>Debbie gave a pretzel to Dean (PD) Debbie gave Dean a pretzel (DOD)</p>
    <p>[Alishahi &amp; Stevenson, 2008]</p>
  </div>
  <div class="page">
    <p>Generalization to Test Linguistic Knowledge</p>
    <p>Childrens knowledge of language is examined by generalization tasks:  Mapping novel words to new/familiar objects.  Using a new verb in unheard structures.  Applying morphological rules to new words.</p>
    <p>Can AI models pass these generalization tasks?</p>
  </div>
  <div class="page">
    <p>Nature of Nature</p>
    <p>Abstract knowledge (priors/inductive biases/constraints) guides our generalization.</p>
    <p>What are the origins of our abstract knowledge? Can it be learned from experience?</p>
    <p>Thanks!</p>
  </div>
  <div class="page">
    <p>Language learning and processing in people and machines</p>
    <p>Aida Nematzadeh, Richard Futrell, and Roger Levy</p>
    <p>Part II: Human language processing</p>
  </div>
  <div class="page">
    <p>Goals of part II of tutorial  Overview of human language processing</p>
    <p>Theoretically deep questions about language and mind  Helps establish long-term benchmarks for human-like AI</p>
    <p>systems for language  Main points:</p>
    <p>How we can study human language processing  First-cut theory  Limitations for first-cut theory:</p>
    <p>Memory considerations  Character of input representations</p>
    <p>More advanced theory  Open frontiers</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>The woman who was given the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>Simple past Past participle bring brought brought give gave given</p>
  </div>
  <div class="page">
    <p>Structure and surprise</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>who was</p>
    <p>The woman given the sandwich from the kitchen tripped.</p>
    <p>Meaning can help us avoid surprise, too:</p>
    <p>The evidence examined by the lawyer from the firm was unreliable.</p>
    <p>Simple past Past participle bring brought brought give gave given</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>NP VP</p>
    <p>SMain Verb</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>Main Verb</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>NP VP</p>
    <p>SMain Verb Reduced Relative</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>NP VP</p>
    <p>SMain Verb Reduced Relative</p>
    <p>NP VP</p>
    <p>S</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>NP VP</p>
    <p>SMain Verb Reduced Relative</p>
    <p>(who was)</p>
    <p>NP VP</p>
    <p>S</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>NP VP</p>
    <p>SMain Verb Reduced Relative</p>
    <p>NP VP</p>
    <p>S</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>People fail to understand it most of the time</p>
    <p>NP VP</p>
    <p>SMain Verb Reduced Relative</p>
    <p>NP VP</p>
    <p>S</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Anatomy of ye olde garden path sentence  Classic example of incrementality in comprehension</p>
    <p>People fail to understand it most of the time  People are likely to misunderstand ite.g.,</p>
    <p>The woman who brought the sandwich from the kitchen tripped</p>
    <p>The woman brought the sandwich from the kitchen and tripped  Whats a kitchen tripped?</p>
    <p>NP VP</p>
    <p>SMain Verb Reduced Relative</p>
    <p>NP VP</p>
    <p>S</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(c.f. The horse raced past the barn fell; Bever, 1970)</p>
  </div>
  <div class="page">
    <p>Measuring human incremental processing state  Eye movements in the visual world  Word-by-word reading times</p>
    <p>Self-paced reading  Eye movements during natural reading</p>
    <p>Recordings of brain activity  Electrophysiological (EEG/ERP)  Magneto-encephalography (MEG)  functional Magnetic Resonance Imaging (fMRI)  Electrocorticography (ECoG)</p>
  </div>
  <div class="page">
    <p>Measuring human incremental processing state  Eye movements in the visual world  Word-by-word reading times</p>
    <p>Self-paced reading  Eye movements during natural reading</p>
    <p>Recordings of brain activity  Electrophysiological (EEG/ERP)  Magneto-encephalography (MEG)  functional Magnetic Resonance Imaging (fMRI)  Electrocorticography (ECoG)</p>
    <p>Behavioral</p>
  </div>
  <div class="page">
    <p>Measuring human incremental processing state  Eye movements in the visual world  Word-by-word reading times</p>
    <p>Self-paced reading  Eye movements during natural reading</p>
    <p>Recordings of brain activity  Electrophysiological (EEG/ERP)  Magneto-encephalography (MEG)  functional Magnetic Resonance Imaging (fMRI)  Electrocorticography (ECoG)</p>
    <p>Behavioral</p>
    <p>Neural</p>
  </div>
  <div class="page">
    <p>Eye movements in the visual world</p>
  </div>
  <div class="page">
    <p>Eye movements in the visual world</p>
  </div>
  <div class="page">
    <p>Eye movements in the visual world</p>
  </div>
  <div class="page">
    <p>Eye movements in the visual world</p>
  </div>
  <div class="page">
    <p>A visual world experiment</p>
    <p>Eye camera</p>
    <p>Scene camera</p>
    <p>Allopenna, Magnuson &amp; Tanenhaus (1998)(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Instruction to experimental participant:</p>
    <p>A visual world experiment</p>
    <p>Eye camera</p>
    <p>Scene camera</p>
    <p>Allopenna, Magnuson &amp; Tanenhaus (1998)(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Instruction to experimental participant:</p>
    <p>Pick up the beaker</p>
    <p>A visual world experiment</p>
    <p>Eye camera</p>
    <p>Scene camera</p>
    <p>Allopenna, Magnuson &amp; Tanenhaus (1998)(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>+</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage Time</p>
    <p>P ro</p>
    <p>po rti</p>
    <p>on o</p>
    <p>f f ix</p>
    <p>at io</p>
    <p>ns</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage Time</p>
    <p>P ro</p>
    <p>po rti</p>
    <p>on o</p>
    <p>f f ix</p>
    <p>at io</p>
    <p>ns</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker. Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Data from human eye movements</p>
    <p>Target = beaker</p>
    <p>Cohort = beetle</p>
    <p>Unrelated = carriage Time</p>
    <p>P ro</p>
    <p>po rti</p>
    <p>on o</p>
    <p>f f ix</p>
    <p>at io</p>
    <p>ns</p>
    <p>Trial Number</p>
    <p>Time +</p>
    <p>Pick up the beaker.</p>
    <p>cohort unrelated</p>
    <p>Look at the cross.</p>
    <p>(Slide courtesy of Mike Tanenhaus)</p>
  </div>
  <div class="page">
    <p>Allopenna, Magnuson &amp; Tanenhaus (1998)</p>
  </div>
  <div class="page">
    <p>Self-paced reading</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>----------------------------------------------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>While ----------------------------------------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>----- the ------------------------------------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>--------- clouds -----------------------------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>---------------- crackled, -------------------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>-------------------------- above -------------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>-------------------------------- the ---------------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>------------------------------------ glider --------------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>------------------------------------------- soared -------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>Readers arent allowed to backtrack</p>
    <p>------------------------------------------- soared -------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Self-paced reading  Participant presses a button to reveal each successive</p>
    <p>word and mask previous words:</p>
    <p>Readers arent allowed to backtrack  Duration between button presses=reading time for each</p>
    <p>word</p>
    <p>------------------------------------------- soared -------------------</p>
    <p>(Mitchell, 1984)</p>
  </div>
  <div class="page">
    <p>Language processing signal from the eyes</p>
    <p>(movie by Piers Cornelissen)</p>
    <p>Leaves a fine-grained trace of the real-time language comprehension record  we will put this to use later in the tutorial!</p>
    <p>(Rayner, 1998)</p>
  </div>
  <div class="page">
    <p>Language processing signal from the eyes</p>
    <p>(movie by Piers Cornelissen)</p>
    <p>Leaves a fine-grained trace of the real-time language comprehension record  we will put this to use later in the tutorial!</p>
    <p>(Rayner, 1998)</p>
  </div>
  <div class="page">
    <p>Electroencephalography (EEG/ERP)</p>
  </div>
  <div class="page">
    <p>Rapid Serial Visual Presentation</p>
    <p>*</p>
  </div>
  <div class="page">
    <p>Rapid Serial Visual Presentation</p>
  </div>
  <div class="page">
    <p>The N400 ERP component in language comprehension  Differing degrees of semantic congruity:</p>
    <p>He took a sip from the drink. (normal)  He took a sip from the waterfall. (moderate incongruity)  He took a sip from the transmitter. (strong incongruity)</p>
    <p>(Kutas &amp; Hillyard, 1980, 1984)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>(Osterhout et al., 1997)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>Definitional mismatch (manherself)</p>
    <p>(Osterhout et al., 1997)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>Definitional mismatch (manherself)</p>
    <p>(Osterhout et al., 1997)</p>
    <p>Definitional match (manhimself)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>Mismatches to stereotypical semantic properties induce similar violations</p>
    <p>The nurse prepared himself for the operation. 16(Osterhout et al., 1997; see also reading time studies by Sturt, 2003; Duffy &amp; Keir, 2004, inter alia)</p>
    <p>Definitional mismatch (manherself)</p>
    <p>(Osterhout et al., 1997)</p>
    <p>Definitional match (manhimself)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>Mismatches to stereotypical semantic properties induce similar violations</p>
    <p>The nurse prepared himself for the operation. 16(Osterhout et al., 1997; see also reading time studies by Sturt, 2003; Duffy &amp; Keir, 2004, inter alia)</p>
    <p>Definitional mismatch (manherself)</p>
    <p>(Osterhout et al., 1997)</p>
    <p>Definitional match (manhimself)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>Mismatches to stereotypical semantic properties induce similar violations</p>
    <p>The nurse prepared himself for the operation. 16(Osterhout et al., 1997; see also reading time studies by Sturt, 2003; Duffy &amp; Keir, 2004, inter alia)</p>
    <p>Definitional mismatch (manherself)</p>
    <p>Stereotypical mismatch</p>
    <p>(Osterhout et al., 1997)</p>
    <p>Definitional match (manhimself)</p>
  </div>
  <div class="page">
    <p>The P600 ERP component in language comprehension</p>
    <p>Mismatches to lexically specified (definitional*) semantic properties induce measurable expectation violations</p>
    <p>The man prepared herself for the interview.</p>
    <p>Mismatches to stereotypical semantic properties induce similar violations</p>
    <p>The nurse prepared himself for the operation. 16(Osterhout et al., 1997; see also reading time studies by Sturt, 2003; Duffy &amp; Keir, 2004, inter alia)</p>
    <p>Definitional mismatch (manherself)</p>
    <p>Stereotypical mismatch</p>
    <p>Stereotypical match(Osterhout et al., 1997)</p>
    <p>Definitional match (manhimself)</p>
  </div>
  <div class="page">
    <p>fMRI recordings during comprehension  MRI measures changes in brain</p>
    <p>associated with blood flow  Slow, but good spatial resolution</p>
    <p>for which parts of the brain are active in processing</p>
  </div>
  <div class="page">
    <p>fMRI recordings during comprehension  MRI measures changes in brain</p>
    <p>associated with blood flow  Slow, but good spatial resolution</p>
    <p>for which parts of the brain are active in processing</p>
  </div>
  <div class="page">
    <p>fMRI recordings during comprehension  MRI measures changes in brain</p>
    <p>associated with blood flow  Slow, but good spatial resolution</p>
    <p>for which parts of the brain are active in processing</p>
  </div>
  <div class="page">
    <p>Functional brain specificity for language</p>
  </div>
  <div class="page">
    <p>Electrocorticography  Pre-surgical epilepsy patients get electrode arrays directly</p>
    <p>implanted on the surface of the cortex</p>
    <p>During pre-surgical monitoring many patients generously donate their energy &amp; attention for experiments 19</p>
    <p>http://med.stanford.edu/neurosurgery/research/NPTL/research2/_jcr_content/main/panel_builder/panel_0/text_image.img.620.high.png</p>
    <p>https://commons.wikimedia.org/wiki/ File:Intracranial_electrode_grid_for_electrocorticography.png</p>
  </div>
  <div class="page">
    <p>Neural phonemic representations</p>
  </div>
  <div class="page">
    <p>Neural consonant representations</p>
  </div>
  <div class="page">
    <p>Scientific opportunity: Comprehensive theory to account for patterns of human language use &amp; representation</p>
    <p>Engineering opportunity: Better prediction of human language understanding, and more human-like AI language-using agents</p>
  </div>
  <div class="page">
    <p>Rational analysis  Background assumption: cognitive agent is optimized via</p>
    <p>evolution and learning to solve everyday tasks effectively 1. Specify precisely the goals of the cognitive system 2. Formalize model of the environment adapted to 3. Make minimal assumptions re: computational limitations 4. Derive predicted optimal behavior given 13 5. Compare predictions with empirical data 6. If necessary, iterate 15</p>
  </div>
  <div class="page">
    <p>Incrementality and Rationality  Real-time language understanding is hard  But lots of information sources can be usefully brought to</p>
    <p>bear to help with the task  Therefore, it would be rational for people to use all the</p>
    <p>information available, whenever possible  This is what incrementality is  We have lots of evidence that people do this often</p>
    <p>Put the apple on the towel in the box. (Tanenhaus et al., 1995, Science)</p>
  </div>
  <div class="page">
    <p>Enter probabilistic grammars from computational linguistics...</p>
  </div>
  <div class="page">
    <p>Probabilistic Context-Free Grammars A probabilistic context-free grammar (PCFG) consists of a tuple (N,V,S,R,P) such that:</p>
    <p>! N is a finite set of non-terminal symbols; ! V is a finite set of terminal symbols; ! S is the start symbol; ! R is a finite set of rules of the form X   where X  N and  is a sequence of symbols drawn from N  V;</p>
    <p>! P is a mapping from R into probabilities, such that for each X  N,</p>
    <p>!</p>
    <p>[X]R P(X  ) = 1</p>
    <p>PCFG derivations and derivation trees are just like for CFGs. The probability P(T) of a derivation tree is simply the product of the probabilities of each rule application.</p>
  </div>
  <div class="page">
    <p>Example PCFG 1 S NP VP 0.8 NP Det N 0.2 NP NP PP 1 PP P NP 1 VP V</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>PP</p>
    <p>P</p>
    <p>near</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>cat</p>
    <p>VP</p>
    <p>V</p>
    <p>growled</p>
    <p>P(T) = 1  0.2  0.8  1  0.5  1  1  0.8  1  0.5  1  1</p>
    <p>= 0.032</p>
  </div>
  <div class="page">
    <p>Incrementality:</p>
  </div>
  <div class="page">
    <p>A zeroth-cut theory of incremental comprehension</p>
    <p>Human knowledge described by a probabilistic grammar</p>
    <p>Incremental input interpretation follows Bayes Rule:</p>
    <p>P(! | words)  P(words | T )P(T )</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>?</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>?</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .NP</p>
    <p>. . .RC</p>
    <p>. . .VP</p>
    <p>. . .PP</p>
    <p>. . .NP</p>
    <p>. . .N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .V</p>
    <p>tripped</p>
    <p>NP</p>
    <p>RC</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>Strong garden-pathing</p>
    <p>The woman brought the sandwich from the kitchen tripped.</p>
    <p>S</p>
    <p>. . .VP</p>
    <p>. . .V</p>
    <p>tripped</p>
    <p>NP</p>
    <p>RC</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>N</p>
    <p>kitchen</p>
    <p>Det</p>
    <p>the</p>
    <p>P</p>
    <p>from</p>
    <p>NP</p>
    <p>N</p>
    <p>sandwich</p>
    <p>Det</p>
    <p>the</p>
    <p>Part</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
    <p>Comprehension only successful if the earlierdisfavored interpretation is still available!!!</p>
    <p>(Levy, Reali, &amp; Griffiths, 2009)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>difficulty here (68ms/char)</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>Compare with:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched, the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched its owner the vet and his new assistant removed the muzzle.</p>
    <p>difficulty here (68ms/char)</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>Compare with:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched, the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched its owner the vet and his new assistant removed the muzzle.</p>
    <p>difficulty here (68ms/char)</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>Compare with:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched, the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched its owner the vet and his new assistant removed the muzzle.</p>
    <p>difficulty here (68ms/char)</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>Compare with:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched, the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched its owner the vet and his new assistant removed the muzzle.</p>
    <p>difficulty here (68ms/char)</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>But not all garden paths are catastrophic:  Heres another type of local syntactic ambiguity:</p>
    <p>Compare with:</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched, the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched its owner the vet and his new assistant removed the muzzle.</p>
    <p>difficulty here (68ms/char)</p>
    <p>easier (50ms/char)</p>
    <p>(Frazier &amp; Rayner, 1982)</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines!</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>chat?</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>chat? wash?</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>chat? wash? get warm?</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>the children went outside to</p>
    <p>chat? wash? get warm?</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>the children went outside to</p>
    <p>play</p>
    <p>chat? wash? get warm?</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>the children went outside to</p>
    <p>Predictable words are read faster (Ehrlich &amp; Rayner, 1981) and have distinctive EEG responses (Kutas &amp; Hillyard 1980)</p>
    <p>play</p>
    <p>chat? wash? get warm?</p>
  </div>
  <div class="page">
    <p>A first-cut theory of incremental comprehension:  Stick with probabilistic grammars and Bayesian inference  But let a words difficulty be its surprisal given its context:</p>
    <p>Captures the expectation intuition: the more we expect an event, the easier it is to process  Brains are prediction engines! my brother came inside to</p>
    <p>the children went outside to</p>
    <p>Predictable words are read faster (Ehrlich &amp; Rayner, 1981) and have distinctive EEG responses (Kutas &amp; Hillyard 1980)</p>
    <p>Probabilistic grammars give grammatical expectations 32(Hale, 2001, NAACL; Levy, 2008, Cognition)</p>
    <p>play</p>
    <p>chat? wash? get warm?</p>
  </div>
  <div class="page">
    <p>The surprisal graph</p>
    <p>S ur</p>
    <p>pr is</p>
    <p>al (b</p>
    <p>its )</p>
  </div>
  <div class="page">
    <p>A small PCFG for this sentence type</p>
    <p>S  SBAR S 0.3 Conj  and 1 Adj  new 1 S  NP VP 0.7 Det  the 0.8 VP  V NP 0.5 SBAR  COMPL S 0.3 Det  its 0.1 VP  V 0.5 SBAR  COMPL S COMMA 0.7 Det  his 0.1 V  scratched 0.25 COMPL  When 1 N  dog 0.2 V  removed 0.25 NP  Det N 0.6 N  vet 0.2 V  arrived 0.5 NP  Det Adj N 0.2 N  assistant 0.2 COMMA  , 1 NP  NP Conj NP 0.2 N  muzzle 0.2</p>
    <p>N  owner 0.2</p>
    <p>(analysis in Levy, 2013)</p>
  </div>
  <div class="page">
    <p>A small PCFG for this sentence type</p>
    <p>S  SBAR S 0.3 Conj  and 1 Adj  new 1 S  NP VP 0.7 Det  the 0.8 VP  V NP 0.5 SBAR  COMPL S 0.3 Det  its 0.1 VP  V 0.5 SBAR  COMPL S COMMA 0.7 Det  his 0.1 V  scratched 0.25 COMPL  When 1 N  dog 0.2 V  removed 0.25 NP  Det N 0.6 N  vet 0.2 V  arrived 0.5 NP  Det Adj N 0.2 N  assistant 0.2 COMMA  , 1 NP  NP Conj NP 0.2 N  muzzle 0.2</p>
    <p>N  owner 0.2</p>
    <p>(analysis in Levy, 2013)</p>
  </div>
  <div class="page">
    <p>Two incremental trees</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.826</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>Ultimately-correct analysis S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.826</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>Ultimately-correct analysis S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.174</p>
    <p>P(T|w1...10) = 0.826</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>Ultimately-correct analysis S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.174</p>
    <p>P(T|w1...10) = 0.826</p>
    <p>removed?</p>
    <p>(analysis in Levy, 2013)</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>Ultimately-correct analysis S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.174</p>
    <p>P(T|w1...10) = 0.826</p>
    <p>removed?</p>
    <p>removed?</p>
    <p>(analysis in Levy, 2013)</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>Ultimately-correct analysis S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.174</p>
    <p>Disambiguating word probability marginalizes over incremental trees:</p>
    <p>P(T|w1...10) = 0.826</p>
  </div>
  <div class="page">
    <p>Two incremental trees  Garden-path analysis:</p>
    <p>Ultimately-correct analysis S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>S</p>
    <p>NP VP</p>
    <p>V</p>
    <p>P(T|w1...10) = 0.174</p>
    <p>Disambiguating word probability marginalizes over incremental trees:</p>
    <p>P(removed|w1...10) = !</p>
    <p>T</p>
    <p>P(removed|T)P(T|w1...10)</p>
    <p>P(T|w1...10) = 0.826</p>
    <p>= 0  0.826 + 0.25  0.174</p>
    <p>(analysis in Levy, 2013)</p>
  </div>
  <div class="page">
    <p>Preceding context can disambiguate  its owner takes up the object slot of scratched</p>
    <p>S</p>
    <p>SBAR</p>
    <p>COMPL</p>
    <p>When</p>
    <p>S</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>dog</p>
    <p>VP</p>
    <p>V</p>
    <p>scratched</p>
    <p>NP</p>
    <p>Det</p>
    <p>its</p>
    <p>N</p>
    <p>owner</p>
    <p>S</p>
    <p>NP</p>
    <p>NP</p>
    <p>Det</p>
    <p>the</p>
    <p>N</p>
    <p>vet</p>
    <p>Conj</p>
    <p>and</p>
    <p>NP</p>
    <p>Det</p>
    <p>his</p>
    <p>Adj</p>
    <p>new</p>
    <p>N</p>
    <p>assistant</p>
    <p>VP</p>
    <p>V</p>
    <p>Condition Surprisal at Resolution NP absent 4.2 NP present 2</p>
  </div>
  <div class="page">
    <p>Sensitivity to verb argument structure  A superficially similar example:</p>
    <p>When the dog arrived the vet and his new assistant removed the muzzle.</p>
    <p>(Staub, 2007)</p>
  </div>
  <div class="page">
    <p>Sensitivity to verb argument structure  A superficially similar example:</p>
    <p>When the dog arrived the vet and his new assistant removed the muzzle.</p>
    <p>Easier here</p>
    <p>(Staub, 2007)</p>
  </div>
  <div class="page">
    <p>Sensitivity to verb argument structure  A superficially similar example:</p>
    <p>When the dog arrived the vet and his new assistant removed the muzzle.</p>
    <p>Easier here</p>
    <p>(Staub, 2007)</p>
    <p>But harder here!</p>
  </div>
  <div class="page">
    <p>Sensitivity to verb argument structure  A superficially similar example:</p>
    <p>When the dog arrived the vet and his new assistant removed the muzzle.</p>
    <p>(c.f. When the dog scratched the vet and his new assistant removed the muzzle.)</p>
    <p>Easier here</p>
    <p>(Staub, 2007)</p>
    <p>But harder here!</p>
  </div>
  <div class="page">
    <p>S  SBAR S 0.3 Conj  and 1 Adj  new 1 S  NP VP 0.7 Det  the 0.8 VP  V NP 0.5 SBAR  COMPL S 0.3 Det  its 0.1 VP  V 0.5 SBAR  COMPL S COMMA 0.7 Det  his 0.1 V  scratched 0.25 COMPL  When 1 N  dog 0.2 V  removed 0.25 NP  Det N 0.6 N  vet 0.2 V  arrived 0.5 NP  Det Adj N 0.2 N  assistant 0.2 COMMA  , 1 NP  NP Conj NP 0.2 N  muzzle 0.2</p>
    <p>N  owner 0.2</p>
    <p>Modeling argument-structure sensitivity</p>
  </div>
  <div class="page">
    <p>S  SBAR S 0.3 Conj  and 1 Adj  new 1 S  NP VP 0.7 Det  the 0.8 VP  V NP 0.5 SBAR  COMPL S 0.3 Det  its 0.1 VP  V 0.5 SBAR  COMPL S COMMA 0.7 Det  his 0.1 V  scratched 0.25 COMPL  When 1 N  dog 0.2 V  removed 0.25 NP  Det N 0.6 N  vet 0.2 V  arrived 0.5 NP  Det Adj N 0.2 N  assistant 0.2 COMMA  , 1 NP  NP Conj NP 0.2 N  muzzle 0.2</p>
    <p>N  owner 0.2</p>
    <p>Modeling argument-structure sensitivity</p>
    <p>The context-free assumption doesnt preclude relaxing probabilistic locality:</p>
    <p>(Johnson, 1998; Klein &amp; Manning, 2003)</p>
  </div>
  <div class="page">
    <p>S  SBAR S 0.3 Conj  and 1 Adj  new 1 S  NP VP 0.7 Det  the 0.8 VP  V NP 0.5 SBAR  COMPL S 0.3 Det  its 0.1 VP  V 0.5 SBAR  COMPL S COMMA 0.7 Det  his 0.1 V  scratched 0.25 COMPL  When 1 N  dog 0.2 V  removed 0.25 NP  Det N 0.6 N  vet 0.2 V  arrived 0.5 NP  Det Adj N 0.2 N  assistant 0.2 COMMA  , 1 NP  NP Conj NP 0.2 N  muzzle 0.2</p>
    <p>N  owner 0.2</p>
    <p>Modeling argument-structure sensitivity</p>
    <p>The context-free assumption doesnt preclude relaxing probabilistic locality:</p>
    <p>(Johnson, 1998; Klein &amp; Manning, 2003)</p>
  </div>
  <div class="page">
    <p>S  SBAR S 0.3 Conj  and 1 Adj  new 1 S  NP VP 0.7 Det  the 0.8 VP  V NP 0.5 SBAR  COMPL S 0.3 Det  its 0.1 VP  V 0.5 SBAR  COMPL S COMMA 0.7 Det  his 0.1 V  scratched 0.25 COMPL  When 1 N  dog 0.2 V  removed 0.25 NP  Det N 0.6 N  vet 0.2 V  arrived 0.5 NP  Det Adj N 0.2 N  assistant 0.2 COMMA  , 1 NP  NP Conj NP 0.2 N  muzzle 0.2</p>
    <p>N  owner 0.2</p>
    <p>Modeling argument-structure sensitivity</p>
    <p>The context-free assumption doesnt preclude relaxing probabilistic locality:</p>
    <p>(Johnson, 1998; Klein &amp; Manning, 2003)</p>
    <p>VP  V NP 0.5 VP  Vtrans NP 0.45</p>
    <p>VP  V 0.5 Replaced by</p>
    <p>VP  Vtrans 0.05</p>
    <p>V  scratched 0.25 VP  Vintrans 0.45</p>
    <p>V  removed 0.25 VP  Vintrans NP 0.05</p>
    <p>V  arrived 0.5 Vtrans  scratched 0.5</p>
    <p>Vtrans  removed 0.5</p>
    <p>Vintrans  arrived 1</p>
  </div>
  <div class="page">
    <p>Result</p>
    <p>When the dog arrived the vet and his new assistant removed the muzzle.</p>
    <p>When the dog scratched the vet and his new assistant removed the muzzle.</p>
    <p>ambiguity onset ambiguity resolution</p>
    <p>Transitivity-distinguishing PCFG Condition Ambiguity onset Resolution Intransitive (arrived) 2.11 3.20 Transitive (scratched) 0.44 8.04</p>
  </div>
  <div class="page">
    <p>Surprisal vs. predictability in general</p>
    <p>But is there evidence for surprisal as the specific function relating probability to processing difficulty?</p>
  </div>
  <div class="page">
    <p>Estimating probability/time curve shape</p>
  </div>
  <div class="page">
    <p>Estimating probability/time curve shape  As a proxy for processing difficulty, reading time in two</p>
    <p>different methods: self-paced reading &amp; eye-tracking</p>
  </div>
  <div class="page">
    <p>Estimating probability/time curve shape  As a proxy for processing difficulty, reading time in two</p>
    <p>different methods: self-paced reading &amp; eye-tracking  Challenge: we need big data to estimate curve shape, but</p>
    <p>probability correlated with confounding variables</p>
  </div>
  <div class="page">
    <p>Estimating probability/time curve shape  As a proxy for processing difficulty, reading time in two</p>
    <p>different methods: self-paced reading &amp; eye-tracking  Challenge: we need big data to estimate curve shape, but</p>
    <p>probability correlated with confounding variables</p>
    <p>(5K words) (50K words)</p>
  </div>
  <div class="page">
    <p>Hypothesized curve shapes</p>
  </div>
  <div class="page">
    <p>Estimating probability/time curve shape  GAM regression:</p>
    <p>total contribution of word (trigram) probability to RT near-linear over 6 orders of magnitude!</p>
    <p>(Smith &amp; Levy, 2013; more recent validation by Goodkind &amp; Bicknell, 2018)</p>
    <p>at</p>
    <p>or ig</p>
    <p>To ta</p>
    <p>l a m</p>
    <p>ou nt</p>
    <p>o f s</p>
    <p>lo w</p>
    <p>do w</p>
    <p>n (m</p>
    <p>s) 0</p>
    <p>at</p>
    <p>or ig</p>
    <p>To ta</p>
    <p>l a m</p>
    <p>ou nt</p>
    <p>o f s</p>
    <p>lo w</p>
    <p>do w</p>
    <p>n (m</p>
    <p>s) 0</p>
    <p>Reading times in self-paced reading</p>
    <p>Gaze durations in eye-tracking</p>
  </div>
  <div class="page">
    <p>Integration with deep learning  Humans condition extremely flexibly on context  Goal: symbolic grammars + neural generatization  Enabling step: action sequence for structure building</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe (S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>Action Meaning String gloss</p>
    <p>NT(X) Push a new open non-terminal on top of the stack (X</p>
    <p>Gen(w) Generate word w as a terminal node and put it on top of the stack (as a closed node)</p>
    <p>w</p>
    <p>REDUCE Pop closed nodes N1i-1 from the top of the stack until encountering open node Ni; close Ni</p>
    <p>)</p>
    <p>END Finish parsing (iff the sole stack element is a closed S) n/a</p>
  </div>
  <div class="page">
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat )</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me REDUCE (S | (NP the hungry cat ) | (VP | chased | (NP me )</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me REDUCE (S | (NP the hungry cat ) | (VP | chased | (NP me ) REDUCE (S | (NP the hungry cat ) | (VP chased (NP me ) )</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me REDUCE (S | (NP the hungry cat ) | (VP | chased | (NP me ) REDUCE (S | (NP the hungry cat ) | (VP chased (NP me ) ) REDUCE (S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me REDUCE (S | (NP the hungry cat ) | (VP | chased | (NP me ) REDUCE (S | (NP the hungry cat ) | (VP chased (NP me ) ) REDUCE (S (NP the hungry cat ) (VP chased (NP me ) ) ) END</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me REDUCE (S | (NP the hungry cat ) | (VP | chased | (NP me ) REDUCE (S | (NP the hungry cat ) | (VP chased (NP me ) ) REDUCE (S (NP the hungry cat ) (VP chased (NP me ) ) ) END</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
  </div>
  <div class="page">
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased NT(NP) (S | (NP the hungry cat ) | (VP | chased | (NP Gen(me) (S | (NP the hungry cat ) | (VP | chased | (NP | me REDUCE (S | (NP the hungry cat ) | (VP | chased | (NP me ) REDUCE (S | (NP the hungry cat ) | (VP chased (NP me ) ) REDUCE (S (NP the hungry cat ) (VP chased (NP me ) ) ) END</p>
    <p>(S (NP the hungry cat ) (VP chased (NP me ) ) )</p>
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>If we put a conditional probability distribution on actions, we have a probabilistic grammar!</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away)</p>
    <p>away Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP)</p>
    <p>PP Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP) NT(NP)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP) NT(NP)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP) NT(NP)</p>
    <p>Knowledge characterization: P(action|context)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP) NT(NP)</p>
    <p>Knowledge characterization: P(action|context)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>NP</p>
    <p>me</p>
    <p>chased</p>
    <p>NP</p>
    <p>cathungrythe</p>
    <p>???</p>
    <p>Gen(away) REDUCE NT(PP) NT(NP)</p>
    <p>Knowledge characterization: P(action|context)</p>
    <p>Action Stack NT(S) (S NT(NP) (S | (NP Gen(the) (S | (NP | the Gen(hungry) (S | (NP | the | hungry Gen(cat) (S | (NP | the | hungry | cat REDUCE (S | (NP the hungry cat ) NT(VP) (S | (NP the hungry cat ) | (VP Gen(chased) (S | (NP the hungry cat ) | (VP | chased</p>
    <p>(Henderson, 2003; Dyer et al. 2016; Kuncoro et al., 2017)</p>
  </div>
  <div class="page">
    <p>Recurrent Neural Network Grammars (RNNGs)</p>
    <p>(Recurrent Neural Network Grammar; Dyer et al, 2016)</p>
    <p>NP The hungry cat NP</p>
    <p>+(S (VP cat hungry The</p>
    <p>NT(S)</p>
    <p>NT(NP)</p>
    <p>GEN(The)</p>
    <p>GEN(hungry)</p>
    <p>GEN(cat)</p>
    <p>REDUCE</p>
    <p>NT(VP)</p>
    <p>GEN(meows)</p>
    <p>p(at) 0</p>
    <p>GEN NT(NP) NT(PP)</p>
    <p>NT(S) NT(NP) GEN(The) GEN(hungry) GEN(cat) REDUCE NT(VP)</p>
    <p>Test</p>
    <p>Peng Qian</p>
    <p>September 2018</p>
    <p>y P(x1 . . . xk1, xk, y)P y P(x1 . . . xk1, y)</p>
    <p>X</p>
    <p>yi</p>
    <p>X</p>
    <p>yi!i+1</p>
    <p>P(xi+1, yi!i+1|x1,...i, yi)P(yi|x1,...i)</p>
    <p>S</p>
    <p>.VP</p>
    <p>meows</p>
    <p>NP</p>
    <p>cathungryThe</p>
    <p>Test</p>
    <p>Peng Qian</p>
    <p>September 2018</p>
    <p>y P(x1 . . . xk1, xk, y)P y P(x1 . . . xk1, y)</p>
    <p>X</p>
    <p>yi</p>
    <p>X</p>
    <p>yi!i+1</p>
    <p>P(xi+1, yi!i+1|x1,...i, yi)P(yi|x1,...i)</p>
    <p>S</p>
    <p>.VP</p>
    <p>meows</p>
    <p>NP</p>
    <p>cathungryThe</p>
    <p>(Dyer et al., 2016; Kuncoro et al., 2017)</p>
    <p>Stack History</p>
    <p>Buffer</p>
    <p>Evidence of human-like language processing: Kuncoro et al., 2018 (ACL) Futrell et al., 2019 (NAACL) Hale et al., 2018 (ACL) Wilcox et al., 2019 (NAACL)</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the I saw the child</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>I saw the child</p>
    <p>I saw the childs dog</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>I saw the child</p>
    <p>I saw the childs dog</p>
    <p>I saw the child leave</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>I saw the child</p>
    <p>I saw the childs dog</p>
    <p>I saw the child leave</p>
    <p>I saw the childs dog leave</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>I saw the child</p>
    <p>I saw the childs dog</p>
    <p>I saw the child leave</p>
    <p>I saw the childs dog leave</p>
    <p>I saw the child left</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>I saw the child</p>
    <p>I saw the childs dog</p>
    <p>I saw the child leave</p>
    <p>I saw the childs dog leave</p>
    <p>I saw the child left</p>
    <p>I saw the childs dog left</p>
  </div>
  <div class="page">
    <p>An inferential challenge</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>I saw the child</p>
    <p>I saw the childs dog</p>
    <p>I saw the child leave</p>
    <p>I saw the childs dog leave</p>
    <p>I saw the child left</p>
    <p>I saw the childs dog left</p>
    <p>There is a potentially unbounded number of treegeneration operations just to get to the next word!</p>
  </div>
  <div class="page">
    <p>Inference using beam search</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the A word-synchronous beam, beam size=4</p>
    <p>(Stern et al., 2017)</p>
    <p>Natural account of strong garden-pathing effects (the woman brought the sandwich tripped):</p>
    <p>The needed analysis falls off the beam</p>
  </div>
  <div class="page">
    <p>Inference using beam search</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>Context C</p>
    <p>A word-synchronous beam, beam size=4</p>
    <p>(Stern et al., 2017)</p>
    <p>Natural account of strong garden-pathing effects (the woman brought the sandwich tripped):</p>
    <p>The needed analysis falls off the beam</p>
  </div>
  <div class="page">
    <p>Inference using beam search</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>Context C</p>
    <p>A word-synchronous beam, beam size=4</p>
    <p>(Stern et al., 2017)</p>
    <p>Natural account of strong garden-pathing effects (the woman brought the sandwich tripped):</p>
    <p>The needed analysis falls off the beam</p>
  </div>
  <div class="page">
    <p>Inference using beam search</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>Context C Actions A</p>
    <p>A word-synchronous beam, beam size=4</p>
    <p>(Stern et al., 2017)</p>
    <p>Natural account of strong garden-pathing effects (the woman brought the sandwich tripped):</p>
    <p>The needed analysis falls off the beam</p>
  </div>
  <div class="page">
    <p>Inference using beam search</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>Context C Actions A log P(A | C )</p>
    <p>A word-synchronous beam, beam size=4</p>
    <p>(Stern et al., 2017)</p>
    <p>Natural account of strong garden-pathing effects (the woman brought the sandwich tripped):</p>
    <p>The needed analysis falls off the beam</p>
  </div>
  <div class="page">
    <p>Inference using beam search</p>
    <p>(S (NP I ) (VP saw (NP the</p>
    <p>(S (NP I ) (VP saw (NP (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP the</p>
    <p>(S (NP I ) (VP saw (S (NP (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP the</p>
    <p>(S (NP I ) (VP saw (SBAR (NP (NP the</p>
    <p>Rank on beam</p>
    <p>Context C Actions A log P(A | C )</p>
    <p>A word-synchronous beam, beam size=4</p>
    <p>(Stern et al., 2017)</p>
    <p>Natural account of strong garden-pathing effects (the woman brought the sandwich tripped):</p>
    <p>The needed analysis falls off the beam</p>
  </div>
  <div class="page">
    <p>Challenges for surprisal theory</p>
    <p>Limitations in the memory representations available during real-time comprehension</p>
    <p>Accounting for input uncertainty from noise &amp; speaker error</p>
  </div>
  <div class="page">
    <p>Structural Forgetting and the Noisy Channel</p>
  </div>
  <div class="page">
    <p>Structural Forgetting and the Noisy Channel</p>
  </div>
  <div class="page">
    <p>Structural Forgetting and the Noisy Channel</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>the apartment</p>
    <p>that</p>
    <p>the maid</p>
    <p>who</p>
    <p>the cleaning service sent over</p>
    <p>cleaned</p>
    <p>was well decorated</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>the apartment</p>
    <p>that</p>
    <p>the maid</p>
    <p>who</p>
    <p>the cleaning service sent over</p>
    <p>was well decorated</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>the apartment</p>
    <p>that</p>
    <p>the maid</p>
    <p>who</p>
    <p>the cleaning service sent over</p>
    <p>was well decorated</p>
    <p>????????????</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>Structural forgetting effect: part of the sentence is forgotten by the time you get to the end (Gibson &amp; Thomas, 1999; Frazier, 1985; Fodor, p.c.)</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>Structural forgetting effect: part of the sentence is forgotten by the time you get to the end (Gibson &amp; Thomas, 1999; Frazier, 1985; Fodor, p.c.)</p>
    <p>The ungrammatical sentence seems better than the grammatical one.</p>
    <p>A &quot;grammaticality illusion&quot;: how could we define grammaticality in this case?</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>But the effect is language-dependent (Vasishth et al., 2010; Frank et al., 2016).</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>But the effect is language-dependent (Vasishth et al., 2010; Frank et al., 2016).</p>
    <p>In German (and Dutch), people prefer 2 over 1.</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>But the effect is language-dependent (Vasishth et al., 2010; Frank et al., 2016).</p>
    <p>In German (and Dutch), people prefer 2 over 1.  What is the difference between English and German?</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>But the effect is language-dependent (Vasishth et al., 2010; Frank et al., 2016).</p>
    <p>In German (and Dutch), people prefer 2 over 1.  What is the difference between English and German?  Frank et al. (2016) show that at recurrent neural network gives</p>
    <p>higher probability to (1) in English, but (2) in German.</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>But the effect is language-dependent (Vasishth et al., 2010; Frank et al., 2016).</p>
    <p>In German (and Dutch), people prefer 2 over 1.  What is the difference between English and German?  Frank et al. (2016) show that at recurrent neural network gives</p>
    <p>higher probability to (1) in English, but (2) in German.  But why?</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>These contexts are more common in German than English (Roland et al., 2007).</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>These contexts are more common in German than English (Roland et al., 2007).</p>
    <p>English: the maid [that cleaned the apartment] the apartment [that the maid cleaned]</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>These contexts are more common in German than English (Roland et al., 2007).</p>
    <p>English: the maid [that cleaned the apartment] the apartment [that the maid cleaned]</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>These contexts are more common in German than English (Roland et al., 2007).</p>
    <p>English: the maid [that cleaned the apartment] the apartment [that the maid cleaned]</p>
  </div>
  <div class="page">
    <p>Structural Forgetting</p>
    <p>These contexts are more common in German than English (Roland et al., 2007).</p>
    <p>English: the maid [that cleaned the apartment] the apartment [that the maid cleaned]</p>
    <p>German: das Dienstmdchen, [das die Wohnung reinigte] die Wohnung, [die das Dienstmdchen reinigte]</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence:</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
    <p>(d) The coach smiled at the player who was tossed the frisbee.</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
    <p>(d) The coach smiled at the player who was tossed the frisbee.</p>
    <p>Readers boggle at tossed in (a), but not in (b-d)</p>
    <p>Tabor et al. (2004, JML)</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
    <p>(d) The coach smiled at the player who was tossed the frisbee.</p>
    <p>Readers boggle at tossed in (a), but not in (b-d)</p>
    <p>Tabor et al. (2004, JML)</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
    <p>(d) The coach smiled at the player who was tossed the frisbee.</p>
    <p>Readers boggle at tossed in (a), but not in (b-d)</p>
    <p>Tabor et al. (2004, JML)</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
    <p>(d) The coach smiled at the player who was tossed the frisbee.</p>
    <p>Readers boggle at tossed in (a), but not in (b-d)</p>
    <p>Tabor et al. (2004, JML)</p>
  </div>
  <div class="page">
    <p>An incremental inference puzzle for surprisal  Try to understand this sentence: (a) The coach smiled at the player tossed the frisbee.</p>
    <p>and contrast this with:</p>
    <p>(b) The coach smiled at the player thrown the frisbee.</p>
    <p>(c) The coach smiled at the player who was thrown the frisbee.</p>
    <p>(d) The coach smiled at the player who was tossed the frisbee.</p>
    <p>Readers boggle at tossed in (a), but not in (b-d)</p>
    <p>Tabor et al. (2004, JML)</p>
    <p>RT spike in (a)</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>verb? participle?</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>But now context should rule out the garden path:</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>But now context should rule out the garden path:  The coach smiled at the player tossed</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>But now context should rule out the garden path:  The coach smiled at the player tossed</p>
    <p>verb? participle?</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>But now context should rule out the garden path:  The coach smiled at the player tossed</p>
    <p>verb? participle?</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>But now context should rule out the garden path:  The coach smiled at the player tossed</p>
    <p>verb? participle?</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Why is tossed/thrown interesting?  As with classic garden-paths, part-of-speech ambiguity leads</p>
    <p>to misinterpretation  The woman brought the sandwichtripped</p>
    <p>But now context should rule out the garden path:  The coach smiled at the player tossed</p>
    <p>A challenge for rational models: failure to condition on relevant context</p>
    <p>verb? participle?</p>
    <p>verb? participle?</p>
    <p>S</p>
    <p>VP</p>
    <p>. . .V</p>
    <p>brought</p>
    <p>NP</p>
    <p>N</p>
    <p>woman</p>
    <p>Det</p>
    <p>the</p>
  </div>
  <div class="page">
    <p>Rational analysis Background assumption: cognitive agent is optimized via evolution and learning to solve everyday tasks effectively 1. Specify precisely the goals of the cognitive system 2. Formalize model of the environment adapted to 3. Make minimal assumptions re: computational limitations 4. Derive predicted optimal behavior given 13 5. Compare predictions with empirical data 6. If necessary, iterate 15</p>
  </div>
  <div class="page">
    <p>Rational analysis Background assumption: cognitive agent is optimized via evolution and learning to solve everyday tasks effectively 1. Specify precisely the goals of the cognitive system 2. Formalize model of the environment adapted to 3. Make minimal assumptions re: computational limitations 4. Derive predicted optimal behavior given 13 5. Compare predictions with empirical data 6. If necessary, iterate 15</p>
    <p>Failures!</p>
  </div>
  <div class="page">
    <p>Rational analysis Background assumption: cognitive agent is optimized via evolution and learning to solve everyday tasks effectively 1. Specify precisely the goals of the cognitive system 2. Formalize model of the environment adapted to 3. Make minimal assumptions re: computational limitations 4. Derive predicted optimal behavior given 13 5. Compare predictions with empirical data 6. If necessary, iterate 15</p>
    <p>Failures!</p>
    <p>Revise somehow</p>
  </div>
  <div class="page">
    <p>Rational analysis Background assumption: cognitive agent is optimized via evolution and learning to solve everyday tasks effectively 1. Specify precisely the goals of the cognitive system 2. Formalize model of the environment adapted to 3. Make minimal assumptions re: computational limitations 4. Derive predicted optimal behavior given 13 5. Compare predictions with empirical data 6. If necessary, iterate 15</p>
    <p>Failures!</p>
    <p>Revise somehow</p>
    <p>Our case study: revise #2, the model of the environment to which the cognitive agent is adapted</p>
  </div>
  <div class="page">
    <p>Uncertain input in language comprehension</p>
  </div>
  <div class="page">
    <p>Uncertain input in language comprehension  Previous state of the art models for ambiguity resolution</p>
    <p>probabilistic incremental parsing</p>
  </div>
  <div class="page">
    <p>Uncertain input in language comprehension  Previous state of the art models for ambiguity resolution</p>
    <p>probabilistic incremental parsing  Simplifying assumption:</p>
    <p>Input is clean and perfectly-formed  No uncertainty about input is admitted</p>
  </div>
  <div class="page">
    <p>Uncertain input in language comprehension  Previous state of the art models for ambiguity resolution</p>
    <p>probabilistic incremental parsing  Simplifying assumption:</p>
    <p>Input is clean and perfectly-formed  No uncertainty about input is admitted</p>
    <p>Intuitively seems patently wrong  We sometimes misread things  We can also proofread</p>
  </div>
  <div class="page">
    <p>Uncertain input in language comprehension  Previous state of the art models for ambiguity resolution</p>
    <p>probabilistic incremental parsing  Simplifying assumption:</p>
    <p>Input is clean and perfectly-formed  No uncertainty about input is admitted</p>
    <p>Intuitively seems patently wrong  We sometimes misread things  We can also proofread</p>
    <p>Leads to two questions: 1. What might a model of sentence comprehension under</p>
    <p>uncertain input look like? 2. What interesting consequences might such a model have?</p>
  </div>
  <div class="page">
    <p>Noisy-channel language comprehension</p>
    <p>Levy (2008, EMNLP); Futrell &amp; Levy (2017, EACL)</p>
    <p>P(! | words)  P(words | T )P(T )</p>
  </div>
  <div class="page">
    <p>Noisy-channel language comprehension  Standard probabilistic language comprehension</p>
    <p>Levy (2008, EMNLP); Futrell &amp; Levy (2017, EACL)</p>
    <p>P(! | words)  P(words | T )P(T )</p>
  </div>
  <div class="page">
    <p>Noisy-channel language comprehension  Standard probabilistic language comprehension</p>
    <p>Revision: probabilistic language comprehension where the input is subject to noise and imperfect memory</p>
    <p>Levy (2008, EMNLP); Futrell &amp; Levy (2017, EACL)</p>
    <p>P(! | words)  P(words | T )P(T )</p>
  </div>
  <div class="page">
    <p>Noisy-channel language comprehension  Standard probabilistic language comprehension</p>
    <p>Revision: probabilistic language comprehension where the input is subject to noise and imperfect memory</p>
    <p>Levy (2008, EMNLP); Futrell &amp; Levy (2017, EACL)</p>
    <p>P(! | words)  P(words | T )P(T )</p>
    <p>P(! | input)  P(input | T )P(T )</p>
  </div>
  <div class="page">
    <p>Noisy-channel language comprehension  Standard probabilistic language comprehension</p>
    <p>Revision: probabilistic language comprehension where the input is subject to noise and imperfect memory</p>
    <p>Levy (2008, EMNLP); Futrell &amp; Levy (2017, EACL)</p>
    <p>P(! | words)  P(words | T )P(T )</p>
    <p>P(! | input)  P(input | T )P(T ) =</p>
    <p>w P(input | w, T )P(w, T )</p>
    <p>Ranges over possible word sequences</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>The coach smiled at the player tossed the frisbee</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee</p>
    <p>(and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (that?) (and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (who?) (that?) (and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (who?) (that?)(that?) (and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (who?) (that?)</p>
    <p>(who?) (that?)</p>
    <p>(and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct: Any of these changes makes tossed a main verb!!!</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (who?) (that?)</p>
    <p>(who?) (that?)</p>
    <p>(and?)</p>
  </div>
  <div class="page">
    <p>Incremental inference under uncertain input</p>
    <p>Near-neighbors make the incorrect analysis correct:</p>
    <p>Hypothesis: the boggle at tossed involves what the comprehender wonders whether she might have seen</p>
    <p>Any of these changes makes tossed a main verb!!!</p>
    <p>The coach smiled at the player tossed the frisbee (as?)</p>
    <p>(and?) (who?) (that?)</p>
    <p>(who?) (that?)</p>
    <p>(and?)</p>
  </div>
  <div class="page">
    <p>The core of the intuition</p>
    <p>the coach smiled</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>tossed</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>tossed</p>
    <p>tossed</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>tossed</p>
    <p>tossed</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>tossed</p>
    <p>tossed</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>tossed is more likely to happen along the bottom path  This creates a large shift in belief in the tossed condition</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>tossed is more likely to happen along the bottom path  This creates a large shift in belief in the tossed condition</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>thrown</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>tossed is more likely to happen along the bottom path  This creates a large shift in belief in the tossed condition</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>thrown</p>
    <p>thrown</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>tossed is more likely to happen along the bottom path  This creates a large shift in belief in the tossed condition</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>thrown</p>
    <p>thrown</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>tossed is more likely to happen along the bottom path  This creates a large shift in belief in the tossed condition</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>thrown</p>
    <p>thrown</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>The core of the intuition  Grammar &amp; input come together to determine two possible</p>
    <p>paths through the partial sentence:</p>
    <p>tossed is more likely to happen along the bottom path  This creates a large shift in belief in the tossed condition</p>
    <p>thrown is very unlikely to happen along the bottom path  As a result, there is no corresponding shift in belief</p>
    <p>the coach smiled</p>
    <p>at (likely)</p>
    <p>the player</p>
    <p>as/and (unlikely)</p>
    <p>the player</p>
    <p>thrown</p>
    <p>thrown</p>
    <p>(line thickness  probability)</p>
  </div>
  <div class="page">
    <p>Experimental design</p>
  </div>
  <div class="page">
    <p>Experimental design  In a free-reading eye-tracking study, we crossed at/toward</p>
    <p>with tossed/thrown:</p>
  </div>
  <div class="page">
    <p>Experimental design  In a free-reading eye-tracking study, we crossed at/toward</p>
    <p>with tossed/thrown: The coach smiled at the player tossed the frisbee The coach smiled at the player thrown the frisbee The coach smiled toward the player tossed the frisbee The coach smiled toward the player thrown the frisbee</p>
  </div>
  <div class="page">
    <p>Experimental design  In a free-reading eye-tracking study, we crossed at/toward</p>
    <p>with tossed/thrown: The coach smiled at the player tossed the frisbee The coach smiled at the player thrown the frisbee The coach smiled toward the player tossed the frisbee The coach smiled toward the player thrown the frisbee</p>
  </div>
  <div class="page">
    <p>Experimental design  In a free-reading eye-tracking study, we crossed at/toward</p>
    <p>with tossed/thrown:</p>
    <p>Prediction: interaction between preposition &amp; part-ofspeech ambiguity in eye movements upon encountering participle</p>
    <p>The coach smiled at the player tossed the frisbee The coach smiled at the player thrown the frisbee The coach smiled toward the player tossed the frisbee The coach smiled toward the player thrown the frisbee</p>
  </div>
  <div class="page">
    <p>Experimental design  In a free-reading eye-tracking study, we crossed at/toward</p>
    <p>with tossed/thrown:</p>
    <p>Prediction: interaction between preposition &amp; part-ofspeech ambiguity in eye movements upon encountering participle</p>
    <p>The coach smiled at the player tossed the frisbee The coach smiled at the player thrown the frisbee The coach smiled toward the player tossed the frisbee The coach smiled toward the player thrown the frisbee</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>The coach smiled at the player tossed ?</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed ?</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed ?</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>The coach smiled at the player tossed ?</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>regressions</p>
    <p>The coach smiled at the player tossed ?</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>regressions</p>
    <p>The coach smiled at the player tossed ?</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>regressions</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>regressions</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n c</p>
    <p>o rr</p>
    <p>e c t a n s w</p>
    <p>e rs</p>
    <p>Comprehension accuracy</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Experimental results a</p>
    <p>t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>First-pass RT</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>Regressions out</p>
    <p>a t a m</p>
    <p>b ig</p>
    <p>a t u n a</p>
    <p>m b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>t o w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>m s</p>
    <p>Go-past RT</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n o</p>
    <p>f tr</p>
    <p>ia ls</p>
    <p>regressions</p>
    <p>a t a</p>
    <p>m b ig</p>
    <p>a t u n</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>a m</p>
    <p>b ig</p>
    <p>to w</p>
    <p>a rd</p>
    <p>u n a m</p>
    <p>b ig</p>
    <p>P ro</p>
    <p>p o rt</p>
    <p>io n c</p>
    <p>o rr</p>
    <p>e c t a n s w</p>
    <p>e rs</p>
    <p>Comprehension accuracy</p>
    <p>The coach smiled at the player tossed</p>
  </div>
  <div class="page">
    <p>Application to structural forgetting</p>
    <p>Cost(wi | C) = log 1</p>
    <p>P(wi | C)</p>
    <p>P(wi | C) =  w1i 1</p>
    <p>P(wi | w1i)P(w1i 1 | C)</p>
  </div>
  <div class="page">
    <p>Noisy channel + surprisal = noisy-context surprisal: for a noisy input context C and next encountered word wi:</p>
    <p>Application to structural forgetting</p>
    <p>Cost(wi | C) = log 1</p>
    <p>P(wi | C)</p>
    <p>P(wi | C) =  w1i 1</p>
    <p>P(wi | w1i)P(w1i 1 | C)</p>
  </div>
  <div class="page">
    <p>Noisy channel + surprisal = noisy-context surprisal: for a noisy input context C and next encountered word wi:</p>
    <p>Comparison with humans: is the ungrammatical version of the sentence costlier?</p>
    <p>Application to structural forgetting</p>
    <p>COST(The apartment that the maid who the cleaning service sent over was well-decorated. ) &lt; COST(The apartment that the maid who the cleaning service sent over cleaned was well-decorated.)</p>
    <p>(Futrell &amp; Levy, 2017)</p>
    <p>Cost(wi | C) = log 1</p>
    <p>P(wi | C)</p>
    <p>P(wi | C) =  w1i 1</p>
    <p>P(wi | w1i)P(w1i 1 | C)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Noisy channel + surprisal = noisy-context surprisal: for a noisy input context C and next encountered word wi:</p>
    <p>Comparison with humans: is the ungrammatical version of the sentence costlier?</p>
    <p>Application to structural forgetting</p>
    <p>COST(NOUN THAT NOUN THAT NOUN VERB VERB) &lt; COST(NOUN THAT NOUN THAT NOUN VERB VERB VERB)</p>
    <p>(Futrell &amp; Levy, 2017)</p>
    <p>Cost(wi | C) = log 1</p>
    <p>P(wi | C)</p>
    <p>P(wi | C) =  w1i 1</p>
    <p>P(wi | w1i)P(w1i 1 | C)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Noisy channel + surprisal = noisy-context surprisal: for a noisy input context C and next encountered word wi:</p>
    <p>Comparison with humans: is the ungrammatical version of the sentence costlier?</p>
    <p>Application to structural forgetting</p>
    <p>COST(2 VERBS) &lt; COST(3 VERBS)</p>
    <p>(Futrell &amp; Levy, 2017)</p>
    <p>Cost(wi | C) = log 1</p>
    <p>P(wi | C)</p>
    <p>P(wi | C) =  w1i 1</p>
    <p>P(wi | w1i)P(w1i 1 | C)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Noisy-Context Surprisal Account of Structural Forgetting  This turns out to work for toy grammars of English and German!</p>
    <p>NOUN VERB</p>
    <p>Rule Probability</p>
    <p>S -&gt; NP VERB 1</p>
    <p>NP -&gt; NOUN 1-m</p>
    <p>NP -&gt; NOUN RC mr</p>
    <p>NP -&gt; NOUN PP m(1-r)</p>
    <p>PP -&gt; PREP NP 1</p>
    <p>RC -&gt; THAT VERB NP s</p>
    <p>RC -&gt; THAT NP VERB 1-s</p>
    <p>NOUN PREP NOUN VERB</p>
    <p>NOUN THAT VERB NOUN VERB</p>
    <p>NOUN THAT NOUN VERB VERB</p>
    <p>NOUN THAT NOUN THAT NOUN</p>
    <p>English: s=0.8 (Roland et al., 2007) German :</p>
    <p>s=0.0 (obligatorily verb-final)</p>
    <p>Generates sequences like:</p>
  </div>
  <div class="page">
    <p>Vasishth et al. (2010)</p>
    <p>Human reading time differences</p>
    <p>Futrell &amp; Levy (2017)</p>
    <p>Model behavior</p>
  </div>
  <div class="page">
    <p>Summary &amp; open questions  NLP and cognitive science offer each other a great deal  NLPcognitive science: formal theory-building for</p>
    <p>understanding human language processing  Cognitive scienceNLP: desiderata for human-like</p>
    <p>language processing systems  Experimental methods can probe human cognitive state</p>
    <p>during language processing in remarkable detail  Principles of rational analysis provide us guidance in</p>
    <p>theory building  Scientific progress good, but many open questions:</p>
    <p>How to fully characterize memory constraints in language?  Key principles of human conversational interaction?  Neural implementation of linguistic computations?</p>
    <p>These are great opportunities for everyone here!!! 68</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>Allopenna, P. D., Magnuson, J. S., &amp; Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. Journal of Memory and Language, 38, 419439.</p>
    <p>Anderson, J. R. (1990). The adaptive character of human thought. Hillsdale, NJ: Lawrence Erlbaum.</p>
    <p>Bever, T. (1970). The cognitive basis for linguistic structures. In J. Hayes (Ed.), Cognition and the development of language (pp. 279362). New York: John Wiley &amp; Sons.</p>
    <p>Duffy, S. A., &amp; Keir, J. A. (2004). Violating stereotypes: Eye movements and comprehension processes when text conflicts with world knowledge. Memory &amp; Cognition, 32(4), 551559.</p>
  </div>
  <div class="page">
    <p>References II Dyer, C., Kuncoro, A., Ballesteros, M., &amp; Smith, N. A. (2016).</p>
    <p>Recurrent Neural Network Grammars. In Proceedings of the 15th Annual Conference of the North American</p>
    <p>Chapter of the Association for Computational Linguistics:</p>
    <p>Human Language Technologies. Fedorenko, E., Behr, M. K., &amp; Kanwisher, N. (2011). Functional</p>
    <p>specificity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39), 1642816433.</p>
    <p>Frank, S. L., Trompenaars, T., &amp; Vasishth, S. (2016). Cross-linguistic differences in processing double-embedded relative clauses: Working-memory constraints or language statistics? Cognitive Science, 40(3), 554578.</p>
    <p>Frazier, L. (1985). Syntactic complexity. Natural language parsing: Psychological, computational, and theoretical</p>
    <p>perspectives, 129189.</p>
  </div>
  <div class="page">
    <p>References III</p>
    <p>Frazier, L., &amp; Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14, 178210.</p>
    <p>Futrell, R., &amp; Levy, R. (2017). Noisy-context surprisal as a human sentence processing cost model. In Proceedings of the 15th Conference of the European Chapter of the</p>
    <p>Association for Computational Linguistics (EACL)</p>
    <p>(pp. 688698). Futrell, R., Wilcox, E., Morita, T., Qian, P., Ballesteros, M., &amp;</p>
    <p>Levy, R. (2019). Neural language models as psycholinguistic subjects: Representations of syntactic state. In Proceedings of the 18th Annual Conference of the North American Chapter of the Association for</p>
    <p>Computational Linguistics: Human Language</p>
    <p>Technologies.</p>
  </div>
  <div class="page">
    <p>References IV Gibson, E., &amp; Thomas, J. (1999). The perception of complex</p>
    <p>ungrammatical sentences as grammatical. Language &amp; Cognitive Processes, 14(3), 225248.</p>
    <p>Goodkind, A., &amp; Bicknell, K. (2018). Predictive power of word surprisal for reading times is a linear function of language model quality. In Proceedings of the 8th workshop on cognitive modeling and computational linguistics (cmcl</p>
    <p>psycholinguistic model. In Proceedings of the second meeting of the north american chapter of the Association</p>
    <p>for Computational Linguistics (pp. 159166). Pittsburgh, Pennsylvania.</p>
    <p>Henderson, J. (2004). Discriminative training of a neural network statistical parser. In Proceedings of the 42nd meeting of the association for computational linguistics</p>
    <p>(ACL04), main volume (pp. 95102).</p>
  </div>
  <div class="page">
    <p>References V</p>
    <p>Johnson, M. (1998). PCFG models of linguistic tree representations. Computational Linguistics, 24(4), 613632.</p>
    <p>Jurafsky, D. (1996). A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20(2), 137194.</p>
    <p>Klein, D., &amp; Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of acl.</p>
    <p>Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C., Neubig, G., &amp; Smith, N. A. (2017). What do Recurrent Neural Network Grammars learn about syntax? In Proceedings of the 15th Conference of the European Chapter of the</p>
    <p>Association for Computational Linguistics (EACL).</p>
  </div>
  <div class="page">
    <p>References VI</p>
    <p>Kuncoro, A., Dyer, C., Hale, J., Yogatama, D., Clark, S., &amp; Blunsom, P. (2018). LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better. In Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: Long</p>
    <p>papers) (pp. 14261436). Melbourne, Australia: Association for Computational Linguistics.</p>
    <p>Kutas, M., &amp; Hillyard, S. A. (1980). Reading senseless sentences: Brain potentials reflect semantic incongruity. Science, 207(4427), 203205.</p>
    <p>Kutas, M., &amp; Hillyard, S. A. (1984). Brain potentials during reading reflect word expectancy and semantic association. Nature, 307, 161163.</p>
  </div>
  <div class="page">
    <p>References VII Levy, R. (2008a). A noisy-channel model of rational human</p>
    <p>sentence comprehension under uncertain input. In Proceedings of the 13th conference on Empirical</p>
    <p>Methods in Natural Language Processing (pp. 234243). Waikiki, Honolulu.</p>
    <p>Levy, R. (2008b). Expectation-based syntactic comprehension. Cognition, 106(3), 11261177.</p>
    <p>Levy, R. (2013). Memory and surprisal in human sentence comprehension. In R. P. G. van Gompel (Ed.), Sentence processing (pp. 78114). Hove: Psychology Press.</p>
    <p>Levy, R., Reali, F., &amp; Griffiths, T. L. (2009). Modeling the effects of memory on human online sentence processing with particle filters. In Proceedings of the 22nd conference on Neural Information Processing Systems (NIPS).</p>
    <p>Mesgarani, N., Cheung, C., Johnson, K., &amp; Chang, E. F. (2014). Phonetic feature encoding in human superior temporal gyrus. Science, 1245994.</p>
  </div>
  <div class="page">
    <p>References VIII Mitchell, D. C. (1984). An evaluation of subject-paced reading</p>
    <p>tasks and other methods for investigating immediate processes in reading. In D. Kieras &amp; M. A. Just (Eds.), New methods in reading comprehension. Hillsdale, NJ: Earlbaum.</p>
    <p>Osterhout, L., Bersick, M., &amp; McLaughlin, J. (1997). Brain potentials reflect violations of gender stereotypes. Memory &amp; Cognition, 25(3), 273285.</p>
    <p>Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124(3), 372422.</p>
    <p>Roland, D., Dick, F., &amp; Elman, J. L. (2007). Frequency of basic English grammatical structures: A corpus analysis. Journal of Memory and Language, 57, 348379.</p>
    <p>Staub, A. (2007). The parser doesnt ignore intransitivity, after all. Journal of Experimental Psychology: Learning, Memory, &amp; Cognition, 33(3), 550569.</p>
  </div>
  <div class="page">
    <p>References IX</p>
    <p>Stern, M., Fried, D., &amp; Klein, D. (2017). Effective inference for generative neural parsing. In Proceedings of the 2017 conference on empirical methods in natural language</p>
    <p>processing (pp. 16951700). Sturt, P. (2003). The time-course of the application of binding</p>
    <p>constraints in reference resolution. Journal of Memory and Language, 48, 542562.</p>
    <p>Tabor, W., Galantucci, B., &amp; Richardson, D. (2004). Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50(4), 355370.</p>
    <p>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K., &amp; Sedivy, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. Science, 268, 16321634.</p>
  </div>
  <div class="page">
    <p>References X</p>
    <p>Vasishth, S., Suckow, K., Lewis, R. L., &amp; Kern, S. (2010). Short-term forgetting in sentence comprehension: Crosslinguistic evidence from verb-final structures. Language &amp; Cognitive Processes, 25(4), 533567.</p>
    <p>Wilcox, E., Qian, P., Futrell, R., Ballesteros, M., &amp; Levy, R. (2019). Structural supervision improves learning of non-local grammatical dependencies. In Proceedings of the 18th Annual Conference of the North American</p>
    <p>Chapter of the Association for Computational Linguistics:</p>
    <p>Human Language Technologies.</p>
  </div>
  <div class="page">
    <p>Cognitive Evaluation and</p>
    <p>Language Evolution and Emergence</p>
    <p>Richard Futrell UC Irvine</p>
    <p>rfutrell@uci.edu @rljfutrell</p>
  </div>
  <div class="page">
    <p>Goals of Part III  Two sections:  Cognitive Evaluation:  Applying methods from psycholinguistics and</p>
    <p>cognitive science to analyze neural networks  Characterizing complex human behavior around</p>
    <p>language as a target for NLP systems  Language Evolution and Emergence  A recently-emerging exciting problem in NLP  Some highlights from 20 years of research from the</p>
    <p>field of Language Evolution about under what circumstances language-like codes emerge in agent-based models</p>
  </div>
  <div class="page">
    <p>Cognitive Evaluation</p>
  </div>
  <div class="page">
    <p>Psycholinguistic Assessment</p>
    <p>?</p>
    <p>Battery of behavioral tests</p>
    <p>Conclusions about form of linguistic knowledge,</p>
    <p>data structures used in online processing, sources of difficulty in production &amp; comprehension</p>
  </div>
  <div class="page">
    <p>What Psycholinguists Do</p>
    <p>Levy et al. (2012)</p>
  </div>
  <div class="page">
    <p>Psycholinguistic Assessment</p>
    <p>?</p>
    <p>Battery of behavioral tests</p>
    <p>Conclusions about form of linguistic knowledge,</p>
    <p>data structures used in online processing, sources of difficulty in production &amp; comprehension</p>
  </div>
  <div class="page">
    <p>Battery of behavioral tests</p>
    <p>Conclusions about form of linguistic knowledge,</p>
    <p>data structures used in online processing, sources of difficulty in production &amp; comprehension</p>
    <p>NN</p>
    <p>Psycholinguistic Assessment</p>
  </div>
  <div class="page">
    <p>(a) *The keys to the cabinet is on the table</p>
    <p>(b) The keys to the cabinet are on the table</p>
    <p>Probing NN Behavior</p>
    <p>(a) is SURPRISING! (b) is UNSURPRISING</p>
    <p>Elman (1991, 1993); Linzen et al. (2016)</p>
  </div>
  <div class="page">
    <p>Probing NN Behavior -lo</p>
    <p>g p(</p>
    <p>w |</p>
    <p>h)</p>
    <p>the keys to the cabinet are</p>
    <p>the keys to the cabinet is</p>
    <p>Penalty for surprising continuation</p>
    <p>h1 h2 h3 h4 h5</p>
    <p>Linzen et al. (2016)</p>
    <p>? ? ?</p>
  </div>
  <div class="page">
    <p>Phenomenon Do NN Language Models Learn It?</p>
    <p>SubjectVerb Agreement ? (Linzen et al., 2016; Gulordava et al., 2018)</p>
    <p>Garden Path Effects  (van Schijndel &amp; Linzen, 2018a,b; Futrell et al., 2018, 2019)</p>
    <p>Filler-Gap Dependencies ?   (Chowdhury &amp; Zamparelli, 2018; McCoy et al, 2018; Wilcox et al., 2018, 2019)</p>
    <p>Island Constraints ? (some) (Chowdhury &amp; Zamparelli, 2018; Wilcox et al., 2018)</p>
    <p>NPI Licensing   (Marvin &amp; Linzen, 2018; Futrell et al., 2018)</p>
    <p>Anaphor Agreement   (Marvin &amp; Linzen, 2018; Futrell et al., 2018)</p>
  </div>
  <div class="page">
    <p>What syntactic structures are easy vs. hard for NN language models?</p>
    <p>They find this contrast easy (Filler-Gap Dependencies: Wilcox et al., 2018, 2019).  I know what the lion standing in the Serengeti devoured _ at sunrise.  *I know what the lion standing in the Serengeti devoured a gazelle at</p>
    <p>sunrise.</p>
    <p>They find this contrast hard (Reflexive Anaphora: Marvin &amp; Linzen, 2018; Futrell et al., 2018)  The king standing next to the queen saw himself  *The king standing next to the queen saw herself</p>
    <p>They dont generalize in a clear way across constructions that humans find similar.</p>
  </div>
  <div class="page">
    <p>Targeted Evaluation Datasets  Marvin &amp; Linzen (2018)  Used in e.g. Shen et al. (2019) [Ordered Neurons]</p>
  </div>
  <div class="page">
    <p>Probing Classifiers  Alain &amp; Bengio (2016); Belinkov et al. (2018); Hupkes,</p>
    <p>Veldhoen &amp; Zuidema (2018)</p>
    <p>Similar to neuroscience methods: Wallis (2018)</p>
  </div>
  <div class="page">
    <p>Other Methods of Peering In  Hewitt &amp; Manning (2019): Structural probe: Does there</p>
    <p>exist a linear transformation of the contextual word embedding space such that the distances reflect syntactic parse trees?</p>
  </div>
  <div class="page">
    <p>Sequence (to Sequence) Models  Do generic sequence (to sequence) models show human</p>
    <p>like generalization?</p>
    <p>Lake &amp; Baroni (2018)</p>
    <p>run =&gt; RUN</p>
  </div>
  <div class="page">
    <p>Sequence (to Sequence) Models</p>
    <p>Lake &amp; Baroni (2018)</p>
  </div>
  <div class="page">
    <p>Embedding Spaces  Standard modern approach in NLP is to embed words and sentences</p>
    <p>into a metric space.  Are human intuitions about word similarity well-modeled by a (Euclidean)</p>
    <p>metric space?</p>
  </div>
  <div class="page">
    <p>Word Similarity</p>
    <p>SimLex</p>
    <p>Other human word similarity datasets:  Free-association Nelson Norms (Nelson et al., 1998)  Small World of Words (smallworldofwords.org)</p>
  </div>
  <div class="page">
    <p>Embedding Spaces  Standard modern approach in NLP is to embed words and sentences</p>
    <p>into a metric space.  Are human intuitions about word similarity well-modeled by a (Euclidean)</p>
    <p>metric space?</p>
    <p>Tversky (1977); Griffiths, Steyvers &amp; Tenenbaum (2007)</p>
    <p>keg, beer  vs. beer, keg</p>
    <p>cobra, snake  vs. snake, cobra</p>
    <p>meow, cat  vs. cat, meow</p>
  </div>
  <div class="page">
    <p>Semantic Networks  Human word similarity judgments are best modeled using</p>
    <p>semantic networks (Steyvers &amp; Tenenbaum, 2005).</p>
  </div>
  <div class="page">
    <p>Semantic Networks</p>
    <p>Steyvers &amp; Tenenbaum (2005)</p>
    <p>Degree distributions in human-derived semantic networks follow a power law:</p>
  </div>
  <div class="page">
    <p>Semantic Networks</p>
    <p>Steyvers &amp; Tenenbaum (2005)</p>
    <p>Degree distributions in semantic networks extracted from distributional embeddings follow an exponential law:</p>
  </div>
  <div class="page">
    <p>Embedding Spaces  Distributionally-derived metric spaces do not capture human</p>
    <p>intuitions about word similarity, nor human free associations between words.  Human data violates symmetry and the triangle</p>
    <p>inequality, but follows minimality.  Human data implies a power-law degree distribution in</p>
    <p>semantic networks, but distributional methods give an exponential degree distribution.</p>
    <p>Premetric spaces (such as defined by KL divergence in information geometry) may be compatible with the human data.</p>
    <p>There is a rich modeling and experimental literature to draw from to define these spaces.</p>
    <p>Tversky (1977); Steyvers &amp; Tenenbaum (2005); Griffiths, Steyvers &amp; Tenenbaum (2007)</p>
  </div>
  <div class="page">
    <p>Theory of Mind</p>
    <p>Baron-Cohen et al. (1985)</p>
  </div>
  <div class="page">
    <p>Theory of Mind as a Question Answering Challenge</p>
    <p>Nematzadeh et al. (2018)</p>
    <p>bAbi (Weston et al., 2006)</p>
  </div>
  <div class="page">
    <p>Question Answering</p>
    <p>Nematzadeh et al. (2018)</p>
  </div>
  <div class="page">
    <p>Cognitive Evaluation  Behavioral work in cognitive science can feed into NLP in two</p>
    <p>ways:  Providing careful analytical techniques for evaluating black</p>
    <p>box models.  Reveals structural representations and inductive biases</p>
    <p>in neural models.  Providing challenging datasets and phenomena.  Compositionality &amp; systematicity  Non-metric nature of human similarity judgments  Question answering involving Theory of Mind  Many more!</p>
  </div>
  <div class="page">
    <p>Language Evolution and Emergence</p>
  </div>
  <div class="page">
    <p>Language Evolution and Emergence</p>
    <p>If you have something like deep reinforcement learning agents trying to cooperate to solve a task, when will they evolve a language-like code for communication?  Havrylov &amp; Titov (2017); Lazaridou et al. (2017, 2018);</p>
    <p>Mordatch &amp; Abbeel (2017); Chaabouni et al. (2019); Lee et al. (2018)</p>
    <p>A potential new way to model what language is.  Ill present some high-level takeaways from over 20 years</p>
    <p>of research in agent-based models of Evolution of Language.</p>
  </div>
  <div class="page">
    <p>Emergence of Symbols  Simplest setting:</p>
    <p>David Lewiss Signaling Game</p>
    <p>Lewis (1969). Convention: A Philosophical Study</p>
  </div>
  <div class="page">
    <p>Emergence of Symbols  Three requirements for emergence of learned signalling:  Availability of referential-interpretative information  Bias against ambiguity  Information loss</p>
    <p>Spike, Stadler, Kirby &amp; Smith (2017)</p>
  </div>
  <div class="page">
    <p>From Symbols to Linguistic Structure</p>
    <p>Two hallmarks of human language:  Combinatoriality  Compositionality</p>
    <p>Combinatoriality:  A small set of meaningless units (phonemes/letters)</p>
    <p>combine together to form a large set of meaningful units (morphemes/words) according to an arbitrary function.</p>
    <p>/k/ + // + /t/ = /kt/, cat</p>
  </div>
  <div class="page">
    <p>From Symbols to Linguistic Structure  Two hallmarks of human language:  Combinatoriality  Compositionality</p>
    <p>Compositionality:  A large set of meaningful units (morphemes/words)</p>
    <p>combine together to form an infinite set of meaningful sentences (Montague, 1970) according to a simple function.</p>
    <p>The + cat + meows Meaning = f(f(the, cat), meows)</p>
    <p>Duality of patterning</p>
  </div>
  <div class="page">
    <p>Emergence of Combinatoriality  Nowak &amp; Krakauer (1999)  Imagine you are</p>
    <p>communicating about K objects in a Lewis signaling game.</p>
    <p>Imagine it is hard to perceive the difference between signals.</p>
    <p>Then it is better for a signal to consist of multiple discriminable parts (for redundancy), rather than each signal consisting of one atomic part.</p>
    <p>Verhoef (2012); Tria (2012); Del Giudice (2012); Hofer, Tenenbaum &amp; Levy (2019)</p>
  </div>
  <div class="page">
    <p>Emergence of Combinatoriality</p>
    <p>Related: Chaabouni et al. (2019) find that emergent languages in deep reinforcement learning agents favor long utterances due to discriminability.</p>
  </div>
  <div class="page">
    <p>Defining Compositionality</p>
    <p>Montague (1970); Andreas (2019)</p>
  </div>
  <div class="page">
    <p>Emergence of Compositionality</p>
    <p>Kirby, Cornish &amp; Smith (2008)</p>
    <p>Iterated language learning experiments  Compositionality emerges from a transmission bottleneck  which</p>
    <p>implements a simplicity constraint.</p>
    <p>Compositionality = Simplicity + Communicativity</p>
  </div>
  <div class="page">
    <p>Simple Compositionality in Agent-Based Modeling</p>
    <p>Abbeel &amp; Mordatch (2017)</p>
    <p>An implementation of compositionality = simplicity + communicativity</p>
  </div>
  <div class="page">
    <p>High-level Generalizations about Human Language</p>
    <p>Modeling targets for language emergence experiments beyond combinatoriality &amp; compositionality.  The set of phonemes used in any language is much</p>
    <p>smaller than the set of all pronounceable phonemes used in all languages.</p>
    <p>The set of phonemes in a language has a lot of repeated substructure in terms of phonetic features.</p>
    <p>The set of phonemes in a language has a pressure to be maximally acoustically distinct.</p>
  </div>
  <div class="page">
    <p>Languages usually have on the order of 10^1 phonemes and on the order of 10^4 morphemes: relatively invariant sequences of phonemes which correspond to atomic components of the meaning of an utterance.  A hierarachy problem for natural language.  In contrast, animal communication systems usually have 10^1</p>
    <p>symbols with no internal structure.  Morphemes vary in length; frequent/more predictable morphemes are</p>
    <p>shorter (Zipf, 1949; Piantadosi et al., 2011)  Compare Chaabouni et al. (2019)</p>
    <p>Morphemes contain a great deal of repeated substructure in their sequences of phonemes (phonotactics).</p>
    <p>Phonotactics is formally characterizable as k-tier-based strictly local languages with k=~2 (Heinz, 2011)</p>
    <p>High-level Generalizations about Human Language</p>
  </div>
  <div class="page">
    <p>Utterances consist of sequences of multiple morphemes.  Utterances vary in length.  The overall meaning of an utterance is compositional: it is a simple</p>
    <p>function of the meanings of the morphemes and their order.  There are an unbounded number of possible utterances.  Utterances have tree-like hierarchical structure  In these structures, one word composes typically with one other</p>
    <p>word in the computation of the meaning of the utterance (defining the dependency tree). This property is called endocentricity (Jakobson, 1961).</p>
    <p>The set of possible utterances is characterizable as a Multiple Context Free Language (Seki et al., 1991), with block degree ~2 (Weir, 1988; Kuhlmann, 2013).</p>
    <p>High-level Generalizations about Human Language</p>
  </div>
  <div class="page">
    <p>Language Evolution  There is a vast literature! (see evolang.org)  Evolution of Language Conference every 2 years</p>
    <p>Requirements for learned signaling: referential feedback, ambiguity avoidance, information loss</p>
    <p>Requirements for combinatoriality: noise in communication  Requirements for compositionality: simplicity +</p>
    <p>communicativity</p>
    <p>Natural language provides a number of modeling targets!</p>
  </div>
  <div class="page">
    <p>Wrapping Up</p>
  </div>
  <div class="page">
    <p>Wrapping Up</p>
    <p>Cognitive modeling provides inspiration, challenges, and analytical tools for NLP.</p>
    <p>Language is a human objectcreated by humans, for humans.  The human cognitive side is especially important!</p>
    <p>A vast unexplored territory in characterizing human language learning, human language processing, and emergence of language  The bottleneck in the field is a lack of computationally</p>
    <p>skilled researchers!</p>
  </div>
  <div class="page">
    <p>Thanks all!</p>
  </div>
</Presentation>
