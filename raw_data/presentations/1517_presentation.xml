<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>EiovaR Efficient IOMMU Intra-Opera4ng System Protec4on</p>
    <p>Moshe Malka, Nadav Amit, and Dan Tsafrir Technion - Israel Ins:tute of Technology</p>
  </div>
  <div class="page">
    <p>IOMMU = I/O memory management unit IOVA = I/O virtual address</p>
    <p>I/ O d ev ic e</p>
    <p>CP U</p>
    <p>I/ O d ev ic e</p>
    <p>M</p>
    <p>M U</p>
    <p>TL B</p>
    <p>I/ O d ev ic e</p>
    <p>ph ys ic al m</p>
    <p>em or y</p>
    <p>I/ O d ev ic e</p>
    <p>I/ O d ev ic e</p>
    <p>I/ O d ev ic e</p>
    <p>IO</p>
    <p>M M U</p>
    <p>IO TL B</p>
    <p>virtual address</p>
    <p>physical address</p>
    <p>physical address</p>
    <p>I/O virtual address</p>
  </div>
  <div class="page">
    <p>Direct memory access is dangerous</p>
    <p>physical memory</p>
    <p>device A device B</p>
    <p>physical memory</p>
    <p>device</p>
    <p>guest A guest B</p>
    <p>Virtual machines</p>
  </div>
  <div class="page">
    <p>IOMMU provides memory protec4on</p>
    <p>physical memory</p>
    <p>device A device B</p>
    <p>physical memory</p>
    <p>device</p>
    <p>guest A guest B</p>
    <p>Inter-OS protec:on Intra-OS protec.on</p>
    <p>IOMMU IOMMU</p>
    <p>Virtual machines</p>
  </div>
  <div class="page">
    <p>How Intra-OS protec4on works (strict)</p>
    <p>IOVA allocator</p>
    <p>IOTLB main memory</p>
    <p>Before the DMA</p>
    <p>page table</p>
  </div>
  <div class="page">
    <p>access the memory using virtual address</p>
    <p>IOVA allocator</p>
    <p>IOTLB main memory</p>
    <p>page table</p>
    <p>How Intra-OS protec4on works (strict)</p>
  </div>
  <div class="page">
    <p>ALer the DMA</p>
    <p>transla:on from page table</p>
    <p>IOVA allocator</p>
    <p>IOTLB main memory</p>
    <p>page table</p>
    <p>How Intra-OS protec4on works (strict)</p>
  </div>
  <div class="page">
    <p>strict vs. defer Protec4on tradeoff</p>
    <p>ALer the DMA</p>
    <p>transla:on from page table</p>
    <p>IOVA allocator</p>
    <p>IOTLB main memory</p>
    <p>page table</p>
  </div>
  <div class="page">
    <p>whats the performance cost? NIC (mlx4) throughput without IOMMU</p>
    <p>15008/</p>
  </div>
  <div class="page">
    <p>whats the performance cost? NIC throughput with IOMMU (mlx4)</p>
    <p>15008/</p>
  </div>
  <div class="page">
    <p>NIC throughput with IOMMU (mlx4)</p>
    <p>whats the performance cost?</p>
    <p>15008/</p>
  </div>
  <div class="page">
    <p>Common wisdom: HW is to blame!  Correla.on between the # of cycles it takes to process a packet and the throughput  Our claim: SW is guil.er!  OSes drive IOMMUs with subop:mal soLware  Could be much improved  All OSes we examined (not just Linux)</p>
    <p>Why is IOMMU so costly?</p>
  </div>
  <div class="page">
    <p>Overhead breakdown - same bench as before</p>
  </div>
  <div class="page">
    <p>Why is IOVA (de)alloca4on so costly?</p>
    <p>Cached node</p>
    <p>Virtual address space 90 91 92 93 94 95 96 97 98 99</p>
    <p>IOVA alloca:on</p>
  </div>
  <div class="page">
    <p>Why is IOVA (de)alloca4on so costly?</p>
    <p>search starts here</p>
    <p>search ends here</p>
    <p>IOVA alloca:on aLer ..98 is reallocated</p>
    <p>Cached node</p>
    <p>Virtual address space 90 91 92 93 94 95 96 97 98 99</p>
  </div>
  <div class="page">
    <p>EiovaR: efficient IOVA allocator</p>
    <p>..FB</p>
    <p>..F9</p>
    <p>..F8 ..FA</p>
    <p>..FE</p>
    <p>..FD ..FF</p>
    <p>..FC</p>
    <p>cache</p>
    <p>. . .</p>
    <p>Lists of free IOVA ranges</p>
    <p>Regular IOVA data structure (tree)</p>
    <p>on IOVA free</p>
    <p>Push the IOVA to the corresponding list</p>
    <p>on IOVA alloca:on</p>
  </div>
  <div class="page">
    <p>Results</p>
  </div>
  <div class="page">
    <p>How many cycles we reduced?</p>
  </div>
  <div class="page">
    <p>How many cycles we reduced?</p>
  </div>
  <div class="page">
    <p>NIC throughput with IOMMU (mlx4)</p>
    <p>15008/</p>
    <p>Performance improvement</p>
  </div>
  <div class="page">
    <p>Performance improvement</p>
    <p>ASPLOS 2015</p>
    <p>NIC throughput with IOMMU (mlx4)</p>
  </div>
  <div class="page">
    <p>Micro- and macro-benchmarks (mlx)</p>
    <p>protec.on benchmark baseline EiovaR diff</p>
    <p>strict Netperf Stream 1 2.37 +137% Netperf RR 1 1.27 +27% Apache 1MB 1 3.65 +265% Apache 1KB 1 2.35 +135% Memcached 1 4.58 +358%</p>
    <p>defer Netperf Stream 1 1.71 +71% Netperf RR 1 1.07 +7% Apache 1MB 1 1.21 +21% Apache 1KB 1 1.11 +11% Memcached 1 1.25 +25%</p>
  </div>
  <div class="page">
    <p>Generalizing</p>
    <p>So far, all results are related to Linux  We found, however, that other OSes similarly drive the IOMMU with subop:mal SW  In the paper, we show that FreeBSD is worse than Linux  (We even describe a patch that we already managed to push to FreeBSD that improves throughput by 76 %)</p>
    <p>We also found that Solaris and Darwin have similar deficiencies (not in the paper)</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Common wisdom is IOMMU is slow  Not true!  Subop:mal SW makes it much slower than it should be</p>
    <p>We find that characteris:cs of the workload experienced by the IOMMU allow for even our basic &amp; minimal freelist op:miza:on to deliver high performance (which BTW consists of about 250- lines patch)  Although our analysis focuses on Linux, other OSes suffer from issues</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
  </div>
</Presentation>
