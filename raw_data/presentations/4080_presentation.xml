<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>EndEnd--toto--End Internet Video End Internet Video Traffic Dynamics: Statistical Traffic Dynamics: Statistical Study and Analysis Study and Analysis Dmitri LoguinovDmitri Loguinov City University of New YorkCity University of New York</p>
    <p>Hayder RadhaHayder Radha Michigan State UniversityMichigan State University</p>
  </div>
  <div class="page">
    <p>Overview of the Talk</p>
    <p>Motivation</p>
    <p>Experimental setup and experiment overview</p>
    <p>Results  Packet loss</p>
    <p>Underflow events</p>
    <p>Round-trip delay</p>
    <p>Delay jitter</p>
    <p>Packet reordering</p>
    <p>Asymmetric paths</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>MotivationMotivation  Market research by Telecommunications</p>
    <p>Reports International (TRI) from August 2001</p>
    <p>Paid dialup ISPs</p>
    <p>Free dialup ISPs</p>
    <p>Cable DSL Internet TV Satellite</p>
    <p>h o</p>
    <p>u s</p>
    <p>e h</p>
    <p>o ld</p>
    <p>s (</p>
    <p>m ill</p>
    <p>io n</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Motivation (contd)Motivation (contd)  Consider broadband (cable, DSL, and satellite)</p>
    <p>Internet access vs. narrowband (dialup and webTV)</p>
    <p>88% of households use modems and 12% broadband</p>
    <p>dialup broadband</p>
    <p>su bs</p>
    <p>cr ib</p>
    <p>er s</p>
    <p>(p er</p>
    <p>ce nt</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Motivation (contd)Motivation (contd)  Our study was conducted in late 1999  early</p>
    <p>Path properties of dialup ISPs and the view of the Internet from the angle of home users have not been documented</p>
    <p>Furthermore, large-scale performance of end-user video streaming in the current Internet has not been reported</p>
    <p>Main question  what is the main impediment to streaming? Is it delay, loss, or something else?</p>
  </div>
  <div class="page">
    <p>Experimental SetupExperimental Setup  MPEG-4 client-server real-time streaming architecture</p>
    <p>NACK-based retransmission and fixed streaming bitrate (i.e., no congestion control)</p>
    <p>Stream S1 at 14 kb/s (16.0 kb/s IP rate), Nov-Dec 1999</p>
    <p>Stream S2 at 25 kb/s (27.4 kb/s IP rate), Jan-May 2000</p>
    <p>National dial-up</p>
    <p>ISP</p>
    <p>UUNET (MCI)</p>
    <p>Internet</p>
    <p>server</p>
    <p>T1 link</p>
    <p>client</p>
    <p>Philips Research</p>
    <p>modem link</p>
  </div>
  <div class="page">
    <p>OverviewOverview  Three ISPs (Earthlink, AT&amp;T WorldNet, IBM Global</p>
    <p>Net)  phone database included 1,813 dialup points in 1,188 cities</p>
    <p>The experiment covered 1,003 points in 653 US cities</p>
    <p>Over 34,000 long-distance phone calls</p>
    <p>85 million video packets, 27.1 GBytes of video data</p>
    <p>End-to-end paths with 5,266 Internet router interfaces</p>
    <p>51% of routers from dialup ISPs and 45% from UUnet</p>
    <p>Sets D1p and D2p contain successful sessions with streams S1 and S2, respectively</p>
  </div>
  <div class="page">
    <p>Overview (contd)  Cities per state that participated in the experiment</p>
    <p>Legend (total 653)</p>
  </div>
  <div class="page">
    <p>Overview (contd)Overview (contd)  Streaming success rate during the day shown below</p>
    <p>Varied between 80% during the night (midnight  6 am) to 40% during the day (9 am  noon)</p>
    <p>time (EDT)</p>
    <p>p er</p>
    <p>ce n</p>
    <p>t s u</p>
    <p>cc es</p>
    <p>sf u</p>
    <p>l</p>
  </div>
  <div class="page">
    <p>Overview (contd)Overview (contd)  Average end-to-end hop count 11.3 in D1p and</p>
    <p>end-to-end hops</p>
    <p>pe rc</p>
    <p>en t r</p>
    <p>ou te</p>
    <p>s</p>
    <p>Stream1 Stream2</p>
  </div>
  <div class="page">
    <p>Packet LossPacket Loss  Average packet loss was 0.5% in both datasets</p>
    <p>38% of sessions with no packet loss</p>
    <p>75% with loss below 0.3%</p>
    <p>91% with loss below 2%</p>
    <p>During the day, average packet loss varied between 0.2% (3 am - 6 am) and 0.8% (9am - 6pm EDT)</p>
    <p>Average per-state packet loss varied between 0.2% (Idaho) to 1.4% (Oklahoma), but did not depend on the average RTT or the average number of end-toend hops in the state</p>
  </div>
  <div class="page">
    <p>Packet Loss (contd)Packet Loss (contd)</p>
    <p>A K</p>
    <p>A Z C T</p>
    <p>F L IA IN L A</p>
    <p>M E</p>
    <p>M O</p>
    <p>N C</p>
    <p>N H</p>
    <p>N V</p>
    <p>O K R I</p>
    <p>T N</p>
    <p>V A W</p>
    <p>I</p>
    <p>state</p>
    <p>lo s</p>
    <p>s</p>
    <p>h o</p>
    <p>p s</p>
    <p>Average Loss Average hops</p>
  </div>
  <div class="page">
    <p>Packet Loss (contd)Packet Loss (contd)  207,384 loss bursts and 431,501 lost packets</p>
    <p>Loss burst lengths PDF:</p>
    <p>P D</p>
    <p>F pe</p>
    <p>rc en</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Packet Loss (contd)Packet Loss (contd)  Most bursts contained no more than 7 packets (however,</p>
    <p>the tail reached to over 100 packets)</p>
    <p>RED was disabled on the backbone; still 74% of loss bursts contained only 1 packet apparently dropped in FIFO queues</p>
    <p>Average burst length was 2.04 packets in D1p and 2.10 packets in D2p</p>
    <p>Conditional probability of packet loss was 51% and 53%, respectively</p>
    <p>Over 90% of loss burst durations were under 1 second (maximum 36 seconds)</p>
    <p>The average distance between lost packets was 21 and 27 seconds in D1p and D2p, respectively</p>
  </div>
  <div class="page">
    <p>Packet Loss (contd)Packet Loss (contd)  Apparently heavy-tailed distributions of loss burst lengths</p>
    <p>Pareto with  = 1.34; however, note that data was nonstationary (time of day or access point non-stationarity)</p>
    <p>-0 4</p>
    <p>-0 3</p>
    <p>-0 2</p>
    <p>-0 1</p>
    <p>+ 0 0</p>
    <p>P (</p>
    <p>X &gt;</p>
    <p>= x</p>
    <p>)</p>
    <p>Loss Burst Lengths Pow er (Loss Burst Lengths) Expon. (Loss Burst Lengths)</p>
  </div>
  <div class="page">
    <p>Underflow eventsUnderflow events  Missing packets (frames) at their decoding deadlines cause</p>
    <p>buffer underflows at the receiver</p>
    <p>Startup delay used in the experiment was 2.7 seconds</p>
    <p>63% (271,788) of all lost packets were discovered to be missing before their deadlines</p>
    <p>Out of these 63% of lost packets:  94% were recovered in time</p>
    <p>3.3% were recovered late</p>
    <p>2.1% were never recovered</p>
    <p>Retransmission appears quite effective in dealing with packet loss, even in the presence of large end-to-end delays</p>
  </div>
  <div class="page">
    <p>Underflow events (contd)Underflow events (contd)  37% (159,713) of lost packets were discovered to be</p>
    <p>missing after their deadlines had passed</p>
    <p>This effect was caused by large one-way delay jitter</p>
    <p>Additionally, one-way delay jitter caused 1,167,979 data packets to be late for decoding</p>
    <p>Overall, 1,342,415 packets were late (1.7% of all sent packets), out of which 98.9% were late due to large one-way delay jitter rather than due to packet loss combined with large RTT</p>
    <p>All late packets caused the freeze-frame effect for 10.5 seconds on average in D1p and 8.5 seconds in D2p (recall that each session was 10 minutes long)</p>
  </div>
  <div class="page">
    <p>Underflow events (contd)Underflow events (contd)  90% of late retransmissions missed the deadline by no more</p>
    <p>than 5 seconds, 99% by no more than 10 seconds</p>
    <p>90% of late data packets missed the deadline by no more than 13 seconds, 99% by no more than 27 seconds</p>
    <p>seconds late</p>
    <p>C D</p>
    <p>F p</p>
    <p>er ce</p>
    <p>n t</p>
    <p>data retransmissions</p>
  </div>
  <div class="page">
    <p>RoundRound--trip Delaytrip Delay  660,439 RTT samples</p>
    <p>75% of samples below 600 ms and 90% below 1 second</p>
    <p>Average RTT was 698 ms in D1p and 839 ms in D2p  Maximum RTT was over 120 seconds</p>
    <p>Data-link retransmission combined with low-bitrate connection were responsible for pathologically high RTTs</p>
    <p>However, we found access points with 6-7 second IPlevel buffering delays</p>
  </div>
  <div class="page">
    <p>RoundRound--trip Delay (contd)trip Delay (contd)  Distributions of the RTT in both datasets (PDF) were</p>
    <p>similar and contained a very long tail</p>
    <p>P D</p>
    <p>F p</p>
    <p>er ce</p>
    <p>nt</p>
    <p>D1p D2p</p>
  </div>
  <div class="page">
    <p>RoundRound--trip Delay (contd)trip Delay (contd)  Distribution tails closely matched hyperbolic distributions</p>
    <p>(Pareto with  between 1.16 and 1.58)</p>
    <p>-0 6</p>
    <p>-0 4</p>
    <p>-0 2</p>
    <p>+ 00</p>
    <p>P D</p>
    <p>F pe</p>
    <p>rc en</p>
    <p>t</p>
    <p>D1p D2p Pow er (D2p) Pow er (D1p)</p>
  </div>
  <div class="page">
    <p>RoundRound--trip Delay (contd)trip Delay (contd)  The average RTT varied during the day between 574 ms (3</p>
    <p>am  6 am) and 847 ms (3 pm  6 pm) in D1p  Between 723 ms and 951 ms in D2p  Relatively small increase in the RTT during the day (by only</p>
    <p>Per-state RTT varied between 539 ms (Maine) and 1,053 ms (Alaska); Hawaii and New Mexico also had average RTTs above 1 second</p>
    <p>Little correlation between the RTT and geographical distance of the state from NY</p>
    <p>However, much stronger positive correlation between the number of hops and the average state RTT:  = 0.52</p>
  </div>
  <div class="page">
    <p>Packet ReorderingPacket Reordering  Average reordering rates were low, but noticeable</p>
    <p>6.5% of missing packets (or 0.04% of sent) were reordered</p>
    <p>Out of 16,852 sessions, 1,599 (9.5%) experienced at least one reordering event</p>
    <p>The highest reordering rate per ISP occurred in AT&amp;T WorldNet, where 35% of missing packets (0.2% of sent packets) were reordered</p>
    <p>In the same set, almost half of the sessions (47%) experienced at least one reordering event</p>
    <p>Earthlink had a session where 7.5% of sent packets were reordered</p>
  </div>
  <div class="page">
    <p>Packet Reordering (contd)Packet Reordering (contd)  Reordering delay Dr is time between detecting a missing</p>
    <p>packet and receiving the reordered packet</p>
    <p>90% of samples Dr below 150 ms, 97% below 300 ms, 99% below 500 ms, and the maximum sample was 20 seconds</p>
    <p>Reordering delay (ms)</p>
    <p>P D</p>
    <p>F pe</p>
    <p>rc en</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Packet Reordering (contd)Packet Reordering (contd)  Reordering distance is the number of packets received</p>
    <p>during the reordering delay (84.6% of the time a single packet, 6.5% exactly 2 packets, 4.5% exactly 3 packets)</p>
    <p>TCPs triple-ACK avoids 91.1% of redundant retransmits and quadruple-ACK avoids 95.7%</p>
    <p>P D</p>
    <p>F p</p>
    <p>er ce</p>
    <p>n t</p>
  </div>
  <div class="page">
    <p>Path AsymmetryPath Asymmetry  Asymmetry detected by analyzing the TTL of the returned</p>
    <p>packets during the initial traceroute</p>
    <p>Each router reset the TTL to a default value (such as 255) when sending a TTL expired ICMP message</p>
    <p>If the number of forward and reverse hops was different, the path was definitely asymmetric</p>
    <p>Otherwise, the path was possibly (or probably) symmetric</p>
    <p>No fail-proof way of establishing path symmetry using endto-end measurements (even using two traceroutes in reverse directions)</p>
  </div>
  <div class="page">
    <p>Path Asymmetry (contd)Path Asymmetry (contd)  72% of sessions operated over definitely asymmetric paths</p>
    <p>Almost all paths with 14 or more end-to-end hops were asymmetric</p>
    <p>Even the shortest paths (with as low as 6 hops) were prone to asymmetry</p>
    <p>Hot potato routing is more likely to cause asymmetry in longer paths, because they are more likely to cross AS borders than shorter paths</p>
    <p>Longer paths also exhibited a higher reordering probability than shorter paths</p>
  </div>
  <div class="page">
    <p>ConclusionConclusion  Dialing success rates were quite low during the day (as low</p>
    <p>as 40%)</p>
    <p>Retransmission worked very well even for delay-sensitive traffic and high-latency end-to-end paths</p>
    <p>Both RTT and packet-loss bursts appeared to be heavytailed</p>
    <p>Our clients experienced huge end-to-end delays both due to large IP buffers as well as persistent data-link retransmission</p>
    <p>Reordering was fairly frequent even given our low bitrates</p>
    <p>Most paths were in fact asymmetric, where longer paths were more likely to be identified as asymmetric</p>
  </div>
</Presentation>
