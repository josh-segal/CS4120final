<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Making Sense of Performance in Data Analytics Frameworks</p>
    <p>Kay Ousterhout, Ryan Rasti, Sylvia Ratnasamy, Scott Shenker, Byung-Gon Chun</p>
  </div>
  <div class="page">
    <p>Large-scale data analytics has become widespread</p>
  </div>
  <div class="page">
    <p>More resourceefficient Faster</p>
  </div>
  <div class="page">
    <p>Task Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Spark (or Hadoop/Dryad/etc.) task</p>
  </div>
  <div class="page">
    <p>Task Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Spark (or Hadoop/Dryad/etc.) task</p>
  </div>
  <div class="page">
    <p>Task Task</p>
    <p>Task</p>
    <p>Task Task Task</p>
    <p>Task</p>
    <p>Task</p>
  </div>
  <div class="page">
    <p>Stragglers Scarlett [EuroSys 11], SkewTune [SIGMOD 12], LATE [OSDI 08], Mantri [OSDI 10], Dolly [NSDI 13], GRASS [NSDI 14], Wrangler [SoCC 14]</p>
    <p>Disk Themis [SoCC 12], PACMan [NSDI 12], Spark [NSDI 12], Tachyon [SoCC 14]</p>
    <p>Network Load balancing: VL2 [SIGCOMM 09], Hedera [NSDI 10], Sinbad [SIGCOMM 13] Application semantics: Orchestra [SIGCOMM 11], Baraat [SIGCOMM 14], Varys [SIGCOMM 14] Reduce data sent: PeriSCOPE [OSDI 12], SUDO [NSDI 12] In-network aggregation: Camdoop [NSDI 12] Better isolation and fairness: Oktopus [SIGCOMM 11], EyeQ [NSDI 12], FairCloud [SIGCOMM 12]</p>
  </div>
  <div class="page">
    <p>Disk Themis [SoCC 12], PACMan [NSDI 12], Spark [NSDI 12], Tachyon [SoCC 14]</p>
    <p>Stragglers Scarlett [EuroSys 11], SkewTune [SIGMOD 12], LATE [OSDI 08], Mantri [OSDI 10], Dolly [NSDI 13], GRASS [NSDI 14], Wrangler [SoCC 14]</p>
    <p>Network Load balancing: VL2 [SIGCOMM 09], Hedera [NSDI 10], Sinbad [SIGCOMM 13] Application semantics: Orchestra [SIGCOMM 11], Baraat [SIGCOMM 14], Varys [SIGCOMM 14] Reduce data sent: PeriSCOPE [OSDI 12], SUDO [NSDI 12] In-network aggregation: Camdoop [NSDI 12] Better isolation and fairness: Oktopus [SIGCOMM 11]), EyeQ [NSDI 12], FairCloud [SIGCOMM 12] Missing: whats most important to</p>
    <p>end-to-end performance?</p>
  </div>
  <div class="page">
    <p>Disk Themis [SoCC 12], PACMan [NSDI 12], Spark [NSDI 12], Tachyon [SoCC 14]</p>
    <p>Stragglers Scarlett [EuroSys 11], SkewTune [SIGMOD 12], LATE [OSDI 08], Mantri [OSDI 10], Dolly [NSDI 13], GRASS [NSDI 14], Wrangler [SoCC 14]</p>
    <p>Network Load balancing: VL2 [SIGCOMM 09], Hedera [NSDI 10], Sinbad [SIGCOMM 13] Application semantics: Orchestra [SIGCOMM 11], Baraat [SIGCOMM 14], Varys [SIGCOMM 14] Reduce data sent: PeriSCOPE [OSDI 12], SUDO [NSDI 12] In-network aggregation: Camdoop [NSDI 12] Better isolation and fairness: Oktopus [SIGCOMM 11]), EyeQ [NSDI 12], FairCloud [SIGCOMM 12]</p>
    <p>Widely-accepted mantras:</p>
    <p>Network and disk I/O are bottlenecks</p>
    <p>Stragglers are a major issue with unknown causes</p>
  </div>
  <div class="page">
    <p>(1) How can we quantify performance bottlenecks? Blocked time analysis</p>
    <p>(2) Do the mantras hold?</p>
    <p>Takeaways based on three workloads run with Spark</p>
    <p>This work</p>
  </div>
  <div class="page">
    <p>Takeaways based on three Spark workloads:</p>
    <p>Network optimizations can reduce job completion time by at most 2%</p>
    <p>CPU (not I/O) often the bottleneck</p>
    <p>&lt;19% reduction in completion time from optimizing disk</p>
    <p>Many straggler causes can be identified and fixed</p>
  </div>
  <div class="page">
    <p>Takeaways will not hold for every single analytics workload</p>
    <p>nor for all time</p>
  </div>
  <div class="page">
    <p>Accepted mantras are often not true</p>
    <p>Methodology to avoid performance misunderstandings in the future</p>
    <p>This work:</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Methodology: How can we measure bottlenecks?</p>
    <p>Workloads: What workloads did we use?</p>
    <p>Results: How well do the mantras hold?</p>
    <p>Why?: Why do our results differ from past work?</p>
  </div>
  <div class="page">
    <p>What is the jobs bottleneck?</p>
    <p>time</p>
    <p>tasks</p>
    <p>compute network</p>
    <p>disk</p>
    <p>Task x: may be bottlenecked on different resources at</p>
    <p>different times</p>
    <p>Time t: different tasks may be bottlenecked on different resources</p>
  </div>
  <div class="page">
    <p>How does network affect the jobs completion time?</p>
    <p>time</p>
    <p>tasks</p>
    <p>:Time when task is blocked on the</p>
    <p>network</p>
    <p>Blocked time analysis: how much faster would the job complete if tasks never blocked on the network?</p>
  </div>
  <div class="page">
    <p>Blocked time analysis</p>
    <p>tasks</p>
    <p>(2) Simulate how job completion time would change</p>
    <p>(1) Measure time when tasks are blocked on the</p>
    <p>network</p>
  </div>
  <div class="page">
    <p>network read compute disk write</p>
    <p>Original task runtime : time blocked on network</p>
    <p>compute</p>
    <p>task runtime if network were infinitely fast</p>
    <p>: time blocked on disk</p>
    <p>Best case</p>
    <p>(1) Measure time when tasks are blocked on network</p>
    <p>: time to handle one record</p>
  </div>
  <div class="page">
    <p>(2) Simulate how job completion time would change</p>
    <p>Task 0</p>
    <p>Task 1</p>
    <p>Task 2 time</p>
    <p>ts</p>
    <p>to: Original job completion time</p>
    <p>Task 0</p>
    <p>Task 1</p>
    <p>Task 2</p>
    <p>ts</p>
    <p>Incorrectly computed time: doesnt account for task scheduling</p>
    <p>: time blocked on network</p>
    <p>tn: Job completion time with infinitely fast network</p>
  </div>
  <div class="page">
    <p>Blocked time analysis: how quickly could a job have completed if a resource</p>
    <p>were infinitely fast?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Methodology: How can we measure bottlenecks?</p>
    <p>Workloads: What workloads did we use?</p>
    <p>Results: How well do the mantras hold?</p>
    <p>Why?: Why do our results differ from prior work?</p>
  </div>
  <div class="page">
    <p>Large-scale traces? Dont have enough instrumentation for</p>
    <p>blocked-time analysis</p>
  </div>
  <div class="page">
    <p>SQL Workloads run on Spark</p>
    <p>TPC-DS (20 machines, 850GB; 60 machines, 2.5TB; 200 machines, 2.5TB) Big Data Benchmark (5 machines, 60GB)</p>
    <p>Databricks (Production; 9 machines, tens of GB)</p>
    <p>Only 3 workloads</p>
    <p>Small cluster sizes</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Methodology: How can we measure bottlenecks?</p>
    <p>Workloads: What workloads did we use?</p>
    <p>Results: How well do the mantras hold?</p>
    <p>Why?: Why do our results differ from prior work?</p>
  </div>
  <div class="page">
    <p>How much faster could jobs get from optimizing network performance?</p>
    <p>!!&quot;</p>
  </div>
  <div class="page">
    <p>How much faster could jobs get from optimizing network performance?</p>
    <p>!!&quot;</p>
    <p>!!&quot;</p>
    <p>Percentiles</p>
    <p>Median improvement: 2% 95%ile improvement: 10%</p>
  </div>
  <div class="page">
    <p>How much faster could jobs get from optimizing network performance?</p>
    <p>Median improvement at most 2%</p>
    <p>Percentiles</p>
    <p>!!&quot;</p>
  </div>
  <div class="page">
    <p>How much faster could jobs get from optimizing disk performance?</p>
    <p>Median improvement at most 19%</p>
    <p>!!&quot;</p>
  </div>
  <div class="page">
    <p>How important is CPU?</p>
    <p>!</p>
    <p>CPU much more highly utilized than disk or network!</p>
  </div>
  <div class="page">
    <p>What about stragglers?</p>
    <p>Can explain &gt;60% of stragglers in &gt;75% of jobs Fixing underlying cause can speed up other tasks too!</p>
  </div>
  <div class="page">
    <p>Takeaways based on three Spark workloads:</p>
    <p>Network optimizations can reduce job completion time by at most 2%</p>
    <p>CPU (not I/O) often the bottleneck</p>
    <p>&lt;19% reduction in completion time from optimizing disk</p>
    <p>Many straggler causes can be identified and fixed</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Methodology: How can we measure bottlenecks?</p>
    <p>Workloads: What workloads did we use?</p>
    <p>Results: How well do the mantras hold?</p>
    <p>Why?: Why do our results differ from past work? network</p>
    <p>&gt;</p>
  </div>
  <div class="page">
    <p>Why are our results so different than whats stated in prior work?</p>
    <p>Are the workloads we measured unusually network-light?</p>
    <p>How can we compare our workloads to largescale traces used to motivate prior work?</p>
  </div>
  <div class="page">
    <p>How much data is transferred per CPU second?</p>
    <p>Microsoft 09-10: 1.96.35 Mb / task second Google 04-07: 1.341.61 Mb / machine second</p>
  </div>
  <div class="page">
    <p>Why are our results so different than whats stated in prior work?</p>
    <p>Our workloads are network light</p>
  </div>
  <div class="page">
    <p>When is the network used? map task</p>
    <p>map task</p>
    <p>map task</p>
    <p>reduce task</p>
    <p>reduce task</p>
    <p>reduce task</p>
    <p>Input data (read</p>
    <p>locally)</p>
    <p>Output data</p>
    <p>(1) To shuffle intermediate</p>
    <p>data</p>
    <p>(2) To replicate</p>
    <p>output data</p>
    <p>Some work focuses only on</p>
    <p>the shuffle</p>
  </div>
  <div class="page">
    <p>How does the data transferred over the network compare to the input data?</p>
    <p>Not realistic to look only at shuffle! Or to use workloads where all input is shuffled</p>
    <p>Shuffled data is only ~1/3 of input data!</p>
    <p>Even less output data</p>
  </div>
  <div class="page">
    <p>Prior work conflates CPU and network time</p>
    <p>To send data over network: (1) Serialize objects into</p>
    <p>bytes (2) Send bytes</p>
    <p>(1) and (2) often conflated.</p>
    <p>Reducing application data sent reduces both!</p>
  </div>
  <div class="page">
    <p>When does the network matter?</p>
    <p>Network important when: (1) Computation optimized (2) Serialization time low</p>
    <p>(3) Large amount of data sent over network</p>
  </div>
  <div class="page">
    <p>Why are our results so different than whats stated in prior work?</p>
    <p>Our workloads are network light</p>
  </div>
  <div class="page">
    <p>Limitations Only three workloads</p>
    <p>Industry-standard workloads Results sanity-checked with larger production traces</p>
    <p>Small cluster sizes</p>
    <p>Results dont change when we move between cluster sizes One framework (Spark)</p>
    <p>Results sanity-checked with production traces from other frameworks We instrumented and evaluated Hadoop, with consistent results</p>
  </div>
  <div class="page">
    <p>Limitations arent fatal Only three workloads</p>
    <p>Industry-standard workloads Results sanity-checked with larger production traces</p>
    <p>Small cluster sizes</p>
    <p>Takeaways dont change when we move between cluster sizes One framework (Spark)</p>
    <p>Results sanity-checked with production traces from other frameworks We instrumented and evaluated Hadoop, with consistent results</p>
  </div>
  <div class="page">
    <p>Network optimizations can reduce job completion time by at most 2%</p>
    <p>CPU (not I/O) often the bottleneck &lt;19% reduction in completion time from optimizing disk</p>
    <p>Many straggler causes can be identified and fixed</p>
    <p>All traces publicly available: tinyurl.com/nsdi-traces</p>
    <p>Takeaway: performance understandability should be a first-class concern!</p>
    <p>Instrument systems for blocked time analysis (almost) All Instrumentation now part of Spark</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Why is the CPU time so high?</p>
    <p>Compression and serialization are costly</p>
  </div>
  <div class="page">
    <p>What can be done to reduce compute time?</p>
  </div>
</Presentation>
