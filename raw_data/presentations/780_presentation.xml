<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ICML 2003 1</p>
    <p>Linear Programming Boosting for Uneven Datasets</p>
    <p>Jurij Leskovec, Joef Stefan Institute, Slovenia</p>
    <p>John Shawe-Taylor, Royal Holloway University of London,</p>
    <p>UK</p>
  </div>
  <div class="page">
    <p>ICML 2003 2</p>
    <p>Motivation  There are 800 million of</p>
    <p>Europeans and 2 million of them are Slovenians</p>
    <p>Want to build a classifier to distinguish Slovenians from the rest of Europeans</p>
    <p>A traditional unaware classifier (e.g. politician) would not even notice Slovenia as an entity</p>
    <p>We dont want that!</p>
  </div>
  <div class="page">
    <p>ICML 2003 3</p>
    <p>Problem setting</p>
    <p>Unbalanced Dataset  2 classes:</p>
    <p>positive (small)  negative (large)</p>
    <p>Train a binary classifier to separate highly unbalanced classes</p>
  </div>
  <div class="page">
    <p>ICML 2003 4</p>
    <p>Our solution framework  We will use Boosting</p>
    <p>Combine many simple and inaccurate categorization rules (weak learners) into a single highly accurate categorization rule</p>
    <p>The simple rules are trained sequentially; each rule is trained on examples which are most difficult to classify by preceding rules</p>
  </div>
  <div class="page">
    <p>ICML 2003 5</p>
    <p>Outline</p>
    <p>Boosting algorithms  Weak learners  Experimental setup  Results  Conclusions</p>
  </div>
  <div class="page">
    <p>ICML 2003 6</p>
    <p>Related approaches: AdaBoost  given training examples (x1,y1), (xm,ym)  initialize D0(i) = 1/m yi  {+1,</p>
    <p>-1}  for t = 1T</p>
    <p>pass distribution Dt to weak learner get weak hypothesis ht: X   R choose t (based on performance of ht) update Dt+1(i) = Dt(i) exp(-t yi ht(xi)) / Zt</p>
    <p>final hypothesis: f(x) = t t ht(x)</p>
  </div>
  <div class="page">
    <p>ICML 2003 7</p>
    <p>AdaBoost - Intuition  weak hypothesis h(x)</p>
    <p>sign of h(x) is the predicted binary label  magnitude |h(x)| as a confidence</p>
    <p>t controls the influence of each ht(x)</p>
  </div>
  <div class="page">
    <p>ICML 2003 8</p>
    <p>More Boosting Algorithms  Algorithms differ in the way of initializing</p>
    <p>weights D0(i) (misclassification costs) and updating them</p>
    <p>4 boosting algorithms:  AdaBoost  Greedy approach  UBoost  Uneven loss function + greedy  LPBoost  Linear Programming (optimal</p>
    <p>solution)  LPUBoost  Our proposed solution (LP +</p>
    <p>uneven)</p>
  </div>
  <div class="page">
    <p>ICML 2003 9</p>
    <p>given training examples (x1,y1), (xm,ym)  initialize D0(i) = 1/m yi</p>
    <p>{+1, -1}  for t = 1T</p>
    <p>pass distribution Dt to weak learner  get weak hypothesis ht: X   R  choose t  update Dt+1(i) = Dt(i) exp(-t yi ht(xi)) / Zt</p>
    <p>final hypothesis: f(x) = t t ht(x)</p>
    <p>Boosting Algorithm Differences</p>
    <p>Boosting Algorithms differ</p>
    <p>in these 2 lines</p>
  </div>
  <div class="page">
    <p>ICML 2003 10</p>
    <p>UBoost - Uneven Loss Function  set:</p>
    <p>D0(i) so that D0(positive) / D0(negative) =</p>
    <p>update Dt+1(i): increase weight of false negatives more</p>
    <p>than on false positives decrease weight of true positives less than</p>
    <p>on true negatives  Positive examples maintain higher</p>
    <p>weight (misclassification cost)</p>
  </div>
  <div class="page">
    <p>ICML 2003 11</p>
    <p>LPBoost  Linear Programming  set:</p>
    <p>D0(i) = 1/m</p>
    <p>update Dt+1: solve LP: argmin LPBeta, s.t. i (D(i) yi hk(xi))  LPBeta; k = 1t</p>
    <p>where 1 / A &lt; D(i) &lt; 1 / B  set  to Lagrangian multipliers  if i D(i) yi ht(xi) &lt; LPBeta, optimal</p>
    <p>solution</p>
  </div>
  <div class="page">
    <p>ICML 2003 12</p>
    <p>LPBoost  Intuition</p>
    <p>argmin LPBeta s.t. i (D(i) yi hk(xi))  LPBeta k = 1...t</p>
    <p>where 1 / A &lt; D(i) &lt; 1 / B</p>
    <p>D(1) D(2) D(3)  D(m)</p>
    <p>h1 + - +</p>
    <p>h2 - - + +</p>
    <p>LPBeta</p>
    <p>ht + - + +</p>
    <p>Training Example Weights</p>
    <p>Weak</p>
    <p>Learners</p>
  </div>
  <div class="page">
    <p>ICML 2003 13</p>
    <p>LPBoost  Example</p>
    <p>D(1) D(2) D(3)</p>
    <p>h1 + 0.3 D(1)</p>
    <p>+ 0.7 D(2)</p>
    <p>- 0.2 D(3)  LPBeta</p>
    <p>h2 + 0.1 D(1)</p>
    <p>- 0.4 D(2)</p>
    <p>- 0.5 D(3)  LPBeta</p>
    <p>h3 + 0.5 D(1)</p>
    <p>- 0.1 D(2)</p>
    <p>- 0.3 D(3)  LPBeta</p>
    <p>Training Example Weights</p>
    <p>argmin LPBeta s.t. i (yi hk(xi) D(i))  LPBeta k = 1...3</p>
    <p>where 1 / A &lt; D(i) &lt; 1 / B</p>
    <p>ConfidenceIncorrectly Classified</p>
    <p>Correctly Classified</p>
    <p>Weak</p>
    <p>Learners</p>
  </div>
  <div class="page">
    <p>ICML 2003 14</p>
    <p>LPUBoost - Uneven Loss + LP  set:</p>
    <p>D0(i) so that D0(positive) / D0(negative) =   update Dt+1:</p>
    <p>solve LP, minimize LPBeta but set different misclassification cost bounds for D(i) ( times higher for positive examples)</p>
    <p>the rest as in LPBoost</p>
    <p>Note:  is input parameter. LPBeta is Linear Programming optimization variable</p>
  </div>
  <div class="page">
    <p>ICML 2003 15</p>
    <p>Summary of Boosting Algorithms</p>
    <p>Uneven loss function</p>
    <p>Converges to global</p>
    <p>optimum</p>
    <p>AdaBoost</p>
    <p>UBoost</p>
    <p>LPBoost</p>
    <p>LPUBoost</p>
  </div>
  <div class="page">
    <p>ICML 2003 16</p>
    <p>Weak Learners  One-level decision tree (IF-THEN rule):</p>
    <p>if word w occurs in a document X return P else return N  P and N are real numbers chosen based on</p>
    <p>misclassification cost weights Dt(i)</p>
    <p>interpret the sign of P and N as the predicted binary label</p>
    <p>magnitude |P| and |N| as the confidence</p>
  </div>
  <div class="page">
    <p>ICML 2003 17</p>
    <p>Experimental setup  Reuters newswire articles (Reuters</p>
    <p>Recall = TP / (TP + FN) F1 = 2Prec Rec / (Prec + Rec)</p>
  </div>
  <div class="page">
    <p>ICML 2003 18</p>
    <p>Typical situations  Balanced training dataset</p>
    <p>all learning algorithms show similar performance</p>
    <p>Unbalanced training dataset  AdaBoost overfits  LPUBoost does not overfit  converges</p>
    <p>fast using only a few weak learners  UBoost and LPBoost are somewhere in</p>
    <p>between</p>
  </div>
  <div class="page">
    <p>ICML 2003 19</p>
    <p>Balanced dataset Typical behavior</p>
  </div>
  <div class="page">
    <p>ICML 2003 20</p>
    <p>Unbalanced Dataset AdaBoost overfits</p>
  </div>
  <div class="page">
    <p>ICML 2003 21</p>
    <p>Unbalanced dataset</p>
    <p>LPUBoost</p>
    <p>Few iterations (10)</p>
    <p>Stop after no suitable feature is left</p>
  </div>
  <div class="page">
    <p>ICML 2003 22</p>
    <p>Reuters categories</p>
    <p>F1 on test set</p>
    <p>even</p>
    <p>uneven</p>
    <p>Category (size) Ada U LP LPU SVM</p>
    <p>EARN (2877) 0.97 0.97 0.97 0.91 0.98 ACQ (1650) 0.91 0.94 0.88 0.84 0.94 MONEY-FX (538) 0.65 0.70 0.63 0.65 0.76 INTEREST (347) 0.65 0.69 0.59 0.66 0.65 CORN (181) 0.81 0.87 0.82 0.83 0.80 GNP (101) 0.78 0.80 0.64 0.66 0.81 CARCASS (50) 0.49 0.65 0.63 0.65 0.52 COTTON (39) 0.68 0.89 0.95 0.95 0.68 MEAL-FEED (30) 0.59 0.77 0.65 0.81 0.45 PET-CHEM (20) 0.03 0.16 0.03 0.19 0.17 LEAD (15) 0.20 0.67 0.24 0.45 0 SOY-MEAL (13) 0.30 0.73 0.35 0.38 0.21 GROUNDNUT (5) 0 0 0.22 0.75 0 PLATINUM (5) 0 0 0.20 1.00 0.32 POTATO (3) 0.53 0.53 0.29 0.86 0.15 NAPHTHA (2) 0 0 0.20 0.89 0</p>
    <p>AVERAGE 0.47 0.59 0.52 0.72 0.46</p>
  </div>
  <div class="page">
    <p>ICML 2003 23</p>
    <p>LPUBoost vs. UBoost</p>
  </div>
  <div class="page">
    <p>ICML 2003 24</p>
    <p>Most important features (stemmed words)</p>
    <p>EARN (2877)  50: ct, net, profit, dividend, shr  INTEREST (347)  70: rate, bank, company, year, pct  CARCASS (50)  30: beef, pork, meat, dollar, chicago  SOY-MEAL (13)  3: meal, soymeal, soybean  GROUNDNUT (5)  2: peanut, cotton (F1=0.75)  PLATINUM (5)  1: platinum (F1=1.0)  POTATO (3)  1: potato (F1=0.86)</p>
    <p>Category size LPU model size (number of features / words)</p>
  </div>
  <div class="page">
    <p>ICML 2003 25</p>
    <p>Computational efficiency  AdaBoost and UBoost are the fastest</p>
    <p>the simplest  LPBoost and LPUBoost are a little</p>
    <p>slower  LP computation takes much of the time</p>
    <p>but since LPUBoost chooses fewer weak hypotheses the times get comparable to those of AdaBoost</p>
  </div>
  <div class="page">
    <p>ICML 2003 26</p>
    <p>Conclusions  LPUBoost is suitable for text</p>
    <p>categorization for highly unbalanced datasets</p>
    <p>All benefits (well-defined stopping criteria, unequal loss function) show up</p>
    <p>No overfitting: it is able to find simple (small) and complicated (large) hypotheses</p>
  </div>
</Presentation>
