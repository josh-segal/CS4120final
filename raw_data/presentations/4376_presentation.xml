<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness</p>
  </div>
  <div class="page">
    <p>Authors</p>
    <p>2</p>
    <p>Amulya Gupta Email: guptaam@iastate.edu https://github.com/amulyahwr/ acl2018</p>
    <p>Zhu (Drew) Zhang Email: zhuzhang@iastate.edu</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>3</p>
    <p>Classical world</p>
    <p>Alternate world</p>
    <p>Our contribution</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Problem Statement</p>
    <p>4</p>
    <p>Given two sentences, determine the semantic similarity between them.</p>
  </div>
  <div class="page">
    <p>Tasks</p>
    <p>5</p>
    <p>Semantic relatedness for sentence pairs.</p>
    <p>Paraphrase detection for question pairs.</p>
    <p>Essence: Given two sentences, determine the semantic similarity between them. Introduction</p>
  </div>
  <div class="page">
    <p>Datasets used</p>
    <p>6</p>
    <p>Semantic relatedness for sentence pairs.</p>
    <p>test)</p>
    <p>Paraphrase detection for question pairs.</p>
  </div>
  <div class="page">
    <p>Examples</p>
    <p>7</p>
    <p>SICK</p>
    <p>MSRpar</p>
    <p>Quora</p>
    <p>The badger is burrowing a hole A hole is being burrowed by the badger 4.9</p>
    <p>The reading for both August and July is the best seen since the survey began in August 1997.</p>
    <p>It is the highest reading since the index was created in August 1997.</p>
    <p>What is bigdata? Is bigdata really doing well? 0</p>
  </div>
  <div class="page">
    <p>Linear</p>
    <p>8</p>
    <p>Generally, a sentence is read in a linear form.</p>
    <p>English (Left to Right):</p>
    <p>Urdu (Right to Left):</p>
    <p>Traditional Chinese (Top to Bottom):</p>
    <p>Classical world</p>
    <p>The badger is burrowing a hole.</p>
    <p>. (Google Translate)</p>
  </div>
  <div class="page">
    <p>Long Short Term Memory (LSTM)</p>
    <p>9</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>e_The e_badger e_is e_burrowing e_a e_hole</p>
    <p>o1 o2 o3 o4 o5 o6</p>
    <p>Introduction Classical</p>
    <p>world</p>
  </div>
  <div class="page">
    <p>Long Short Term Memory (LSTM)</p>
    <p>10</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>LSTM cell</p>
    <p>e_The e_badger e_is e_burrowing e_a e_hole</p>
    <p>o1 o2 o3 o4 o5</p>
    <p>Introduction Classical</p>
    <p>world</p>
    <p>LSTM cell</p>
    <p>o6</p>
  </div>
  <div class="page">
    <p>Attention mechanism</p>
    <p>11</p>
    <p>Neural Machine Translation (NMT) (Bahdanau et al., 2014)</p>
    <p>Introduction Classical</p>
    <p>world</p>
    <p>Global Attention Model (GAM) (Luong et al., 2015)</p>
  </div>
  <div class="page">
    <p>Tree</p>
    <p>12</p>
    <p>DependencyConstituency</p>
    <p>nsubj aux</p>
    <p>dobj</p>
    <p>det det</p>
    <p>burrowing</p>
    <p>badger</p>
    <p>The</p>
    <p>is hole</p>
    <p>a</p>
    <p>Introduction Alternate</p>
    <p>world Classical</p>
    <p>world</p>
  </div>
  <div class="page">
    <p>Tree-LSTM (Tai et al., 2015)</p>
    <p>13</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>e_The</p>
    <p>o1 e_badger</p>
    <p>o2</p>
    <p>e_is</p>
    <p>o3</p>
    <p>e_hole</p>
    <p>o6</p>
    <p>e_a</p>
    <p>o5</p>
    <p>e_burrowing</p>
    <p>o4</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world</p>
    <p>T-LSTM cell</p>
  </div>
  <div class="page">
    <p>Attention mechanism</p>
    <p>14</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world</p>
  </div>
  <div class="page">
    <p>Aggregate</p>
    <p>Decomposable Attention (Parikh et al., 2016)</p>
    <p>15</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world</p>
    <p>Sentence R e1 e2 e3 e4</p>
    <p>No structural encoding</p>
    <p>Sentence L e1 e2 e3 e4 e5 e6</p>
    <p>No structural encoding</p>
    <p>e7 e8 Attend: Attention matrix</p>
    <p>Compare</p>
  </div>
  <div class="page">
    <p>Modification 2</p>
    <p>Modification 1</p>
    <p>h+ (Absolute Distance similarity:</p>
    <p>Element wise absolute difference)</p>
    <p>Modified Decomposable Attention (MDA)</p>
    <p>16</p>
    <p>HL</p>
    <p>o3</p>
    <p>o1 o2</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>Sentence L</p>
    <p>T-LSTM cell</p>
    <p>HR</p>
    <p>o3 o1</p>
    <p>o2</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>Sentence R</p>
    <p>T-LSTM cell</p>
    <p>Attention matrix</p>
    <p>hx (Sign similarity:</p>
    <p>Element wise multiplication)</p>
    <p>output</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
    <p>MDA is employed after encoding sentences.</p>
  </div>
  <div class="page">
    <p>Testset Results</p>
    <p>17</p>
    <p>MSRpar</p>
    <p>Linear Constituency Dependency</p>
    <p>w/o Attention MDA w/o Attention MDA w/o Attention MDA</p>
    <p>Pearsons r 0.327 0.3763 0.3981 0.3991 0.4921 0.4016 Spearmans</p>
    <p>0.2205 0.3025 0.315 0.3237 0.4519 0.331</p>
    <p>MSE 0.8098 0.729 0.7407 0.722 0.6611 0.7243</p>
    <p>SICK</p>
    <p>Linear Constituency Dependency</p>
    <p>w/o Attention MDA w/o Attention MDA w/o Attention MDA</p>
    <p>Pearsons r 0.8398 0.7899 0.8582 0.779 0.8676 0.8239</p>
    <p>Spearmans  0.7782 0.7173 0.7966 0.7074 0.8083 0.7614</p>
    <p>MSE 0.3024 0.3897 0.2734 0.4044 0.2532 0.3326</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
  </div>
  <div class="page">
    <p>Progressive Attention (PA)</p>
    <p>18</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>Sentence R</p>
    <p>T-LSTM cell</p>
    <p>Phase 1</p>
    <p>a2 Attention</p>
    <p>vector</p>
    <p>Gating mechanismo3</p>
    <p>o3</p>
    <p>o3</p>
    <p>HR</p>
    <p>o3 o1</p>
    <p>o2</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>Sentence L</p>
    <p>T-LSTM cell</p>
    <p>HL</p>
    <p>Start</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
  </div>
  <div class="page">
    <p>Progressive Attention (PA)</p>
    <p>19</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>Sentence R</p>
    <p>T-LSTM cell</p>
    <p>Phase 1</p>
    <p>a2 Attention</p>
    <p>vector</p>
    <p>Gating mechanismo3</p>
    <p>o3</p>
    <p>o3</p>
    <p>HR</p>
    <p>o3 o1</p>
    <p>o2</p>
    <p>T-LSTM cell</p>
    <p>T-LSTM cell</p>
    <p>Sentence L</p>
    <p>T-LSTM cell</p>
    <p>HL</p>
    <p>Start</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
    <p>HL HR</p>
    <p>T</p>
    <p>T- T</p>
    <p>T</p>
    <p>T- T</p>
  </div>
  <div class="page">
    <p>Progressive Attention (PA)</p>
    <p>20</p>
    <p>h+ (Absolute Distance similarity:</p>
    <p>Element wise absolute difference)</p>
    <p>hx (Sign similarity:</p>
    <p>Element wise multiplication)</p>
    <p>output</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
    <p>PA is employed during encoding sentences.</p>
    <p>T</p>
    <p>T- T</p>
    <p>T- T- T- T</p>
    <p>HL HR</p>
    <p>T- T</p>
    <p>T- T</p>
    <p>T- T</p>
  </div>
  <div class="page">
    <p>Effectiveness of PA</p>
    <p>21</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
    <p>ID Sentence 1 Sentence 2 Gold Linear Constituency Dependency</p>
    <p>No attn</p>
    <p>PA No attn</p>
    <p>PA No attn PA</p>
    <p>A hole is being burrowed by the badger</p>
  </div>
  <div class="page">
    <p>Testset Results</p>
    <p>MSRpar</p>
    <p>Linear Constituency Dependency</p>
    <p>w/o Attention MDA PA</p>
    <p>w/o Attention MDA PA</p>
    <p>w/o Attention MDA PA</p>
    <p>Pearsons r 0.327 0.3763 0.4773 0.3981 0.3991 0.5104 0.4921 0.4016 0.4727</p>
    <p>Spearmans  0.2205 0.3025 0.4453 0.315 0.3237 0.4764 0.4519 0.331 0.4216</p>
    <p>MSE 0.8098 0.729 0.6758 0.7407 0.722 0.6436 0.6611 0.7243 0.6823</p>
    <p>SICK</p>
    <p>Linear Constituency Dependency</p>
    <p>w/o Attention MDA PA</p>
    <p>w/o Attention MDA PA</p>
    <p>w/o Attention MDA PA</p>
    <p>Pearsons r 0.8398 0.7899 0.8550 0.8582 0.779 0.8625 0.8676 0.8239 0.8424</p>
    <p>Spearmans  0.7782 0.7173 0.7873 0.7966 0.7074 0.7997 0.8083 0.7614 0.7733</p>
    <p>MSE 0.3024 0.3897 0.2761 0.2734 0.4044 0.2610 0.2532 0.3326 0.2963</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
    <p>Is it because attention can be considered as an implicit form of structure which complements the explicit form of syntactic structure?</p>
    <p>If yes, does there exist some tradeoff between modeling efforts invested in syntactic and attention structure? Does this mean there is a closer affinity between dependency structure and compositional semantics?</p>
    <p>If yes, is it because dependency structure embody more semantic information?</p>
    <p>Structural Information</p>
    <p>Attention Impact</p>
    <p>Linear Constituency Dependency</p>
    <p>Gildea (2004): Dependencies vs. Constituents for Tree-Based Alignment</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Summary</p>
    <p>Introduction Classical</p>
    <p>world Alternate</p>
    <p>world Our</p>
    <p>contribution</p>
    <p>Proposed a modified decomposable attention (MDA) and a novel progressive attention (PA) model on tree based structures.</p>
    <p>Investigated the impact of proposed attention models on syntactic structures in linguistics.</p>
    <p>Summary</p>
  </div>
</Presentation>
