<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DFS: A Filesystem for Virtualized Flash Disks</p>
    <p>William Josephson</p>
    <p>wkj@CS.Princeton.EDU</p>
  </div>
  <div class="page">
    <p>Why Flash?</p>
    <p>Tape is Dead; Disk is Tape; Flash is Disk; RAM Locality is King</p>
    <p>-Jim Gray (2006)</p>
    <p>Why Flash?</p>
    <p>Non-volatile storage</p>
    <p>No mechanical components</p>
    <p>Moores law does not apply to seeks</p>
    <p>Inexpensive and getting cheaper</p>
    <p>Potential for significant power savings</p>
    <p>Real-world performance is much better than in 2006</p>
    <p>Bottom line: disks for $/GB; flash for $/IOPS</p>
  </div>
  <div class="page">
    <p>Why not Battery-Backed DRAM?</p>
    <p>Flash costs less than DRAM and is getting cheaper</p>
    <p>Both markets are volatile, however (e.g., new iPhones)</p>
    <p>Memory subsystems that support large memory are expensive</p>
    <p>Think of flash as a new level in the memory hierarchy</p>
    <p>Last weeks spot prices put SLC : DRAM at 1 : 3.6 and MLC at 1 : 9.8</p>
  </div>
  <div class="page">
    <p>Flash Memory Review</p>
    <p>Non-volatile solid state memory</p>
    <p>Individual cells are comparable in size to a transistor</p>
    <p>Not sensitive to mechanical shock</p>
    <p>Re-write requires prior bulk erase</p>
    <p>Limited number of erase/write cycles</p>
    <p>Two categories of flash:</p>
    <p>NOR flash: random access, used for firmware</p>
    <p>NAND flash: block access, used for mass storage</p>
    <p>Two types of memory cells:</p>
    <p>SLC: single level cell that encodes a single bit per cell</p>
    <p>MLC: multi-level cell that encodes multiple bits per cell</p>
  </div>
  <div class="page">
    <p>NAND Flash</p>
    <p>Economics</p>
    <p>Individual cells are simple</p>
    <p>Improved fabrication yield</p>
    <p>1st to use new process technology</p>
    <p>Already must deal with failures, so just mark fab defects</p>
    <p>High volume for many consumer applications</p>
    <p>Organization</p>
    <p>Data is organized into pages for transfer (512B-4K)</p>
    <p>Pages are grouped into erase blocks (EBs) (16K-16MB+)</p>
    <p>Must erase an entire EB before writing again</p>
  </div>
  <div class="page">
    <p>NAND Flash Challenges</p>
    <p>Block oriented interface</p>
    <p>Must read or write multiples of the page size</p>
    <p>Must erase an entire EB at once</p>
    <p>Bulk erasure of EBs requires copying rather than update-in-place</p>
    <p>Limited number of erase cycles requires wear-leveling</p>
    <p>Less of an issue if you are copying for performance anyway</p>
    <p>Additional error correction often necessary for reliability</p>
    <p>Performance requires HW parallelism and software support</p>
  </div>
  <div class="page">
    <p>Why Another Filesystem?</p>
    <p>There are many filesystems designed for spinning rust</p>
    <p>e.g., FFS, extN , XFS, VxFS, FAT, NTFS, etc.</p>
    <p>Layout not designed with flash in mind</p>
    <p>Firmware/driver still implements a level of indirection</p>
    <p>Indirection supports wear-leveling and copying for performance</p>
    <p>There are also several filesystems designed specifically for flash</p>
    <p>e.g., JFFS/JFFS2 (NOR), YAFFS/YAFFS2 (SLC NAND)</p>
    <p>Log-structured; implement wear-leveling &amp; additional ECC</p>
    <p>Intended for embedded applications</p>
    <p>Small numbers of files, small total filesystem sizes</p>
    <p>Some must scan entire device at boot</p>
    <p>Often expect to manage raw flash</p>
    <p>In a server environment, we end up with two storage managers!</p>
  </div>
  <div class="page">
    <p>DFS: Idea</p>
    <p>Idea: Instead of running two storage managers, delegate</p>
    <p>Filesystem still responsible for directory management, access control</p>
    <p>Flash disk storage manager responsible for block allocation</p>
    <p>May take advantage of features not in traditional disk interface</p>
    <p>Longer term question: what should storage interface look like?</p>
  </div>
  <div class="page">
    <p>DFS: Requirements</p>
    <p>Currently relies on four features of underlying flash disk</p>
    <p>All are a natural outgrowth of high-performance flash storage</p>
    <p>(1) follows from block-remapping for copying and failed blocks</p>
    <p>(2) and (3) follow from log-structured storage for write peformance</p>
    <p>(4) already exists on most flash devices as a hint to GC</p>
  </div>
  <div class="page">
    <p>Block Diagram of Existing Approach vs DFS</p>
    <p>!&quot;!&quot;!&quot;</p>
    <p>!&quot;#$%&amp;'()$*%</p>
    <p>+,-.&quot;/01-#%2#034%&amp;)0,-5$%6-'$,%</p>
    <p>&amp;$3)0,%% 7$-.% 8,&quot;)$%</p>
    <p>&amp;$3)0,%%</p>
    <p>!&quot;#$%&amp;'()*%+,-.'*%/%</p>
    <p>#$%&amp;'&quot;</p>
    <p>&lt;-5$%</p>
    <p>&lt;-5$%</p>
    <p>;; ;%</p>
    <p>=&gt;=9%!#-(?%@$*0,'%</p>
    <p>!+6%A7$*-BB&quot;15C%</p>
    <p>&lt;-5$% ,$-.%</p>
    <p>&amp;0#&quot;.%&amp;)-)$%9&quot;(4%</p>
    <p>#$%&amp;'&quot;</p>
    <p>&lt;-5$%</p>
    <p>&lt;-5$%</p>
    <p>;; ;%</p>
    <p>=&gt;=9%!#-(?%@$*0,'%</p>
    <p>!+6%A7$*-BB&quot;15C%</p>
    <p>&lt;-5$% ,$-.%</p>
    <p>&amp;0#&quot;.%&amp;)-)$%9&quot;(4%</p>
    <p>!&quot;!&quot;!&quot;</p>
    <p>E&quot;,)F-#&quot;G$.%!#-(?%&amp;)0,-5$%6-'$,%</p>
    <p>A7$*-BB&quot;15H%8$-,I6$J$#&quot;15H%7$#&quot;-:&quot;#&quot;)'C%</p>
    <p>%!&quot;#$%&amp;'()*%+,-.'*%0'(1123(.'*%444%</p>
    <p>;%;%;%</p>
    <p>#$%&amp;'&quot;</p>
    <p>&lt;-5$%</p>
    <p>&lt;-5$%</p>
    <p>;; ;%</p>
    <p>=&gt;=9%!#-(?%@$*0,'%</p>
    <p>K01),0##$,%</p>
    <p>&quot;09,&quot;J$%</p>
    <p>&lt;-5$% ,$-.H%D,&quot;)$%%</p>
    <p>#$%&amp;'&quot;</p>
    <p>&lt;-5$%</p>
    <p>&lt;-5$%</p>
    <p>;; ;%</p>
    <p>=&gt;=9%!#-(?%@$*0,'%</p>
    <p>K01),0##$,%</p>
    <p>&quot;09,&quot;J$%</p>
    <p>&lt;-5$% ,$-.H%D,&quot;)$%%</p>
    <p>(a) Traditional layers of abstractions (b) Our layers of abstractions</p>
    <p>&gt;2?-3(1%71238% 9&quot;@A#-3(1%#-B'=%%</p>
    <p>()*&quot; +,-.&quot;/01-#% 9-)-:-($%</p>
    <p>+,-.&quot;/01-#% !&quot;#$%&amp;'()$*%</p>
  </div>
  <div class="page">
    <p>DFS: Logical Address Translation</p>
    <p>I-node contains base virtual address for files extent</p>
    <p>Base address, logical block #, and offset yield virtual address</p>
    <p>Flash storage manager translates virtual address to physical</p>
  </div>
  <div class="page">
    <p>DFS: File Layout</p>
    <p>Divide virtual address space into contiguous allocation chunks</p>
    <p>Flash storage manager maintains sparse virtual-to-physical mapping</p>
    <p>First chunk used for boot block, super block, and I-nodes</p>
    <p>Subsequent chunks contain either one large file or several small files</p>
    <p>Size of allocation chunk and small file chosen at initialization</p>
  </div>
  <div class="page">
    <p>DFS: Directories</p>
    <p>Directory implementation that peforms is work in progress</p>
    <p>Evaluation platform does not yet export atomic multi-block update</p>
    <p>Plan to implement directories as sparse hash tables</p>
    <p>Current implementation uses UFS/FFS directory metadata</p>
    <p>Requires additional logging of directory updates only</p>
  </div>
  <div class="page">
    <p>Evaluation Platform</p>
    <p>Linux 2.6.27.9 on a 4-core amd64 @ 2.4GHz with 4GB DRAM</p>
    <p>FusionIO ioDrive with 160GB SLC NAND flash (formatted capacity)</p>
    <p>Sits on PCIe bus rather than SATA/SCSI bus</p>
    <p>Hardware op latency is 50s</p>
    <p>Theoretical peak throughput of 120, 000 IOPS</p>
    <p>Version of device driver we are using limits throughput further</p>
    <p>OS-specific device driver exports block device interface</p>
    <p>Other features of the device can be separately exported</p>
    <p>Functionality split between hardware, software, &amp; host device driver</p>
    <p>Device driver consumes host CPU and memory</p>
  </div>
  <div class="page">
    <p>Microbenchmark: Random Reads</p>
    <p>Random 4KB I/Os per second as function of number of threads</p>
    <p>Need multiple threads to take advantage of hardware parallelism</p>
    <p>On our particular hardware, peak performance is about 100K IOPS</p>
    <p>Host CPU/memory performance has substantial effect, too</p>
    <p>Read IOPS x 1K raw dfs ext3</p>
  </div>
  <div class="page">
    <p>Microbenchmark: Random Writes</p>
    <p>Random 4KB I/Os per second as function of number of threads</p>
    <p>Once again need multiple threads to get best agregate performance</p>
    <p>There is an additional garbage collector thread in device driver</p>
    <p>We consider CPU expended per I/O in a moment</p>
    <p>Write IOPS x 1K raw dfs ext3</p>
  </div>
  <div class="page">
    <p>Microbenchmark: CPU Utilization</p>
    <p>Improvement in CPU usage for DFS vs. Ext3 at peak throughput</p>
    <p>i.e., larger, positive number is better</p>
    <p>About the same for reads; improvement for writes at low concurrency</p>
    <p>4 threads+4 cores: improved performance at higher cost due to GC</p>
    <p>Threads Read Random</p>
    <p>Read Write</p>
    <p>Random</p>
    <p>Write</p>
  </div>
  <div class="page">
    <p>Application Benchmark: Description</p>
    <p>Applications Description I/O Patterns</p>
    <p>Quicksort A quicksort on a large dataset Mem-mapped I/O</p>
    <p>N-Gram A hash table index for n-grams</p>
    <p>collected on the web</p>
    <p>Direct, random read</p>
    <p>KNNImpute Missing-value estimation for</p>
    <p>bioinformatics microarray data</p>
    <p>Mem-mapped I/O</p>
    <p>VM-Update Simultaneous update of an OS</p>
    <p>on several virtual machines</p>
    <p>Sequential read &amp; write</p>
    <p>TPC-H Standard benchmark for</p>
    <p>Decision Support</p>
    <p>Mostly sequential read</p>
  </div>
  <div class="page">
    <p>Application Benchmark: Performance</p>
    <p>Wall Time</p>
    <p>Application Ext3 DFS Speedup</p>
    <p>Quick Sort 1268 822 1.54</p>
    <p>N-Gram (Zipf) 4718 1912 2.47</p>
    <p>KNNImpute 303 248 1.22</p>
    <p>VM Update 685 640 1.07</p>
    <p>TPC-H 5059 4154 1.22</p>
    <p>Lower per-file lock contention</p>
    <p>I/Os to adjacent locations merged into fewer but larger requests</p>
    <p>Simplified get block can more easily issue contiguous I/O requests</p>
  </div>
  <div class="page">
    <p>Some Musings on Future Directions</p>
    <p>CPU overhead of device driver is not trivial</p>
    <p>Particularly write side suffers from GC overhead</p>
    <p>Push storage management onto flash device or into network?</p>
    <p>No compelling reason to interact with flash as ordinary mass storage</p>
    <p>Useful innovation at interface to new level in memory hierarchy?</p>
    <p>Key/value pair interface implemented in hardware/firmware?</p>
    <p>First class object store with additional metadata?</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>With a little secret sauce, NAND flash becomes interesting</p>
    <p>Secret sauce includes hardware, firmware, and possibly device driver</p>
    <p>No need for flash to sit behind traditional mass storage bus</p>
    <p>Delegating storage management to flash vendors hardware/software:</p>
    <p>Allows simplification of system software</p>
    <p>Simulatenously provides opportunity for improved performance</p>
    <p>Does not require changes to storage interfaces or protocols</p>
    <p>There may be benefit to innovation in the storage interface</p>
    <p>Allows vendors to improve the secret sauce independently</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
    <p>David Flynn at FusionIO</p>
    <p>Garrett Swart at Oracle</p>
  </div>
</Presentation>
