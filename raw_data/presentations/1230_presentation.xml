<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>PinK: High-speed In-storage Key-value Store with Bounded Tails</p>
    <p>Junsu Im, Jinwook Bae, Chanwoo Chung*,</p>
    <p>Arvind*, and Sungjin Lee</p>
    <p>Daegu Gyeongbuk Institute of Science &amp; Technology (DGIST)</p>
    <p>*Massachusetts Institute of Technology (MIT)</p>
    <p>DATA-INTENSIVE</p>
    <p>COMPUTING SYSTEMS</p>
    <p>LABORATORY</p>
  </div>
  <div class="page">
    <p>Key-Value store (KVS) has become a necessary infrastructure</p>
    <p>Key-Value Store is Everywhere!</p>
    <p>Web indexing, Caching, Storage systems  Algorithm  SILK (ATC19),</p>
    <p>Dostoevsky (SIGMOD18)</p>
    <p>Monkey (SIGMOD17)</p>
    <p>System  FlashStore (VLDB10)  Wisckey (FAST16)  LOCS (Eurosys14)</p>
    <p>Architecture  Bluecache (VLDB16)</p>
  </div>
  <div class="page">
    <p>Key-Value Interface</p>
    <p>KV-SSD Device Driver</p>
    <p>KV-SSD</p>
    <p>Host KVS Engine</p>
    <p>Block Device Driver</p>
    <p>Block-SSD</p>
    <p>Key-Value (KV) Storage Device</p>
    <p>Web indexing, Caching, Storage systems</p>
    <p>Offloading KVS functionality</p>
    <p>Fewer Host Resources Low Latency High Throughput</p>
    <p>capacitior</p>
  </div>
  <div class="page">
    <p>Key-Value Interface</p>
    <p>KV-SSD Device Driver</p>
    <p>KV-SSD</p>
    <p>Host KVS Engine</p>
    <p>Block Device Driver</p>
    <p>Block-SSD</p>
    <p>Key-Value (KV) Storage Device</p>
    <p>Web indexing, Caching, Storage systems</p>
    <p>Offloading KVS functionality</p>
    <p>Fewer Host Resources Low Latency High Throughput</p>
    <p>Academia  LightStore (ASPLOS19),</p>
    <p>KV-SSD (SYSTOR19), iLSM-SSD(MASCOTS19) KAML (HPCA17), NVMKV(ATC15), Bluecache (VLDB16)</p>
    <p>Industry  Samsungs KV-SSD</p>
  </div>
  <div class="page">
    <p>1. Limited DRAM resource</p>
    <p>SSDs usually have DRAM as much as 0.1% of NAND for indexing!</p>
    <p>Logical block: 4KB &gt; KV-pair: 1KB on average</p>
    <p>DRAM scalability slower than NAND!</p>
    <p>Key Challenges of Designing KV-SSD</p>
    <p>NAND Scalability DRAM Scalability</p>
    <p>DRAM</p>
    <p>DRAM</p>
    <p>Technology and Cost Trends at Advanced Nodes, 2020,</p>
    <p>https://semiwiki.com/wp-content/uploads/2020/03/Lithovision-2020.pdf</p>
  </div>
  <div class="page">
    <p>Key Challenges of Designing KV-SSD (Cont.)</p>
    <p>Which algorithm is better for KV-SSD with these limitations, Hash or Log-structured Merge-tree (LSM-tree)?</p>
    <p>2. Limited CPU performance</p>
    <p>SSDs have low power CPU (ARM based)</p>
    <p>x86 CPU ARM CPU</p>
  </div>
  <div class="page">
    <p>Samsung KV-SSD prototype</p>
    <p>hash-based KV-SSD*</p>
    <p>Benchmark</p>
    <p>KV-SSD: KVBench**, 32B key and 1KB value read request</p>
    <p>Block-SSD: FIO,</p>
    <p>Experiments using Hash-based KV-SSD</p>
    <p>What is the reason?</p>
    <p>/ / / / / /</p>
    <p>Long tail latency Performance drop</p>
    <p>*KV-PM983, **Samsung KV-SSD benchmark tool</p>
  </div>
  <div class="page">
    <p>Problem of Hash-based KV-SSD</p>
    <p>Full key (32B) Pointer to value (4B)</p>
    <p>Signature (2B) Pointer to KV (4B)</p>
    <p>SSD: 4TB, DRAM:4GB</p>
    <p>KAML (HPCA17)</p>
    <p>Flashstore (VLDB10)</p>
    <p>Key: 32B, Value: 1KB</p>
    <p>Value</p>
    <p>Full key and Value</p>
    <p>Hash bucket</p>
  </div>
  <div class="page">
    <p>probing</p>
    <p>In-flash hash buckets</p>
    <p>cached hash buckets</p>
    <p>Flash Access</p>
    <p>Read other KV-pair</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Problem of Hash-based KV-SSD Get (key 7)</p>
    <p>Bucket</p>
    <p>Hash Function</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket 5</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket</p>
    <p>Signature Ptr</p>
    <p>Bucket 9</p>
    <p>Signature Ptr</p>
    <p>KEY: 7,Value</p>
    <p>Bucket 10</p>
    <p>Signature Ptr</p>
    <p>LRU Cache</p>
    <p>Bucket 10 Signature: 1000</p>
    <p>Signature Collision</p>
    <p>Cache miss Performance Drop</p>
    <p>Long tail latency</p>
    <p>KEY: 10, ValueKEY:16, Value</p>
    <p>Key is not 7Key is not 7</p>
  </div>
  <div class="page">
    <p>Another Option LSM-tree</p>
    <p>Low DRAM requirement</p>
    <p>No collision</p>
    <p>Easy to serve range query</p>
    <p>LSM-tree?</p>
    <p>Is the LSM-tree really good enough?</p>
  </div>
  <div class="page">
    <p>Problem of LSM-tree-based KV-SSD</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Value Value Value Indices</p>
    <p>Bloom filter</p>
    <p>Indices</p>
    <p>Bloom filter</p>
    <p>Level 0: Memtable</p>
    <p>Indices</p>
    <p>Bloom filter</p>
    <p>Level 1 Level 2 Level h</p>
    <p>Get (key 7)</p>
    <p>Indices</p>
    <p>fh(7)</p>
    <p>V1 V2 V4 V8</p>
    <p>Indices</p>
    <p>fh(7)</p>
    <p>V1 V3 V11 V12</p>
    <p>Indices</p>
    <p>fh(7)</p>
    <p>V4 V5 V6 V7</p>
    <p>no key 7 : false positive no key 7 : false positive finally key 7 found</p>
    <p>1. Long tail latency! In the worst case, h-1 flash accesses for 1 KV</p>
    <p>(h = height of LSM-tree)</p>
    <p>pass pass pass</p>
  </div>
  <div class="page">
    <p>2. CPU overhead!</p>
    <p>Merge sort in compaction</p>
    <p>Building bloom filters</p>
    <p>3. I/O overhead!</p>
    <p>Compaction I/O added by LSM-tree</p>
    <p>Problem of LSM-tree-based KV-SSD</p>
    <p>ARM CPU</p>
    <p>Level N</p>
    <p>Level N+1 6 5 4 3 2 1</p>
    <p>New Level N+1</p>
    <p>Bloom filter</p>
  </div>
  <div class="page">
    <p>Experiments using LSM-tree-based KV-SSD</p>
    <p>Lightstore*: LSM-tree-based KV-SSD</p>
    <p>Key-value separation (Wisckey**) and Bloom filter (Monkey***)</p>
    <p>Benchmark</p>
    <p>Lightstore: YCSB-LOAD and YCSB-C (Read only), 32B key and 1KB value</p>
    <p>YCSB-CCompaction time-breakdown</p>
    <p>Long tail latency</p>
    <p>*ASPLOS19, **FAST16, ***SIGMOD17</p>
  </div>
  <div class="page">
    <p>PinK : New LSM-tree-based KV-SSD</p>
    <p>Long tail latency?</p>
    <p>Using Level-pinning</p>
    <p>CPU overhead?</p>
    <p>No Bloom filter</p>
    <p>HW accelerator for compaction</p>
    <p>I/O overhead?</p>
    <p>Reducing compaction I/O by level-pinning</p>
    <p>Optimizing GC by reinserting valid data to LSM-tree</p>
    <p>L0</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>L0</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Bloom filter</p>
    <p>Level N</p>
    <p>Level N+1 Level N+1</p>
    <p>Level N</p>
    <p>Level N+1</p>
    <p>Level N+1</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>PinK</p>
    <p>Overview of LSM-tree in PinK</p>
    <p>Bounding tail latency</p>
    <p>Memory requirement</p>
    <p>Reducing search overhead</p>
    <p>Reducing compaction I/O</p>
    <p>Reducing sorting time</p>
    <p>Experiments</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Level list (sorted array)</p>
    <p>Skiplist</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Level 0 KV KV KV KV</p>
    <p>PinK is based on key-value separated LSM-tree</p>
    <p>Overview of LSM-tree in PinK</p>
    <p>Meta segment area Data segment area</p>
    <p>Pointer to KVMeta segment Data segment</p>
    <p>Level 1 2 23</p>
    <p>Level 2</p>
    <p>Level h-1</p>
    <p>Start key</p>
    <p>Address pointer</p>
  </div>
  <div class="page">
    <p>Bounding Tail Latency</p>
    <p>GET</p>
    <p>LSM-tree: # of Levels 5</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>GET</p>
    <p>PinK</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Level list</p>
    <p>Meta segment</p>
    <p>Bloom filter</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>In worst case, 1 flash access!</p>
    <p>In worst case, 4 flash access!</p>
    <p>LSM-tree with bloom filter</p>
    <p>Memory usage?</p>
    <p>Binary search</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>L4</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>L4</p>
  </div>
  <div class="page">
    <p>4TB SSD, 4GB DRAM (32B key, 1KB value)  Total # of levels: 5</p>
    <p>Memory Requirement</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Level list</p>
    <p>Meta segment</p>
    <p>KV KV KV KVSkip list (L0) 8MB</p>
    <p>Only one flash access for indexing</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>L4</p>
  </div>
  <div class="page">
    <p>Fractional cascading</p>
    <p>Reducing Search Overhead</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>search complexity is</p>
    <p>Binary search on overlapped range</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>(2 log()) ( log())</p>
    <p>Burdensome!</p>
    <p>Binary search</p>
    <p>Range pointer</p>
    <p>h</p>
    <p>T</p>
    <p>T</p>
    <p>T</p>
  </div>
  <div class="page">
    <p>Reducing Search Overhead</p>
    <p>Prefix</p>
    <p>Less compare overhead</p>
    <p>Cache efficient search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search</p>
    <p>Binary search on same prefix</p>
    <p>Binary search on keys</p>
    <p>Prefix (4B)</p>
    <p>Key (32B) Ptr (4B)</p>
    <p>Prefix and range pointer memory usage: about 10% of level list</p>
  </div>
  <div class="page">
    <p>Reducing Compaction I/O</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>PinK without level-pinning PinK with level-pinning</p>
    <p>Update level list</p>
    <p>Update level list</p>
    <p>Full Full</p>
    <p>Burdensome!</p>
    <p>capacitior</p>
  </div>
  <div class="page">
    <p>Reducing Sorting Time</p>
    <p>ARM CPU</p>
    <p>PinK</p>
    <p>New level list of Ln+1</p>
    <p>Ln</p>
    <p>Ln+1 Key Comparator</p>
    <p>(==, &gt;, &lt;)</p>
    <p>Ln Meta segment addresses</p>
    <p>Ln+1 Meta segment addresses</p>
    <p>New address for Meta segments</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>DRAM</p>
    <p>Flash Read DRAM or Flash</p>
    <p>Write DRAM or Flash</p>
  </div>
  <div class="page">
    <p>Long tail latency?</p>
    <p>Using level-pinning</p>
    <p>CPU overhead?</p>
    <p>Removing Bloom filter</p>
    <p>Optimizing binary search</p>
    <p>Adopting HW accelerator</p>
    <p>I/O overhead?</p>
    <p>Reducing compaction I/O</p>
    <p>Optimizing GC by reinserting valid data to LSM-tree</p>
    <p>PinK Summary</p>
    <p>L0</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>L0</p>
    <p>L1</p>
    <p>L2</p>
    <p>L3</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Bloom filter</p>
    <p>Please refer to the paper!</p>
    <p>ARM CPU</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>PinK</p>
    <p>Experiments</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Zynq Ultrascale+ SoC</p>
    <p>(Quad-core ARM Cortex-A53</p>
    <p>with FPGA)</p>
    <p>Raw NAND</p>
    <p>Flash chips</p>
    <p>(256GB)</p>
    <p>Expansion Card</p>
    <p>Connectors</p>
    <p>Artix7 FPGA</p>
    <p>Xilinx ZCU102</p>
    <p>Custom</p>
    <p>Flash Card</p>
    <p>Custom KV-SSD Prototype and Setup</p>
    <p>All algorithms for KV-SSD were implemented on ZCU102 board</p>
    <p>For fast experiments: 64GB SSD, 64 MB DRAM (0.1% of NAND capacity)</p>
    <p>Xeon E5-2640</p>
    <p>(20 cores @ 2.4 GHz)</p>
    <p>KV-SSD platformClient Server</p>
  </div>
  <div class="page">
    <p>Benchmark Setup</p>
    <p>YCSB: 32B key, 1KB value</p>
    <p>Two phases</p>
    <p>Load: issue unique 44M KV pairs (44GB, 70% of total SSD)</p>
    <p>Run: issue 44M KV pairs following workload description</p>
    <p>Load A B C D E F</p>
    <p>R:W ratio 0:100 50:50 95:5 100:0 95:5 95:5 50:50(RMW)</p>
    <p>Query type Point Range read Point</p>
    <p>Request</p>
    <p>distribution Uniform Zipfian</p>
    <p>Latest (Highest locality)</p>
    <p>Zipfian</p>
  </div>
  <div class="page">
    <p>Testing Algorithms</p>
    <p>Hash</p>
    <p>8-bit signature: total 320MB buckets</p>
    <p>LSM-tree</p>
    <p>The conventional LSM-tree implementation based on Lightstore*</p>
    <p>Total 5 levels (1~4 level in flash)</p>
    <p>PinK</p>
    <p>Total 5 levels (pinning top 3 levels, one level in flash)</p>
    <p>PinK+HW</p>
    <p>Using HW accelerator for compaction based on PinK</p>
    <p>Hash LSM-tree PinK, PinK+HW</p>
    <p>Bloom filter (55MB)</p>
    <p>Level list + prefix, range (10MB)</p>
    <p>Level-pinning (54MB)</p>
    <p>*ASPLOS19</p>
  </div>
  <div class="page">
    <p>Experiment: Throughput</p>
    <p>Read Only</p>
  </div>
  <div class="page">
    <p>Experiment: Latency</p>
  </div>
  <div class="page">
    <p>Experiment: Impact of Level-pinning</p>
  </div>
  <div class="page">
    <p>Settings  PinK (NO-OPT): PinK without prefix and range pointer</p>
    <p>Benchmark: YCSB-Load and YCSB-C</p>
    <p>Experiment: Search Optimization</p>
  </div>
  <div class="page">
    <p>LSM-tree PinK</p>
    <p>Benchmark: YCSB-C</p>
    <p># of total levels: 4 ~ 8</p>
    <p>PinK</p>
    <p># of levels: 4, 5</p>
    <p>Unpinned-level: 1</p>
    <p># of levels: 6, 7</p>
    <p>Unpinned-level: 2</p>
    <p># of levels:8</p>
    <p>Unpinned-level: 3</p>
    <p>Experiment: Level-pinning on Higher LSM-tree</p>
    <p>Same memory</p>
    <p>Bad write performance</p>
    <p>Good write performance Bad read performance</p>
    <p>Good write performance</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>PinK</p>
    <p>Experiments</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Since the conventional KV-SSDs algorithms did not consider the embedded systems limitations well, they have suffered from long tail latency and throughput degradation</p>
    <p>PinK</p>
    <p>Pinning KV indices of top levels of LSM-tree to DRAM to reduce latency</p>
    <p>Using HW accelerator for compaction sorting</p>
    <p>Benefits</p>
    <p>99 percentile tail latency: 73%</p>
    <p>Average latency: 42%</p>
    <p>Throughput : 37%</p>
  </div>
  <div class="page">
    <p>Thank You !</p>
    <p>Junsu Im (junsu_im@dgist.ac.kr)</p>
  </div>
</Presentation>
