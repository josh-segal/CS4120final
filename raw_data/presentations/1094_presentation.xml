<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Understanding Ephemeral Storage for</p>
    <p>Serverless Analytics</p>
    <p>Ana Klimovic*, Yawen Wang*, Christos Kozyrakis*, Patrick Stuedi+, Jonas Pfefferle+, Animesh Trivedi+</p>
    <p>*Stanford University, +IBM Research</p>
    <p>USENIX ATC 2018</p>
  </div>
  <div class="page">
    <p>Introduction  Serverless computing enables launching short-lived</p>
    <p>tasks with high elasticity and fine-grain resource billing</p>
    <p>This makes serverless computing appealing for interactive analytics</p>
  </div>
  <div class="page">
    <p>Introduction  Serverless computing enables launching short-lived</p>
    <p>tasks with high elasticity and fine-grain resource billing</p>
    <p>This makes serverless computing appealing for interactive analytics</p>
    <p>The challenge: tasks (lambdas) need an efficient way to communicate intermediate results</p>
    <p>ephemeral data</p>
  </div>
  <div class="page">
    <p>In traditional analytics...</p>
    <p>mapper 0</p>
    <p>mapper 1</p>
    <p>mapper 2</p>
    <p>mapper 3</p>
    <p>reducer 0</p>
    <p>reducer 1</p>
    <p>Ephemeral data is exchanged directly between tasks</p>
  </div>
  <div class="page">
    <p>In traditional analytics...</p>
    <p>mapper 0</p>
    <p>mapper 1</p>
    <p>mapper 2</p>
    <p>mapper 3</p>
    <p>reducer 0</p>
    <p>reducer 1</p>
    <p>Ephemeral data is exchanged directly between tasks</p>
  </div>
  <div class="page">
    <p>Direct communication between lambdas is difficult:</p>
    <p>Lambdas are short-lived and stateless  Users have no control over lambda scheduling</p>
    <p>In serverless analytics...</p>
  </div>
  <div class="page">
    <p>Direct communication between lambdas is difficult:</p>
    <p>Lambdas are short-lived and stateless  Users have no control over lambda scheduling</p>
    <p>In serverless analytics...</p>
    <p>mapper 0</p>
    <p>mapper 1</p>
    <p>mapper 2</p>
    <p>mapper 3</p>
    <p>reducer 0</p>
    <p>reducer 1</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>The natural approach is to share data through a common data store</p>
    <p>In serverless analytics...</p>
  </div>
  <div class="page">
    <p>The natural approach is to share data through a common data store</p>
    <p>In serverless analytics...</p>
    <p>reducer 0</p>
    <p>reducer 1</p>
    <p>mapper 0</p>
    <p>mapper 1</p>
    <p>mapper 2</p>
    <p>mapper 3</p>
  </div>
  <div class="page">
    <p>The natural approach is to share data through a common data store</p>
    <p>In serverless analytics...</p>
    <p>reducer 0</p>
    <p>reducer 1</p>
    <p>However, it is not clear whether existing storage systems are a good fit for ephemeral data sharing.</p>
  </div>
  <div class="page">
    <p>Questions:</p>
  </div>
  <div class="page">
    <p>Ephemeral Data Capacity Ephemeral I/O Throughput: Write (dotted), Read (solid)</p>
    <p>Time (s)</p>
    <p>T o</p>
    <p>ta l G</p>
    <p>B /s</p>
    <p>Application Type</p>
    <p>Distributed Compilation</p>
    <p>High throughput and IOPS due to high parallelism: lambdas each compile independent files</p>
    <p>Archiving and linking lambdas are serialized as they depend on previous lambdas  low parallelism, low I/O rate</p>
  </div>
  <div class="page">
    <p>Ephemeral Data Capacity Ephemeral I/O Throughput: Write (dotted), Read (solid)</p>
    <p>Time (s)</p>
    <p>T o</p>
    <p>ta l G</p>
    <p>B /s</p>
    <p>Application Type</p>
    <p>Distributed Compilation</p>
    <p>MapReduce</p>
    <p>High throughput due to high I/O intensity and parallelism</p>
    <p>(up to 7.5 GB/s with 500 concurrent lambdas)</p>
  </div>
  <div class="page">
    <p>Ephemeral Data Capacity Ephemeral I/O Throughput: Write (dotted), Read (solid)</p>
    <p>T o</p>
    <p>ta l G</p>
    <p>B /s</p>
    <p>Application Type</p>
    <p>Distributed Compilation</p>
    <p>MapReduce</p>
    <p>Video Analytics</p>
  </div>
  <div class="page">
    <p>Wide range of I/O sizes (bytes to 100s of MBs)</p>
    <p>Ephemeral I/O Size</p>
    <p>Thus, an ephemeral storage system should support high throughput and low latency.</p>
  </div>
  <div class="page">
    <p>We focus on three different categories:</p>
  </div>
  <div class="page">
    <p>We focus on three different categories:</p>
  </div>
  <div class="page">
    <p>We focus on three different categories:</p>
  </div>
  <div class="page">
    <p>We focus on three different categories:</p>
  </div>
  <div class="page">
    <p>Latency sensitivity</p>
    <p>Distributed compilation job shows some sensitivity to latency due to small I/Os</p>
    <p>As concurrency increases, job runtime becomes</p>
    <p>dominated by the sequential portion of the application</p>
  </div>
  <div class="page">
    <p>The impact of application parallelism</p>
    <p>Distributed compilation (gg-cmake) with up to 650 concurrent lambdas using S3</p>
    <p>Lambda #</p>
    <p>Ephemeral read I/O Compute Ephemeral write I/O</p>
    <p>Figure based on Fig. 6 in A thunk to remember: make -j1000 (and other jobs) on functions-as-a-service infrastructure (preprint). Fouladi, S., et al.</p>
  </div>
  <div class="page">
    <p>The impact of application parallelism</p>
    <p>Distributed compilation (gg-cmake) with up to 650 concurrent lambdas using Redis</p>
    <p>Each lambda spends less time on I/O</p>
    <p>But job runtime is the same as with S3</p>
    <p>Lambda #</p>
    <p>Ephemeral read I/O Compute Ephemeral write I/O</p>
    <p>Figure based on Fig. 6 in A thunk to remember: make -j1000 (and other jobs) on functions-as-a-service infrastructure (preprint). Fouladi, S., et al.</p>
  </div>
  <div class="page">
    <p>The impact of application parallelism</p>
    <p>Distributed compilation (gg-cmake) with up to 650 concurrent lambdas using Redis</p>
    <p>Runtime is limited by dependencies on compute-bound lambdas</p>
    <p>Lambda #</p>
    <p>Ephemeral read I/O Compute Ephemeral write I/O</p>
    <p>Figure based on Fig. 6 in A thunk to remember: make -j1000 (and other jobs) on functions-as-a-service infrastructure (preprint). Fouladi, S., et al.</p>
  </div>
  <div class="page">
    <p>The impact of application parallelism</p>
    <p>Distributed compilation (gg-cmake) with up to 650 concurrent lambdas using Redis</p>
    <p>Lambda #</p>
    <p>Applications with inherently limited parallelism have lower ephemeral</p>
    <p>I/O throughput demands</p>
    <p>Ephemeral read I/O Compute Ephemeral write I/O</p>
    <p>Figure based on Fig. 6 in A thunk to remember: make -j1000 (and other jobs) on functions-as-a-service infrastructure (preprint). Fouladi, S., et al.</p>
  </div>
  <div class="page">
    <p>High I/O intensity</p>
    <p>MapReduce sort (100 GB) demands high throughput</p>
    <p>Input/Output I/O Compute Ephemeral data I/O</p>
    <p>Original input/output data I/O</p>
  </div>
  <div class="page">
    <p>High I/O intensity</p>
    <p>MapReduce sort (100 GB) demands high throughput</p>
    <p>S3 does not provide sufficient throughput</p>
    <p>S3 also does not provide sufficient IOPS scalability</p>
    <p>Compute Ephemeral data I/O</p>
    <p>Original input/output data I/O</p>
  </div>
  <div class="page">
    <p>High I/O intensity</p>
    <p>MapReduce sort (100 GB) demands high throughput</p>
    <p>Compute Ephemeral data I/O</p>
    <p>Original input/output data I/O</p>
    <p>Similar performance with Flash and DRAM</p>
  </div>
  <div class="page">
    <p>High I/O and compute intensity</p>
    <p>Video analytics has both high I/O and compute intensity</p>
    <p>Similar performance with Flash and DRAM</p>
  </div>
  <div class="page">
    <p>Compare throughput:capacity ratios of DRAM, Flash, HDD</p>
    <p>DRAM: 20 GB/s / 64 GB = 0.3</p>
    <p>Flash: 3.2 GB/s / 500 GB = 0.006</p>
    <p>Disk: 0.7 GB/s / 6000 GB = 0.0001</p>
    <p>T o</p>
    <p>ta l e</p>
    <p>p h</p>
    <p>e m</p>
    <p>e ra</p>
    <p>l G B</p>
    <p>/s p</p>
    <p>e r</p>
    <p>G B</p>
  </div>
  <div class="page">
    <p>Compare throughput:capacity ratios of DRAM, Flash, HDD</p>
    <p>DRAM: 20 GB/s / 64 GB = 0.3</p>
    <p>Flash: 3.2 GB/s / 500 GB = 0.006</p>
    <p>Disk: 0.7 GB/s / 6000 GB = 0.0001</p>
    <p>Application throughput:capacity ratios are in DRAM - Flash regimes</p>
    <p>T o</p>
    <p>ta l e</p>
    <p>p h</p>
    <p>e m</p>
    <p>e ra</p>
    <p>l G B</p>
    <p>/s p</p>
    <p>e r</p>
    <p>G B</p>
  </div>
  <div class="page">
    <p>Compare throughput:capacity ratios of DRAM, Flash, HDD</p>
    <p>DRAM: 20 GB/s / 64 GB = 0.3</p>
    <p>Flash: 3.2 GB/s / 500 GB = 0.006</p>
    <p>Disk: 0.7 GB/s / 6000 GB = 0.0001</p>
    <p>Application throughput:capacity ratios are in DRAM - Flash regimes</p>
    <p>Using Flash vs. DRAM, jobs achieve similar performance</p>
    <p>at lower cost per bit</p>
    <p>T o</p>
    <p>ta l e</p>
    <p>p h</p>
    <p>e m</p>
    <p>e ra</p>
    <p>l G B</p>
    <p>/s p</p>
    <p>e r</p>
    <p>G B</p>
  </div>
  <div class="page">
    <p>Putting it all together...</p>
    <p>Ephemeral storage wishlist for serverless analytics:</p>
    <p>Existing systems provide some but not all of these properties</p>
    <p>High throughput and IOPS</p>
    <p>Low latency, particularly important for small requests</p>
    <p>Fine-grain, elastic scaling to adapt to elastic application load</p>
    <p>Automatic rightsizing of resource allocations</p>
    <p>Low cost, pay-what-you-use</p>
  </div>
  <div class="page">
    <p>Our analysis motivates the design of an ephemeral storage service that supports automatic, fine-grain storage capacity and throughput allocation</p>
    <p>Ephemeral I/O requirements depend on a jobs latency sensitivity, inherent parallelism and its I/O vs. compute intensity</p>
    <p>Flash is an appealing storage media for ephemeral I/O performance-cost requirements</p>
    <p>Conclusion</p>
  </div>
</Presentation>
