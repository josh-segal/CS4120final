<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MICA: A Holistic Approach to Fast In-Memory Key-Value Storage</p>
    <p>Hyeontaek Lim1</p>
    <p>Dongsu Han,2 David G. Andersen,1 Michael Kaminsky3</p>
  </div>
  <div class="page">
    <p>Goal: Fast In-Memory Key-Value Store</p>
    <p>Improve per-node performance (op/sec/node)</p>
    <p>Less expensive</p>
    <p>Easier hotspot mitigation</p>
    <p>Lower latency for multi-key queries</p>
    <p>Target: small key-value items (fit in single packet)</p>
    <p>Non-goals: cluster architecture, durability</p>
  </div>
  <div class="page">
    <p>Q: How Good (or Bad) are Current Systems?</p>
    <p>Workload: YCSB [SoCC 2010]</p>
    <p>Single-key operations</p>
    <p>In-memory storage</p>
    <p>Logging turned off in our experiments</p>
    <p>End-to-end performance over the network</p>
    <p>Single server node</p>
  </div>
  <div class="page">
    <p>Memcached RAMCloud MemC3 Masstree MICA</p>
    <p>End-to-End Performance Comparison</p>
    <p>Throughput (M operations/sec)</p>
    <p>- Published results; Logging on RAMCloud/Masstree</p>
    <p>- Using Intel DPDK (kernel bypass I/O); No logging</p>
    <p>- (Write-intensive workload)</p>
  </div>
  <div class="page">
    <p>Memcached RAMCloud MemC3 Masstree MICA</p>
    <p>End-to-End Performance Comparison</p>
    <p>Throughput (M operations/sec)</p>
    <p>Performance collapses under heavy writes</p>
    <p>- Published results; Logging on RAMCloud/Masstree</p>
    <p>- Using Intel DPDK (kernel bypass I/O); No logging</p>
    <p>- (Write-intensive workload)</p>
  </div>
  <div class="page">
    <p>Memcached RAMCloud MemC3 Masstree MICA</p>
    <p>End-to-End Performance Comparison</p>
    <p>Throughput (M operations/sec)</p>
    <p>Maximum packets/sec attainable using UDP 4x</p>
  </div>
  <div class="page">
    <p>MICA Approach</p>
    <p>MICA: Redesigning in-memory key-value storage</p>
    <p>Applies new SW architecture and data structures to general-purpose HW in a holistic way</p>
    <p>Client CPU</p>
    <p>NIC CPU</p>
    <p>Memory</p>
    <p>Server node</p>
  </div>
  <div class="page">
    <p>Parallel Data Access</p>
    <p>Modern CPUs have many cores (8, 15, )</p>
    <p>How to exploit CPU parallelism efficiently?</p>
    <p>Client CPU</p>
    <p>NIC CPU</p>
    <p>Memory</p>
    <p>Server node</p>
  </div>
  <div class="page">
    <p>Parallel Data Access Schemes</p>
    <p>CPU core</p>
    <p>CPU core Memory</p>
    <p>CPU core</p>
    <p>CPU core</p>
    <p>Partition</p>
    <p>Partition</p>
    <p>Concurrent Read Concurrent Write</p>
    <p>Exclusive Read Exclusive Write</p>
    <p>+ Good load distribution - Limited CPU scalability</p>
    <p>(e.g., synchronization) - Cross-NUMA latency</p>
    <p>+ Good CPU scalability - Potentially low performance under skewed workloads</p>
  </div>
  <div class="page">
    <p>In MICA, Exclusive Outperforms Concurrent</p>
    <p>Throughput (Mops)</p>
    <p>Concurrent Access Exclusive Access</p>
    <p>Uniform, 50% GET</p>
    <p>Uniform, 95% GET</p>
    <p>Skewed, 50% GET</p>
    <p>Skewed, 95% GET</p>
    <p>End-to-end performance with kernel bypass I/O</p>
  </div>
  <div class="page">
    <p>Request Direction</p>
    <p>Sending requests to appropriate CPU cores for better data access locality</p>
    <p>Exclusive access benefits from correct delivery</p>
    <p>Each request must be sent to corresp. partitions core</p>
    <p>Client CPU</p>
    <p>NIC CPU</p>
    <p>Memory</p>
    <p>Server node</p>
  </div>
  <div class="page">
    <p>Request Direction Schemes</p>
    <p>Client CPU</p>
    <p>CPU</p>
    <p>Server node</p>
    <p>Client</p>
    <p>Classification using 5-tuple</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>Server node</p>
    <p>Classification depends on request content</p>
    <p>Key 1</p>
    <p>Key 1</p>
    <p>Key 2</p>
    <p>Key 2 Client</p>
    <p>Client NIC NIC</p>
    <p>+ Good locality for flows (e.g., HTTP over TCP)</p>
    <p>- Suboptimal for small key-value processing</p>
    <p>+ Good locality for key access - Client assist or special HW support needed for efficiency</p>
    <p>Flow-based Affinity Object-based Affinity</p>
  </div>
  <div class="page">
    <p>Crucial to Use NIC HW for Request Direction</p>
    <p>Throughput (Mops)</p>
    <p>Using exclusive access for parallel data access</p>
    <p>Request direction done solely by software</p>
    <p>Client-assisted hardware-based request direction</p>
    <p>Uniform</p>
    <p>Skewed</p>
  </div>
  <div class="page">
    <p>Key-Value Data Structures</p>
    <p>Significant impact on key-value processing speed</p>
    <p>New design required for very high op/sec for both read and write</p>
    <p>Cache and store modes</p>
    <p>Client CPU</p>
    <p>NIC CPU</p>
    <p>Memory</p>
    <p>Server node</p>
  </div>
  <div class="page">
    <p>MICAs Cache Data Structures</p>
    <p>Each partition has:</p>
    <p>Circular log (for memory allocation)</p>
    <p>Lossy concurrent hash index (for fast item access)</p>
    <p>Exploit Memcached-like cache semantics</p>
    <p>Lost data is easily recoverable (not free, though)</p>
    <p>Favor fast processing</p>
    <p>Provide good memory efficiency &amp; item eviction</p>
  </div>
  <div class="page">
    <p>Circular Log</p>
    <p>Allocates space for key-value items of any length</p>
    <p>Conventional logs + Circular queues</p>
    <p>Simple garbage collection/free space defragmentation</p>
    <p>New item is appended at tail</p>
    <p>Head Tail</p>
    <p>Head Tail</p>
    <p>Evict oldest item at head (FIFO)</p>
    <p>Insufficient space for new item?</p>
    <p>(fixed log size)</p>
    <p>Support LRU by reinserting recently accessed items</p>
  </div>
  <div class="page">
    <p>Lossy Concurrent Hash Index</p>
    <p>Indexes key-value items stored in the circular log</p>
    <p>Set-associative table</p>
    <p>Full bucket? Evict oldest entry from it</p>
    <p>Fast indexing of new key-value items</p>
    <p>Key,Val</p>
    <p>bucket 0</p>
    <p>bucket 1</p>
    <p>bucket N-1</p>
    <p>Circular log</p>
    <p>Hash index hash(Key)</p>
  </div>
  <div class="page">
    <p>MICAs Store Data Structures</p>
    <p>Required to preserve stored items</p>
    <p>Achieve similar performance by trading memory</p>
    <p>Circular log -&gt; Segregated fits</p>
    <p>Lossy index -&gt; Lossless index (with bulk chaining)</p>
    <p>See our paper for details</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Going back to end-to-end evaluation</p>
    <p>Throughput &amp; latency characteristics</p>
  </div>
  <div class="page">
    <p>Throughput Comparison</p>
    <p>Throughput (Mops)</p>
    <p>End-to-end performance with kernel bypass I/O</p>
    <p>Memcached MemC3 RAMCloud Masstree MICA</p>
    <p>Uniform, 50% GET</p>
    <p>Uniform, 95% GET</p>
    <p>Skewed, 50% GET</p>
    <p>Skewed, 95% GET</p>
    <p>Bad at high write ratios</p>
    <p>Similar performance regardless of skew/write</p>
    <p>Large performance</p>
    <p>gap</p>
  </div>
  <div class="page">
    <p>Throughput-Latency on Ethernet</p>
    <p>Average latency (s)</p>
    <p>Throughput (Mops)</p>
    <p>Original Memcached MICA</p>
    <p>Original Memcached using standard socket I/O; both use UDP</p>
  </div>
  <div class="page">
    <p>MICA</p>
    <p>Redesigning in-memory key-value storage</p>
    <p>65.6+ Mops/node even for heavy skew/write</p>
    <p>Source code: github.com/efficient/mica</p>
    <p>Client CPU</p>
    <p>NIC CPU</p>
    <p>Memory</p>
    <p>Server node</p>
  </div>
</Presentation>
