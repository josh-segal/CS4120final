<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning with Probabilistic Features for Improved Pipeline Models</p>
    <p>Razvan C. Bunescu Electrical Engineering and Computer Science</p>
    <p>Ohio University Athens, OH</p>
    <p>bunescu@ohio.edu</p>
    <p>EMNLP, October 2008</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>Semantic Role Labeling</p>
    <p>Named Entity Recognition</p>
    <p>Question Answering</p>
    <p>NLP systems often depend on the output of other NLP systems.</p>
  </div>
  <div class="page">
    <p>Traditional Pipeline Model: M1</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x )(, xzx )( xy</p>
    <p>)|(maxarg)(</p>
    <p>))(,,(maxarg)(</p>
    <p>)(</p>
    <p>)(</p>
    <p>xzpxz</p>
    <p>xzyxwxy</p>
    <p>xZz</p>
    <p>xYy</p>
    <p>The best annotation from one stage is used in subsequent stages.</p>
    <p>Problem: Errors propagate between pipeline stages!</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Model: M2</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x )(, xZx )( xy</p>
    <p>)(</p>
    <p>)(</p>
    <p>),,()|(),(</p>
    <p>),(maxarg)(</p>
    <p>xZz</p>
    <p>xYy</p>
    <p>zyxxzpyx</p>
    <p>yxwxy</p>
    <p>All possible annotationsfrom one stage are used in subsequent stages.</p>
    <p>Problem: Z(x) has exponential cardinality!</p>
    <p>probabilistic features</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Model: M2</p>
    <p>When original is are count features, it can be shown that:</p>
    <p>)(</p>
    <p>),,()|(),( xZz</p>
    <p>ii zyxxzpyx</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>Feature-wise formulation:</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xZyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
  </div>
  <div class="page">
    <p>An instance of feature i , i.e. the actual evidence used from example (x,y,z).</p>
    <p>Probabilistic Pipeline Model</p>
    <p>When original is are count features, it can be shown that:</p>
    <p>)(</p>
    <p>),,()|(),( xZz</p>
    <p>ii zyxxzpyx</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xZyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>Feature-wise formulation:</p>
  </div>
  <div class="page">
    <p>The set of all instances of feature i in (x,y,z), across all annotations zZ(x).</p>
    <p>Probabilistic Pipeline Model</p>
    <p>When original is are count features, it can be shown that:</p>
    <p>)(</p>
    <p>),,()|(),( xZz</p>
    <p>ii zyxxzpyx</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xZyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>Feature-wise formulation:</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>RB VBD</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>RB VBD</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
    <p>RB VBD</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>RB VBD</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>RB VBD</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>RB VBD</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>RB VBD</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>y~</p>
    <p>z~</p>
    <p>)|~( xzp</p>
    <p>Feature i  RB  VBD</p>
    <p>The set of feature instances Fi is:</p>
    <p>N(N-1) feature instances in Fi .</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
    <p>RB VBD</p>
  </div>
  <div class="page">
    <p>Example: POS Dependency Parsing</p>
    <p>the set of feature instances Fi has cardinality N(N-1).</p>
    <p>computing takes O(N|P|2) time using a constrained version of</p>
    <p>the forward-backward algorithm:</p>
    <p>Therefore, computing i takes O(N3|P|2) time.</p>
    <p>)|~( xzp</p>
    <p>)|VBD ,RB( is )|~( xttpxzp ji</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xZyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Model: M2</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xZyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>),(maxarg)( )(</p>
    <p>yxwxy xYy</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x )(, xZx )( xy</p>
    <p>All possible annotations from one stage are used in subsequent stages.</p>
    <p>polynomial time</p>
    <p>In general, the time complexity of computing i depends on the complexity of the evidence used by feature i. z</p>
    <p>~</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Model: M3</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xzyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>),(maxarg)( )(</p>
    <p>yxwxy xYy</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x )(, xzx )( xy</p>
    <p>The best annotationfrom one stage is used in subsequent stages, together with its probabilistic confidence:</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Model: M3</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xzyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>),(maxarg)( )(</p>
    <p>yxwxy xYy</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x )(, xzx )( xy</p>
    <p>The best annotationfrom one stage is used in subsequent stages, together with its probabilistic confidence:</p>
    <p>The set of instances of feature i using only the best annotation z</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Model: M3</p>
    <p>Like the traditional pipeline model M1, except that it uses the probabilistic confidence values associated with annotation features.</p>
    <p>More efficient than M2, but less accurate.</p>
    <p>Example: POS  Dependency Parsing  shows features generated by template ti  tj and their</p>
    <p>probabilities.</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>DT1 NNS2 RB3 VBD4 EX5 MD6 VB7 NNS8 IN9 DT10 NN11</p>
    <p>x:</p>
    <p>y: :z</p>
  </div>
  <div class="page">
    <p>Probabilistic Pipeline Models</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xzyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>),(maxarg)( )(</p>
    <p>yxwxy xYy</p>
    <p>nii yxyx ..1)],([),(</p>
    <p>))(,,()~,~,~(</p>
    <p>)|~( ),( xZyxFzyx</p>
    <p>i</p>
    <p>i</p>
    <p>xzpyx</p>
    <p>),(maxarg)( )(</p>
    <p>yxwxy xYy</p>
    <p>Model M2 Model M3</p>
  </div>
  <div class="page">
    <p>Two Applications</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x )( xz )( xy</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x</p>
    <p>)(1 xz</p>
    <p>Named Entity Recognition</p>
    <p>)( xy</p>
    <p>)(2 xz</p>
    <p>)(1 xz</p>
  </div>
  <div class="page">
    <p>Use MSTParser [McDonald et al. 2005]:  The score of a dependency tree  the sum of the edge scores:</p>
    <p>Feature templates use words and POS tags at positions u and v and their neighbors u  1 and v  1.</p>
    <p>Use CRF [Lafferty et al. 2001] POS tagger:  Compute probabilistic features using a constrained</p>
    <p>forward-backward procedure.  Example: feature titj has probability p(ti, tj)</p>
    <p>constrain the state transitions to pass through tags ti and tj. 22</p>
    <p>yvu</p>
    <p>vuxyx ),(),(</p>
    <p>)|~( xzp</p>
  </div>
  <div class="page">
    <p>Two approximations of model M2:  Model M2:</p>
    <p>Consider POS tags independent:  p(ti  RB,tj  VBD|x)  p(ti  RB|x)  p(tj  VBD|x)</p>
    <p>Ignore tags with low marginal probability:  p(ti)  1/(|P|)</p>
    <p>Model M2:</p>
    <p>Like M2, but use constrained forward-backward to compute marginal probabilities when the tag chunks are less than 4 tokens apart.</p>
  </div>
  <div class="page">
    <p>Train MSTParser on sections 2-21 of Pen WSJ Treebank using gold POS tagging.</p>
    <p>Test MST Parser on section 23, using POS tags from CRF tagger.</p>
    <p>Absolute error reduction of only 0.19% :  But POS tagger has a very high accuracy of 96.25%.</p>
    <p>Expect more substantial improvement when upstream stages in the pipeline are less accurate.</p>
    <p>M1 M2 (1) M2 (2) M2 (4) M2 (4)</p>
  </div>
  <div class="page">
    <p>Model NER as a sequence tagging problem using CRFs:</p>
    <p>The1 sailors2 mistakenly3 thought4 there5 must6 be7 diamonds8 in9 the10 soil11</p>
    <p>DT1 NNS2 RB3 VBD4 EX5 MD6 VB7 NNS8 IN9 DT10 NN11</p>
    <p>x:</p>
    <p>z2: z1:</p>
    <p>y: O I O O O O O O O O O</p>
    <p>Flat features: unigram, bigram and trigram that extend either left or right:  sailors, the sailors, sailors RB, sailors RB thought</p>
    <p>Tree features: unigram, bigram and trigram that extend in any direction in the undirected dependency tree:  sailors  thought, sailors  thought  RB, NNS  thought  RB,</p>
  </div>
  <div class="page">
    <p>Named Entity Recognition: Model M2</p>
    <p>Syntactic Parsing</p>
    <p>POS Tagging</p>
    <p>x</p>
    <p>)(1 xz</p>
    <p>Named Entity Recognition</p>
    <p>)( xy</p>
    <p>)(2 xz</p>
    <p>)(1 xz</p>
    <p>)|~(),~|~()|~,~()|~( 11221 xzpxzzpxzzpxzp</p>
    <p>)|,(),,|342()|~( 3232 xRBNNSpxRBNNSpxzp</p>
    <p>Probabilistic features:</p>
    <p>Example feature NNS2 thought4  RB3:</p>
  </div>
  <div class="page">
    <p>Named Entity Recognition: Model M3</p>
    <p>M3 is an approximation of M3 in which confidence scores are computed as follows:  Consider POS tagging and dependency parsing independent.  Consider POS tags independent.  Consider dependency arcs independent.  Example feature NNS2 thought4  RB3:</p>
    <p>Need to compute marginals p(uv|x).</p>
    <p>)|43()|42()|~(</p>
    <p>)|()|()|~(</p>
    <p>)|~()|~()|~(</p>
    <p>xpxpxzp</p>
    <p>xRBtpxNNStpxzp</p>
    <p>xzpxzpxzp</p>
  </div>
  <div class="page">
    <p>Probabilistic Dependency Features</p>
    <p>To compute probabilistic POS features, we used a constrained version of the forward-backward algorithm.</p>
    <p>To compute probabilistic dependency features, we use a constrained version of Eisners algorithm:  Compute normalized scores n(uv | x) using the softmax function:</p>
    <p>Transform scores n(uv|x) into probabilities p(uv|x) using isotonic regression [Zadrozny &amp; Elkan, 2002].</p>
    <p>)(</p>
    <p>),(</p>
    <p>),(</p>
    <p>),(</p>
    <p>),(</p>
    <p>xYy</p>
    <p>yxs</p>
    <p>yvuxYy</p>
    <p>yxs</p>
    <p>e</p>
    <p>e</p>
    <p>vuxn</p>
  </div>
  <div class="page">
    <p>Named Entity Recognition: Results</p>
    <p>Implemented the CRF models in MALLET [McCallum, 2002]  Trained and tested on the standard split from the ACE 2002 + 2003</p>
    <p>corpus (674 training, 97 testing).</p>
    <p>POS tagger and MSTParser were trained on sections 2-21 of WSJ Treebank  Isotonic regression for MSTParser on section 23.</p>
    <p>Model Tree Flat Tree+Flat</p>
    <p>M3 76.78 77.02 77.96</p>
    <p>M1 74.38 76.53 77.02</p>
    <p>Area under PR curve</p>
  </div>
  <div class="page">
    <p>Named Entity Recognition: Results</p>
    <p>M3 (probabilistic) vs. M1 (traditional) using tree features:</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Related Work</p>
    <p>A general method for improving the communication between consecutive stages in pipeline models:  based on computing expectations for count features.</p>
    <p>an efective method for associating probabilities with output substructures.</p>
    <p>adds polynomial time complexity to pipeline whenever the inference step at each stage is done in polynomial time.</p>
    <p>Can be seen as complementary to the sampling approach of [Finkel et al. 2006]:  approximate vs. exact in polynomial time.  used in testing vs. used in training and testing.</p>
  </div>
  <div class="page">
    <p>Future Work</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
