<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MapReduce Online</p>
    <p>Tyson Condie</p>
    <p>UC Berkeley</p>
    <p>Joint work with</p>
    <p>Neil Conway, Peter Alvaro, and Joseph M. Hellerstein (UC Berkeley)</p>
    <p>Khaled Elmeleegy and Russell Sears (Yahoo! Research)</p>
  </div>
  <div class="page">
    <p>MapReduce Programming Model</p>
    <p>Think datacentric  Apply a two step transformaMon to data sets</p>
    <p>Map step: Map(k1, v1) &gt; list(k2, v2)  Apply map funcMon to input records  Assign output records to groups</p>
    <p>Reduce step: Reduce(k2, list(v2)) &gt; list(v3)  Consolidate groups from the map step  Apply reduce funcMon to each group</p>
  </div>
  <div class="page">
    <p>MapReduce System Model  Sharednothing architecture  Tuned for massive data parallelism  Many maps operate on porMons of the input  Many reduces, each assigned specific groups</p>
    <p>Batchoriented computaMons over massive data  RunMmes range in minutes to hours  Execute on tens to thousands of machines  Failures common (fault tolerance crucial)</p>
    <p>Fault tolerance via operator restart since   Operators complete before producing any output  Atomic data exchange between operators</p>
  </div>
  <div class="page">
    <p>Life Beyond Batch</p>
    <p>MapReduce oXen used for analyMcs on streams of data that arrive con/nuously  Click streams, network traffic, web crawl data,</p>
    <p>Batch approach: buffer, load, process  High latency  Hard to scale for realMme analysis</p>
    <p>Online approach: run MR jobs con/nuously  Analyze data as it arrives</p>
  </div>
  <div class="page">
    <p>Online Query Processing  Two domains of interest (at massive scale):</p>
    <p>Blocking operators are a poor fit  Final answers only  No infinite streams</p>
    <p>Operators need to pipeline  BUT we must retain fault tolerance</p>
  </div>
  <div class="page">
    <p>A Brave New MapReduce World</p>
    <p>Pipelined MapReduce  Maps can operate on infinite data (Stream processing)  Reduces can export early answers (Online aggrega8on)</p>
    <p>Hadoop Online Prototype (HOP)  Preserves Hadoop interfaces and APIs  Pipelining fault tolerance model</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Wordcount Job</p>
    <p>Map step  Parse input into a series of words  For each word, output &lt;word, 1&gt;</p>
    <p>Reduce step  For each word, list of counts  Sum counts and output &lt;word, sum&gt;</p>
    <p>Combine step (op8onal)  Preaggregate map output  Same as the reduce step in wordcount</p>
  </div>
  <div class="page">
    <p>Master Client</p>
    <p>Submit wordcount</p>
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>schedule</p>
  </div>
  <div class="page">
    <p>Block 1</p>
    <p>HDFS</p>
    <p>Cat Rabbit</p>
    <p>Cat</p>
    <p>Rabbit</p>
    <p>Dog</p>
    <p>Turtle</p>
    <p>Map step</p>
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Cat, 1</p>
    <p>Rabbit, 1</p>
    <p>Apply map funcMon to the input block  Assign a group id (color) to output records  group id = hash(key) mod # reducers</p>
    <p>Cat, 1 Turtle, 1</p>
    <p>Rabbit, 1 Dog, 1</p>
  </div>
  <div class="page">
    <p>Group step (opMonal)  Sort map output by group id and key</p>
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Cat, 1 Cat, 1 Dog, 1</p>
    <p>Rabbit, 1 Rabbit, 1 Turtle, 1</p>
  </div>
  <div class="page">
    <p>Combine step (opMonal)</p>
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Apply combiner funcMon to map output o Usually reduces the output size</p>
    <p>Cat, 2 Dog, 1</p>
    <p>Rabbit, 2 Turtle, 1</p>
  </div>
  <div class="page">
    <p>Commit step</p>
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Cat, 2 Dog, 1</p>
    <p>Rabbit, 2 Turtle, 1</p>
    <p>Final output stored on local file system  Register file locaMon with TaskTracker</p>
    <p>Local FS</p>
  </div>
  <div class="page">
    <p>Master</p>
    <p>Workers</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Local FS</p>
    <p>Map finished Map output loca8on</p>
  </div>
  <div class="page">
    <p>Workers</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Local FS</p>
    <p>Shuffle step</p>
    <p>HTTP get</p>
    <p>Reduce tasks pull data from map output locaMons</p>
  </div>
  <div class="page">
    <p>Workers</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Group step (required)  When all sorted runs are received  mergesort runs (opMonally apply combiner)</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>Cat, 5,1,3,4, Dog, 1,4,2,5,</p>
    <p>Rabbit, 2,5,1,7, Turtle, 4,2,3,3,</p>
    <p>Map 1, Map 2, . . ., Map k</p>
    <p>Map 1, Map 2, . . ., Map k</p>
  </div>
  <div class="page">
    <p>Workers</p>
    <p>Reduce step  Call reduce funcMon on each &lt;key, list of values&gt;  Write final output to HDFS</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>HDFS</p>
    <p>Cat, 25 Dog, 14</p>
    <p>Rabbit, 23 Turtle, 16</p>
    <p>Cat, 5,1,3,4, Dog, 1,4,2,5,</p>
    <p>Rabbit, 2,5,1,7, Turtle, 4,2,3,3,</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Hadoop Online Prototype (HOP)</p>
    <p>Pipelining between operators  Data pushed from producers to consumers  Data transfer scheduled concurrently with operator computaMon</p>
    <p>HOP API  No changes required to exisMng clients</p>
    <p>Pig, Hive, Jaql sMll work + ConfiguraMon for pipeline/block modes + JobTracker accepts a series of jobs</p>
  </div>
  <div class="page">
    <p>Master</p>
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Schedule Schedule + Map loca8on (ASAP)</p>
    <p>pipeline request</p>
  </div>
  <div class="page">
    <p>Pipelining Data Unit</p>
    <p>IniMal design: pipeline eagerly (each record)  Prevents map side group and combine step  Map computaMon can block on network I/O</p>
    <p>Revised design: pipeline small sorted runs (spills)  Task thread: apply (map/reduce) funcMon, buffer output</p>
    <p>Spill thread: sort &amp; combine buffer, spill to a file  TaskTracker: service consumer requests</p>
  </div>
  <div class="page">
    <p>Simple AdapMve Policy</p>
    <p>Halt pipeline when  1. Unserviced spill files backup OR 2. EffecMve combiner</p>
    <p>Resume pipeline by first   merging &amp; combining accumulated spill files into a single file</p>
    <p>Map tasks adapMvely take on more work</p>
  </div>
  <div class="page">
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Pipelined shuffle step  Each map task can send mulMple sorted runs</p>
  </div>
  <div class="page">
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Pipelined shuffle step</p>
    <p>Merge and combine</p>
    <p>Merge and combine</p>
    <p>Each map task can send mulMple sorted runs  Reducers perform early group + combine during shuffle  Also done in blocking but more so when pipelining</p>
  </div>
  <div class="page">
    <p>Pipelined Fault Tolerance (PFT)</p>
    <p>Simple PFT design:  Reduce treats inprogress map output as tenta/ve  If map dies then throw away its output  If map succeeds then accept its output</p>
    <p>Revised PFT design:  Spill files have determinisMc boundaries and are assigned a sequence number</p>
    <p>Correctness: Reduce tasks ensure spill files are idempotent  Op8miza8on: Map tasks avoid sending redundant spill files</p>
  </div>
  <div class="page">
    <p>HDFS</p>
    <p>Write Snapshot Answer</p>
    <p>HDFS</p>
    <p>Block 1</p>
    <p>Block 2</p>
    <p>Read Input File map</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Execute reduce task on intermediate data  Intermediate results published to HDFS</p>
    <p>Online AggregaMon</p>
  </div>
  <div class="page">
    <p>Example ApproximaMon Query  The data:  Wikipedia traffic staMsMcs (1TB)  Webpage clicks/hour  5066 compressed files (each file = 1 hour click logs)</p>
    <p>The query:  group by language and hour  count clicks</p>
    <p>The approximaMon:  Final answer  (intermediate click count * scaleup factor) 1. Job progress: 1.0 / fracMon of input received by reducers 2. Sample frac8on: total # of hours / # hours sampled</p>
    <p>and fracMon of hour</p>
    <p>(each file = 1 hour click logs)</p>
  </div>
  <div class="page">
    <p>Bar graph shows results for a single hour (1600)  Taken less than 2 minutes into a ~2 hour job!</p>
    <p>!&quot;#$!!%</p>
    <p>&amp;&quot;#$!'%</p>
    <p>(&quot;#$!'%</p>
    <p>)&quot;#$!'%</p>
    <p>*&quot;#$!'%</p>
    <p>+&quot;#$!'%</p>
    <p>,&quot;#$!'%</p>
    <p>-&quot;#$!'%</p>
    <p>./ 01 2%</p>
    <p>=&lt; &gt;&lt; 43 83 %</p>
    <p>&gt;? 678 2%</p>
    <p>&gt;? :0 /5 /3 83 %</p>
    <p>:/ 88 7&lt; 4%</p>
    <p>@74&lt;6%&lt;48A3:% B&lt;;&gt;63%9:&lt;1C?4% D?E%&gt;:?5:388%</p>
  </div>
  <div class="page">
    <p>Approxima8on error: |es/mate  actual| / actual  Job progress assumes hours are uniformly sampled  Sample fracMon  sample distribuMon of each hour</p>
    <p>!&quot;</p>
    <p>!#$&quot;</p>
    <p>!#%&quot;</p>
    <p>!#&amp;&quot;</p>
    <p>!#'&quot;</p>
    <p>!#(&quot;</p>
    <p>!#)&quot;</p>
    <p>!#*&quot;</p>
    <p>!#+&quot;</p>
    <p>! &quot;</p>
    <p>% ' ! &quot;</p>
    <p>' + ! &quot;</p>
    <p>* % ! &quot;</p>
    <p>, ) ! &quot;</p>
    <p>$ % ! ! &quot;</p>
    <p>$ ' ' ! &quot;</p>
    <p>$ ) + ! &quot;</p>
    <p>$ , % ! &quot;</p>
    <p>% $ ) ! &quot;</p>
    <p>% ' ! ! &quot;</p>
    <p>% ) ' ! &quot;</p>
    <p>% + + ! &quot;</p>
    <p>&amp; $ % ! &quot;</p>
    <p>&amp; &amp; ) ! &quot;</p>
    <p>&amp; ) ! ! &quot;</p>
    <p>&amp; + ' ! &quot;</p>
    <p>' ! + ! &quot;</p>
    <p>' &amp; % ! &quot;</p>
    <p>' ( ) ! &quot;</p>
    <p>( &amp; ' ! &quot;</p>
    <p>! &quot;# $ % # &amp;% '( &amp;&amp; ) &amp;'</p>
    <p>*+,-'./-0/1'</p>
    <p>-./&quot;01.21344&quot; 567083&quot;916:;.&lt;&quot;</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Large vs. Small Block Size</p>
    <p>Map input is a single block (Hadoop default)  Increasing block size =&gt; fewer maps with longer runMmes</p>
    <p>Wordcount on 100GB randomly generated words  20 extralarge EC2 nodes: 4 cores, 15GB RAM</p>
    <p>Slot capacity: 80 maps (4 per node), 60 reduces (3 per node)  Two jobs: large vs. small block size</p>
    <p>Job 1 (large): 512MB (240 maps/blocks)  Job 2 (small): 32MB (3120 maps/blocks)</p>
    <p>Both jobs hard coded to use 60 reduce tasks</p>
  </div>
  <div class="page">
    <p>Poor CPU and I/O overlap  Especially in blocking mode</p>
    <p>Pipelining + adapMve policy less sensiMve to block sizes  BUT incurs extra sorMng between shuffle and reduce steps</p>
    <p>!&quot;#</p>
    <p>$!&quot;#</p>
    <p>%!&quot;#</p>
    <p>&amp;!&quot;#</p>
    <p>'!&quot;#</p>
    <p>(!!&quot;#</p>
    <p>!# )# (!# ()# $!# $)# *!# *)# %!# %)#</p>
    <p>! &quot;# $ &quot;% &amp;&amp; '</p>
    <p>()*%'+*),-.%&amp;/'</p>
    <p>+,-#-./0.122# 314561#-./0.122#</p>
    <p>!&quot;#</p>
    <p>$!&quot;#</p>
    <p>%!&quot;#</p>
    <p>&amp;!&quot;#</p>
    <p>'!&quot;#</p>
    <p>(!!&quot;#</p>
    <p>!# )# (!# ()# $!# $)# *!# *)# %!# %)#</p>
    <p>! &quot;# $ &quot;% &amp;&amp; '</p>
    <p>()*%'+*),-.%&amp;/'</p>
    <p>+,-#-./0.122# 314561#-./0.122#</p>
    <p>Large Block Size Job compleMon Mme Reduce idle</p>
    <p>period Reduce idle period on final mergesort</p>
    <p>Reduce step (75%100%)</p>
    <p>Shuffle step (0%75%)</p>
  </div>
  <div class="page">
    <p>Improves CPU and I/O overlap  BUT idle periods sMll exist in blocking mode shuffle step  AND increases scheduler overhead (3120 maps)  AND increases HDFS (NameNode) memory pressure</p>
    <p>AdapMve policy finds the right degree of pipelined parallelism  Based on runMme dynamics (reducer load, network capacity, etc.)</p>
    <p>!&quot;#</p>
    <p>$!&quot;#</p>
    <p>%!&quot;#</p>
    <p>&amp;!&quot;#</p>
    <p>'!&quot;#</p>
    <p>(!!&quot;#</p>
    <p>!# )# (!# ()# $!# $)# *!# *)# %!#</p>
    <p>! &quot;# $ &quot;% &amp;&amp; '</p>
    <p>()*%'+*),-.%&amp;/'</p>
    <p>+,-#-./0.122# 314561#-./0.122#</p>
    <p>!&quot;#</p>
    <p>$!&quot;#</p>
    <p>%!&quot;#</p>
    <p>&amp;!&quot;#</p>
    <p>'!&quot;#</p>
    <p>(!!&quot;#</p>
    <p>!# )# (!# ()# $!# $)# *!# *)# %!#</p>
    <p>! &quot;# $ &quot;% &amp;&amp; '</p>
    <p>()*%'+*),-.%&amp;/'</p>
    <p>+,-#-./0.122# 314561#-./0.122#</p>
    <p>Small Block Size Job compleMon Mme</p>
    <p>&lt;&lt; 1 minute &lt; 1 minute</p>
    <p>Reduce idle period</p>
  </div>
  <div class="page">
    <p>Future Work</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>More informaMon: hvp://boom.cs.berkeley.edu</p>
    <p>HOP code: hvp://code.google.com/p/hop/</p>
  </div>
  <div class="page">
    <p>Simple wordcount on two (small) EC2 nodes 1. Map machine: 2 map slots 2. Reduce machine: 2 reduce slots</p>
    <p>Input 2GB data, 512MB block size  So job contains 4 maps and (a hardcoded) 2 reduces</p>
    <p>Blocking vs. Pipelining 2 maps sort and send output to</p>
    <p>reducer</p>
    <p>to reduce</p>
    <p>Final map finishes, sorts and sends to reduce</p>
  </div>
  <div class="page">
    <p>Simple wordcount on two (small) EC2 nodes 1. Map machine: 2 map slots 2. Reduce machine: 2 reduce slots</p>
    <p>Input 2GB data, 512MB block size  So job contains 4 maps and (a hardcoded) 2 reduces</p>
    <p>Blocking vs. Pipelining</p>
    <p>Reduce task 6.5 minute idle period</p>
    <p>Reduce task performing final</p>
    <p>mergesort</p>
    <p>Job compleMon when reduce finishes</p>
    <p>No significant idle periods during the</p>
    <p>shuffle phase</p>
  </div>
  <div class="page">
    <p>Operators block  Poor CPU and I/O overlap  Reduce task idle periods</p>
    <p>Only the final answer is fetched  So more data is fetched resulMng in  Network traffic spikes  Especially when a group of maps finish</p>
    <p>Recall in blocking mode</p>
  </div>
  <div class="page">
    <p>CPU UMlizaMon</p>
    <p>Mapper CPU</p>
    <p>Reducer CPU</p>
    <p>Amazon Cloudwatch</p>
    <p>Map tasks loading 2GB</p>
    <p>of data</p>
    <p>Reduce task 6.5 minute idle period</p>
    <p>Pipelining reduce tasks start working</p>
    <p>(presorMng) early</p>
    <p>Map step more I/O bound</p>
  </div>
  <div class="page">
    <p>Operators block  Poor CPU and I/O overlap  Reduce task idle periods</p>
    <p>Only the final answer is fetched  So more data is fetched at once resulMng in  Network traffic spikes  Especially when a group of maps finish</p>
    <p>Recall in blocking mode</p>
  </div>
  <div class="page">
    <p>Network Traffic (map machine network out)</p>
    <p>Amazon Cloudwatch</p>
    <p>First 2 maps finish and send output</p>
    <p>Last map finishes and sends output</p>
    <p>Steady network traffic</p>
  </div>
  <div class="page">
    <p>Benefits of Pipelining</p>
    <p>Online aggregaMon  An early view of the result from a running computaMon  InteracMve data analysis (you say when to stop)</p>
    <p>Stream processing  Tasks operate on infinite data streams  RealMme data analysis</p>
    <p>Performance? Pipelining can   Improve CPU and I/O overlap  Steady network traffic (fewer load spikes)  Improve cluster uMlizaMon (reducers do more work)</p>
  </div>
  <div class="page">
    <p>Stream Processing</p>
    <p>Map and reduce tasks run con/nuously  Scheduler: wait for required slot capacity</p>
    <p>Map tasks stream spill files  Input taken from arbitrary source</p>
    <p>MapReduce job, TCP socket, log files, etc.  Garbage collecMon handled by system</p>
    <p>Window management done at reducer  Reduce output is an infinite series of windowed results  Window boundary based on Mme, record counts, etc.</p>
  </div>
  <div class="page">
    <p>RealMme Monitoring System</p>
    <p>Use MapReduce to monitor MapReduce  Economy of Mechanism</p>
    <p>Agents monitor machines  Implemented as a conMnuous map task  Record staMsMcs of interest (/proc, log files, etc.)</p>
    <p>Aggregators group agentlocal staMsMcs  Implemented as reduce tasks  Aggregate staMsMcs along machine, rack, datacenter  Reduce windows: 1, 5, and 15 second load averages</p>
  </div>
  <div class="page">
    <p>Monitor /proc/vmstat for swapping  Alert triggered aXer some threshold</p>
    <p>Alert reported around a second aXer passing threshold  Faster than the (~5 second) TaskTracker reporMng interval ? Feedback loop to the JobTracker for bever scheduling</p>
    <p>!&quot;</p>
    <p>#!!!!&quot;</p>
    <p>$!!!!&quot;</p>
    <p>%!!!!&quot;</p>
    <p>&amp;!!!!&quot;</p>
    <p>'!!!!&quot;</p>
    <p>(!!!!&quot;</p>
    <p>)!!!!&quot;</p>
    <p>*!!!!&quot;</p>
    <p>+!!!!&quot;</p>
    <p>#!!!!!&quot;</p>
    <p>!&quot; '&quot; #!&quot; #'&quot; $!&quot; $'&quot; %!&quot;</p>
    <p>! &quot; # $ %&amp; %'</p>
    <p>&quot; ( ( $ ) &amp;</p>
    <p>*+,$&amp;-%$./0)%1&amp;</p>
  </div>
  <div class="page">
    <p>Workers</p>
    <p>map</p>
    <p>reduce</p>
    <p>reduce</p>
    <p>Pipelined shuffle step  Each map task can send mulMple sorted runs  Reducers perform early group + combine during shuffle  Also done in blocking but more so when pipelining</p>
  </div>
  <div class="page">
    <p>Hadoop Architecture</p>
    <p>Hadoop MapReduce  Single master node (JobTracker), many worker nodes (TaskTrackers)</p>
    <p>Client submits a job to the JobTracker  JobTracker splits each job into tasks (map/reduce)  Assigns tasks to TaskTrackers on demand</p>
    <p>Hadoop Distributed File System (HDFS)  Single name node, many data nodes  Data is stored as fixedsize (e.g., 64MB) blocks  HDFS typically holds map input and reduce output</p>
  </div>
  <div class="page">
    <p>Performance</p>
    <p>Why block?  EffecMve combiner  Reduce step is a bovleneck</p>
    <p>Why pipeline?  Improve cluster uMlizaMon  Smooth out network traffic</p>
  </div>
</Presentation>
