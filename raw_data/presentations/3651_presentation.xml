<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>High Accuracy and High Fidelity Extraction of Neural Networks</p>
    <p>Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot</p>
  </div>
  <div class="page">
    <p>MLaaS</p>
  </div>
  <div class="page">
    <p>Model Extraction</p>
  </div>
  <div class="page">
    <p>This Talk  Taxonomy</p>
    <p>Motivation</p>
    <p>Learning Extraction  Algorithms  Limitations</p>
    <p>Direct Recovery Extraction  Prior Work  Improvements</p>
  </div>
  <div class="page">
    <p>Why would someone do this?</p>
    <p>Data and engineers are expensive (theft)...</p>
  </div>
  <div class="page">
    <p>Why would someone do this?</p>
    <p>Data and engineers are expensive (theft)... ...and models are vulnerable to</p>
    <p>attack (reconnaissance)!</p>
  </div>
  <div class="page">
    <p>Taxonomy  Theft</p>
    <p>Accuracy</p>
    <p>Reconnaissance  Fidelity  Functional Equivalence</p>
    <p>Adversaries also have specific access restrictions  Full model output vs class label  Rate limiting</p>
  </div>
  <div class="page">
    <p>Algorithms for Extraction  Consider a linear model: f(x) = w.x  We could try learning:</p>
    <p>Do machine learning on (xi, f(xi)) pairs</p>
    <p>But notice also:  f([1, 0, , 0]) = w0  f([0, 1, , 0]) = w1</p>
    <p>We can directly recover linear models!  What about neural networks?</p>
  </div>
  <div class="page">
    <p>Learning-based Extraction - Active Learning (also here!)</p>
    <p>Active Learning: progressively growing a labeled dataset</p>
    <p>Chandrasekharan et al: https://arxiv.org/abs/1811.02054</p>
  </div>
  <div class="page">
    <p>Learning-based Extraction - Semi-Supervised Learning</p>
    <p>Semi-Supervised Learning: label a small dataset, but also use the unlabeled dataset</p>
  </div>
  <div class="page">
    <p>Learning-based Extraction  Semi-supervised learning</p>
    <p>Scales to deep learning + complex datasets  Requires large unlabeled dataset</p>
    <p>Label efficient!</p>
    <p>Dataset Queries Baseline Accuracy SemiSup Accuracy</p>
    <p>SVHN 250 79.25% 95.82%</p>
    <p>CIFAR-10 250 53.35% 87.98%</p>
    <p>ImageNet (top 5) ~140000 83.5% 86.17%</p>
  </div>
  <div class="page">
    <p>Limitations of Learning-based Extraction - Nondeterminism</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)  Linear model direct recovery isnt</p>
    <p>easily extended to neural networks</p>
    <p>We focus on 2-layer ReLU networks, following Milli et al. [1]</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
    <p>ReLU(x) = max(0, x)</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
  </div>
  <div class="page">
    <p>Improving Fidelity - Direct Recovery (Milli et al.)</p>
    <p>[1] Milli et al: https://arxiv.org/abs/1807.05185</p>
  </div>
  <div class="page">
    <p>Our Functionally Equivalent  Make Milli et al. work in practice - improved search and precision</p>
    <p>Parameters 25,000 50,000 100,000</p>
    <p>Fidelity 100% 100% 99.98%</p>
    <p>Queries ~150,000 ~300,000 ~600,000</p>
    <p>Effectiveness of our Direct Recovery Attack</p>
  </div>
  <div class="page">
    <p>Wrapping Up  See the paper for more!</p>
    <p>Hardness results  Nondeterminism  Adversarial example transferability  Our functionally equivalent attack  Hybrid attacks</p>
    <p>Future Work  More efficient, realistic, effective attacks!  Defenses for accuracy, fidelity,</p>
    <p>functionally equivalent?</p>
    <p>Thank you! Ask me questions!</p>
  </div>
  <div class="page">
    <p>Credits  Papers</p>
    <p>Chandrasekharan et al.: https://arxiv.org/abs/1811.02054  Milli et al.: https://arxiv.org/abs/1807.05185</p>
    <p>Images  Alice: Eysa Lee https://ccs.neu.edu/~eysa/  Neural Network Diagram: http://alexlenail.me/NN-SVG/index.html  Affiliations: https://ai.googleblog.com/, https://en.wikipedia.org/wiki/Northeastern_University,</p>
    <p>https://en.wikipedia.org/wiki/University_of_Toronto  Slide 6: https://hackernoon.com/hn-images/1*be2sR_HIKjY36cWuWRcu-Q.jpeg  Slide 6: https://imgs.xkcd.com/comics/machine_learning_2x.png  Slide 6: https://www.labsix.org/media/2017/10/31/cat_adversarial.png</p>
  </div>
</Presentation>
