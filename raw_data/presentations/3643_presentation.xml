<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sparse Coding of Neural Word Embeddings for Multilingual</p>
    <p>Sequence Labeling</p>
    <p>Gbor Berend</p>
  </div>
  <div class="page">
    <p>Continuous word representations</p>
    <p>apple [1 0 0 0  0 0 0 0 0  0] [3.2 -1.5]</p>
    <p>...</p>
    <p>banana [0 0 0 0  1 0 0 0 0  0] [2.8 -1.6]</p>
    <p>...</p>
    <p>door [0 0 0 0  0 0 1 0 0  0] [-1.1 12.6]</p>
    <p>zebra [0 0 0 0  0 0 0 0 0  1] [0.8 0.5]</p>
  </div>
  <div class="page">
    <p>Sparse &amp; continuous representations</p>
    <p>apple [3.2 -1.5] [ 0 1.7 0 0 -0.2 0 ]</p>
    <p>...</p>
    <p>banana [2.8 -1.6] [ 0 1.1 0 0 -0.4 0 ]</p>
    <p>...</p>
    <p>door [-1.1 12.6] [1.7 0 -2.1 0 0 -0.8]</p>
    <p>zebra [0.8 0.5] [ 0 0 1.3 0 -1.2 0 ]</p>
  </div>
  <div class="page">
    <p>Assuming trained word embeddings wi (i=1,,|V|)</p>
    <p>Creating sparse word representations</p>
    <p>Sparse coefficients</p>
    <p>Embedding vector (m)</p>
    <p>Dictionary (mxk)</p>
    <p>min DC ,</p>
    <p>i=1</p>
    <p>|V|</p>
    <p>wiD i2 2 +i1</p>
  </div>
  <div class="page">
    <p>Creating sparse word representations</p>
    <p>Assuming trained word embeddings wi (i=1,,|V|)</p>
    <p>Sparse coefficients</p>
    <p>Embedding vector (m)</p>
    <p>Dictionary (mxk)</p>
    <p>Sparsity inducing regularization</p>
    <p>min DC ,</p>
    <p>i=1</p>
    <p>|V|</p>
    <p>wiD i2 2 +i1</p>
  </div>
  <div class="page">
    <p>Assuming trained word embeddings wi (i=1,,|V|)</p>
    <p>Convex set of matrices s.t. d</p>
    <p>i  1</p>
    <p>Creating sparse word representations</p>
    <p>Sparse coefficients</p>
    <p>Embedding vector (m)</p>
    <p>Dictionary (mxk)</p>
    <p>Sparsity inducing regularization</p>
    <p>min DC ,</p>
    <p>i=1</p>
    <p>|V|</p>
    <p>wiD i2 2 +i1</p>
  </div>
  <div class="page">
    <p>Assuming trained word embeddings wi (i=1,,|V|)</p>
    <p>Similar formulation to Faruqui et al. (2015)</p>
    <p>Creating sparse word representations</p>
    <p>Convex set of matrices s.t. d</p>
    <p>i  1</p>
    <p>Sparse coefficients</p>
    <p>Embedding vector (m)</p>
    <p>Dictionary (mxk)</p>
    <p>Sparsity inducing regularization</p>
    <p>min DC ,</p>
    <p>i=1</p>
    <p>|V|</p>
    <p>wiD i2 2 +i1</p>
  </div>
  <div class="page">
    <p>Calculate a set of (surface form) features using feature functions j</p>
    <p>j could check for capitalization, suffixes, prefixes, neighboring words, etc.</p>
    <p>Classical sequence labeling</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>:</p>
  </div>
  <div class="page">
    <p>Calculate a set of (surface form) features using feature functions j</p>
    <p>j could check for capitalization, suffixes, prefixes, neighboring words, etc.</p>
    <p>Classical sequence labeling</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>: pre2=Fr pre2=fl pre2=li pre2=a pre2=ba pre2=. suf2=it suf2=es suf2=ke suf2=a suf2=na suf2=.</p>
  </div>
  <div class="page">
    <p>Calculate a set of (surface form) features using feature functions j</p>
    <p>j could check for capitalization, suffixes, prefixes, neighboring words, etc.</p>
    <p>Classical sequence labeling</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>: pre2=Fr pre2=fl pre2=li pre2=a pre2=ba pre2=. suf2=it suf2=es suf2=ke suf2=a suf2=na suf2=.</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Rely on the sparse coefficients from</p>
    <p>Sequence labeling using sparse word representation</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>:</p>
    <p>(wi)={sign(i [ j]) ji [ j]0 }</p>
  </div>
  <div class="page">
    <p>Rely on the sparse coefficients from</p>
    <p>E.g.</p>
    <p>Sequence labeling using sparse word representation</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>:</p>
    <p>Fruit1.1d280.4d171</p>
    <p>(wi)={sign(i [ j]) ji [ j]0 }</p>
  </div>
  <div class="page">
    <p>Rely on the sparse coefficients from</p>
    <p>E.g.</p>
    <p>Sequence labeling using sparse word representation</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>: P28 N171</p>
    <p>Fruit1.1d280.4d171</p>
    <p>(wi)={sign(i [ j]) ji [ j]0 }</p>
  </div>
  <div class="page">
    <p>Rely on the sparse coefficients from</p>
    <p>E.g.</p>
    <p>Sequence labeling using sparse word representation</p>
    <p>X: Fruit flies like a banana .</p>
    <p>Y: NN NN VB DT NN PUNCT</p>
    <p>: P28 P77 N11 N88 P28 N21 N171 P88 N62 N40 N210 P67</p>
    <p>...</p>
    <p>Fruit1.1d280.4d171</p>
    <p>(wi)={sign(i [ j]) ji [ j]0 }</p>
  </div>
  <div class="page">
    <p>Experimental setup</p>
    <p>Linear chain CRF (CRFsuite implementation)  Part of Speech tagging</p>
    <p>12 languages from the CoNLL-X shared task</p>
    <p>Google Universal Tag Set (12 tags)</p>
  </div>
  <div class="page">
    <p>Experimental setup</p>
    <p>Linear chain CRF (CRFsuite implementation)  Part of Speech tagging</p>
    <p>12 languages from the CoNLL-X shared task</p>
    <p>Google Universal Tag Set (12 tags)</p>
    <p>Hyperparameter settings  polyglot/w2v/Glove</p>
    <p>m=64</p>
    <p>k=1024</p>
    <p>Varying s</p>
    <p>Sparse coefficients</p>
    <p>Embedding vector (m)</p>
    <p>Dictionary (mxk)</p>
    <p>min DC ,</p>
    <p>i=1</p>
    <p>|V|</p>
    <p>wiD i2 2 +i1</p>
  </div>
  <div class="page">
    <p>Baselines</p>
    <p>Feature rich baseline (FR)  Standard feature set borrowed from CRFsuite</p>
    <p>Previous, next word, word combinations,</p>
    <p>2 variants:  Character+word level features (FRw+c)</p>
    <p>Word level features alone (FRw)</p>
  </div>
  <div class="page">
    <p>Baselines</p>
    <p>Feature rich baseline (FR)  Standard feature set borrowed from CRFsuite</p>
    <p>Previous, next word, word combinations,</p>
    <p>2 variants:  Character+word level features (FRw+c)</p>
    <p>Word level features alone (FRw) FR</p>
    <p>w+c FR</p>
    <p>w</p>
  </div>
  <div class="page">
    <p>Baselines</p>
    <p>Feature rich baseline (FR)  Standard feature set borrowed from CRFsuite</p>
    <p>Previous, next word, word combinations,</p>
    <p>2 variants:  Character+word level features (FRw+c)</p>
    <p>Word level features alone (FRw)  Brown clustering</p>
    <p>Derive features from prefixes of Brown cluster IDs</p>
  </div>
  <div class="page">
    <p>Baselines</p>
    <p>(wi)={ j : i [ j] j1,, 64 }</p>
    <p>Brown clustering  Derive features from prefixes of Brown cluster IDs</p>
    <p>Features from dense embeddings</p>
    <p>Feature rich baseline (FR)  Standard feature set borrowed from CRFsuite</p>
    <p>Previous, next word, word combinations,</p>
    <p>2 variants:  Character+word level features (FRw+c)</p>
    <p>Word level features alone (FRw)</p>
  </div>
  <div class="page">
    <p>Results averaged over 12 languages</p>
    <p>Key inspections  polyglot &gt; CBOW &gt; SG &gt; Glove</p>
    <p>Continuous vs. sparse embeddings</p>
    <p>Dense S p a r s e polyglot 91.17% 94.44% CBOW 88.30% 93.74% SG 86.89% 93.63% Glove 81.53% 91.92%</p>
  </div>
  <div class="page">
    <p>Results averaged over 12 languages</p>
    <p>Key inspections  polyglot &gt; CBOW &gt; SG &gt; Glove</p>
    <p>Sparse embeddings &gt;&gt; dense embeddings</p>
    <p>Continuous vs. sparse embeddings</p>
    <p>Dense S p a r s e Improvement polyglot 91.17% 94.44% +3.3 CBOW 88.30% 93.74% +5.4 SG 86.89% 93.63% +6.7 Glove 81.53% 91.92% +10.4</p>
  </div>
  <div class="page">
    <p>Results on Hungarian</p>
  </div>
  <div class="page">
    <p>Results on Hungarian</p>
  </div>
  <div class="page">
    <p>Experiments on generalization</p>
    <p>Training data artificially decreased  First 150 and 1500 sentences</p>
  </div>
  <div class="page">
    <p>Comparison with biLSTMs</p>
    <p>POS tagging experiments on UD v1.2 treebanks  Same settings as before (k=1024, =0.1)  biLSTM results from Plank et al. (2016)</p>
    <p>Method Avg. accuracy biLSTM</p>
    <p>w 92.40%</p>
    <p>SC-CRF 93.15%</p>
  </div>
  <div class="page">
    <p>Comparison with biLSTMs</p>
    <p>POS tagging experiments on UD v1.2 treebanks  Same settings as before (k=1024, =0.1)  biLSTM results from Plank et al. (2016)</p>
    <p>Method Avg. accuracy biLSTM</p>
    <p>w 92.40%</p>
    <p>SC-CRF 93.15% SC+WI-CRF 93.73%</p>
  </div>
  <div class="page">
    <p>Comparison with biLSTMs</p>
    <p>POS tagging experiments on UD v1.2 treebanks  Same settings as before (k=1024, =0.1)  biLSTM results from Plank et al. (2016)</p>
    <p>Method Avg. accuracy biLSTM</p>
    <p>w 92.40%</p>
    <p>SC-CRF 93.15% SC+WI-CRF 93.73% biLSTM</p>
    <p>w+c 95.99%</p>
  </div>
  <div class="page">
    <p>Further experiments in the paper</p>
    <p>Quantifying the effects of further hyperparameters  Different window sizes for training dense embeddings</p>
    <p>Comparison of different sparse coding techniques  E.g. non-negativity constraint</p>
    <p>NER experiments (on 3 languages)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Simple, yet accurate approach  Robust across languages and tasks  Favorable generalization properties  Competitive results to biLSTMs  Sparse representations accessible:</p>
    <p>begab.github.io</p>
  </div>
</Presentation>
