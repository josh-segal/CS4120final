<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>2009 Hewlett-Packard Development Company, L.P. The information contained herein is subject to change without notice</p>
    <p>Capture, conversion, and analysis of an intense NFS workload Eric Anderson HP Labs</p>
    <p>aka Industrial strength NFS tracing</p>
  </div>
  <div class="page">
    <p>Industrial strength NFS tracing  Wanted to collect customer NFS traces  Applying existing techniques failed  Going to explain how we did it Many incremental improvements Need most of them Details in paper</p>
    <p>Summary:  If you take traces, re-read the paper, apply the lessons Our workload is quite different from previous ones</p>
  </div>
  <div class="page">
    <p>Why do we take traces?  Understand real workloads How many operations occur? How big are the files? How cacheable are they? How sequential are the accesses? What trends are present?</p>
    <p>Evaluate new systems Figure out new possible designs Estimate performance on real workloads</p>
  </div>
  <div class="page">
    <p>Why new traces?</p>
    <p>Complete bunk</p>
    <p>us</p>
    <p>us</p>
    <p>Existing tools insufficient Develop new ones</p>
    <p>Workloads highly variable Collect many more traces</p>
  </div>
  <div class="page">
    <p>Improved tools</p>
  </div>
  <div class="page">
    <p>Overall trace analysis process</p>
    <p>Details in paper Tools, traces are open source</p>
    <p>environment raw form</p>
    <p>raw cooked</p>
    <p>cooked data</p>
    <p>data information</p>
  </div>
  <div class="page">
    <p>The customer  Feature animation (movie)</p>
    <p>company  Read models, textures, animation</p>
    <p>curves  Write intermediates and pictures  ~3 years/movie</p>
    <p>Dramatis personae:  Thousands of clients (render</p>
    <p>farm)  Tens of NFS servers  Twenties of NFS caches  Many rack switches  Few core routers</p>
    <p>NFS server NFS server</p>
    <p>NFS cache</p>
    <p>NFS cache</p>
    <p>Core router</p>
    <p>Rack switch</p>
    <p>Clients</p>
    <p>Rack switch</p>
    <p>Clients</p>
  </div>
  <div class="page">
    <p>Capture (2003)  Challenge:</p>
    <p>conversion 4. NFS traffic bursts</p>
    <p>&gt;1Gbit/s 5. Prefer long capture</p>
    <p>times (days)</p>
    <p>Solution: 1. Port mirroring on</p>
    <p>switch 2. Full packet capture 3. Capture to parallel</p>
    <p>JBOD 4. Special Linux-specific</p>
    <p>capture tool (lindump) 5. Dynamic compression</p>
    <p>via tmpfs buffer</p>
  </div>
  <div class="page">
    <p>Capture, improved  2004: new switches with smaller buffers</p>
    <p>2007: sustained 5Gb/s Special capture card (endacedump) Integrated dynamic compression</p>
  </div>
  <div class="page">
    <p>Capture: observed rates</p>
    <p>Measured workload is bursty</p>
    <p>Capture tool can sustain 5Gb/s</p>
    <p>Capture tool can burst up to 7.5Gb/s</p>
  </div>
  <div class="page">
    <p>Capture: discussion  No more papers reporting packet drops</p>
    <p>Expected data rate, available resources</p>
    <p>Use lindump</p>
    <p>(2003 tool)</p>
    <p>&lt;1Gbit/s &lt; 3Gbit/s,</p>
    <p>Kernel hacker</p>
    <p>Implement driverdump</p>
    <p>(2004 tool)</p>
    <p>Purchase endacedump</p>
    <p>(2007 tool)</p>
    <p>&lt;10Gbit/s,</p>
    <p>Capital budget</p>
  </div>
  <div class="page">
    <p>Conversion</p>
    <p>Challenge: 1. Flexible logical</p>
    <p>representation</p>
    <p>Solution: 1. Relational data</p>
    <p>model, multiple tables</p>
  </div>
  <div class="page">
    <p>Analysis techniques</p>
    <p>Challenge: 1. Huge (50 billion row)</p>
    <p>data sets</p>
    <p>grouping options 4. Bursty, non-normally</p>
    <p>distributed data</p>
    <p>Solution: 1. Custom DataSeries</p>
    <p>analysis 2. Streaming analysis</p>
  </div>
  <div class="page">
    <p>Graphing/Reporting techniques  Challenge:</p>
    <p>Solution: 1. Store data in SQL</p>
    <p>database 2. Select with mercury</p>
    <p>plot</p>
    <p>Example mercury-plot command: plot quantile as x, value as y from nfs_hostinfo_cube</p>
    <p>where operation = 'read' and direction = 'send'</p>
  </div>
  <div class="page">
    <p>Collect more traces</p>
  </div>
  <div class="page">
    <p>Analysis: distribution of operation rate  Shows</p>
    <p>NFS-level burstiness</p>
    <p>Validates use of quantiles rather than mean and stddev</p>
  </div>
  <div class="page">
    <p>Analysis: distribution of file sizes Each accessed file counted once Most files are small Moderately wide size distribution</p>
    <p>Horizontal line is NFS read and write size</p>
  </div>
  <div class="page">
    <p>Analysis: reads in a single group</p>
    <p>File A read read read read read</p>
    <p>&gt;30s&lt;30s</p>
    <p>Group 1 Group 2</p>
    <p>Each group is the set of reads with a maximum inter-read gap of 30 seconds</p>
  </div>
  <div class="page">
    <p>Analysis: reads in a single group</p>
    <p>Most I/Os all alone</p>
    <p>Side effect of many small files.</p>
    <p>Occasional large groups (~100 I/Os)</p>
    <p>Need cross-file prefetching</p>
  </div>
  <div class="page">
    <p>Conclusion  Capture techniques</p>
    <p>no more packet loss  Conversion and analysis techniques</p>
    <p>handle huge datasets on moderate hardware  Workload is very different: Very intense Small files</p>
    <p>Much more detail and discussion in paper  Tools and traces open source</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>Author/Speaker: eric.anderson4@hp.com</p>
    <p>Software: http://tesla.hpl.hp.com/opensource/</p>
    <p>Datasets: http://apotheca.hpl.hp.com/pub/datasets/animation-bear/</p>
    <p>http://iotta.snia.org/traces/list/NFS</p>
    <p>Tracing BoF: 8:30-9:30 pm, San Francisco A</p>
  </div>
  <div class="page"/>
</Presentation>
