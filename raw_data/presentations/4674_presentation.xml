<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Comparison of variational Bayes and Gibbs sampling in reconstruction of</p>
    <p>missing values with probabilistic principal component analysis</p>
    <p>Luis De Alba, Alexander Ilin, and Tapani Raiko Adaptive Informatics Research Center</p>
    <p>Aalto University</p>
  </div>
  <div class="page">
    <p>Collaborative filtering  Consider a table of ratings of movies (m1,m2,...)</p>
    <p>given by different people (p1,p2,...)</p>
    <p>The task is to make personalized recommendations</p>
    <p>This can be seen as reconstructing missing ratings in the table given the observed ones</p>
    <p>p1 p2 p3 p4 p5 p6 m1 4 5 ? 5 2 m2 5 ? 5 3 m3 1 3 5 ?</p>
  </div>
  <div class="page">
    <p>Principal Component Analysis (PCA)</p>
    <p>Data Y consists of n d-dimensional vectors</p>
    <p>Matrix Y is decomposed in to a product of smaller matrices such that the square reconstruction error is minimized</p>
    <p>Comparisonof VariationalBayes and Gibbs Sampling</p>
    <p>in Reconstructionof Missing Values</p>
    <p>with ProbabilisticPrincipal ComponentAnalysis</p>
    <p>Luis Gabriel De Alba Rivera!</p>
    <p>! Aalto University School of Science and Technology</p>
    <p>Department of Information and Computer Science</p>
    <p>luis.dealbar@gmail.com</p>
    <p>Alexander Ilin</p>
    <p>Aalto University School of Science and Technology</p>
    <p>Department of Information and Computer Science</p>
    <p>firstname.lastname@tkk.fi</p>
    <p>Tapani Raiko</p>
    <p>Abstract</p>
    <p>Lately there has been the interest of categorization and pattern detection in large data sets, including</p>
    <p>the recovering of the dataset missing values. In this project the objective will be to recover the subset</p>
    <p>of missing values as accurately as possible from a movie rating data set. Initially the data matrix is</p>
    <p>preprocessed and its elements are divided in training and test sets. Thereafter the resulting matrices</p>
    <p>are factorized and reconstructed according to probabilistic principal component analysis (PCA). We</p>
    <p>compare the quality of reconstructions done with sampling and variational Bayesian (VB) approach.</p>
    <p>The results of the experiments showed that sampling improved the quality of the recovered missing</p>
    <p>values over VB-PCA typically after roughly 100 steps of Gibbs sampling.</p>
    <p>Human preferences (the quality tags we put on</p>
    <p>things) are language terms that can be easily trans</p>
    <p>lated into a numerical domain. We could assign low</p>
    <p>values to odd things and high values to enjoyable</p>
    <p>things, i.e.; rate things according to our experience.</p>
    <p>These ratings serve us to easily (and grossly) classify</p>
    <p>and order our preferences from the ones we like the</p>
    <p>most to the ones we dislike the most. Of course we</p>
    <p>are limited: we can not rate what we do not know,</p>
    <p>however; it may be of our interest to know the possi</p>
    <p>ble ratings of these unknowns.</p>
    <p>In this project we will be working with large and</p>
    <p>sparse matrices of movies ratings. The objective will</p>
    <p>be to recover a subset of the missing values as accu</p>
    <p>rately as possible. Recovering these missing values</p>
    <p>equal topredictingmovies ratingsand, therefore;pre</p>
    <p>dicting movies preferences for different users. The</p>
    <p>idea of correctly recoveringmovies ratings for differ</p>
    <p>ent users has been a hot topic during the last years</p>
    <p>motivated by the Netflix prize.</p>
    <p>The conceptof mining users preferences to predict</p>
    <p>apreferenceofa thirduser is calledCollaborativeFil</p>
    <p>tering, it involves largedata sets andhasbeenusedby</p>
    <p>stores like Amazon and iTunes.</p>
    <p>We can start by considering that the preferencesof</p>
    <p>the users are determined by a number of unobserved</p>
    <p>factors (that later we will call components). These</p>
    <p>hidden variables can be, for example, music, screen</p>
    <p>play, special effects, etc. These variables weight dif</p>
    <p>ferent and are rated independently,however; they, to</p>
    <p>gether, sum up for the final rating, the one we ob</p>
    <p>serve. Therefore; if we can factorize the original ma</p>
    <p>trix (the one with the ratings) in a set of sub-matrices</p>
    <p>that represent these hidden factors, we may have a</p>
    <p>better chance to find the components and values to</p>
    <p>recover the missing ratings [1]. One approach to find</p>
    <p>these matrices is to use SVD (Single Value Decom</p>
    <p>position), a matrix factorization method. With SVD</p>
    <p>the objective is to find matrices U V minimizing the</p>
    <p>sum-squared distance to the target matrix R [2].</p>
    <p>For this project we consider matrix Y to be our</p>
    <p>only informative input. Matrix Y is, usually, large</p>
    <p>anddisperse, i.e.;with lotsofmissingvalues. Theob</p>
    <p>servable valuesare the ratingsgiven to movies (rows)</p>
    <p>by users (columns). Our objective is to recover the</p>
    <p>missing values, or a subset of them, with a small er</p>
    <p>ror. We can factorize matrix Y such that</p>
    <p>Y ! WX + m, (1)</p>
    <p>where the bias vector m is added to each column of</p>
    <p>the matrix WX. Matrices W X m will let us re</p>
    <p>cover the missing values, of course, the quality of the</p>
    <p>recovering depends on the quality of these matrices.</p>
    <p>Sampling will let us improve the fitness of matrices</p>
  </div>
  <div class="page">
    <p>Algorithms for PCA</p>
    <p>Eigenvalue decomposition (standard approach)</p>
    <p>Compute the covariance matrix and its eigenvectors</p>
    <p>EM algorithm</p>
    <p>Iterates between updates of W and X</p>
  </div>
  <div class="page">
    <p>PCA with missing values</p>
    <p>Red and blue data points are reconstructed based on only one of the two dimensions</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
  </div>
  <div class="page">
    <p>Adapting the algorithms for missing values</p>
    <p>Iterative imputation</p>
    <p>Alternately 1) fill in missing values and 2) solve normal PCA with the standard approach</p>
    <p>EM algorithm becomes a bit more involved</p>
    <p>Can be extended, and was thus used here</p>
  </div>
  <div class="page">
    <p>Overfitting in case of sparse data</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
    <p>Overfitted solution Regularized solution</p>
  </div>
  <div class="page">
    <p>Regularization against Overfitting</p>
    <p>Penalizing the use of large parameter values</p>
    <p>Estimating the distribution of unknown parameters</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X dn</p>
    <p>A dc</p>
    <p>S cn</p>
    <p>, c  d  n</p>
    <p>Minimized cost function:</p>
    <p>Ce =</p>
    <p>(i,j)O</p>
    <p>( xij</p>
    <p>c</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>)2</p>
    <p>Reconstruction:</p>
    <p>xij = c</p>
    <p>k=1</p>
    <p>aikskj , (i, j) / O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A  arg max A</p>
    <p>Ce , S  arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A  A   Ce A</p>
    <p>, S  S   Ce S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>1 x Ce + A</p>
    <p>+ c</p>
    <p>k=1</p>
    <p>v1sk Sk: 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, aik), q(skj) = N</p>
    <p>( skj; skj, skj</p>
    <p>)</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>{ ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>} = Cbr + C(aik, skj)</p>
    <p>Extra term C(aik, skj) accounts for posterior uncertainty  aik, aik, skj, skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>#i   (</p>
    <p>2C</p>
    <p>2i</p>
    <p>) C</p>
    <p>i</p>
    <p>= 0: gradient descent,  = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with  = 0 (gradient) and with  = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
  </div>
  <div class="page">
    <p>Modelling the uncertainty</p>
    <p>The optimal reconstruction is an integral over the unknown variables of the model</p>
    <p>There is no analytical solution to the integral so approximative methods must be used</p>
    <p>Variational Bayes and Gibbs sampling are such methods</p>
    <p>W X m to better recovermatrix Y. We can use VB</p>
    <p>PCA (Variational Bayes PCA) for the initial decom</p>
    <p>position of the input matrix Y. VB-PCA is known</p>
    <p>to be less prone to over-fitting and more accurate for</p>
    <p>lager-scale data sets with lots of missing values com</p>
    <p>pared to traditional PCA methods [3, 4]. However;</p>
    <p>VB-PCA is not compulsory for sampling, a random</p>
    <p>initialization method is also explored in this project.</p>
    <p>Sampling can be seen as the generation of numerical</p>
    <p>values with the characteristicsof a given distribution.</p>
    <p>Sampling is used when other approaches are not fea</p>
    <p>sible.</p>
    <p>Forhigh-dimensionalprobabilisticmodelsMarkov</p>
    <p>chainMonteCarlomethodsareused togoover the in</p>
    <p>tegrals with good accuracy. Gibbs sampling is a well</p>
    <p>known MCMC method [5, 6]. In Gibbs approach we</p>
    <p>sample one variable, for example W, conditioned to</p>
    <p>the remaining variables, X m. In the following step</p>
    <p>we sample another variable fixing the rest; we repeat</p>
    <p>thisprocessgeneratingasmanysamplesasnecessary.</p>
    <p>In our project we have matrix Y that is a joint dis</p>
    <p>tribution of the form Y = WX + m+noise to predict the missing values in Y we need to solve:</p>
    <p>P (YMIS|YOBS ) =</p>
    <p>! P (YMIS|W, X, m) (2)</p>
    <p>P (W, X, m|YOBS ) dW dX dm.</p>
    <p>Solving the integral is complex, therefore; we</p>
    <p>make use of Gibbs sampling to approximate its</p>
    <p>solution. To recover matrices W X m we need</p>
    <p>to solve P (W|YOBS , X, m), P (X|YOBS , W, m) and P (m|YOBS , W, X) each one followinga Gaussian distribution, contrary to P (W, X, m|YOBS ) that follows an unknown and complex distribution.</p>
    <p>The mean matrices, X W m, and covariance matri</p>
    <p>ces, x w m, are calculated according to the for</p>
    <p>mulas provided at [4] Appendix D; this is done as</p>
    <p>follows:</p>
    <p>X:j = (W T j Wj + vI)</p>
    <p>!1 W</p>
    <p>T j (Y:j  mj ) (3)</p>
    <p>!x,j = v(W T j Wj + vI)</p>
    <p>!1 (4)</p>
    <p>Wi: = (Yi:  mi) T X</p>
    <p>T i (XiX</p>
    <p>T i + v diag(w</p>
    <p>!1 k )) (5)</p>
    <p>!w,i = v(XiX T i + v diag(w</p>
    <p>!1 k )) (6)</p>
    <p>mi = wm</p>
    <p>|Oi|(wm + v/|Oi|)</p>
    <p>&quot; j!Oi</p>
    <p>[yij  Wi:X:j ] (7)</p>
    <p>mi = vwm</p>
    <p>|Oi|(wm + v/|Oi|) . (8)</p>
    <p>Indices j = 1, . . . , p and i = 1, . . . , m go over the rows (people)and columns (movies)of matrix Y,</p>
    <p>and yij is the ijth element of matrix Y. X:j is the column j of matrix X, Wi: is row i of matrix W, mi is element i of vector m. v and wm are hyperparameters. Y is the data matrix where the missing</p>
    <p>valueshave been replacedwith zeroes. O is the set of indices ij for which yij is observed. Oi is the set of indices j for which yij is observed. |Oi| is the number of elements in Oi. I is the identity matrix. diag is the diagonalizingof the referredvalues. Wj is ma</p>
    <p>trix W in which an ith row is replaced with zeros if yij is missing, mj is vector m in which each ith element is replacedwith zero if yij is missing, and Xi is the matrix X in which a jth column is replaced with zeros if yij is missing. Using the mean and covariance matrices we are</p>
    <p>able to sample W&quot; X&quot; and m&quot; using the methodspre</p>
    <p>sented in [6]. With the sampled and mean matrices</p>
    <p>we recover a full matrix Y&quot;, i.e.; including the miss</p>
    <p>ing values; more of this is explained in the following</p>
    <p>subsections.</p>
    <p>To recover the matrix Y we need to multiply ma</p>
    <p>trix W by X and add the m bias vector to each col</p>
    <p>umn. Referring to the ideas presented by [1], matrix</p>
    <p>W represents the different and weighted factors that</p>
    <p>conform a movie. On the other hand, matrix X rep</p>
    <p>resents the values assigned to each factor by the dif</p>
    <p>ferent users. The resulting matrix Y&quot; has, therefore,</p>
    <p>the ratings given to movies m by users p. The bias</p>
    <p>term, m, is used to compensate the differences in re</p>
    <p>sults from the recovered matrix Y&quot; and the original</p>
    <p>observed values used during the training.</p>
    <p>To prove the quality of the ratings in the recovered</p>
    <p>matrix Y&quot; it is necessary to have a test set different</p>
    <p>from the training set. At every step during sampling</p>
    <p>when the values are recovered we calculate the Root</p>
    <p>Mean Square Error, RMSE, using the test set as base</p>
    <p>line. RMSE is a well known measure to quantify the</p>
    <p>amount by which a predictor differs from the value</p>
    <p>being predicted.</p>
    <p>The samplingand recoveringprocess is as follows:</p>
    <p>x using W i by Eqs. (3)(4).</p>
  </div>
  <div class="page">
    <p>Variational Bayes</p>
    <p>Distribution with a fixed form is fitted to the true distribution</p>
    <p>The misfit between the two distributions is minimized iteratively</p>
  </div>
  <div class="page">
    <p>Gibbs sampling</p>
    <p>The distribution of W and m given Y and X has a simple form</p>
    <p>The distribution of X given Y, W and m has a simple form</p>
    <p>Draw random samples alternately from these</p>
    <p>trix !w using X i by Eqs. (5)(6).</p>
    <p>W i X</p>
    <p>i by Eqs. (7)(8).</p>
    <p>This can be graphically visualized at Figure 1. At ev</p>
    <p>ery loop, when calculating the mean matrices W X</p>
    <p>(steps 2 and 6), we use the original matrix Y, this</p>
    <p>leads to an improvement in the recovered values (bet</p>
    <p>ter representing the original matrix with the observed</p>
    <p>values) and hence and improvement in the future</p>
    <p>sampled matrices.</p>
    <p>X</p>
    <p>W calculate</p>
    <p>loopsample</p>
    <p>W</p>
    <p>X</p>
    <p>W</p>
    <p>Y</p>
    <p>Y Starting point</p>
    <p>Initial W From PCA Full/Diag</p>
    <p>Randomly initialized</p>
    <p>recover</p>
    <p>Figure 1: Sampling PCA process.</p>
    <p>Every time matrix Y! is calculated (steps 3 and 7)</p>
    <p>the missing values are recovered. At every recovering</p>
    <p>step the missing values are averaged with the previ</p>
    <p>ously recovered ones</p>
    <p>yk+1 = kyk + yk+1</p>
    <p>k + 1 , (9)</p>
    <p>where k is the step, y is the average of the previous values and y are the new recovered values. Using the average will lead to better results than just using the</p>
    <p>single-samples alone. The more samples are aver</p>
    <p>aged, the close the approximation is to the true in</p>
    <p>tegral in Equation 2.</p>
    <p>The Sampling PCA method was tested with an arti</p>
    <p>ficial data set and the MovieLens data set. For the</p>
    <p>MovieLens test the missing values were also pre</p>
    <p>dicted randomly to observe how close a random pre</p>
    <p>diction is from the sampling approach, i.e.; to grossly</p>
    <p>measure the benefit of using sampling. With the ar</p>
    <p>tificial data we will focus on recovering all missing</p>
    <p>values while with MovieLens data only a subset of</p>
    <p>the missing values.</p>
    <p>The initial testing was done using artificially gener</p>
    <p>ated data. The artificial data consists on generating</p>
    <p>matrices W[m, c] (normally distributed N (0, 1), random values); X[c, p] (uniformly distributed [0 . . . 1], random values) and, an additional noise matrix</p>
    <p>N[m, p] (normally distributed N (0, var) where noise variance (var) is given in the table below). Matrix</p>
    <p>Y[m, p] is generated as Y = WX + N. From matrix Y a given percentage of ratings is selected at ran</p>
    <p>dom and set to N aN in matrix Yt, i.e.; set to be missing values1.</p>
    <p>Three data sets were generated with the following</p>
    <p>characteristics:</p>
    <p>Set m p c Noise Var Missing Values</p>
    <p>A 100 125 8 0.05 50%</p>
    <p>B 150 200 15 0.3 70%</p>
    <p>C 300 450 18 0.5 85%</p>
    <p>Using the VB-PCA approach, PCA FULL function</p>
    <p>[4], we recover W X and m (plus hyper-parameters)</p>
    <p>from matrix Yt. We do this using 10, 20 and 30</p>
    <p>components. With the recovered matrices we run the</p>
    <p>Sampling PCA algorithm; 500 samples are generated</p>
    <p>from each input.</p>
    <p>We can observe at Table 1, how the noise, size and</p>
    <p>proportion of missing values of the original matrix Y</p>
    <p>affect the quality of the recovered missing values. It</p>
    <p>is also noticeable that when the problem is simple, as</p>
    <p>and c for number of components.</p>
  </div>
  <div class="page">
    <p>MovieLens data</p>
    <p>100000 ratings from 1-5 given by 943 people to 1682 movies</p>
    <p>We used 95000 ratings for training and 5000 ratings for testing</p>
    <p>94% missing values</p>
  </div>
  <div class="page">
    <p>Results with MovieLens data  Sampling against variational Bayes with different</p>
    <p>number of components c=10,20,30</p>
    <p>RMS reconstruction error of test ratings</p>
    <p>c=10</p>
    <p>P C</p>
    <p>A F</p>
    <p>U L</p>
    <p>L</p>
    <p>c=20</p>
    <p>c=30</p>
    <p>c=10</p>
    <p>P C</p>
    <p>A D</p>
    <p>IA G</p>
    <p>c=20</p>
    <p>c=30</p>
    <p>Figure 5: RMSE for first samples after random ini</p>
    <p>tialization (discarded samples).</p>
    <p>P C</p>
    <p>A F</p>
    <p>U L</p>
    <p>L</p>
    <p>c=10</p>
    <p>c=20</p>
    <p>c=30</p>
    <p>P C</p>
    <p>A D</p>
    <p>IA G</p>
    <p>c=10</p>
    <p>c=20</p>
    <p>c=30</p>
    <p>Figure 6: Sampling process for randominitialization.</p>
    <p>This project lead to interesting results. The artifi</p>
    <p>cial tests let us know that small matrices with small</p>
    <p>portion of missing values are not easily improved by</p>
    <p>sampling. For the MovieLens test we observed that</p>
    <p>sampling improved the quality of the recoveredmiss</p>
    <p>ing values over VB-PCA using the later as an initial</p>
    <p>step. We also noticed that the random initialization</p>
    <p>doesnotaffectsamplingand the resultsaregood. The</p>
    <p>best results were obtained using PCA DIAG and 20</p>
    <p>components; the worst results were obtained using</p>
    <p>PCA FULL and 10 components. A future improve</p>
    <p>ment could be achieved rounding the recovered val</p>
    <p>ues that are outside the range of the expected ones,</p>
    <p>i.e.; values ! 1 to 1 and &quot; 5 to 5. A look at the recovered vector, for the best results, shows 6 values</p>
    <p>below 1 and 32 above 5.</p>
    <p>Luis De Alba is supported by the Program Al</p>
    <p>Ban, the European Union Program of High Level</p>
    <p>Scholarships for Latin America, scholarship No.</p>
    <p>E07M402627MX. Alexander Ilin and Tapani Raiko</p>
    <p>are supported by the Academy of Finland.</p>
    <p>References</p>
    <p>[1] Simon Funk. Netflix update: Try this at home.</p>
    <p>December 2006.</p>
    <p>http://sifter.org/#simon/journal/20061211.html</p>
    <p>[2] Ruslan Salakhutdinov and Andriy Mnih. Prob</p>
    <p>abilistic Matrix Factorization. 2008. Advances</p>
    <p>in Neural Information Processing Systems 20.</p>
    <p>Cambridge, MA. MIT Press.</p>
    <p>[3] TapaniRaiko,Alexander IlinandJuhaKarhunen.</p>
    <p>Principal Component Analysis for Large Scale</p>
    <p>Problems with Lots of Missing Values. 2007.Pro</p>
    <p>ceedingsof the18thEuropeanconferenceonMa</p>
    <p>chine Learning.</p>
    <p>[4] Alexander Ilin, Tapani Raiko. Practical Ap</p>
    <p>proaches to Principal Component Analysis in</p>
    <p>the Presence of Missing Values. Technical re</p>
    <p>port TKK-ICS-R6, 2008. Helsinki University of</p>
    <p>Technology. Later version accepted for publica</p>
    <p>tion in Journal of Machine Learning Research,</p>
    <p>[5] Ruslan Salakhutdinov and Andriy Mnih.</p>
    <p>Bayesian Probabilistic Matrix Factorization</p>
    <p>using Markov Chain Monte Carlo. 2008. Pro</p>
    <p>ceedings of the 25th InternationalConference on</p>
    <p>Machine learning.</p>
    <p>[6] Christopher M. Bishop. Pattern recognition and</p>
    <p>Machine Learning. Springer. 2006. Chapter 11</p>
    <p>Sampling Methods.</p>
    <p>[7] MovieLens.Movie Recommendations.Group</p>
    <p>Lens Research at the University of Minnesota.</p>
    <p>http://movielens.umn.edu</p>
    <p>Dataset: http://www.grouplens.org/node/73</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>When the data becomes very sparse, modelling uncertainty in PCA becomes important</p>
    <p>Sampling required about 100 samples to surpass variational Bayesian reconstructions</p>
  </div>
</Presentation>
