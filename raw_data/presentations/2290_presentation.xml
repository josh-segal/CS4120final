<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>iShuffle: Improving Hadoop Performance with Shuffle-on-Write YA N F EI G U O , J I A R A O, X I A O B O Z H O U C S D E PA R T M E N T, U N I V E R S I T Y O F C O L O R A D O , C O L O R A D O S P R I N G S</p>
    <p>P R E S E N T E D B Y Y A N F E I G U O</p>
  </div>
  <div class="page">
    <p>MapReduce A framework for processing parallelizable problems across huge data sets using a large number of machines  Invented and used by Google [OSDI04]  Many implementations</p>
    <p>Apache Hadoop, Dryad</p>
    <p>From interactive query to massive/batch computation  Nutch, Hive, HBase</p>
  </div>
  <div class="page">
    <p>MapReduce Model</p>
    <p>If by your art, my dearest father, you have Put the wild waters in this roar, allay them.</p>
    <p>If = 8 By = 5 Your = 7 Art = 1</p>
    <p>If By Your Art</p>
    <p>Apply a common function to the problems input</p>
    <p>Generate intermediate data</p>
    <p>Process intermediate data for answer</p>
    <p>Map M a p ( k 1 , v 1 )l i s t ( k 2 , k 2 )</p>
    <p>Reduce R e d u c e ( k 2 , l i s t ( v 2 ) )l i s t ( v 3 )</p>
    <p>ICAC'13 ISHUFFLE</p>
  </div>
  <div class="page">
    <p>MapReduce</p>
    <p>Programming and Execution Model</p>
    <p>Map ReducePartition Combine Shuffle/Sort</p>
    <p>M a p ( k 1 , v 1 )l i s t ( k 2 , k 2 ) R e d u c e ( k 2 , l i s t ( v 2 ) )l i s t ( v 3 )</p>
    <p>Map Partition Combine</p>
    <p>Map Partition Combine</p>
    <p>ReduceShuffle/Sort</p>
    <p>ReduceShuffle/Sort</p>
  </div>
  <div class="page">
    <p>Hadoop Implementation Map  Buffered output  Spill to disk</p>
    <p>Reduce</p>
    <p>OutputCollector Collect()</p>
    <p>M ap</p>
    <p>pe r</p>
    <p>M ap</p>
    <p>()</p>
    <p>Output Buffer</p>
    <p>Partitioner Combiner</p>
    <p>v1, v2, v3 ...k1</p>
    <p>...k2</p>
    <p>......</p>
    <p>v1, v2, v3 ...k7</p>
    <p>...k8</p>
    <p>......</p>
    <p>v1, v2, v3 ...k9</p>
    <p>... k1 0</p>
    <p>......</p>
    <p>P1</p>
    <p>P2</p>
    <p>...</p>
    <p>v1, v2, v3 ...k1</p>
    <p>...k2</p>
    <p>......</p>
    <p>v1, v2, v3 ...k7</p>
    <p>...k8</p>
    <p>......</p>
    <p>v1, v2, v3 ...k9</p>
    <p>... k1 0</p>
    <p>......</p>
    <p>P1</p>
    <p>P2</p>
    <p>...</p>
    <p>SpillThread</p>
    <p>v1, v2, v3 ...k1</p>
    <p>...k2</p>
    <p>......</p>
    <p>v1, v2, v3 ...k7</p>
    <p>...k8</p>
    <p>......</p>
    <p>v1, v2, v3 ...k9</p>
    <p>...k10</p>
    <p>......</p>
    <p>P1</p>
    <p>P2</p>
    <p>...</p>
    <p>&lt;k, v&gt;</p>
    <p>Input Split</p>
    <p>Map Task</p>
  </div>
  <div class="page">
    <p>Hadoop Key Designs Shuffle  All-to-all input data fetching phase in a reduce task  The reduce function will not be performed until its completion  Disk I/O and network intensive</p>
    <p>Overlapping shuffle with map tasks  Hadoop allows an early start of the shuffle phase as soon as part of the</p>
    <p>reduce input is available  By default, shuffle is started when 5% of map tasks finished</p>
    <p>Fair sharing  Hadoop enforces fairness among users/jobs  Fair share of map and reduce slots</p>
  </div>
  <div class="page">
    <p>Issues Input data skew among reduce tasks  Non-uniform key distribution  Different partition size  Lead to disparity in reduce completion time</p>
    <p>Inflexible Scheduling of Reduce Tasks  Reduce tasks are created during job initialization  Tasks are scheduled in ascending order of their ID  Reduce tasks can not start even if all their input partitions are available</p>
    <p>Tight coupling of shuffle and reduce  Shuffle starts only when the corresponding reduce is scheduled  Leaving parallelism within and between jobs unexploited</p>
  </div>
  <div class="page">
    <p>A Motivating Example</p>
    <p>Workload: tera-sort with 4GB dataset Platform: 10-node Hadoop cluster 1 map and 1 reduce slots per node</p>
  </div>
  <div class="page">
    <p>Related Work Map Scheduling in Hadoop  Accelerating straggler Task: [OSDI08]  Enforcing Fairness: [Middleware10], [EuroSys10]</p>
    <p>Improving reduce performance  Push-based shuffling: [NSDI10]  RDMA-based acceleration: [SC11]  Specially designed partitioner: [TPDS12]</p>
    <p>Not applicable to reduce tasks</p>
    <p>Requiring hardware support or not effective in multiple wave execution</p>
  </div>
  <div class="page">
    <p>Our Approach Decouple shuffle phase from reduce tasks  Shuffle as a platform service provided by Hadoop  Pro-actively and deterministically push map output to different slave nodes</p>
    <p>Balancing the partition placement  Predict partition sizes during task execution  Determine which node should a partition been shuffled to  Mitigate data skew</p>
    <p>Flexible reduce task scheduling  Assign partitions to reduce tasks only when scheduled</p>
  </div>
  <div class="page">
    <p>iShuffle Design</p>
    <p>iShuffle  Shuffler  Shuffle Manager  Task Scheduler</p>
    <p>Features  User-Transparent Shuffle Service  Shuffle-on-Write  Automated Map Output Placement  Flexible Reduce Task Scheduling</p>
  </div>
  <div class="page">
    <p>Shuffle-on-Write shuffle when Hadoop stores intermediate results</p>
    <p>Map output collection  MapOutputCollector  DataSpillHandler</p>
    <p>Data shuffling  Queuing and Dispatching  Data Size Predictor  Shuffle Manager</p>
    <p>Map output merging  Merger  Priority-Queue merge sort</p>
  </div>
  <div class="page">
    <p>Partition Placement Prediction of Partition Sizes  Task characteristics: input data size, map selectivity  Linear model between partition size and input data size  Metrics measured during the task execution</p>
    <p>, =  +</p>
    <p>Partition Placement Problem  Minimizes the difference of total partition size on different nodes</p>
    <p>= 1  =1</p>
  </div>
  <div class="page">
    <p>Heuristic Placement Algorithm Largest Partition First (LPF)  Pick the largest partition first  Place it to node with the least total</p>
    <p>partition size</p>
  </div>
  <div class="page">
    <p>Flexible Reduce Scheduling Assign Partitions to Reduce Tasks at Runtime  Override the partition assignment at job initialization  Allow tasks to run on any node</p>
    <p>Multiple Job Scheduling  Fair scheduling for map tasks  Disabled fair share for reduce tasks  Prevent wasted cluster cycles for waiting unfinished maps</p>
  </div>
  <div class="page">
    <p>Experiments 32-node Hadoop Cluster  1 namenode, 1 jobtracker, 30 slave nodes  4 map slots and 2 reduce slots per slave  HDFS Block size = 64 MB  Hadoop version 1.1.1</p>
    <p>Hardware  Intel Xeon E5530, 4-core, 2.4 GHz  4 GB Memory  1 Gbps Ethernet</p>
  </div>
  <div class="page">
    <p>Benchmark Purdue MapReduce Benchmark Suite (PUMA)  Real data set from Wikipedia, Netflix  Shuffle-heavy and shuffle-light</p>
    <p>Job Input Size (GB) # Map # Reduce Shuffle Vol (GB)</p>
    <p>Self-join 250 4000 180 246</p>
    <p>Tera-sort 300 4800 180 300</p>
    <p>Ranked-inverted-index 220 3520 180 235</p>
    <p>K-means 30 480 6 43</p>
    <p>Inverted-index 250 4000 180 57</p>
    <p>Term-vector 250 4000 180 59</p>
    <p>wordcount 250 4000 180 49</p>
    <p>Histogram-movies 200 3200 180 0.002</p>
    <p>Histogram-ratings 200 3200 180 0.0012</p>
    <p>Grep 250 4000 180 0.0013</p>
    <p>Shuffle-heavy</p>
    <p>Shuffle-light</p>
  </div>
  <div class="page">
    <p>iShuffle Performance Execution Trace  Slow start of Hadoop does not eliminate</p>
    <p>shuffle delay for multiple reduce wave  Overhead of remote disk access of</p>
    <p>Hadoop-A [SC11]  iShuffle has almost no shuffle delay</p>
  </div>
  <div class="page">
    <p>iShuffle Performance (contd) Reducing Job Completion Time  30% and 21% less than vanilla Hadoop and Hadoop-A</p>
    <p>Reducing Shuffle Delay  10x less than vanilla Hadoop in jobs with large shuffle volume  2x to 3x less than Hadoop-A</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>Jo b</p>
    <p>Ex ec</p>
    <p>ut io</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>Hadoop</p>
    <p>Hadoop-A</p>
    <p>iShuffle 0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>S hu</p>
    <p>ff le</p>
    <p>D el</p>
    <p>ay</p>
    <p>Hadoop</p>
    <p>Hadoop-A</p>
    <p>iShuffle</p>
  </div>
  <div class="page">
    <p>Balanced Partition Placement Performance improvement by a Balanced Partition Placement  8-12% shorter job completion time</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>Jo b</p>
    <p>Ex ec</p>
    <p>ut io</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>Hadoop</p>
    <p>Hadoop-A</p>
    <p>iShuffle</p>
  </div>
  <div class="page">
    <p>Multiple Job Performance Shuffle-heavy + Shuffle-heavy  8% and 23% improvement on</p>
    <p>tera_sort and inverted-index</p>
    <p>Shuffle-heavy + Shuffle-light  16% and 25% improvement on</p>
    <p>tera_sort and histogram-movies</p>
    <p>tera_sort inverted-index</p>
    <p>Jo b</p>
    <p>Ex ec</p>
    <p>ut io</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>(s )</p>
    <p>Separate iShuffle</p>
    <p>iShuffle w/ HFS</p>
    <p>iShuffle w/ HFS_mod</p>
    <p>tera_sort histogram-movies</p>
    <p>Jo b</p>
    <p>Ex ec</p>
    <p>ut io</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>(s )</p>
    <p>Separate iShuffle</p>
    <p>iShuffle w/ HFS</p>
    <p>iShuffle w/ HFS_mod</p>
  </div>
  <div class="page">
    <p>Conclusions Motivations o Tight coupling of shuffle of reduce o Inefficient reduce scheduling o Parallelism unexploited</p>
    <p>iShuffle o Proactively push shuffle data o Balancing map output to mitigate data skew o Flexible reduce scheduling</p>
    <p>Results o Significantly reducing completion time for shuffle-heavy jobs</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>iShuffle v.s. Random Placement iShuffle outperforms randomization -based placement algorithms</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>Jo b</p>
    <p>Ex ec</p>
    <p>ut io</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>GREEDY(2)</p>
    <p>LPF-GREEDY(2)</p>
    <p>iShuffle</p>
  </div>
</Presentation>
