<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Virtual Filtering Platform</p>
    <p>A retrospective on 8 years of shipping Host SDN in the Public Cloud</p>
    <p>Daniel Firestone Tech Lead and Manager, Azure Networking Host SDN Team</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Azure and Scale  Why Virtual Switches for SDN?  Early implementations of Azure Host SDN  Azure Host SDN Platform Goals  VFP  Our platform for host SDN  VFPv2  Addressing Challenges of Scale  Hardware Offloads  Conclusion and Future</p>
  </div>
  <div class="page">
    <p>mediacaching identity service bus</p>
    <p>mobile services</p>
    <p>cloud services</p>
    <p>virtual machines</p>
    <p>Data Services tableData Lake blob storage</p>
    <p>SQL database</p>
    <p>App Services</p>
    <p>media</p>
    <p>hpcintegration analytics</p>
    <p>caching identity service bus</p>
    <p>web apps mobile services</p>
    <p>cloud services</p>
    <p>Infrastructure Services cdnvirtual machines virtual network vpn traffic manager</p>
    <p>Microsoft Azure</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Pbps10s of TbpsDatacenter Network</p>
  </div>
  <div class="page">
    <p>Fortune 500 using Microsoft Cloud</p>
    <p>&gt;85%</p>
    <p>&gt;60 TRILLIONAzure storage objects</p>
    <p>&gt;9 MILLIONAzure Active Directory Orgs</p>
    <p>&gt; 3 TRILLION Azure Event Hubs events/week</p>
    <p>&gt;110 BILLION Azure DB requests/day</p>
    <p>Azure Scale &amp; Momentum</p>
    <p>&gt;18 BILLIONAzure Active Directory authentications/week</p>
    <p>New Azure customers a month</p>
    <p>&gt;120,000</p>
  </div>
  <div class="page">
    <p>Why do we need Virtual Switch SDN Policy?</p>
    <p>Policy application at the host is more scalable!</p>
  </div>
  <div class="page">
    <p>IaaS VM</p>
    <p>Example #1: LB (From Ananta, SIGCOMM 13)</p>
    <p>All infrastructure runs behind an LB to enable high availability and application scale  How do we make application load balancing scale to the cloud?  Challenges:  How do you load balance the load balancers?  Hardware LBs are expensive, and cannot support the rapid creation/deletion of LB endpoints required in the cloud  Support 10s of Gbps per cluster  Need a simple provisioning model</p>
    <p>LB</p>
    <p>Web Server VM</p>
    <p>Web Server VM</p>
    <p>SQL Service</p>
    <p>IaaS VM</p>
    <p>SQL Service</p>
  </div>
  <div class="page">
    <p>NAT</p>
    <p>SDN Approach: Software LB with NAT in VMSwitch</p>
    <p>MUX</p>
    <p>VM DIP 10.1.1.2</p>
    <p>VM DIP 10.1.1.3</p>
    <p>Azure VMSwitch</p>
    <p>Stateless Tunnel</p>
    <p>Edge Routers</p>
    <p>Client</p>
    <p>VIP</p>
    <p>VIP</p>
    <p>DIPDIP</p>
    <p>Direct Return: VIP</p>
    <p>VIP</p>
    <p>MUX</p>
    <p>VM DIP 10.1.1.4</p>
    <p>VM DIP 10.1.1.5</p>
    <p>Azure VMSwitch</p>
    <p>NAT SLBM</p>
    <p>Tenant Definition: VIPs, # DIPs</p>
    <p>Mappings</p>
    <p>Goal of an LB: Map a Virtual IP (VIP) to a Dynamic IP (DIP) set of a cloud service  Two steps: Load Balance (select a DIP) and NAT (translate VIP-&gt;DIP and ports)  Pushing the NAT to the vswitch makes the MUXes stateless (ECMP) and enables direct return  Single controller abstracts out LB/vswitch interactions</p>
    <p>NAT</p>
  </div>
  <div class="page">
    <p>Example #2: Vnet</p>
    <p>Ideas from VL2 (SIGCOMM 09)</p>
    <p>Goal is to map Customer Addresses (e.g. BYO IP space) to Provider Addresses (real 10/8 addresses on the physical network)</p>
    <p>This requires a translation of *every* packet on the network  no hardware device on our network is scalable enough to handle this load along with all of the relevant policy</p>
    <p>Enables companies to create their own virtual network in the cloud, defining their own topologies, security groups, middleboxes and more</p>
  </div>
  <div class="page">
    <p>VNET Forwarding Policy: Traffic to on-prem</p>
    <p>Node1: 10.1.1.5</p>
    <p>Blue VM1 10.1.1.2</p>
    <p>Green VM1 10.1.1.2</p>
    <p>VMSwitchSrc:10.1.1.2 Dst:10.2.0.9 Src:10.1.1.2 Dst:10.2.0.9</p>
    <p>Policy lookup: 10.2/16 routes to GW on host with PA 10.1.1.7</p>
    <p>NM</p>
    <p>Src:10.1.1.5 Dst:10.1.1.7 GRE:Green Src:10.1.1.2 Dst:10.2.0.9</p>
    <p>L3 Forwarding Policy</p>
    <p>Node3: 10.1.1.7</p>
    <p>Green VPN GW VM 10.1.2.1</p>
    <p>VMSwitch</p>
    <p>Green Enterpise Network 10.2/16</p>
    <p>VPN GW</p>
    <p>Src:10.1.1.2 Dst:10.2.0.9L3VPN PPP</p>
  </div>
  <div class="page">
    <p>Even More VSwitch</p>
    <p>5-tuple ACLs  Infrastructure Protection  User-defined Protection</p>
    <p>Billing  Metering traffic to internet</p>
    <p>Rate limiting  Security Guards  Spoof, ARP, DHCP, and other attacks</p>
    <p>More in development all the time</p>
    <p>NM/TM</p>
    <p>Node2: 10.2.1.6</p>
    <p>VM1 10.1.1.2</p>
    <p>VM2 10.1.1.3</p>
    <p>VMSwitch</p>
    <p>Tenant Description</p>
    <p>ACL: VM2 can talk to other green VMs ACL: VM2 can talk to VM3 but not VM4 Meter all traffic from VM2 outside of 10/8 Rate limit VM2 to 800mbps</p>
    <p>VM3 10.1.1.4</p>
    <p>VM4 10.1.1.5 Billing</p>
    <p>VM2 sent 23MB of public internet traffic</p>
  </div>
  <div class="page">
    <p>Early Approach to Azure Vswitch (2009-2011): Stacked SDN drivers per app  Each SDN application is a driver module hard compiled into the vswitch, handling packets on its own</p>
    <p>Changes to SDN policy require kernel space changes, and an OS update</p>
    <p>Was revolutionary for us in shipping LB and VNET and Host SDN  but not easy to add new SDN Apps</p>
    <p>After a couple of years we decided we needed a more flexible Host SDN platform</p>
    <p>Azure Virtual Switch</p>
    <p>ACLs</p>
    <p>LB</p>
    <p>VNET</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Azure and Scale  Why Virtual Switches for SDN?  Early implementations of Azure Host SDN  Azure Host SDN Platform Goals  VFP  Our platform for host SDN  VFPv2  Addressing Challenges of Scale  Hardware Offloads  Conclusion and Future</p>
  </div>
  <div class="page">
    <p>Original Goals for Azure Host SDN Platform</p>
    <p>Goal 1: Provide a programming model allowing for multiple simultaneous, independent network controllers to program network applications, minimizing cross-controller dependencies</p>
    <p>Goal 2: Provide a MAT programming model capable of using connections as a base primitive, rather than just packets  stateful rules as first class objects</p>
    <p>Goal 3: Provide a programming model that allows controllers to define their own policy and actions, rather than implementing fixed sets of network policies for predefined scenarios</p>
  </div>
  <div class="page">
    <p>What is VFP?</p>
  </div>
  <div class="page">
    <p>VMSwitch</p>
    <p>vNIC</p>
    <p>VM</p>
    <p>NIC vNIC</p>
    <p>VM</p>
    <p>SLB (NAT)</p>
    <p>VNET</p>
    <p>ACLs, Metering, Security</p>
    <p>VFP</p>
    <p>Virtual Filtering Platform (VFP) Azures SDN Dataplane</p>
    <p>Plugin module for WS2012+ VMSwitch  Provides core SDN functionality for Azure networking services, including:  Address Virtualization for VNET  VIP -&gt; DIP Translation for SLB  ACLs, Metering, and Security Guards</p>
    <p>Uses programmable rule/flow tables to perform per-packet actions  Programmed by multiple Azure SDN controllers, supports all dataplane policy at line rate with offloads</p>
  </div>
  <div class="page">
    <p>VFP Translates L2 extensibility (ingress/egress to switch) to L3 extensibility (inbound/outbound to VM)</p>
    <p>VM</p>
    <p>Metering</p>
    <p>VNET</p>
    <p>SLB</p>
    <p>ACLs</p>
    <p>Inbound (Egress) Outbound (Ingress)</p>
    <p>Egress -&gt; Inbound</p>
    <p>Ingress -&gt; Outbound</p>
    <p>VMSwitch</p>
    <p>vNIC</p>
    <p>VM</p>
    <p>NIC vNIC</p>
    <p>VM</p>
    <p>SLB (NAT)</p>
    <p>VNET</p>
    <p>ACLs, Metering, Security</p>
    <p>VFP Egress</p>
    <p>Egress</p>
    <p>Ingress</p>
    <p>Ingress</p>
  </div>
  <div class="page">
    <p>Goal: All Policy is in the Controller VFP is a Fast, Flexible Implementation of Policy  To enable agility, allow controllers to specify exactly what they want to do at the flow/packet level, so they can implement new SDN scenarios without dataplane driver changes  VFP focuses on integrating multi-controller policies and scaling the host dataplane  perf and offloads without sacrificing flexibility  3 Key Primitives we expose to controllers:  Layers  independent flow tables per controller to order the pipeline  Rule Matches  define which packets match which rule  Rule Actions  what to do with a packet for a given rule</p>
  </div>
  <div class="page">
    <p>Node: 10.4.1.5</p>
    <p>VFP</p>
    <p>Key Primitive: Match Action Tables</p>
    <p>Blue VM1 10.1.1.2NIC</p>
    <p>Controllers</p>
    <p>Tenant Description VNet Description</p>
    <p>Flow Action</p>
    <p>VNet Routing Policy ACLsNAT</p>
    <p>Endpoints</p>
    <p>Flow ActionFlow Action</p>
    <p>TO: 10.2/16 Encap to GW</p>
    <p>TO: 10.1.1.5 Encap to 10.5.1.7</p>
    <p>TO: !10/8 NAT out of VNET</p>
    <p>Flow ActionFlow Action</p>
    <p>TO: 79.3.1.2 DNAT to 10.1.1.2</p>
    <p>TO: !10/8 SNAT to 79.3.1.2</p>
    <p>Flow Action</p>
    <p>TO: 10.1.1/24 Allow</p>
    <p>TO: !10/8 Allow</p>
    <p>VFP exposes a typed MatchAction-Table API to the agents/controllers  One table (Layer) per policy  Inspired by OpenFlow and other MAT designs, but designed for multi-controller, stateful, scalable host SDN applications</p>
    <p>VNET SLB NAT ACLS</p>
  </div>
  <div class="page">
    <p>Layers</p>
    <p>A VFP layer is not a built-in function  it is a generic set of rule/flow tables  Any layer can be created at any time  it is only an LB layer or a VNET layer based on what rules are plumbed into it  Resources like NAT pools or PA&gt;CA mapping pools are available to any layer to implement special functionality (e.g. SLB or VNET)</p>
  </div>
  <div class="page">
    <p>Everything is Stateful  The core primitive of most policy is a (TCP, UDP, ) connection  translates to a twoway flow  5-tuple ACLs, VIP-DIP SLB NAT, dynamic outbound SNAT, and more  Stateful rules make it easy to reason about asymmetric policy  rules apply to whichever side started the flow, and the reverse happens automatically for the other direction  Flow state managed by TCP connection tracker</p>
    <p>VM</p>
    <p>OUT Rules</p>
    <p>IN Rules</p>
    <p>Flow Flow</p>
    <p>IN Flow Table OUT Flow Table</p>
    <p>VFP</p>
    <p>Layer</p>
  </div>
  <div class="page">
    <p>Example: Software LB Support VM</p>
    <p>Dynam ic NAT Rules</p>
    <p>Static NAT Rules</p>
    <p>OUT Rules</p>
    <p>Decap Rules</p>
    <p>IN Flow Table OUT Flow Table</p>
    <p>IN Flow Table OUT Flow Table</p>
    <p>FlowFlow</p>
    <p>SLB NAT Layer</p>
    <p>SLB Decap Layer</p>
    <p>FlowFlow NAT Ranges</p>
    <p>NAT Pool</p>
    <p>Rules can reference Resources, like dynamic NAT pools or PA-CA</p>
    <p>mapping tables</p>
    <p>Similarly, VNET can be expressed as a series of (encap, decap, rewrite, etc) rules, rather than fixed policy</p>
  </div>
  <div class="page">
    <p>Cool Uses of Stateful Flows  LB Fastpath</p>
    <p>VFP</p>
    <p>SLB Decap/Fastpath</p>
    <p>SLB NAT</p>
    <p>Storage</p>
    <p>Decap</p>
    <p>VFP</p>
    <p>SLB Decap/Fastpath</p>
    <p>SLB NAT</p>
    <p>VM</p>
    <p>DecapEncap</p>
    <p>MUX Redirect Packet</p>
    <p>FASTPATH</p>
  </div>
  <div class="page">
    <p>Example VFP Layers: Support for LB, VNET, Security Groups, and Billing</p>
    <p>VM</p>
    <p>ACLs</p>
    <p>VNET</p>
    <p>SLB NAT</p>
    <p>ILB</p>
    <p>SLB Decap / Fastpath</p>
    <p>Metering</p>
    <p>Successfully deployed across Azure in 2012</p>
  </div>
  <div class="page">
    <p>Agility Example: Internal Load Balancing</p>
    <p>LB team wanted to offer CAspace LB in addition to PAspace LB</p>
    <p>All they had to do was create a new layer  added new policy by specifying CA-space rule matches for NAT rules</p>
    <p>No new work in VFP, because we picked the right primitives</p>
    <p>VM</p>
    <p>ACLs</p>
    <p>VNET</p>
    <p>SLB NAT</p>
    <p>ILB</p>
    <p>SLB Decap / Fastpath</p>
    <p>Metering</p>
    <p>SLB Controller</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Azure and Scale  Why Virtual Switches for SDN?  Early implementations of Azure Host SDN  Azure Host SDN Platform Goals  VFP  Our platform for host SDN  VFPv2  Addressing Challenges of Scale  Hardware Offloads  Conclusion and Future</p>
  </div>
  <div class="page">
    <p>Scaling Up SDN: NIC Speeds in Azure</p>
    <p>2009: 1Gbps  2012: 10Gbps  2015: 40Gbps  2017: 50Gbps  Soon: 100Gbps?</p>
    <p>We got a 50x improvement in network throughput, but not a 50x improvement in CPU power!</p>
    <p>NIC Speed, Gbps</p>
  </div>
  <div class="page">
    <p>New Goals for VFPv2 (2013-2014)</p>
    <p>Goal 4: Provide a serviceability model allowing for frequent deployments and updates without requiring reboots or interrupting VM connectivity for stateful flows, and strong service monitoring</p>
    <p>Goal 5: Provide very high packet rates, even with a large number of tables and rules, via extensive caching</p>
    <p>Goal 6: Implement an efficient mechanism to offload flow policy to programmable NICs, without assuming complex rule processing</p>
  </div>
  <div class="page">
    <p>VFPv1 Layers - Challenges VM</p>
    <p>Metering</p>
    <p>VNET</p>
    <p>SLB NAT</p>
    <p>ACLs</p>
    <p>SLB Decap / Fastpath</p>
    <p>ILB</p>
    <p>Holdover from original vswitch design  every layer independently handles, parses, and modifies packets</p>
    <p>Most of our layers want to be stateful  but this means independent connection tracking and flow state at each layer</p>
    <p>As host SDN became easy to program and widely used, people wanted to add new layers all the time</p>
    <p>Couldnt keep adding layers and scaling up</p>
    <p>PARSE</p>
    <p>PARSE PARSE</p>
    <p>PARSE</p>
    <p>PARSE PARSE</p>
    <p>PARSE</p>
    <p>Modify</p>
    <p>Modify</p>
    <p>Modify</p>
    <p>Modify</p>
    <p>We need a better primitive for actions!</p>
  </div>
  <div class="page">
    <p>VM</p>
    <p>Metering</p>
    <p>VNET</p>
    <p>SLB NAT</p>
    <p>ACLs</p>
    <p>SLB Decap / Fastpath</p>
    <p>ILB</p>
    <p>PARSE</p>
    <p>MODIFY</p>
    <p>Unified FlowID</p>
    <p>HEADER</p>
    <p>HEADER</p>
    <p>HEADER</p>
    <p>MATCH</p>
    <p>Transposition Engine</p>
    <p>Composite Transposition</p>
    <p>ASIC Pipeline Model: Parse Once, Modify Once</p>
    <p>TRANSPOSE</p>
    <p>Shipped in 2014</p>
  </div>
  <div class="page">
    <p>Header Transposition - Actions Header Parameters</p>
    <p>Outer Ethernet Source MAC, Dest MAC</p>
    <p>Outer IP Source IP, Dest IP</p>
    <p>Encap Encap Type, GRE Key / VXLAN VNI</p>
    <p>Inner Ethernet Source MAC, Dest MAC</p>
    <p>Inner IP Source IP, Dest IP</p>
    <p>TCP/UDP Source Port, Dest Port (note: does not support Push/Pop)</p>
    <p>Action Notes</p>
    <p>Pop Remove this header. No params supported.</p>
    <p>Push Push this header onto the packet. All params must be specified.</p>
    <p>Modify Modify this header. All params are optional, but at least one must be specified.</p>
    <p>Ignore Leave this header as is. No params supported.</p>
    <p>Not Present This header is not expected to be present (based on the match conditions). No params supported.</p>
    <p>Headers</p>
    <p>Header Actions</p>
  </div>
  <div class="page">
    <p>Header Transposition  Example Actions</p>
    <p>Header NAT Encap Decap Encap+NAT Decap+NAT</p>
    <p>Outer Ethernet Ignore Push (SMAC,DMAC) Pop Push (SMAC,DMAC) Pop</p>
    <p>Outer IP Modify (SIP,DIP) Push (SIP,DIP) Pop Push (SIP,DIP) Pop</p>
    <p>GRE / VxLAN Not Present Push (Key) Pop Push (Key) Pop</p>
    <p>Inner Ethernet Not Present Modify (DMAC) Ignore Modify (DMAC) Ignore</p>
    <p>Inner IP Not Present Ignore Ignore Modify (SIP,DIP) Modify (SIP,DIP)</p>
    <p>TCP/UDP Modify (SPt,DPt) Ignore Ignore Modify (SPt,DPt) Modify (SPort,DPt)</p>
    <p>Allows rules to express more complex actions across headers</p>
  </div>
  <div class="page">
    <p>Unified Parsing and Matching Condition Notes</p>
    <p>Source VPort N/A</p>
    <p>(Outer) Source MAC Address N/A</p>
    <p>(Outer) Destination MAC Address N/A</p>
    <p>(Outer) Source IP Address IPv4 or IPv6</p>
    <p>(Outer) Destination IP Address IPv4 or IPv6</p>
    <p>(Outer) IP Protocol N/A</p>
    <p>Source Port Applies if Protocol == TCP or UDP</p>
    <p>Destination Port Applies if Protocol == TCP or UDP</p>
    <p>ICMP Type Applies if Protocol == ICMP (v4 or v6)</p>
    <p>Destination Vport N/A</p>
    <p>GRE Key / VxLAN VNI (Tenant ID) Applies if Outer Protocol == GRE / VxLAN</p>
    <p>(Inner) Source MAC Address N/A</p>
    <p>(Inner) Destination MAC Address N/A</p>
    <p>(Inner) Source IP Address IPv4 or IPv6</p>
    <p>(Inner) Destination IP Address IPv4 or IPv6</p>
    <p>(Inner) IP Protocol N/A</p>
  </div>
  <div class="page">
    <p>Header Transpositions Complete our Generic Southbound API Capability Story  In order to enable agility, we want controllers to be able to define new types of policy dynamically without needing to change VFP.  We already provide flexibility in:  Layers: Controllers can define new layers dynamically for their own policy without interfering with other controllers layers  Rules: Controllers can define which rules match which packets via a consistent 5-tuple match API, nothing specific to special policies</p>
    <p>Header transpositions provide the key third primitive: Ability to specify what exactly a rule does once it is matched  All built in rules define HTs, but controllers can define their own rules by creating new ones out of HTs on the fly</p>
  </div>
  <div class="page">
    <p>Unified Flow Tables  A Fastpath Through VFP</p>
    <p>Transposition Engine</p>
    <p>Rew rite</p>
    <p>Transposition</p>
    <p>SLB Decap SLB NAT VNET ACL Metering Rule Action Rule ActionRule Action Rule Action Rule Action Rule Action</p>
    <p>Decap* DNAT* Rewrite* Allow* Meter*First Packet</p>
    <p>Second+ Packet</p>
    <p>Flow Action Decap, DNAT, Rewrite, Meter1.2.3.1-&gt;1.3.4.1, 62362-&gt;80</p>
    <p>Rule Lookups (Expensive)</p>
    <p>Hash Lookups (Cheap)</p>
    <p>VFP</p>
  </div>
  <div class="page">
    <p>Unified Flow Tables</p>
    <p>Single hash lookup for each packet after flow is created  Leaves room for new layers w/o perf impact (e.g. ILB, etc)  Single flow table per VM can be sized with VM size  All VFP actions can be expressed as header transpositions  e.g. encap/decap/l3 rewrite/l4 NAT  Any set of header transpositions can be composed and expressed as one transposition  Unified Flow Table: One match (per entire flowid, inner and outer) and one action (header transposition) per flow</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Azure and Scale  Why Virtual Switches for SDN?  Early implementations of Azure Host SDN  Azure Host SDN Platform Goals  VFP  Our platform for host SDN  VFPv2  Addressing Challenges of Scale  Hardware Offloads  Conclusion and Future</p>
  </div>
  <div class="page">
    <p>Single Root IO Virtualization (SR-IOV): Native Performance for Virtualized Workloads</p>
    <p>Parent Partition VM1 VM2</p>
    <p>TCP/IP TCP/IP</p>
    <p>VF Driver VF Driver Network Virtual Service Provider</p>
    <p>NICNIC Embedded Switch</p>
    <p>External Switch</p>
    <p>VF VFPF</p>
    <p>But where is the SDN Policy?</p>
  </div>
  <div class="page">
    <p>Goal: Offload a cache of our internal (unified) flow table to the NIC</p>
    <p>Package Header Transpositions and Unified Flow IDs into hardware API</p>
    <p>Allows us to enable SR-IOV, applying virtualization policy in hardware and bypassing the host completely</p>
    <p>Future of Host SDN: New Hardware/Software co-design models, programmable acceleration for transports, QoS, crypto, and more!</p>
  </div>
  <div class="page">
    <p>Results Azure Accelerated Networking: Fastest Cloud Network!  Highest bandwidth VMs of any cloud  DS15v2 &amp; D15v2 VMs get up to 25Gbps</p>
    <p>Consistent low latency network performance  Provides SR-IOV to the VM  10x latency improvement  Increased packets per second (PPS)  Reduced jitter means more consistency in workloads</p>
    <p>Enables workloads requiring native performance to run in cloud VMs  &gt;2x improvement for many DB and OLTP applications</p>
  </div>
  <div class="page">
    <p>Host Networking makes Physical Network Fast and Scalable</p>
    <p>Massive, distributed 40/100GbE network built on commodity hardware  No Hardware per tenant ACLs  No Hardware NAT  No Hardware VPN / overlay  No Vendor-specific control, management or data plane</p>
    <p>All policy is in software on hosts  and everythings a VM!  Network services deployed like all other services</p>
    <p>VFP, battle tested in the cloud, is now available in Microsoft Azure Stack for private cloud as well!</p>
    <p>T2-11</p>
    <p>T2-12</p>
    <p>T2-18</p>
    <p>T31</p>
    <p>T32</p>
    <p>T33</p>
    <p>T34</p>
    <p>Row Spine</p>
    <p>T2-41</p>
    <p>T2-42</p>
    <p>T2-44</p>
    <p>Data Center Spine</p>
    <p>T1-1 T1-8T1-7T1-2</p>
    <p>Regional Spine</p>
    <p>T1-1 T1-8T1-7T1-2 T1-1 T1-8T1-7T1-2</p>
    <p>Rack T0-1 T0-2 T020</p>
    <p>T0-1 T0-2</p>
    <p>T020</p>
    <p>T0-1 T0-2</p>
    <p>T020</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>VFP Developers  Yue Zuo, Harish Kumar Chandrappa, Praveen Balasubramanian, Vikas Bhardwaj, Somesh Chaturmohta, Milan Dasgupta, Mahmoud Elhaddad, Luis Hernandez, Nathan Hu, Alan Jowett, Hadi Katebi, Fengfen Liu, Keith Mange, Randy Miller, Claire Mitchell, Sambhrama Mundkur, Chidambaram Muthu, Gaurav Poothia, Madhan Sivakumar, Ethan Song, Khoa To, Kelvin Zou, and Qasim Zuhair</p>
    <p>Design Influence  Alireza Dabagh, Deepak Bansal, Pankaj Garg, Changhoon Kim, Hemant Kumar, Parveen Patel, Parag Sharma, Nisheeth Srivastava, Venkat Thiruvengadam, Narasimhan Venkataramaiah, Haiyong Wang</p>
    <p>Dave Maltz, Mark Russinovich, and Albert Greenberg for years of support</p>
  </div>
  <div class="page">
    <p>Want to come build the next generation of scalable Host SDN? Were hiring!</p>
    <p>fstone@microsoft.com</p>
  </div>
</Presentation>
