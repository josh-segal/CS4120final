<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Generating Impact-Based Summaries for Scientific</p>
    <p>Literature Qiaozhu Mei, ChengXiang Zhai</p>
    <p>University of Illinois at Urbana-Champaign</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Fast growth of publications  &gt;100k papers in DBLP; &gt; 10 references per paper</p>
    <p>Summarize a scientific paper  Authors view: Abstracts, introductions</p>
    <p>May not be what the readers received  May change over time</p>
    <p>Readers view: impact of the paper  Impact Factor: numeric</p>
    <p>Summary of the content?</p>
    <p>Authors view: Proof of xxx;</p>
    <p>new definition of xxx; apply xxx technique</p>
    <p>State-of-the-art algorithm;</p>
    <p>Evaluation metric Readers view 20 years</p>
    <p>later 2</p>
  </div>
  <div class="page">
    <p>What should an impact summary look like?</p>
  </div>
  <div class="page">
    <p>Citation Contexts  Impact, but</p>
    <p>Describes how other authors view/comment on the paper  Implies the impact</p>
    <p>Similar to anchor text on web graph, but:</p>
    <p>Usually more than one sentences (informative).</p>
    <p>Usually mixed with discussions/comparison about other papers (noisy).</p>
    <p>They have been also successfully used in part of speech tagging [7], machine translation [3, 5], information retrieval [4, 20], transliteration [13] and text summarization [14]. ... For example, Ponte and Croft [20] adopt a language modeling approach to information retrieval.</p>
  </div>
  <div class="page">
    <p>Our Definition of Impact Summary</p>
    <p>Solution: Citation context  infer impact; Original content  summary</p>
    <p>Abstract:. Introduction: ..</p>
    <p>Content:</p>
    <p>References: .</p>
    <p>Ponte and Croft [20] adopt a language modeling approach to information retrieval.</p>
    <p>probabilistic models, as well as to the use of other recent models [19, 21], the statistical properties</p>
    <p>Author picked sentences: good for summary, but doesnt reflect the impact</p>
    <p>Reader composed sentences: good signal of impact, but too noisy to be used as summary</p>
    <p>Citation Contexts</p>
    <p>Target: extractive summary (pick sentences) of the impact of a paper</p>
  </div>
  <div class="page">
    <p>Rest of this Talk</p>
    <p>An Feasibility study:</p>
    <p>A Language modeling based approach  Sentence retrieval</p>
    <p>Estimation of impact language models</p>
    <p>Experiments</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Language Modeling in Information Retrieval</p>
    <p>d1</p>
    <p>d2</p>
    <p>dN</p>
    <p>Doc LMDocuments</p>
    <p>Nd</p>
    <p>q q)||D(- 1dq</p>
    <p>Query LM</p>
    <p>)||D(1dq</p>
    <p>)||D(1dq</p>
    <p>Rank with neg. KL Divergence</p>
    <p>CSmooth using collection LM</p>
  </div>
  <div class="page">
    <p>Impact-based Summarization as Sentence Retrieval</p>
    <p>s1</p>
    <p>s2</p>
    <p>sN</p>
    <p>Sent LMSentences</p>
    <p>Ns</p>
    <p>D I)||D(- 1sI</p>
    <p>Impact LM</p>
    <p>)||D(2sI</p>
    <p>)||D(- sI N</p>
    <p>Rank with neg. KL Divergence</p>
    <p>D c1</p>
    <p>c2</p>
    <p>cM</p>
    <p>D</p>
    <p>Use top ranked sentences as a summary</p>
    <p>Key problem: estimate I</p>
  </div>
  <div class="page">
    <p>Estimating Impact Language Models</p>
    <p>Interpolation of document language model and citation language model</p>
    <p>Dc1</p>
    <p>c2</p>
    <p>cM</p>
    <p>I DC</p>
    <p>)|()|()1()|( CdI wpwpwp</p>
    <p>c</p>
    <p>Cc I</p>
    <p>d</p>
    <p>wpdwc wp</p>
    <p>||</p>
    <p>)|(),( )|(</p>
    <p>Constant coefficient:</p>
    <p>Dirichlet smoothing:</p>
    <p>M</p>
    <p>j Cj</p>
    <p>j j</p>
    <p>C j wpwp</p>
    <p>)|( 1</p>
    <p>)|(</p>
    <p>MC</p>
    <p>Set j with features of cj : ...)()()( 321  jjjj cfcfcf</p>
    <p>f1(cj) = |cj|, and 9</p>
  </div>
  <div class="page">
    <p>Specific Feature  Citation-based Authority</p>
    <p>Assumption: High authority paper has more trustable comments (citation context)</p>
    <p>Weight more in impact language model</p>
    <p>Authority  pagerank on the citation graph</p>
    <p>d1</p>
    <p>d2</p>
    <p>)|()(2 dcdPgcf jj</p>
    <p>ddd doutDeg</p>
    <p>dPg</p>
    <p>N dPg</p>
    <p>',' )'(</p>
    <p>)'(1 )1()(</p>
  </div>
  <div class="page">
    <p>Specific Feature  Citation Context Proximity</p>
    <p>Weight citation sentences according to the proximity to the citation label</p>
    <p>k  distance to the citation label</p>
    <p>There has been a lot of effort in applying the notion of language modeling and its variations to other problems. For example, Ponte and Croft [20] adopt a language modeling approach to information retrieval. They argue that much of the difficulty for IR lies in the lack of an adequate indexing model. Instead of making prior parametric assumptions about the similarity of documents, they propose a non-parametric approach to retrieval based probabilistic language modeling. Empirically, their approach significantly outperforms traditional tf*idf weighting on two different collections and query sets.</p>
    <p>kjj ccf</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Gold standard:  human generated summary  14 most cited papers in SIGIR</p>
    <p>Baselines:  Random; LEAD (likely to cover abs/intro.);  MEAD  Single Doc;  MEAD  Doc + Citations; (multi-document)</p>
    <p>Evaluation Metric:  ROUGE-1, ROUGE-L</p>
    <p>(unigram cooccurrence; longest common sequence)</p>
  </div>
  <div class="page">
    <p>Basic Results</p>
    <p>Length Metric Random LEAD MEADDoc</p>
    <p>MEADDoc +Cite</p>
    <p>LM (KL-Div)</p>
  </div>
  <div class="page">
    <p>Component Study</p>
    <p>Impact language model:</p>
    <p>Document LM &lt;&lt; Citation Context LM</p>
    <p>&lt;&lt; Interpolation (Doc LM, Cite LM)  Dirichlet interpolation &gt; constant coefficient</p>
    <p>Metric Impact LM = Doc LM</p>
    <p>Impact LM = Citation LM</p>
    <p>Interpolation</p>
    <p>ConstCoef Dirichlet</p>
    <p>ROUGE-1 0.529 0.635 0.643 0.647</p>
    <p>ROUGE-L 0.501 0.607 0.619 0.623</p>
  </div>
  <div class="page">
    <p>Component Study (Cont.)</p>
    <p>Authority and Proximity  Both Pagerank and Proximity improves</p>
    <p>Pagerank + Proximity improves marginally</p>
    <p>Q: How to combine pagerank and proximity?</p>
    <p>PageRank Proximity = Off Pr(s) = 1/k</p>
    <p>Off 0.685 0.711</p>
    <p>On 0.708 0.712</p>
  </div>
  <div class="page">
    <p>Non-impact-based Summary Paper = A study of smoothing methods for language models</p>
    <p>applied to ad hoc information retrieval</p>
    <p>Good big picture of the field (LMIR),</p>
    <p>but not about contribution of the paper (smoothing in LMIR)</p>
  </div>
  <div class="page">
    <p>Impact-based Summary</p>
    <p>Paper = A study of smoothing methods for language models applied to ad hoc information retrieval</p>
    <p>Specific to smoothing LM in IR;</p>
    <p>especially for the concrete smoothing techniques (Dirichlet and JM)</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Text summarization (extractive)  E.g., Luhn 58; McKeown and Radev 95; Goldstein et al. 99; Kraaij et</p>
    <p>al. 01 (using language modeling)</p>
    <p>Technical paper summarization  Paice and Jones 93; Saggion and Lapalme 02; Teufel and Moens 02</p>
    <p>Citation context  Ritchie et al. 06; Schwartz et al. 07</p>
    <p>Anchor text and hyperlink structure</p>
    <p>Language Modeling for information retrieval  Ponte and Croft 98; Zhai and Lafferty 01; Lafferty and Zhai 01</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Novel problem of Impact-based Summarization</p>
    <p>Language Modeling approach  Citation context  Impact language model</p>
    <p>Accommodating authority and proximity features</p>
    <p>Feasibility study rather than optimizing</p>
    <p>Future work  Optimize features/methods</p>
    <p>Large scale evaluation</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
  <div class="page">
    <p>Feature Study</p>
    <p>What we have explored:  Unigram language models - doc; citation context;  Length features  Authority features;  Proximity features;  Position-based re-ranking;</p>
    <p>What we havent done:  Redundancy removal (Diversity);  Deeper NLP features; ngram features;  Learning to weight features;</p>
  </div>
  <div class="page">
    <p>Scientific Literature with Citations</p>
    <p>They have been also successfully used in part of speech tagging [7], machine translation [3, 5], information retrieval [4, 20], transliteration [13] and text summarization [14]. ... For example, Ponte and Croft [20] adopt a language modeling approach to information retrieval.</p>
    <p>While the statistical properties of text corpora are fundamental to the use of probabilistic models, as well as to the use of other recent models [19, 21], the statistical properties</p>
    <p>paper</p>
    <p>Citation</p>
    <p>paper</p>
    <p>paper</p>
    <p>Citation</p>
    <p>Citation context 22</p>
  </div>
  <div class="page">
    <p>Language Modeling in Information Retrieval</p>
    <p>Estimate document language models  Unigram multinomial distribution of words</p>
    <p>d: {P(w|d)}</p>
    <p>Ranking documents with query likelihood  R(doc, Q) ~ P(q|d), a special case of</p>
    <p>negative KL-divergence: R(doc, Q) ~ -D(q || d)</p>
    <p>Smooth the document language model  Interpolation-based (p(w|d) ~ pML(w|d) + p(w|REF))</p>
    <p>Dirichlet smoothing empirically performs well</p>
  </div>
</Presentation>
