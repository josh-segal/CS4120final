<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>TDNN: A Two-stage Deep Neural Network</p>
    <p>for Prompt-independent Automated Essay Scoring</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background</p>
    <p>Method</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>What is Automated Essay Scoring (AES)?</p>
    <p>Computer produces summative assessment for evaluation</p>
    <p>Aim: reduce human workload</p>
    <p>AES has been put into practical use by ETS from 1999</p>
  </div>
  <div class="page">
    <p>Prompt-specific and -Independent AES</p>
    <p>Most existing AES approaches are prompt-specific</p>
    <p>Require human labels for each prompt to train</p>
    <p>Can achieve satisfying human-machine agreement</p>
    <p>Quadradic weighted kappa (QWK) &gt; 0.75 [Taghipour &amp; Ng, EMNLP 2016]</p>
    <p>Inter-human agreement: QWK=0.754</p>
    <p>Prompt-independent AES remains a challenge</p>
    <p>Only non-target human labels are available</p>
  </div>
  <div class="page">
    <p>Challenges in Prompt-independent AES</p>
    <p>Prompt 1: Winter Olympics</p>
    <p>Prompt 2: Rugby World Cup</p>
    <p>Prompt 3: Australian Open</p>
    <p>Source Prompts</p>
    <p>Model</p>
    <p>Learn</p>
    <p>World Cup 2018</p>
    <p>Target Prompt</p>
    <p>Predict</p>
  </div>
  <div class="page">
    <p>Challenges in Prompt-independent AES</p>
    <p>Prompt 1: Winter Olympics</p>
    <p>Prompt 2: Rugby World Cup</p>
    <p>Prompt 3: Australian Open</p>
    <p>Source Prompts</p>
    <p>Model</p>
    <p>Learn</p>
    <p>World Cup 2018</p>
    <p>Target Prompt</p>
    <p>Predict</p>
    <p>Unavailability of rated essays written for the target</p>
    <p>prompt</p>
  </div>
  <div class="page">
    <p>Challenges in Prompt-independent AES</p>
    <p>Previous approaches learn on source prompts  Domain adaption [Phandi et al. EMNLP 2015]  Cross-domain learning [Dong &amp; Zhang, EMNLP</p>
    <p>Prompt 1: Winter Olympics</p>
    <p>Prompt 2: Rugby World Cup</p>
    <p>Prompt 3: Australian Open</p>
    <p>Source Prompts</p>
    <p>Model</p>
    <p>Learn</p>
    <p>World Cup 2018</p>
    <p>Target Prompt</p>
    <p>Predict</p>
  </div>
  <div class="page">
    <p>Challenges in Prompt-independent AES</p>
    <p>Prompt 1: Winter Olympics</p>
    <p>Prompt 2: Rugby World Cup</p>
    <p>Prompt 3: Australian Open</p>
    <p>Source Prompts</p>
    <p>Model</p>
    <p>Learn</p>
    <p>World Cup 2018</p>
    <p>Target Prompt</p>
    <p>Predict</p>
    <p>Off-topic: essays written for source prompts are mostly irrelevant</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background</p>
    <p>Method</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>TDNN: A Two-stage Deep Neural Network for Promptindependent AES</p>
    <p>Based on the idea of transductive transfer learning</p>
    <p>Learn on target essays</p>
    <p>Utilize the content of target essays to rate</p>
  </div>
  <div class="page">
    <p>The Two-stage Architecture</p>
    <p>Prompt-independent stage: train a shallow model to create pseudo labels on the target prompt</p>
  </div>
  <div class="page">
    <p>The Two-stage Architecture</p>
    <p>Prompt-dependent stage: learn an end-to-end model to predict essay ratings for the target prompts</p>
  </div>
  <div class="page">
    <p>Prompt-independent stage</p>
    <p>Train a robust prompt-independent AES model</p>
    <p>Using Non-target prompts</p>
    <p>Learning algorithm: RankSVM for AES</p>
    <p>Pre-defined prompt-independent features</p>
    <p>Select confident essays written for the target prompt</p>
  </div>
  <div class="page">
    <p>Prompt-independent stage</p>
    <p>Train a robust prompt-independent AES model</p>
    <p>Using Non-target prompts</p>
    <p>Learning algorithm: RankSVM</p>
    <p>Pre-defined prompt-independent features</p>
    <p>Select confident essays written for the target prompt</p>
    <p>Predicted Scores</p>
  </div>
  <div class="page">
    <p>Prompt-independent stage</p>
    <p>Train a robust prompt-independent AES model</p>
    <p>Using Non-target prompts</p>
    <p>Learning algorithm: RankSVM</p>
    <p>Pre-defined prompt-independent features</p>
    <p>Select confident essays written for the target prompt</p>
    <p>Predicted Scores</p>
    <p>Predicted ratings in [0, 4] as negative examples</p>
  </div>
  <div class="page">
    <p>Prompt-independent stage</p>
    <p>Train a robust prompt-independent AES model</p>
    <p>Using Non-target prompts</p>
    <p>Learning algorithm: RankSVM</p>
    <p>Pre-defined prompt-independent features</p>
    <p>Select confident essays written for the target prompt</p>
    <p>Predicted Scores</p>
    <p>Predicted ratings in [8, 10] as positive examples</p>
  </div>
  <div class="page">
    <p>Prompt-independent stage</p>
    <p>Train a robust prompt-independent AES model</p>
    <p>Using Non-target prompts</p>
    <p>Learning algorithm: RankSVM</p>
    <p>Pre-defined prompt-independent features</p>
    <p>Select confident essays written for the target prompt</p>
    <p>Predicted Scores</p>
    <p>Converted to 0/1 labels</p>
  </div>
  <div class="page">
    <p>Prompt-independent stage</p>
    <p>Train a robust prompt-independent AES model</p>
    <p>Using Non-target prompts</p>
    <p>Learning algorithm: RankSVM</p>
    <p>Pre-defined prompt-independent features</p>
    <p>Select confident essays written for the target prompt</p>
    <p>Common sense: 8 is good, &lt;5 is bad</p>
    <p>Enlarge sample size</p>
  </div>
  <div class="page">
    <p>Prompt-dependent stage</p>
    <p>Train a hybrid deep model for a prompt</p>
    <p>dependent assessment</p>
    <p>An end-to-end neural network with three parts</p>
    <p>of inputs:</p>
    <p>Word semantic embeddings</p>
    <p>Part-of-speech (POS) taggings</p>
    <p>Syntactic taggings</p>
  </div>
  <div class="page">
    <p>Architecture of the hybrid deep model</p>
    <p>Multi-layer structure: Words  (phrases) - Sentences  Essay</p>
  </div>
  <div class="page">
    <p>Architecture of the hybrid deep model</p>
    <p>Glove word embeddings</p>
  </div>
  <div class="page">
    <p>Architecture of the hybrid deep model</p>
    <p>Part-of-speech taggings</p>
  </div>
  <div class="page">
    <p>Architecture of the hybrid deep model</p>
    <p>Syntactic taggings</p>
  </div>
  <div class="page">
    <p>Architecture of the hybrid deep model</p>
    <p>Multi-layer structure: Words  (phrases) - Sentences  Essay</p>
  </div>
  <div class="page">
    <p>Architecture of the hybrid deep model</p>
  </div>
  <div class="page">
    <p>Model Training</p>
    <p>Training loss: MSE on 0/1 pseudo labels</p>
    <p>Validation metric: Kappa on 30% non-target essays</p>
    <p>Select the model that can best rate</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background</p>
    <p>Method</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Dataset &amp; Metrics</p>
    <p>We use the standard ASAP corpus  8 prompts with &gt;10K essays in total</p>
    <p>Prompt-independent AES: 7 prompts are used for training, 1 for testing</p>
    <p>Report on common human-machine agreement metrics  Pearsons correlation coefficient (PCC)</p>
    <p>Spearmans correlation coefficient (SCC)</p>
    <p>Quadratic weighted Kappa (QWK)</p>
  </div>
  <div class="page">
    <p>Baselines  RankSVM based on prompt-independent handcrafted</p>
    <p>features</p>
    <p>Also used in the prompt-independent stage in TDNN</p>
    <p>2L-LSTM [Alikaniotis et al. , ACL 2016]</p>
    <p>Two LSTM layer + linear layer</p>
    <p>CNN-LSTM [Taghipour &amp; Ng, EMNLP 2016]</p>
    <p>CNN + LSTM + linear layer</p>
    <p>CNN-LSTM-ATT [Dong et al. , CoNLL 2017]</p>
    <p>CNN-LSTM + attention</p>
  </div>
  <div class="page">
    <p>High variance of DNN models performance on all 8 prompts  Possibly caused by learning on non-target prompts</p>
    <p>RankSVM appears to be the most stable baseline  Justifies the use of RankSVM in the first stage of TDNN</p>
    <p>RankSVM is the most robust baseline</p>
  </div>
  <div class="page">
    <p>TDNN outperforms the best baseline on 7 out of 8 prompts  Performance improvements gained by learning on the target</p>
    <p>prompt</p>
    <p>Comparison to the best baseline</p>
  </div>
  <div class="page">
    <p>Average performance on 8 prompts</p>
    <p>Method QWK PCC SCC</p>
    <p>Baselines RankSVM .5462 .6072 .5976</p>
    <p>CNN-LSTM .5362 .6569 .6139</p>
    <p>CNN-LSTM-ATT .5057 .6535 .6368</p>
    <p>TDNN TDNN(Sem) .5875 .6779 .6795</p>
    <p>TDNN(Sem+POS) .6582 .7103 .7130</p>
    <p>TDNN(Sem+Synt) .6856 .7244 .7365</p>
    <p>TDNN(POS+Synt) .6784 .7189 .7322</p>
    <p>TDNN(ALL) .6682 .7176 .7258</p>
  </div>
  <div class="page">
    <p>Average performance on 8 prompts</p>
    <p>Method QWK PCC SCC</p>
    <p>Baselines RankSVM .5462 .6072 .5976</p>
    <p>CNN-LSTM .5362 .6569 .6139</p>
    <p>CNN-LSTM-ATT .5057 .6535 .6368</p>
    <p>TDNN TDNN(Sem) .5875 .6779 .6795</p>
    <p>TDNN(Sem+POS) .6582 .7103 .7130</p>
    <p>TDNN(Sem+Synt) .6856 .7244 .7365</p>
    <p>TDNN(POS+Synt) .6784 .7189 .7322</p>
    <p>TDNN(ALL) .6682 .7176 .7258</p>
  </div>
  <div class="page">
    <p>Average performance on 8 prompts</p>
    <p>Method QWK PCC SCC</p>
    <p>Baselines RankSVM .5462 .6072 .5976</p>
    <p>CNN-LSTM .5362 .6569 .6139</p>
    <p>CNN-LSTM-ATT .5057 .6535 .6368</p>
    <p>TDNN TDNN(Sem) .5875 .6779 .6795</p>
    <p>TDNN(Sem+POS) .6582 .7103 .7130</p>
    <p>TDNN(Sem+Synt) .6856 .7244 .7365</p>
    <p>TDNN(POS+Synt) .6784 .7189 .7322</p>
    <p>TDNN(ALL) .6682 .7176 .7258</p>
  </div>
  <div class="page">
    <p>Sanity Check: Relative Precision How the quality of pseudo examples affects the performance of</p>
    <p>TDNN?  The sanctity of the selected essays, namely, the number of positive</p>
    <p>(negative) essays that are better (worse) than all negative (positive)</p>
    <p>essays.</p>
    <p>Such relative precision is at least 80% and mostly beyond 90% on different prompts</p>
    <p>TDNN can at least learn</p>
    <p>from correct 0/1 labels</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>It is beneficial to learn an AES model on the target prompt</p>
    <p>Syntactic features are useful addition to the widely used Word2Vec embeddings</p>
    <p>Sanity check: small overlap between pos/neg examples</p>
    <p>Prompt-independent AES remains an open problem  ETS wants Kappa&gt;0.70</p>
    <p>TDNN can achieve 0.68 at best</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
