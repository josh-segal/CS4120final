<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An Yang, Sujian Li</p>
    <p>Peking University</p>
    <p>ACL 2018</p>
    <p>SciDTB: Discourse Dependency</p>
    <p>Treebank for Scientific Abstracts</p>
  </div>
  <div class="page">
    <p>Background: discourse dependency structure &amp; treebanks</p>
    <p>Main work: details about SciDTB  Annotation framework</p>
    <p>Corpus construction</p>
    <p>Statistical analysis</p>
    <p>SciDTB as evaluation benchmark</p>
    <p>Conclusion &amp; summary</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Discourse Dependency Structure &amp; Treebanks</p>
    <p>Example text: [Syntactic parsing is useful in NLP.]e1 [We present a parsing algorithm,]e2 [which improves classical transition-based approach.]e3</p>
    <p>10 32</p>
    <p>ROOT</p>
    <p>background elaboration</p>
    <p>(ROOT node)</p>
    <p>Discourse dependency tree: [Li. 2014; Yoshida. 2014]</p>
    <p>Advantage: flexible, simple, support non-projection</p>
    <p>Discourse dependency treebanks:  Conversion based dependency treebanks from RST or SDRT representations [Li. 2014; Stede. 2016]  Limitations: conversion errors and not support non-projection  Build a dependency treebank from scratch  Scientific abstracts: short with strong logics</p>
  </div>
  <div class="page">
    <p>Guidelines:</p>
    <p>Generally treats clauses as EDUs [Polanyi. 1988, Mann and Thompson. 1988]</p>
    <p>Subjective and some objective clauses are not segmented [Carlson and Marcu. 2001]</p>
    <p>Strong discourse cues always starts a new EDU</p>
    <p>Annotation Framework: Discourse Segmentation</p>
    <p>Example 1: [The challenge of copying mechanism in Seq2Seq is that new machinery is needed]e1 [to decide when to perform the operation.]e2</p>
    <p>Discourse segmentation: Segment abstracts into elementary discourse units (EDUs)</p>
    <p>Example 2: [Despite bilingual embeddings success,]e1 [the contextual information]e2 [which is important to translation quality,]e3 [was ignored in previous work.]e4</p>
  </div>
  <div class="page">
    <p>Annotation Framework: Obtain Tree Structure</p>
    <p>A tree is composed of relations &lt; ,, &gt;  : the EDU with essential information</p>
    <p>: the EDU with supportive content</p>
    <p>: relation type (17 coarse-grained and 26 fine-grained types)</p>
    <p>Each EDU has one and only one head  One EDU is dominated by ROOT node</p>
    <p>Polynary relations</p>
    <p>1 32 4</p>
    <p>joint</p>
    <p>1 2 4</p>
    <p>process-step</p>
    <p>3 Multi-coordination One-dominates-many</p>
  </div>
  <div class="page">
    <p>Annotation Example in SciDTB</p>
    <p>Abstract from http://www.aclweb.org/anthology/</p>
  </div>
  <div class="page">
    <p>Annotator Recruitment:  5 annotators were selected after test annotation</p>
    <p>EDU Segmentation:  Semi-automatic: pre-trained SPADE [Soricut. 2003] + Manual proofreading</p>
    <p>Tree Annotation:  The annotation lasted 6 months</p>
    <p>63% abstracts were annotated more than twice</p>
    <p>An online tool was developed for annotating and visualizing DT trees</p>
    <p>Corpus Construction</p>
  </div>
  <div class="page">
    <p>Online Annotation Tool</p>
    <p>Website: http://123.56.88.210/demo/depannotate/</p>
  </div>
  <div class="page">
    <p>The consistency of tree annotation is analyzed by 3 metrics:  Unlabeled accuracy score: structural consistency</p>
    <p>Labeled accuracy score: overall consistency</p>
    <p>Cohens Kappa: consistency on relation label conditioned on same structure</p>
    <p>Reliability: Annotation Consistency</p>
    <p>Annotators #Doc. UAS LAS Kappa score</p>
    <p>Annotator 1 &amp; 2 93 0.811 0.644 0.763</p>
    <p>Annotator 1 &amp; 3 147 0.800 0.628 0.761</p>
    <p>Annotator 1 &amp; 4 42 0.772 0.609 0.767</p>
    <p>Annotator 3 &amp; 4 46 0.806 0.639 0.772</p>
    <p>Annotator 4 &amp; 5 44 0.753 0.550 0.699</p>
  </div>
  <div class="page">
    <p>SciDTB is  comparable with PDTB and RST-DT considering size of units and relations</p>
    <p>much larger than existing domain-specific discourse treebanks</p>
    <p>Annotation Scale</p>
    <p>Corpus #Doc. #Doc. (unique) #Text unit #Relation Source Annotation form</p>
    <p>SciDTB 1355 798 18978 18978 Scientific abstracts Dependency trees</p>
    <p>RST-DT 438 385 24828 23611 Wall Street Journal RST trees</p>
    <p>PDTB v2.0 2159 2159 38994 40600 Wall Street Journal Relation pairs</p>
    <p>BioDRB 24 24 5097 5859 Biomedical articles Relation pairs</p>
  </div>
  <div class="page">
    <p>Dependency distance  Most relations (61.6%) occur between neighboring EDUs  The distance of 8.8% relations is greater than 5</p>
    <p>Non-projection: 3% of the whole corpus</p>
    <p>Structural Characteristics</p>
  </div>
  <div class="page">
    <p>We make SciDTB as a benchmark for evaluating discourse dependency parsers</p>
    <p>Data partition: 492/154/152 abstracts for train/dev/test set</p>
    <p>3 baselines are implemented:  Vanilla transition based parser</p>
    <p>Two-stage transition based parser a simpler version of [Wang, 2017]</p>
    <p>Graph based parser</p>
    <p>Model Dev set Test set</p>
    <p>UAS LAS UAS LAS</p>
    <p>Vanilla transition 0.730 0.557 0.702 0.535</p>
    <p>Two-stage transition 0.730 0.577 0.702 0.545</p>
    <p>Graph-based 0.577 0.455 0.576 0.425</p>
    <p>Human 0.806 0.627 0.802 0.622</p>
    <p>SciDTB as Benchmark</p>
  </div>
  <div class="page">
    <p>Summary:  We propose a discourse dependency treebank with following features:</p>
    <p>constructed from scratch</p>
    <p>Scientific abstracts</p>
    <p>comparable with existing treebanks in size</p>
    <p>We further make SciDTB as a benchmark</p>
    <p>Future work:  Consider longer scientific articles</p>
    <p>Develop effective parsers on SciDTB</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Contact: yangan@pku.edu.cn</p>
    <p>SciDTB is available: https://github.com/PKU-TANGENT/SciDTB</p>
    <p>Thank you!</p>
  </div>
</Presentation>
