<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Lee-Ad Gottlieb</p>
    <p>Robert Krauthgamer</p>
    <p>Weizmann Institute</p>
  </div>
  <div class="page">
    <p>Proximity problems  In arbitrary metric space, some proximity problems are hard</p>
    <p>For example, the nearest neighbor search problem requires (n) time</p>
    <p>The doubling dimension</p>
    <p>parameterizes the bad</p>
    <p>case</p>
    <p>q</p>
    <p>~1 ~1</p>
    <p>~1</p>
    <p>~1</p>
    <p>~1</p>
  </div>
  <div class="page">
    <p>Doubling Dimension  Definition: Ball B(x,r) = all points within distance r from x.</p>
    <p>The doubling constant (of a metric M) is the minimum value  such that every ball can be covered by  balls of half the radius  First used by [Ass-83], algorithmically by [Cla-97].  The doubling dimension is dim(M)=log (M) [GKL-03]  A metric is doubling if its doubling dimension is constant</p>
    <p>Packing property of doubling spaces  A set with diameter D and min. inter-point</p>
    <p>distance a, contains at most</p>
    <p>(D/a)O(log) points Here 7.</p>
  </div>
  <div class="page">
    <p>Applications  In the past few years, many algorithmic tasks have been</p>
    <p>analyzed via the doubling dimension  For example, approximate nearest neighbor search can be executed in</p>
    <p>time O(1) log n</p>
    <p>Some other algorithms analyzed via the doubling dimension  Nearest neighbor search [KL-04, BKL-06, CG-06]  Clustering [Tal-04, ABS-08, FM-10]  Spanner construction [GGN-06, CG-06, DPP-06, GR-08]  Routing [KSW-04, Sil-05, AGGM-06, KRXY-07, KRX-08]  Travelling Salesperson [Tal-04]  Machine learning [BLL-09, GKK-10]</p>
    <p>Message: This is an active line of research</p>
  </div>
  <div class="page">
    <p>Problem  Most algorithms developed for doubling spaces are not robust</p>
    <p>Algorithmic guarantees dont hold for nearly-doubling spaces  If a small fraction of the working set possesses high doubling dimension,</p>
    <p>algorithmic performance degrades.</p>
    <p>This problem motivates the following key task  Given an n-point set S and target dimension d*  Remove from S the fewest number of points so that the remaining set</p>
    <p>has doubling dimension at most d*</p>
  </div>
  <div class="page">
    <p>Two paradigms  How can removing a few bad points help? Two models:</p>
    <p>1. Ignore the bad points  Outlier detection.</p>
    <p>[GHPT-05] cluster based on similarity, seek a large subset with low intrinsic dimension.</p>
    <p>Algorithms with slack. Throw bad points into the slack  [KRXY-07] gave a routing algorithm with guarantees for most of the input</p>
    <p>points.  [FM-10] gave a kinetic clustering algorithm for most of the input points.  [GKK-10] gave a machine learning algorithm  small subset doesnt interfere</p>
    <p>with learning</p>
  </div>
  <div class="page">
    <p>Two paradigms  How can removing a few bad points help? Two models:</p>
    <p>2. Tailor a different algorithm for the bad points  Example: Spanner construction. A spanner is an edge subset of the full</p>
    <p>graph  Good points: Low doubling dimension sparse spanner with nice</p>
    <p>properties (low stretch and degree)  Bad points: Take the full graph  If the number of bad points is O(n.5), we have a spanner with O(n) edges</p>
  </div>
  <div class="page">
    <p>Results  Recall our key problem</p>
    <p>Given an n-point set S and target dimension d*  Remove from S the fewest number of points so that the remaining set</p>
    <p>has doubling dimension at most d*</p>
    <p>This problem is NP-hard  Even determining the doubling dimension of a point set exactly is NP</p>
    <p>hard!  Proof on the next slide  But the doubling dimension can be approximated within a constant factor</p>
    <p>Our contribution: bicriteria approximation algorithm  In time 2O(d*) n3, we remove a number of points arbitrarily close to optimal,</p>
    <p>while achieving doubling dimension 4d* + O(1)  We can also achieve near-linear runtime, at the cost of slightly higher</p>
    <p>dimension</p>
  </div>
  <div class="page">
    <p>Warm up  Lemma: It is NP-hard to determine the doubling dimension of a set S</p>
    <p>Reduction: from vertex cover with bounded degree  = n.  the size of any vertex cover is at least n.</p>
    <p>Construction: A set S of n points corresponding to the vertex set V.  Let d(u,v) =  if the cor. vertices are connected by an edge  Let d(u,v) = 1 if the cor. vertices arent connected</p>
    <p>Analysis:  Any subset of S found in a ball of radius  has at most n points - degree of original</p>
    <p>graph  S is a ball of radius 1. The minimum covering of all of S with balls of radius  is equal to</p>
    <p>the minimum vertex cover of V.</p>
    <p>Note: reduction preserves hardness of approximation  Corollary: It is NP-hard to determine if removing k points from S can</p>
    <p>leave a set with doubling dimension d*.  So our problem is hard as well.</p>
  </div>
  <div class="page">
    <p>Bicriteria algorithm  Recall that he doubling constant (of a metric M) is</p>
    <p>the minimum value  such that every r-radius ball can be covered by  balls of half the radius</p>
    <p>Define the related notion of density constant as  the minimum value &gt;0 such that every r-radius ball contains at most</p>
    <p>points at mutual interpoint distance r/2  Nice property: The density constant can only decrease under the</p>
    <p>removal of points, unlike the doubling constant.</p>
    <p>We can show that  (S)  (S)  (S)  its NP-hard to compute the density constant</p>
    <p>(ratio-preserving reduction from independent set)</p>
    <p>=2, =3</p>
  </div>
  <div class="page">
    <p>Bicriteria algorithm  We will give a bicriteria algorithm for the density constant.</p>
    <p>Problem statement:  Given an n-point set S and target density constant *  Remove from S the fewest number of points so that the remaining set</p>
    <p>has density constant at most *</p>
    <p>A bicriteria algorithm for the density constant is itself a bicriteria algorithm for the doubling constant  within a quadratic factor</p>
  </div>
  <div class="page">
    <p>Witness set  Given a set S, a subset S is a witness set</p>
    <p>for the density constant if  All points are at interpoint distance at least r/2  Note that S is a concise proof that the density</p>
    <p>constant of S is at least |S|</p>
    <p>Theorem: Fix a value &lt; (S). A witness set of S of size at least  can be found in time 2O(*) n3  Proof outline:  For each point p and radius r define the r-ball</p>
    <p>of p.  Greedily cover all points in the r-ball with</p>
    <p>disjoint balls of radius r/2.  Then cover all points in each r/2 ball with</p>
    <p>disjoint balls of radius r/4.  Since there exists in S a witness set of size</p>
    <p>(S), there exists a p and r so that  either there are (S) r/2 balls, and these form a</p>
    <p>witness set, or  one r/2 ball covers (S) r/4 balls, and these</p>
    <p>form a witness set.</p>
  </div>
  <div class="page">
    <p>Bicriteria algorithm  Recall our problem</p>
    <p>Given an n-point set S and target density constant *  Remove from S the fewest number of points so that the remaining set</p>
    <p>has density constant at most *</p>
    <p>Our bricriteria solution:  Let k be the true answer (the minimum number of points that must be</p>
    <p>removed).  We remove k c/(c-1) points and the remaining set has density constant</p>
    <p>c2*2</p>
  </div>
  <div class="page">
    <p>Bicriteria algorithm  Algorithm</p>
    <p>Run the subroutine to identify a witness set of size at least c*  Remove it  Repeat</p>
    <p>Analysis  The density constant of the resulting set is not greater than c2*2</p>
    <p>since we terminated without finding a witness set of size at least c*  Every time a witness set of size w&gt;c* is removed by our algorithm, the</p>
    <p>optimal algorithm must remove at least w-* points  or else the true solution would have density constant greater than *</p>
    <p>It follows that are algorithm removes k w/(w-*) &lt; kc/(c-1) points</p>
  </div>
  <div class="page">
    <p>Conclusion  We conclude that there exists a bicriteria algorithm for the</p>
    <p>density constant  We remove k c/(c-1) points and the remaining set has density constant</p>
    <p>c2*2</p>
    <p>It follows that there exists a bricriteria algorithm for the doubling constant  We remove k c/(c-1) points and the remaining set has doubling constant</p>
    <p>c4*4</p>
  </div>
</Presentation>
