<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Characterizing Storage Workloads with Counter Stacks</p>
    <p>Jake Wires, Stephen Ingram, Zachary Drudi, Nicholas J. A. Harvey, Andrew Warfield</p>
    <p>Coho Data, UBC</p>
  </div>
  <div class="page">
    <p>Memory Hierarchies</p>
  </div>
  <div class="page">
    <p>Memory Hierarchies</p>
  </div>
  <div class="page">
    <p>Challenge: Provisioning</p>
  </div>
  <div class="page">
    <p>Challenge: Provisioning</p>
    <p>???</p>
  </div>
  <div class="page">
    <p>Challenge: Placement</p>
  </div>
  <div class="page">
    <p>Workload Characterization</p>
    <p>Provisioning and placement are difficult problems</p>
    <p>What are the key workload characteristics we can use to solve these problems?</p>
  </div>
  <div class="page">
    <p>Optimal</p>
    <p>MIN (Belady, '66): prioritize pages with shortest forward distance</p>
  </div>
  <div class="page">
    <p>Practical</p>
    <p>LRU: prioritize pages with shortest reuse distance</p>
  </div>
  <div class="page">
    <p>Practical</p>
    <p>LRU: prioritize pages with shortest reuse distance</p>
    <p>Warning: past behavior</p>
    <p>does not predict future performance.</p>
  </div>
  <div class="page">
    <p>Reuse Distances</p>
    <p># of distinct symbols since previous reference</p>
    <p>Measure of workload locality  Model of memory behavior</p>
    <p>A B C A A B</p>
  </div>
  <div class="page">
    <p>Miss Ratio Curves</p>
    <p>A plot of miss rate vs. cache size for a given workload under a given replacement policy  With LRU, this is the distribution of reuse</p>
    <p>distances</p>
  </div>
  <div class="page">
    <p>Miss Ratio Curves</p>
  </div>
  <div class="page">
    <p>Miss Ratio Curves you miss this often</p>
    <p>if your cache is this big</p>
  </div>
  <div class="page">
    <p>Miss Ratio Curves</p>
    <p>low value, high cost</p>
    <p>high value, low cost</p>
  </div>
  <div class="page">
    <p>Miss Ratio Curves</p>
    <p>Hardware Monitor Web Proxy Web/SQL Server</p>
  </div>
  <div class="page">
    <p>Miss Ratio Curves</p>
    <p>One Hour Twelve Hours One Week</p>
  </div>
  <div class="page">
    <p>Computing MRCs</p>
    <p>Nave approach</p>
    <p>Simulate workload once at each cache size</p>
  </div>
  <div class="page">
    <p>Computing MRCs</p>
    <p>Nave approach</p>
    <p>Simulate workload once at each cache size</p>
  </div>
  <div class="page">
    <p>Computing MRCs</p>
    <p>Mattson's Stack Algorithm ('70)</p>
    <p>Some replacement policies are inclusive  Larger caches always include contents of smaller</p>
    <p>caches</p>
  </div>
  <div class="page">
    <p>Computing MRCs</p>
    <p>Mattson's Stack Algorithm ('70)</p>
    <p>Some replacement policies are inclusive  Larger caches always include contents of smaller</p>
    <p>caches  LRU, LFU, MIN, ...</p>
    <p>For such policies, simulate all cache sizes in one pass  Hits at size N are hits at all M &gt; N</p>
  </div>
  <div class="page">
    <p>Stack Algorithm for LRU</p>
    <p>To compute miss ratio curves for LRU:</p>
    <p>Compute reuse distance of each request</p>
    <p>Aggregate distances in a histogram</p>
    <p>Compute the cumulative sum (CDF)</p>
  </div>
  <div class="page">
    <p>Stack Algorithm for LRU</p>
    <p>Complexity (N records, M unique symbols):</p>
    <p>Time: O(N * M)  Reduced to O(N * log(N)) (Bennett et al., '75)  Reduced to O(N * log(M)) (Almsi et al., '02)</p>
    <p>Space: O(M)</p>
  </div>
  <div class="page">
    <p>Stack Algorithm for LRU</p>
    <p>Complexity (N records, M unique symbols):</p>
    <p>Time: O(N * M)  Reduced to O(N * log(N)) (Bennett et al., '75)  Reduced to O(N * log(M)) (Almsi et al., '02)</p>
    <p>Space: O(M)  ...</p>
  </div>
  <div class="page">
    <p>Still Not Practical</p>
    <p>92 GB RAM to compute MRC of 3 TB workload</p>
  </div>
  <div class="page">
    <p>Still Not Practical</p>
    <p>92 GB RAM to compute MRC of 3 TB workload</p>
  </div>
  <div class="page">
    <p>Stack Algorithm for LRU</p>
    <p>To compute miss ratio curves for LRU:</p>
    <p>Compute reuse distance of each request</p>
    <p>Aggregate distances in a histogram</p>
    <p>Compute the cumulative sum (CDF)</p>
    <p>Can we do this more efficiently?</p>
  </div>
  <div class="page">
    <p>Stack Algorithm for LRU</p>
    <p>To compute miss ratio curves for LRU:</p>
    <p>Compute reuse distance of each request</p>
    <p>Aggregate distances in a histogram</p>
    <p>Compute the cumulative sum (CDF)</p>
    <p>Can we do this more efficiently? Yes.</p>
    <p>80 MB for approximate MRC of 3 TB workload</p>
  </div>
  <div class="page">
    <p>Counter Stacks</p>
    <p>Measure uniqueness over time</p>
    <p>Observation: computing reuse distances is related to counting distinct elements</p>
    <p>Consider a 'stack' of cardinality counters, one for each request</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
    <p>+0</p>
    <p>+1</p>
    <p>Observation 1: A difference in the change between adjacent counters implies a repeated reference.</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>Reference String: A B C A cardinality counter started at t</p>
    <p>+0</p>
    <p>+1</p>
    <p>Observation 1: A difference in the change between adjacent counters implies a repeated reference.</p>
    <p>Observation 2: The location of the difference stores the reuse distance.</p>
  </div>
  <div class="page">
    <p>Calculating with Counts</p>
    <p>A B C A 1 2 3 3</p>
    <p>A B C A 1 1 1 0</p>
    <p>A B C A 0 0 0 1</p>
    <p>x y</p>
    <p>Matrix C</p>
  </div>
  <div class="page">
    <p>Perfect Counting</p>
    <p>One cardinality counter per request  Quadratic overhead!</p>
  </div>
  <div class="page">
    <p>Perfect Counting</p>
    <p>~5 ZB RAM to compute MRC of 3 TB workload</p>
  </div>
  <div class="page">
    <p>Practical Counting</p>
    <p>C is highly redundant  Space/accuracy tradeoff</p>
    <p>A B C A A C A B C A</p>
  </div>
  <div class="page">
    <p>Practical Counting</p>
    <p>Downsample</p>
    <p>A B C A A C A B C A</p>
  </div>
  <div class="page">
    <p>Practical Counting</p>
    <p>Downsample</p>
    <p>Only output every kth counter</p>
    <p>A B C A A C A B C A</p>
  </div>
  <div class="page">
    <p>Practical Counting</p>
    <p>Downsample</p>
    <p>Only output every kth counter  Only output every kth count</p>
    <p>A B C A A C A B C A</p>
  </div>
  <div class="page">
    <p>Practical Counting</p>
    <p>Prune: discard counters with similar values (i.e., differing less than pruning distance p)</p>
    <p>A B C A A C A B C A</p>
  </div>
  <div class="page">
    <p>Practical Counting</p>
    <p>Prune: discard counters with similar values (i.e., differing less than pruning distance p)</p>
    <p>A B C A A C A B C A</p>
  </div>
  <div class="page">
    <p>Approximate Counting</p>
    <p>Estimate: use probabilistic counters</p>
  </div>
  <div class="page">
    <p>Approximate Counting</p>
    <p>Estimate: use probabilistic counters</p>
    <p>HyperLogLog (Flajolet et al., '07)  Accurate estimates of large multisets with</p>
    <p>sublinear space</p>
  </div>
  <div class="page">
    <p>Counter Stacks</p>
    <p>Sublinear memory overhead  Practical for online computation</p>
  </div>
  <div class="page">
    <p>Counter Stacks</p>
    <p>Sublinear memory overhead  Practical for online computation</p>
    <p>But wait, there's more...</p>
  </div>
  <div class="page">
    <p>Counter Stack Streams</p>
    <p>We can compute x, y, and reuse distances with only the last two columns of C</p>
    <p>We store all columns on disk as a Counter Stack Stream</p>
    <p>Preserves a history of locality</p>
  </div>
  <div class="page">
    <p>Counter Stack Stream Queries</p>
  </div>
  <div class="page">
    <p>Counter Stack Stream Queries</p>
    <p>Search for outliers  Identify phase changes  Explore coarse-grain scheduling</p>
  </div>
  <div class="page">
    <p>How Much Do They Cost?</p>
    <p>Technique RAM Throughput Storage Mattson 92 GB 680 K reqs/sec 2.9 GB</p>
    <p>high-fidelity CS 80.6 MB (1168x) 2.29 M reqs/sec (3.37x) 11 MB (270x)</p>
    <p>low-fidelity CS 78.5 MB (1200x) 2.31 M reqs/sec (3.40x) 747 KB (4070x)</p>
    <p>MSR Cambridge storage traces  2.7 TB unique data  13 servers, 36 volumes, one week  417 million records in 5 GB of gzipped CSV</p>
    <p>compression parameters are tunable: high: k = 106, p = 98% low: k = 106, p = 90%</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Managing data can be data-intensive!</p>
    <p>Counter Stacks measure uniqueness over time</p>
    <p>Low memory and storage overheads</p>
    <p>Easy to capture, process, and store workload histories  Used in production:</p>
    <p>Collecting traces from the field</p>
    <p>Making online placement decisions</p>
    <p>Forecasting benefits of adding more hardware</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>How Well Do They Work?</p>
  </div>
</Presentation>
