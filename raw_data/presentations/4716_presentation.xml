<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>HKUST System Description Toward Integrating Word Sense and Entity Disambiguation into SMT</p>
    <p>Marine CARPUAT Yihai SHEN Xiaofeng YU Dekai WU</p>
    <p>HKUST Human Language Technology Center</p>
    <p>Department of Computer Science University of Science and Technology</p>
    <p>Hong Kong</p>
    <p>{marine, shenyh, xfyu, dekai}@cs.ust.hk</p>
  </div>
  <div class="page">
    <p>The HKUST submission</p>
    <p>Goals for our first-time IWSLT participation:</p>
    <p>Test our current system, designed primarily for Chinese-English text translation</p>
    <p>on various data sets and input conditions Chinese-English text, read speech, spontaneous speech</p>
    <p>on various language pairs from different language families Arabic-English, Chinese-English, Italian-English, Japanese-English</p>
    <p>Investigate integration of semantic processing</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>System description Core phrase-based SMT engine Word Sense Disambiguation for lexical choice Named-Entity translation</p>
    <p>IWSLT results Chinese-English Other language pairs</p>
  </div>
  <div class="page">
    <p>System description Core MT engine uses phrase-based SMT</p>
    <p>Baseline is a phrase-based log-linear model</p>
    <p>Phrasal bilexicon learned from intersection of IBM4 alignments Following Koehn [2003], base features are:</p>
    <p>conditional translation probabilities in both directions lexical weights derived from word translation probabilities</p>
    <p>Decoder Pharaoh [Koehn 2004]</p>
    <p>Language model standard 3-gram model trained using SRI LM toolkit [Stolcke 2002]</p>
  </div>
  <div class="page">
    <p>Integration of semantic processing 1) Word Sense Disambiguation for lexical choice</p>
    <p>Phrase-based SMT makes little use of context information In contrast, WSD approaches generalize across rich contextual features to choose a word sense</p>
    <p>Previous work: Senseval WSD models do not help translation quality when integrated into a word-based SMT model [Carpuat &amp; Wu 2005]</p>
    <p>In this new version, we repurpose the WSD models for SMT: WSD senses are exactly same as SMT translation candidates WSD training data is exactly same as SMT training data WSD scores are added to log linear model feature set</p>
  </div>
  <div class="page">
    <p>The HKUST WSD System</p>
    <p>Proved highly effective at Senseval-3 Placed first on Chinese lexical sample Placed second on Multilingual lexical sample (translation) 71.4% on English lexical sample (median 67.2, best 72.9)</p>
    <p>Classifier ensemble: nave Bayes [Yarowsky &amp; Florian 2002] maximum entropy [Klein &amp; Manning 2002] boosting [Carreras et al. 2002; Wu et al. 2002]: we use boosted decision stumps Kernel PCA model [Wu et al. 2004]</p>
  </div>
  <div class="page">
    <p>Sense prediction</p>
    <p>Nearest Neighbor Classifier</p>
    <p>KPCA model</p>
    <p>training vectors in KPCA feature space</p>
    <p>Unannotated test vector test vector</p>
    <p>in KPCA feature space</p>
    <p>The HKUST WSD System KPCA model is a good candidate to augment ensemble</p>
    <p>We introduced the new Kernel Principal Component Analysis (KPCA) model for WSD in Wu et al. [2004]</p>
    <p>Overview of our KPCA model for WSD</p>
    <p>Kernel Function</p>
    <p>Linear PCA</p>
    <p>Sense annotated feature vectors</p>
    <p>Non linear principal components</p>
    <p>KPCA training</p>
  </div>
  <div class="page">
    <p>The HKUST WSD System KPCA model is a good candidate to augment ensemble</p>
    <p>Is well suited to WSD: can handle high dimensional problems inherently takes into account combinations of predictive features</p>
    <p>Achieves high accuracy as a stand-alone model: outperforms maximum entropy, nave Bayes, and SVMs on Senseval-2 English lexical sample</p>
    <p>Has a different prediction bias than the 3 other voters</p>
  </div>
  <div class="page">
    <p>The HKUST WSD System Contextual Features</p>
    <p>Feature set includes: Bag-of-words context Position sensitive local collocational features Syntactic features</p>
    <p>A WSD model using these features yielded the best classification accuracy in Yarowsky &amp; Florian [2002]</p>
  </div>
  <div class="page">
    <p>Preliminary result Using WSD helps translation quality on all IWSLT dev test sets.</p>
    <p>Test set</p>
    <p>SMT system</p>
    <p>BLEU NIST</p>
    <p>Baseline 40.76 7.938Dev. Test 1</p>
    <p>+ WSD 41.28 7.981</p>
    <p>Baseline 39.81 8.153Dev. Test 2</p>
    <p>+ WSD 39.85 8.175</p>
    <p>Baseline 49.26 9.117</p>
    <p>+ WSD 49.81 9.152</p>
    <p>Baseline 16.13 5.725</p>
    <p>+ WSD 16.27 5.757</p>
    <p>Dev. Test 4</p>
    <p>Dev. Test 3</p>
    <p>Evaluation results on IWSLT 2006 Chinese-English Test Sets</p>
  </div>
  <div class="page">
    <p>Integration of semantic processing 2) Named-Entity Translation</p>
    <p>NE translation is a particular case of WSD that requires specific handling</p>
    <p>Names are rare and often never seen during training Whether a phrase is a NE is context-dependent Specific translation patterns</p>
    <p>Our approach NE recognition: tag both NE boundaries and type</p>
    <p>We distinguish Person, Organization, Location Rule-based translation approach for each category using</p>
    <p>Name-lists Transliteration scheme</p>
    <p>Integration: NE translation candidates are added to phrasal bilexicon using Pharaoh XML markup scheme for input text</p>
  </div>
  <div class="page">
    <p>The HKUST NER system</p>
    <p>Extensively evaluated on European languages at CoNLL shared tasks [Wu et al. 2002, 2003]</p>
    <p>Classifier ensemble: boosting [Carreras et al. 2002; Wu et al. 2002]: we use boosted decision stumps Support Vector Machines [Boser et al. 1992] Transformation-based learning [Brill 1995]</p>
  </div>
  <div class="page">
    <p>The HKUST NER system Feature set</p>
    <p>Lexical features within a window of 2 words around the current word</p>
    <p>Word Lemma</p>
    <p>Morphological features for the current word Prefixes and suffixes of up to 4 characters from the current word Capitalization: is the first letter or the whole word capitalized?</p>
    <p>Syntactic features POS tag within a window of 2 words around the current word 2 previous NE tags history</p>
    <p>Lexicosyntactic features Conjunctions of POS tags and words</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>System description Core phrase-based SMT engine Word Sense Disambiguation for lexical choice Named-Entity translation</p>
    <p>IWSLT results Chinese-English Other language pairs</p>
  </div>
  <div class="page">
    <p>IWSLT tasks</p>
    <p>Chinese-English speech translation Correct Recognition vs. Read Speech vs. Spontaneous Speech We do not perform ASR and simply used provided 1-best recognition results</p>
    <p>Text and read speech only Arabic-English Italian-English Japanese-English</p>
  </div>
  <div class="page">
    <p>Language-specific data preprocessing was kept minimal</p>
    <p>English data was tokenized and case-normalized</p>
    <p>Italian data was processed as if it were English</p>
    <p>Chinese data was word segmented using LDC segmenter</p>
    <p>Japanese data was used directly as provided</p>
    <p>Arabic Converted to Buckwalter romanization scheme Tokenized with ASVMT Morphological Analysis toolkit [Diab 2005]</p>
  </div>
  <div class="page">
    <p>Chinese-English evaluation results</p>
    <p>Metric HKUST result correct recog.</p>
    <p>result range correct recog.</p>
    <p>HKUST result read speech</p>
    <p>result range read speech</p>
    <p>HKUST result spont. speech</p>
    <p>result range spont. speech</p>
    <p>BLEU 18.04 12.84 24.23</p>
    <p>NIST 5.3615 4.0658 6.0961</p>
    <p>METEOR 49.15 46.01 50.33</p>
  </div>
  <div class="page">
    <p>Chinese translations for different input conditions: Example 1</p>
    <p>Translation of both text and read speech are acceptable</p>
  </div>
  <div class="page">
    <p>Chinese translations for different input conditions: Example 1</p>
    <p>Translation of both text and read speech are acceptable But SMT cant recover ASR error in spontaneous speech transcription</p>
  </div>
  <div class="page">
    <p>Chinese translations for different input conditions: Example 2</p>
    <p>Some ASR transcription errors do not affect the translation</p>
  </div>
  <div class="page">
    <p>Chinese translations for different input conditions: Example 2</p>
    <p>Some ASR transcription errors yield same translation Here, read speech transcription is worse than spontaneous speech transcription, which affects the translation</p>
  </div>
  <div class="page">
    <p>Additional language pair results</p>
  </div>
  <div class="page">
    <p>Comparison of translation quality across languages: Example 1</p>
    <p>SMT Input SMT Output</p>
    <p>Reference It is about twenty kilometers away from here.</p>
    <p>Arabic</p>
    <p>On in about twenty kilometers from here.</p>
    <p>Chinese       About twenty kilometers from here.</p>
    <p>Italian  distante circa venti chilometri da</p>
    <p>qui</p>
    <p>Its about twenty kilometers far from here.</p>
    <p>Japanese        About two  kilometers from here.</p>
  </div>
  <div class="page">
    <p>Comparison of translation quality across languages: Example 2</p>
    <p>SMT Input SMT Output</p>
    <p>Reference This wine is from France. Its very famous.</p>
    <p>Arabic</p>
    <p>This wine from France and is very popular.</p>
    <p>Chinese</p>
    <p>This is very famous French made wine.</p>
    <p>Italian questo vino viene dalla francia</p>
    <p>molto famoso</p>
    <p>This wine comes from France is very popular.</p>
    <p>Japanese</p>
    <p>This is s very famous.</p>
  </div>
  <div class="page">
    <p>Comparison of translation quality across languages: Example 3</p>
    <p>SMT Input SMT Output</p>
    <p>Reference Yes. We also have blue, red, yellow and pink.</p>
    <p>Arabic</p>
    <p>Yes, we have a red and my.</p>
    <p>Chinese</p>
    <p>Do you have any blue red yellow and pink.</p>
    <p>Italian s abbiamo anche blu rosso giallo e rosa</p>
    <p>Yes, we have red yellow blue and pink.</p>
    <p>Japanese</p>
    <p>Yes, we have red green yellow pink.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We have described the design of HKUST system which integrates semantic processing into SMT</p>
    <p>We presented results on 4 different IWSLT-06 tasks On Chinese-English, our system achieved reasonable performance, despite being a text-based system We also reported results on 3 other language pairs from different language families</p>
  </div>
</Presentation>
