<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>CSI NN Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel</p>
    <p>Lejla Batina1, Shivam Bhasin2, Dirmanto Jap2, Stjepan Picek3</p>
    <p>USENIX 2019, Santa Clara, USA 13-15 August 2019</p>
  </div>
  <div class="page">
    <p>Machine Learning &amp; Security  Machine learning (ML) has wide applications</p>
    <p>across industries.  Security is just one popular application for ML  US$ 35 Billion Industry by 20241  Optimized ML model are Intellectual property  Leaked models can leak information about</p>
    <p>sensitive training sets !2</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices  To Recover:</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices  To Recover:</p>
    <p>Number of layers</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices  To Recover:</p>
    <p>Number of layers  Number of neurons in each layer</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices  To Recover:</p>
    <p>Number of layers  Number of neurons in each layer</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices  To Recover:</p>
    <p>Number of layers  Number of neurons in each layer  Activation function in each neuron</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>This Work   Reverse Engineering  Through Side-Channel  Measured by Electromagnetic (EM) probes  Of Deep Neural Network (DNN) on embedded devices  To Recover:</p>
    <p>Number of layers  Number of neurons in each layer  Activation function in each neuron  Input weights to each neuron</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA  Non-invasive</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA  Non-invasive  Serious threat to pervasive computing</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA  Non-invasive  Serious threat to pervasive computing  Exploiting unintentional EM leakage</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA  Non-invasive  Serious threat to pervasive computing  Exploiting unintentional EM leakage  Powerful &amp; practical</p>
    <p>Keeloq  FPGA Bitstream encryption  Bitcoin wallets</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA  Non-invasive  Serious threat to pervasive computing  Exploiting unintentional EM leakage  Powerful &amp; practical</p>
    <p>Keeloq  FPGA Bitstream encryption  Bitcoin wallets</p>
    <p>Applications beyond secret key recovery</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Electromagnetic (EM) SCA Simple EM Analysis (SEMA)  Adversary learns secret information by</p>
    <p>visual inspection of (usually single) power/EM measurement</p>
    <p>Ex: observe square &amp; multiply in exponentiation etc.</p>
    <p>Differential EM Analysis (DEMA)  Adversary extract secret information</p>
    <p>statistically from EM trace  Target leakage from function f(x,k) of Secret</p>
    <p>k, input x  EM leakage  L(f(x,k))  Correct key k* maximizes: (t, L(f(x,k)))  Most commonly used leakage model L is</p>
    <p>Hamming Weight (HW)  A microcontroller leaks in Hamming Weight</p>
    <p>when sensitive data is loaded to pre-charged data bus</p>
    <p>!5 Source: https://anysilicon.com/side-channel-attacks-differential-power-analysis-dpa-simple-power-analysis-spa-works/</p>
  </div>
  <div class="page">
    <p>Adversary Model  Recover the neural network architecture using only side-channel</p>
    <p>information  Adversary does not know the architecture of the used network but</p>
    <p>can feed random/known inputs to the DNN and capture corresponding electromagnetic side-channel traces</p>
    <p>No assumption on the type of inputs; we work with real numbers  Assumption: Implementation of the machine learning algorithm with</p>
    <p>no side-channel countermeasures</p>
    <p>!6</p>
  </div>
  <div class="page">
    <p>Experimental Setup  Passive EM Measurement  Near-field probe  30dB pre-amplifier for clear signal  Measurements averaged for noise filtering  For bigger networks, measurements are made</p>
    <p>sequentially for different layers  Targets: ATMEGA AVR328P, ARM Cortex-M3</p>
    <p>!7</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>!8</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>!8Target</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>!8Target</p>
    <p>Probe</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>!8Target</p>
    <p>Probe</p>
    <p>Pre-amplifier</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>!8Target</p>
    <p>Probe</p>
    <p>Pre-amplifier</p>
    <p>Trace</p>
  </div>
  <div class="page">
    <p>Lets Start With Some Visual Inspection!!!!</p>
    <p>!9</p>
  </div>
  <div class="page">
    <p>Identifying Neurons  Simple EM Analysis  Hidden layer with 6 neurons = 6 repeating patterns  Each neuron executes a series of multiplication, followed by activation  Activation Function in this case = Sigmoid</p>
    <p>!10</p>
  </div>
  <div class="page">
    <p>Identifying Neurons  Simple EM Analysis  Hidden layer with 6 neurons = 6 repeating patterns  Each neuron executes a series of multiplication, followed by activation  Activation Function in this case = Sigmoid</p>
    <p>!10</p>
  </div>
  <div class="page">
    <p>Recovering Activation Function  Timing Attack  Each activation function has</p>
    <p>distinct timing pattern  Timing patterns can be pre</p>
    <p>characterized for different NN libraries</p>
    <p>We measure precise timing of activation function using EM measurement on oscilloscope.</p>
    <p>!11</p>
  </div>
  <div class="page">
    <p>Timing Patterns of Various Activation Function</p>
    <p>!12</p>
  </div>
  <div class="page">
    <p>Timing Patterns of Various Activation Function</p>
    <p>!12</p>
    <p>ReLU</p>
  </div>
  <div class="page">
    <p>Timing Patterns of Various Activation Function</p>
    <p>!12</p>
    <p>ReLU</p>
    <p>Sigmoid</p>
  </div>
  <div class="page">
    <p>Timing Patterns of Various Activation Function</p>
    <p>!12</p>
    <p>ReLU</p>
    <p>Sigmoid</p>
    <p>tanh</p>
  </div>
  <div class="page">
    <p>Timing Patterns of Various Activation Function</p>
    <p>!12</p>
    <p>ReLU</p>
    <p>Sigmoid</p>
    <p>tanh</p>
    <p>SoftMax</p>
  </div>
  <div class="page">
    <p>Recovering Weights</p>
    <p>!13</p>
    <p>co rr</p>
    <p>el at</p>
    <p>io n</p>
    <p>Targeted value Incorrect values</p>
    <p>co rr</p>
    <p>el at</p>
    <p>io n</p>
    <p>Targeted value Incorrect values</p>
    <p>First byte recovery (sign and 7-bit exponent)</p>
    <p>Second byte recovery (lsb exponent and mantissa)</p>
  </div>
  <div class="page">
    <p>Recovering Weights</p>
    <p>!13</p>
    <p>co rr</p>
    <p>el at</p>
    <p>io n</p>
    <p>Targeted value Incorrect values</p>
    <p>co rr</p>
    <p>el at</p>
    <p>io n</p>
    <p>Targeted value Incorrect values</p>
    <p>First byte recovery (sign and 7-bit exponent)</p>
    <p>Second byte recovery (lsb exponent and mantissa)</p>
    <p>Recovered by DEMA  Known input, secret weight  Weights in IEEE 754 format (32-bits)  Recovered Weight Precision=0.01</p>
  </div>
  <div class="page">
    <p>Recovering Number of Neurons &amp; Layers</p>
    <p>!14</p>
    <p>One hidden layer 6 neurons</p>
  </div>
  <div class="page">
    <p>Recovering Number of Neurons &amp; Layers</p>
    <p>!14</p>
    <p>One hidden layer 6 neurons</p>
    <p>Two hidden layer (6,5) neurons</p>
    <p>Three hidden layer (6,5,5) neurons</p>
  </div>
  <div class="page">
    <p>Recovering Number of Neurons &amp; Layers</p>
    <p>!14</p>
    <p>One hidden layer 6 neurons</p>
    <p>Two hidden layer (6,5) neurons</p>
    <p>Three hidden layer (6,5,5) neurons</p>
    <p>DEMA on weights used to determine layer boundaries</p>
  </div>
  <div class="page">
    <p>Full Network Recovery</p>
    <p>!15</p>
  </div>
  <div class="page">
    <p>Full Network Recovery</p>
    <p>!15 Recovery is performed layer by layer, neuron by neuron.</p>
    <p>One neuron at a time, starting from input layer</p>
  </div>
  <div class="page">
    <p>Results on ARM Cortex-M3</p>
    <p>!16</p>
    <p>Four hidden layer (50,30,20,50) neurons</p>
    <p>One Neuron in 3rd hidden layer 20 multiplications, 1 ReLU</p>
  </div>
  <div class="page">
    <p>Results on ARM Cortex-M3</p>
    <p>!16</p>
    <p>Four hidden layer (50,30,20,50) neurons</p>
    <p>One Neuron in 3rd hidden layer 20 multiplications, 1 ReLU</p>
    <p>With MNIST: Accuracy 98.16% (original) vs 98.15% (reverse engineered) Average weight error: 0.0025.</p>
  </div>
  <div class="page">
    <p>Extension to CNN on ARM Cortex-M3</p>
    <p>CIFAR-10 dataset.  Target the multiplication operation</p>
    <p>from the input with the weight, similar as in previous experiments.</p>
    <p>fixed-point arithmetic (8-bits).  The original accuracy of the CNN</p>
    <p>equals 78.47% and the accuracy of the recovered CNN is 78.11%.</p>
    <p>!17</p>
  </div>
  <div class="page">
    <p>Conclusions  With an appropriate combination of SEMA and DEMA techniques, all</p>
    <p>sensitive parameters of the network can be recovered.  A serious threat to commercial NN IPs  The attack methodology scales linearly with the size of the network.  The proposed attacks are both generic in nature and more powerful than the</p>
    <p>previous works in this direction.  Can be adapted for recovery of sensitive training/testing data  SCA countermeasures (masking/hiding) would help but overhead will be too</p>
    <p>high for NN. Motivates research for optimised countermeasures.</p>
    <p>!18</p>
  </div>
  <div class="page">
    <p>Thank You !!!</p>
    <p>Questions ???</p>
    <p>!19</p>
  </div>
  <div class="page">
    <p>Full Network Recovery</p>
    <p>!20</p>
    <p>The combination of previously developed individual techniques can thereafter result in full reverse engineering of the network.</p>
    <p>Recovery is performed layer by layer, neuron by neuron, one at a time.  Complexity grows linearly with network size.  The first step is to recover the weight wL0 of each connection from the input layer</p>
    <p>(L0) and the first hidden layer (L1).  In order to determine the output of the sum of the multiplications, the number of</p>
    <p>neurons in the layer must be known.  Using the same set of traces, timing patterns for different inputs to the activation</p>
    <p>function can be built.  The same steps are repeated in the subsequent layers</p>
  </div>
  <div class="page">
    <p>Recovering Weights  Correlation Power Analysis (CPA) i.e., a variant of DPA using the Pearsons correlation</p>
    <p>as a statistical test.  CPA targets the multiplication m = x  w of a known input x with a secret weight w.  Using the HW model, the adversary correlates the activity of the predicted output m for</p>
    <p>all hypothesis of the weight, with side-channel trace t  The correct value of the weight w will result in a higher correlation standing out from all</p>
    <p>other wrong hypotheses w, given enough measurements.  As data is represented in IEEE 754 format, each floating point number is 32 bits. 1</p>
    <p>sign bit, 8 exponent bits and 23 mantissa bits  Exact weight recovery is not required but only up to a precision (we choose 0.01)</p>
    <p>!21</p>
  </div>
</Presentation>
