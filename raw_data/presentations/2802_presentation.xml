<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DETER: Deterministic TCP Replay for Performance Diagnosis</p>
    <p>Yuliang Li, Rui Miao, Mohammad Alizadeh, Minlan Yu</p>
  </div>
  <div class="page">
    <p>TCP performance diagnosis is important</p>
    <p>Apps are more distributed  Increasingly rely on the TCP performance  Tail latency is impactful  A single long latency slows down the entire task</p>
    <p>Need a diagnosis tool for TCP problems in large scale production networks</p>
  </div>
  <div class="page">
    <p>Why diagnosing TCP is hard?</p>
    <p>What I learned in the textbook</p>
    <p>Slow start</p>
    <p>Cong. Avoid.</p>
    <p>Fast recoverySender</p>
    <p>Receiver</p>
    <p>Send ACK</p>
  </div>
  <div class="page">
    <p>TCP is complex!</p>
    <p>Reality</p>
    <p>Congestion control</p>
    <p>Loss recovery</p>
    <p>Send buffer manager</p>
    <p>Socket call manager</p>
    <p>ACK processor</p>
    <p>Packet generator</p>
    <p>Sender</p>
    <p>Timer manager</p>
    <p>Send window manager</p>
    <p>Pacing rate manager</p>
    <p>Nagle test TSO</p>
    <p>Congestion control</p>
    <p>Packet processor</p>
    <p>ACK generator</p>
    <p>Recv buffer manager</p>
    <p>Attack mitigation</p>
    <p>Socket call manager</p>
    <p>Delayed ACK manager</p>
    <p>Receiver</p>
    <p>Recv window manager</p>
    <p>Sender</p>
    <p>Receiver</p>
  </div>
  <div class="page">
    <p>TCP is complex!</p>
    <p>Unexpected interactions between diff components</p>
    <p>Congestion control</p>
    <p>Loss recovery</p>
    <p>Send buffer manager</p>
    <p>Socket call manager</p>
    <p>ACK processor</p>
    <p>Packet generator</p>
    <p>Sender</p>
    <p>Timer manager</p>
    <p>Send window manager</p>
    <p>Pacing rate manager</p>
    <p>Nagle test TSO</p>
    <p>Congestion control</p>
    <p>Packet processor</p>
    <p>ACK generator</p>
    <p>Recv buffer manager</p>
    <p>Attack mitigation</p>
    <p>Socket call manager</p>
    <p>Delayed ACK manager</p>
    <p>Receiver</p>
    <p>Recv window managerRecv window manager</p>
    <p>Loss recovery</p>
    <p>Sender</p>
    <p>No fast recovery</p>
    <p>Receiver</p>
  </div>
  <div class="page">
    <p>TCP is complex!</p>
    <p>Unexpected interactions between diff components</p>
    <p>Congestion control</p>
    <p>Loss recovery</p>
    <p>Send buffer manager</p>
    <p>Socket call manager</p>
    <p>ACK processor</p>
    <p>Packet generator</p>
    <p>Sender</p>
    <p>Timer manager</p>
    <p>Send window manager</p>
    <p>Pacing rate manager</p>
    <p>Nagle test TSO</p>
    <p>Congestion control</p>
    <p>Packet processor</p>
    <p>ACK generator</p>
    <p>Recv buffer manager</p>
    <p>Attack mitigation</p>
    <p>Socket call manager</p>
    <p>Delayed ACK manager</p>
    <p>Receiver</p>
    <p>Recv window manager</p>
    <p>Sender</p>
    <p>Receiver</p>
    <p>Ignore the packet</p>
    <p>No response</p>
    <p>Send window manager</p>
    <p>Attack mitigation</p>
  </div>
  <div class="page">
    <p>TCP is complex!</p>
    <p>Congestion control</p>
    <p>Loss recovery</p>
    <p>Send buffer manager</p>
    <p>Socket call manager</p>
    <p>ACK processor</p>
    <p>Packet generator</p>
    <p>Sender</p>
    <p>Timer manager</p>
    <p>Send window manager</p>
    <p>Pacing rate manager</p>
    <p>Nagle test</p>
    <p>TSO</p>
    <p>Congestion control</p>
    <p>Packet processor</p>
    <p>ACK generator</p>
    <p>Recv buffer manager</p>
    <p>Attack mitigation</p>
    <p>Socket call manager</p>
    <p>Delayed ACK manager</p>
    <p>Receiver</p>
    <p>Recv window manager</p>
    <p>Unexpected interactions between diff components  63 parameters in Linux TCP that tune the behaviors of diff components  Continuous error-prone development:  16 bugs found in July &amp; Aug of 2018 in Linux TCP</p>
  </div>
  <div class="page">
    <p>How do we diagnose TCP today?</p>
    <p>Tcpdump</p>
  </div>
  <div class="page">
    <p>Detailed diagnosis is not scalable</p>
    <p>Tcpdump Tcpdump Tcpdump</p>
    <p>Tcpdump</p>
    <p>Bandwidth</p>
    <p>Too much overhead!</p>
    <p># hosts</p>
  </div>
  <div class="page">
    <p>Tension between more details and low overhead</p>
    <p>Existing tools cannot achieve both  DETER solves it, by introducing replay  Lightweight recording during the runtime  Replay every detail</p>
    <p>Overhead</p>
    <p>Tcpdump</p>
    <p>Tcp probe</p>
    <p>Tcp counters</p>
    <p>DETERebpf</p>
    <p>Lots of details, but high overhead</p>
    <p>Low overhead, but miss lots of details</p>
    <p>All details, low overhead</p>
    <p>Runtime record = Data for diagnosis Runtime record &lt; Data for diagnosis</p>
  </div>
  <div class="page">
    <p>DETER overview</p>
    <p>DETER Recorder</p>
    <p>Runtime Replay</p>
    <p>Replayer</p>
    <p>Lightweight record Run continuously On all hosts</p>
    <p>Deterministic replay Capture packets/counters Trace executions Iterative diagnosis</p>
    <p>Tcpdump</p>
    <p>TCP Probe</p>
    <p>N</p>
  </div>
  <div class="page">
    <p>Lightweight record Deterministic replay</p>
  </div>
  <div class="page">
    <p>Intuition for being lightweight</p>
    <p>TCPsockcall TCP sock call</p>
    <p>Lightweight record Deterministic replayFAI L!</p>
    <p>Record socket calls Automatically generate packets</p>
  </div>
  <div class="page">
    <p>Non-deterministic interactions w/ many parties</p>
    <p>TCPsockcall TCP sock call</p>
  </div>
  <div class="page">
    <p>Non-deterministic interactions w/ many parties</p>
    <p>kernel TCPsockcall</p>
    <p>kernel TCP sockcall</p>
    <p>kernel TCPsockcall</p>
    <p>kernel TCP sockcall</p>
    <p>kernel TCPsockcall</p>
    <p>kernel TCP sockcall</p>
    <p>Key contribution:  Identifying the minimum set of data that enables deterministic replay</p>
    <p>Two challenges:  Network wide: non-deterministic interactions across switches and TCP  On host: non-determinisms within the kernel</p>
    <p>Butterfly effect</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>The closed loop between TCP and switches amplifies small noises</p>
    <p>kernel TCPsockcall</p>
    <p>kernel TCP sockcall</p>
    <p>kernel TCPsockcall</p>
    <p>kernel TCP sockcall</p>
    <p>kernel TCPsockcall</p>
    <p>kernel TCP sockcall</p>
    <p>TCP TCP</p>
    <p>TCP TCP</p>
    <p>TCP TCP</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>enqueue</p>
    <p>drop</p>
    <p>drop</p>
    <p>enqueue Cong_win/=2</p>
    <p>Cong_win++ Cong_win/=2</p>
    <p>Cong_win++</p>
    <p>Sending time variation Switch action variation</p>
    <p>TCP behavior variation Runtime Replay</p>
    <p>s-level: Clock drift, context switching, kernel scheduling, cache state</p>
    <p>TCP sock call</p>
    <p>TCP sock call</p>
    <p>TCP sock call</p>
    <p>TCP sock call</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>drop enqueue</p>
    <p>Sending time variation Switch action variation</p>
    <p>TCP behavior variation Runtime</p>
    <p>TCPsockcall</p>
    <p>TCPsockcall</p>
    <p>TCPsockcall</p>
    <p>Cong_win/=2</p>
    <p>Cong_win++</p>
    <p>Replay</p>
    <p>Cong_win/=2</p>
    <p>Cong_win++</p>
    <p>TCPsockcall</p>
    <p>TCPsockcall</p>
    <p>TCPsockcall</p>
    <p>Butterfly effect</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>To understand the impact of butterfly effect  We try to replay a long latency problem in a 3-host testbed with 3</p>
    <p>flows, by issuing the same set of socket calls as runtime  Replay 100 times, but none of them reproduce the same problem.</p>
    <p>Sending time variation Switch action variation</p>
    <p>TCP behavior variation Butterfly effect</p>
    <p>What if we reduce it?</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Run the same experiment in simulation, while controlling the sending time variation, from 0 to 1000ns</p>
    <p>Sending time variation (ns)</p>
    <p>Sending time variation Switch action variation</p>
    <p>TCP behavior variation Butterfly effect</p>
    <p>Even 1ns variation still cause butterfly effect</p>
    <p>What if we reduce it?</p>
    <p>Reducing sending time variation cannot eliminate butterfly effect</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Sending time variation Switch action variation</p>
    <p>TCP behavior variation Butterfly effect</p>
    <p>TCPTCP sock call</p>
    <p>sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Record&amp;replay Record&amp;replay</p>
    <p>Butterfly effect</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Directly borrow classic kernel replay techniques?</p>
    <p>TCPTCP sock call</p>
    <p>sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Record&amp;replay Record&amp;replay</p>
    <p>High ov erhead</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Directly borrow classic kernel replay techniques?  Solution: record&amp;replay packet stream mutations Runtime</p>
    <p>TCPTCP sock call</p>
    <p>sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Replay</p>
    <p>TCPTCP sockcall sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Drop Mark ECN Packet stream mutations Reordering</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Directly borrow classic kernel replay techniques?  Solution: record&amp;replay packet stream mutations</p>
    <p>Runtime</p>
    <p>TCPTCP sock call</p>
    <p>sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Replay</p>
    <p>TCPTCP sockcall sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Record mutations</p>
    <p>Record mutations</p>
    <p>Drops, ECN, reordering, etc. Drops, ECN, reordering, etc.</p>
    <p>Replay mutations</p>
    <p>Replay mutations</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Solution: record&amp;replay packet stream mutations</p>
    <p>Runtime</p>
    <p>TCPTCP sock call</p>
    <p>sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Replay</p>
    <p>TCPTCP sock call</p>
    <p>sock call TCPTCP sock call</p>
    <p>sock callTCP</p>
    <p>sock call TCP</p>
    <p>sock call</p>
    <p>Record mutations</p>
    <p>Record mutations</p>
    <p>Drops, ECN, reordering, etc. Drops, ECN, reordering, etc.</p>
    <p>Replay mutations</p>
    <p>Replay mutations</p>
    <p>+ Low overhead: Drop rate &lt; 10-4; ECN: 1 bit/packet; Reordering is rare</p>
    <p>+ Replaying each TCP connection is independent Connections interact via drops and ECN, which we replay.</p>
    <p>+ Need no switches for replay</p>
    <p>Resource-efficient replay: - Just need two hosts</p>
  </div>
  <div class="page">
    <p>Challenge 1: butterfly effect</p>
    <p>Solution: record&amp;replay packet stream mutations</p>
    <p>TCPsockcall TCP sock callReplay mutations</p>
    <p>Replay mutations</p>
    <p>TCPsockcall TCP sock callRecord mutations</p>
    <p>Record mutations</p>
    <p>Drop: Drop</p>
    <p>Runtime</p>
    <p>Replay</p>
    <p>Only rec ord on h</p>
    <p>osts</p>
    <p>IP_ID is consecutive</p>
  </div>
  <div class="page">
    <p>Challenge 2: non-determinisms within the kernel</p>
    <p>kernel kernel TCPsockcall TCP</p>
    <p>sock callReplay mutations</p>
    <p>Replay mutations</p>
  </div>
  <div class="page">
    <p>Handling non-determinisms within the kernel</p>
    <p>Other handler function calls (e.g., OS timer calls timeout handler)  Thread scheduling  Order of lock acquisitions of diff threads</p>
    <p>Reading kernel variables (e.g., jiffies)</p>
    <p>kernel</p>
    <p>TCP</p>
    <p>sock</p>
    <p>call</p>
    <p>Record</p>
    <p>mutations</p>
    <p>timer</p>
    <p>Sockcall hdl</p>
    <p>Timeout hdl</p>
    <p>Pkt hdl</p>
    <p>lock</p>
    <p>jiffies</p>
    <p>Runtime</p>
    <p>DETER</p>
    <p>lock</p>
    <p>kernel</p>
    <p>TCP</p>
    <p>sock</p>
    <p>call</p>
    <p>Replay</p>
    <p>mutations</p>
    <p>Hdl caller</p>
    <p>Sockcall hdl</p>
    <p>Timeout hdl</p>
    <p>Pkt hdl</p>
    <p>Replay</p>
    <p>DETER</p>
    <p>lock</p>
    <p>Read_jiffies Read_jiffies</p>
    <p>Very few</p>
    <p>Value changes infrequently, only record new values</p>
    <p>Correct input to TCP</p>
    <p>Normally race conditions are expensive to record and replay  But TCP uses one lock per connection to prevent race conditions  So we record &amp; replay the order of lock acquisitions of diff threads</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Prototype in Linux 4.4  Lightweight recorder (packet stream mutations, 3 types of kernel non-determinism)  Storage: 2.1%~3.1% compared to compressed packet header traces.  CPU: &lt; 1.49%</p>
    <p>All data are recorded on end hosts.  Just need 139 lines of changes to Linux TCP.  Open source</p>
  </div>
  <div class="page">
    <p>An RTO problem in testbed</p>
    <p>Two senders to one receiver  2 long flows (20MB) and 1 short flow (30KB)</p>
    <p>The short flow experiences 49 ms delay (2 orders of magnitude higher than expected)  In contrast, retransmission timeout (RTO) is 16ms</p>
    <p>TCP counters are not enough: they shows 2 RTO, but 2*16 &lt; 49.</p>
  </div>
  <div class="page">
    <p>An RTO problem in testbed</p>
    <p>Diagnosis Info:  2 RTO  Exponential backoff</p>
    <p>Sender Receiver</p>
    <p>Why the receiver doesnt ACK?</p>
  </div>
  <div class="page">
    <p>An RTO problem in testbed</p>
    <p>Diagnosis Info:  2 RTO  Exponential backoff  Delayed ACK</p>
    <p>Counter</p>
    <p>Sender Receiver</p>
    <p>DETER+Tcpdump Monitor</p>
    <p>function call graph (Ftrace)</p>
    <p>Enters delayed ACK function DETER+Ftrace</p>
    <p>TCP expert may guess</p>
  </div>
  <div class="page">
    <p>Case study in Spark</p>
    <p>Terasort 200 GB on 20 servers (4 cores each) on EC2, 6.2K connections  Replay and collect trace for problematic flows</p>
    <p>- The receiver explicitly delays the ACK, because the recv buffer is shrinking - Caused by the slow receiver</p>
  </div>
  <div class="page">
    <p>Case study in RPC</p>
    <p>An RPC application running empirical DC traffic on 20 servers (4 cores each) on EC2, 280K requests</p>
    <p>Late Fast Retransmission: fast retransmit after 10s of dupACKs. - The threshold for dupACK increases, from 3 to 45. - Due to reordering in the past</p>
  </div>
  <div class="page">
    <p>Other use cases</p>
    <p>We can diagnose many other problems in the TCP stack  RTO caused by diff reasons: small messages, misconfiguration of recv buf size</p>
    <p>We can also diagnose problems in the switches  Because we have traces, we can push packets into the network  In simulation (requires modeling switch data plane accurately)  Case study: A temporary blackhole caused by switch buffer sharing</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>DETER enables deterministic TCP replay  Lightweight: always on during runtime  Detailed diagnosis during the replay</p>
    <p>Key challenge: butterfly effect  Record &amp; replay packet stream mutations to break the closed loop between TCP</p>
    <p>and switches.</p>
  </div>
</Presentation>
