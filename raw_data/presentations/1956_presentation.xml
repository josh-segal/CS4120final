<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Marcel Wagenlnder, Luo Mai, Guo Li, Peter Pietzuch Large-Scale Data &amp; Systems (LSDS) Group</p>
    <p>Imperial College London</p>
    <p>Spotnik Designing Distributed Machine Learning for Transient Cloud</p>
    <p>Resources</p>
  </div>
  <div class="page">
    <p>Distributed ML</p>
    <p>Train a machine learning model</p>
    <p>worker 0 worker 1</p>
    <p>worker 2</p>
  </div>
  <div class="page">
    <p>Distributed ML</p>
    <p>Train a machine learning model</p>
    <p>worker 0 worker 1</p>
    <p>worker 2</p>
    <p>Learn a model</p>
  </div>
  <div class="page">
    <p>Distributed ML</p>
    <p>Train a machine learning model</p>
    <p>worker 0 worker 1</p>
    <p>worker 2</p>
    <p>Data parallelism</p>
  </div>
  <div class="page">
    <p>Distributed ML</p>
    <p>Train a machine learning model</p>
    <p>worker 0 worker 1</p>
    <p>worker 2</p>
    <p>Ring all-reduce</p>
  </div>
  <div class="page">
    <p>Challenges of distributed ML</p>
    <p>Distributed ML is resourcehungry</p>
    <p>Accelerated resources are expensive</p>
    <p>Example Megatron-LM3</p>
    <p>Training of BERT-like model  512 V100 GPUs  One epoch (68,507 iterations)</p>
    <p>takes 2.1 days</p>
    <p>Cost on Azure: $92,613</p>
  </div>
  <div class="page">
    <p>Transient cloud resources</p>
    <p>Examples: AWS Spot instances, Azure Spot VMs  Follows the law of a free market  Revocations</p>
    <p>Notifications  Economic incentive</p>
    <p>Offers a cost reduction of up to 90%1</p>
    <p>A Megatron-LM epoch would drop from $92,613 to $15,152</p>
    <p>worker 2</p>
  </div>
  <div class="page">
    <p>Implications of transient resources</p>
    <p>New workers become available or old workers get revoked  System must cope with disappearing resources</p>
    <p>Changes can happen at any time  System must ensure consistency of updates</p>
    <p>worker 0</p>
    <p>worker 1</p>
    <p>worker 2</p>
    <p>worker 3</p>
    <p>worker 4</p>
    <p>Cluster</p>
    <p>Size</p>
  </div>
  <div class="page">
    <p>Implications of transient resources</p>
    <p>New workers become available or old workers get revoked  System must cope with disappearing resources</p>
    <p>Changes can happen at any time  System must ensure consistency of updates</p>
    <p>Cluster sizes are unknown beforehand  System must adapt to different conditions</p>
    <p>HogWild!</p>
    <p>AD-PSGD</p>
    <p>EA-SGD</p>
    <p>SMA S-SGD</p>
    <p>Network efficiency</p>
    <p>Model accuracy</p>
  </div>
  <div class="page">
    <p>Current approach: Checkpoint &amp; recovery</p>
    <p>Tensorflow and Pytorch  Changes to the cluster are not considered  Recovery takes about 20 seconds with ResNet50 and ImageNet</p>
    <p>Recovery Training</p>
    <p>CheckpointTraining</p>
  </div>
  <div class="page">
    <p>Mix dedicated resources with transient resources  Proteus2: Placement of parameter server on dedicated resources so that the</p>
    <p>training state is save</p>
    <p>Parameter server</p>
    <p>Worker 0 Worker 1 Worker 2</p>
    <p>Dedicated resources</p>
    <p>Transient resources</p>
    <p>Current approaches: Hybrid</p>
  </div>
  <div class="page">
    <p>Spotnik</p>
    <p>Challenges Solutions</p>
    <p>Workers become available or get revoked</p>
    <p>Reuse communication channels for synchronisation to repair the cluster</p>
    <p>Changes can happen at any time</p>
    <p>Ensure atomic model updates by waiting for all synchronisations to finish first</p>
    <p>Cluster sizes are unknown beforehand</p>
    <p>Change the synchronisation strategy based on the number of workers</p>
  </div>
  <div class="page">
    <p>Spotnik</p>
    <p>Challenges Solutions</p>
    <p>Workers become available or get revoked</p>
    <p>Reuse communication channels for synchronisation to repair the cluster</p>
    <p>Changes can happen at any time</p>
    <p>Ensure atomic model updates by waiting for all synchronisations to finish first</p>
    <p>Cluster sizes are unknown beforehand</p>
    <p>Change the synchronisation strategy based on the number of workers</p>
  </div>
  <div class="page">
    <p>Revocation recovery algorithm</p>
    <p>Handle revocations within a ring topology  Number of total messages is bounded by messages</p>
    <p>K is the number of simultaneous revocations  N is the number of workers  Scale to many transient resources</p>
    <p>No reliance on revocation notifications</p>
    <p>O(N  K)</p>
  </div>
  <div class="page">
    <p>Revocation recovery algorithm Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile Revocation</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile</p>
    <p>Bypass</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile</p>
    <p>Revocation</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile</p>
    <p>Bypass</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Reconcile</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W1</p>
    <p>W2</p>
    <p>W3</p>
    <p>W4</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Accept</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Repairing a broken all-reduce ring</p>
    <p>W0</p>
    <p>W2</p>
    <p>W3</p>
    <p>W5</p>
    <p>Worker Membership</p>
    <p>Restart</p>
    <p>Revocation recovery algorithm</p>
  </div>
  <div class="page">
    <p>Spotnik</p>
    <p>Challenges Solutions</p>
    <p>Workers become available or get revoked</p>
    <p>Reuse communication channels for synchronisation to repair the cluster</p>
    <p>Changes can happen at any time</p>
    <p>Ensure atomic model updates by waiting for all synchronisations to finish first</p>
    <p>Cluster sizes are unknown beforehand</p>
    <p>Change the synchronisation strategy based on the number of workers</p>
  </div>
  <div class="page">
    <p>Atomic worker state update</p>
    <p>Pipelined synchronisation</p>
    <p>Parameter Parameter</p>
    <p>Sync. Sync.</p>
    <p>Parameter</p>
    <p>Sync.</p>
    <p>Update Update Update</p>
  </div>
  <div class="page">
    <p>Atomic worker state update</p>
    <p>Pipelined synchronisation  Revocations can happen meanwhile  Partial update leads to inconsistency</p>
    <p>Parameter Parameter</p>
    <p>Sync. Sync.</p>
    <p>Parameter</p>
    <p>Sync.</p>
    <p>Update Update Update</p>
  </div>
  <div class="page">
    <p>Atomic worker state update</p>
    <p>Atomicity: Wait for all synchronisation communications to finish  Discard updates if communication fails</p>
    <p>Parameter Parameter</p>
    <p>Sync. Sync.</p>
    <p>Parameter</p>
    <p>Sync.</p>
    <p>Update Update Update</p>
    <p>Barrier</p>
  </div>
  <div class="page">
    <p>Spotnik</p>
    <p>Challenges Solutions</p>
    <p>Workers become available or get revoked</p>
    <p>Reuse communication channels for synchronisation to repair the cluster</p>
    <p>Changes can happen at any time</p>
    <p>Ensure atomic model updates by waiting for all synchronisations to finish first</p>
    <p>Cluster sizes are unknown beforehand</p>
    <p>Change the synchronisation strategy based on the number of workers</p>
  </div>
  <div class="page">
    <p>Adaptive synchronisation strategies</p>
    <p>Support a range of synchronisation primitives  collective and point-to-point synchronisation</p>
    <p>Monitor a metric  Number of workers</p>
  </div>
  <div class="page">
    <p>Adaptive synchronisation strategies  Support a range of synchronisation primitives</p>
    <p>collective and point-to-point synchronisation  Monitor a metric</p>
    <p>Number of workers  Define a policy in the beginning</p>
    <p>When to choose which sync strategy</p>
    <p>W0</p>
    <p>W1W2</p>
    <p>W0</p>
    <p>W3</p>
    <p>W2</p>
    <p>W1</p>
    <p>W4</p>
    <p>W5</p>
    <p>AD-PSGD S-SGD</p>
  </div>
  <div class="page">
    <p>Evaluation How does the recovery latency change with increasing number of revocations?</p>
    <p>Cluster  16 workers Hardware  Azure NC6 VMs</p>
    <p>Nvidia K80 Software  KungFu 0.2.1  Tensorflow 1.15 ML  ResNet50  ImageNet</p>
    <p>No significant increase of recovery latency if the number of revocation increases</p>
  </div>
  <div class="page">
    <p>Evaluation How much does the training slow down if we use atomic worker state updates?</p>
    <p>Cluster  32 workers Hardware  Azure NC6 VMs</p>
    <p>Nvidia K80 Software  KungFu 0.2.1  Tensorflow 1.15</p>
    <p>*different Setup Cluster  16 workers Hardware  Huawei ModelArts</p>
    <p>Nvidia V100  InfiniBand</p>
    <p>Software  KungFu 0.2.1  Tensorflow 1.12</p>
    <p>Throughput decrease is small</p>
  </div>
  <div class="page">
    <p>Evaluation How does the throughput change, if the cluster changes?</p>
    <p>Cluster  up to 32 workers Hardware  Azure NC6 VMs</p>
    <p>Nvidia K80 Software  KungFu 0.2.1  Tensorflow 1.15 ML  ResNet50  ImageNet</p>
    <p>Cluster size 5 10 15 20 25 30</p>
  </div>
  <div class="page">
    <p>Evaluation How does the throughput change, if the cluster changes?</p>
    <p>Cluster  up to 32 workers Hardware  Azure NC6 VMs</p>
    <p>Nvidia K80 Software  KungFu 0.2.1  Tensorflow 1.15 ML  ResNet50  ImageNet</p>
    <p>Cluster size 5 10 15 20 25 30</p>
  </div>
  <div class="page">
    <p>Evaluation How does the throughput change, if the cluster changes?</p>
    <p>Switching from S-SGD to AD- SGD</p>
    <p>Cluster  up to 32 workers Hardware  Azure NC6 VMs</p>
    <p>Nvidia K80 Software  KungFu 0.2.1  Tensorflow 1.15 ML  ResNet50  ImageNet</p>
    <p>Changing clusters need adaptation</p>
    <p>Cluster size 5 10 15 20 25 30</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Transient cloud resources offer potential to save money for ML training  No system that runs exclusively on transient resources and has low overhead</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Transient cloud resources offer potential to save money for ML training  No system that runs exclusively on transient resources and has low overhead</p>
    <p>Spotnik  Repair cluster with low overhead  Ensure consistent model updates  Adapt to changes of the cluster</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Transient cloud resources offer potential to save money for ML training  No system that runs exclusively on transient resources and has low overhead</p>
    <p>Spotnik  Repair cluster with low overhead  Ensure consistent model updates  Adapt to changes of the cluster</p>
    <p>KungFu github.com/lsds/KungFu</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Transient cloud resources offer potential to save money for ML training  No system that runs exclusively on transient resources and has low overhead</p>
    <p>Repair cluster with low overhead  Ensure consistent model updates  Adapt to changes of the cluster</p>
    <p>Website lsds.doc.ic.ac.uk | E-Mail marcel.wagenlander19@imperial.ac.uk Twitter @marwage</p>
    <p>Spotnik KungFu github.com/lsds/KungFu</p>
  </div>
</Presentation>
