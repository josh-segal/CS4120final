<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-core and Network Aware MPI Topology Functions</p>
    <p>Mohammad J. Rashti, Jonathan Green, Pavan Balaji, Ahmad Afsahi, and William D. Gropp</p>
    <p>Department of Electrical and Computer Engineering, Queens University Mathematics and Computer Science, Argonne National Laboratory</p>
    <p>Department of Computer Science, University of Illinois at Urbana-Champaign</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction  Background and Motivation</p>
    <p>MPI Graph and Cartesian Topology Functions  Related Work  Design and Implementation of Topology Functions  Experimental Framework and Performance Results</p>
    <p>Micro-benchmark Results  Applications Results</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>MPI is the main standard for communication in HPC clusters.  Scalability is the major concern for MPI over large-scale</p>
    <p>hierarchical systems.  System topology awareness is essential for MPI scalability:</p>
    <p>Being aware of performance implications in each and every architectural hierarchy of the machine</p>
    <p>Efficiently mapping processes to processor cores, based on applications communication pattern</p>
    <p>Such functionality should be embedded in MPI topology interface</p>
  </div>
  <div class="page">
    <p>Background and Motivation</p>
    <p>MPI topology functions:  Define the communication topology of the application</p>
    <p>o Logical process arrangement or virtual topology  Possibly reorder the processes to efficiently map over the</p>
    <p>system architecture (physical topology) for more performance  Virtual topology models:</p>
    <p>Cartesian topology: multi-dimensional Cartesian arrangement  Graph topology: non-specific graph arrangement</p>
    <p>Graph topology representation  Non-distributed: easier to manage, less scalable  Distributed: new to the standard, more scalable</p>
  </div>
  <div class="page">
    <p>Background and Motivation (II)</p>
    <p>However, topology functions are mostly utilized for the construction of process arrangement (i.e., virtual topology).  Most MPI applications are not utilizing them for performance</p>
    <p>improvement  In addition, MPI implementations offer trivial functionality for</p>
    <p>these functions.  Mainly constructing the virtual topology  No reordering of the ranks; thus no performance improvement</p>
    <p>This work designs topology functions with reorder ability:  Designing non-distributed API functions  Supporting multi-hierarchy nodes and networks</p>
  </div>
  <div class="page">
    <p>MPI Graph and Cartesian Topology Functions  MPI defines a set of virtual topology definition functions for graph</p>
    <p>and Cartesian structures.  MPI_Graph_create and MPI_Cart_create non-distributed</p>
    <p>functions:  Are collective calls that accept a virtual topology  Return a new MPI communicator enclosing the desired topology  The input topology is in a non-distributed form  All nodes have a full view of the entire structure</p>
    <p>o Pass the whole information to the function  If the user opts for reordering, the function may reorder the ranks</p>
    <p>for an efficient process-to-core mapping.</p>
  </div>
  <div class="page">
    <p>MPI Graph and Cartesian Topology Functions (II) MPI_Cart_create(comm_old, ndims, dims, periods, reorder, comm_cart )</p>
    <p>comm_old [in] input communicator without topology (handle)  ndims [in] number of dimensions of Cartesian grid (integer)  dims [in] integer array of size ndims specifying the number</p>
    <p>of processes in each dimension  periods [in] logical array of size ndims specifying whether the</p>
    <p>grid is periodic (true) or not (false) in each dimension  reorder [in] ranking may be reordered (true) or not (false) (logical)  comm_graph [out] communicator with Cartesian topology (handle)</p>
    <p>Dimension #Processes 1 2</p>
    <p>ndims = 2 dims = 4, 2 periods = 1, 0</p>
  </div>
  <div class="page">
    <p>MPI Graph and Cartesian Topology Functions (III) MPI_Graph_create(comm_old, nnodes, index, edges, reorder, comm_graph )</p>
    <p>comm_old [in] input communicator without topology (handle)  nnodes [in] number of nodes in graph (integer)  index [in] array of integers describing node degrees  edges [in] array of integers describing graph edges  reorder [in] ranking may be reordered (true) or not (false)</p>
    <p>(logical)  comm_graph[out] communicator with graph topology added</p>
    <p>(handle)</p>
    <p>Process Neighbors 0 1 2 3</p>
    <p>nnodes = 4 index = 2, 3, 4, 6 edges = 1, 3, 0, 3, 0, 2</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction  Background and Motivation</p>
    <p>MPI Graph and Cartesian Topology Functions  Related Work  Design and Implementation of Topology Functions  Experimental Framework and Performance Results</p>
    <p>Micro-benchmark Results  Applications Results</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Related Work (I)</p>
    <p>Hatazaki, Trff, worked on topology mapping using graph embedding algorithms (Euro PVM/MPI 1998, SC 2002)</p>
    <p>Trff et. al, proposed extending MPI-1 topology interface (HIPS 2003, Euro PVM/MPI 2006)  To support weighted-edge topologies and dynamic process</p>
    <p>reordering, and to  Provide architectural clues to the applications for a better mapping</p>
    <p>MPI Forum introduced distributed topology functionality in MPI2.2 (2009)</p>
    <p>Hoefler et. al, proposed guidelines for efficient implementation of distributed topology functionality (CCPE 2010)</p>
  </div>
  <div class="page">
    <p>Related Work (II)</p>
    <p>Mercier et. al, studied efficient process-to-core mapping (Euro PVM/MPI 2009, EuroPar 2010]  Using external libraries for node architecture discovery and</p>
    <p>graph mapping  Using weighted graphs and/or trees, and outside MPI topology</p>
    <p>interface  How is our work different from the related work?</p>
    <p>Supports a physical topology spanning nodes and the network  Uses edge replication to support weighted edges in virtual</p>
    <p>topology graphs  Integrates the above functionality in MPI non-distributed</p>
    <p>topology interface</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction  Background and Motivation</p>
    <p>MPI Graph and Cartesian Topology Functions  Related Work  Design and Implementation of Topology Functions  Experimental Framework and Performance Results</p>
    <p>Micro-benchmark Results  Applications Results</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Design of MPI Topology Functions (I)</p>
    <p>Both Cartesian and graph interfaces are treated as graph at the underlying layers  Cartesian topology is internally copied to a graph topology</p>
    <p>Virtual topology graph:  Vertices: MPI processes  Edges: existence, or significance, of communication between any</p>
    <p>two processes  Significance of communication : normalized total communication</p>
    <p>volume between any pair of processes, used as edge weights  Edge replication is used to represent graph edge weight</p>
    <p>o Recap: MPI non-distributed interface does not support weighted edges</p>
  </div>
  <div class="page">
    <p>Design of MPI Topology Functions (II)</p>
    <p>Physical topology graph:  Integrated node and network architecture  Vertices: architectural components such as:</p>
    <p>o Network nodes o Cores o Caches</p>
    <p>Edges: communication links between the components  Edge weights: communication performance between components</p>
    <p>o Processor cores: closer cores have higher edge weight o Network nodes: closer nodes have higher edge weight o Farthest on-node cores get higher weight than closest network nodes</p>
  </div>
  <div class="page">
    <p>Physical Topology Distance Example</p>
    <p>d1 will have the highest load value in the graph.  The path between N2 and N3 (d4) will have the lowest load</p>
    <p>value, indicating the lowest performance path.  d1 &gt; d2 &gt; d3 &gt; d4 = 1</p>
    <p>N2</p>
    <p>d4</p>
    <p>d4</p>
    <p>d4 d3</p>
    <p>d3</p>
    <p>S1</p>
    <p>d1</p>
    <p>d2</p>
    <p>S2 S3</p>
    <p>d1</p>
    <p>d2</p>
    <p>d1</p>
    <p>d2</p>
    <p>d1</p>
    <p>d2</p>
    <p>N1 N3</p>
    <p>N4</p>
    <p>d4</p>
  </div>
  <div class="page">
    <p>Tools for Implementation of Topology Functions  HWLOC library for extracting node architecture:</p>
    <p>A tree architecture, with nodes at top level and cores at the leaves  Cores with lower-level parents (such as caches) are considered to</p>
    <p>have higher communication performance  IB subnet manager (ibtracert) for extracting network distances:</p>
    <p>Do the discovery offline, before the application run  Make a pre-discovered network distance file</p>
    <p>Scotch library for mapping virtual to physical topologies:  Source and target graphs are weighted and undirected  Uses recursive bi-partitioning for graph mapping</p>
  </div>
  <div class="page">
    <p>Implementation of Topology Functions</p>
    <p>Communication pattern profiling:  Probes are placed inside MPI library to profile applications</p>
    <p>communication pattern.  Pairwise communication volume is normalized in the range of</p>
    <p>All processes perform node architecture discovery  One process performs network discovery for all  Make the physical architecture view unified across the processes</p>
    <p>(using Allgather)</p>
  </div>
  <div class="page">
    <p>Existing MPICH function</p>
    <p>Graph topology</p>
    <p>Graph topology initialization</p>
    <p>Creating physical topology: by extracting and merging node and network architectures. 1. Initialize Scotch architecture. 2. Extract network topology (if required). 3. Extract node topology. 4. Merge node and network topology. 5. Distribute the merged topology among processes (using allgather). 6. Build Scotch physical topology.</p>
    <p>Constructing a new reordered communicator: using Scotch mapping of the previous step.</p>
    <p>SCOTCH</p>
    <p>HWLOC</p>
    <p>Cartesian topology</p>
    <p>Trivial graph topology creation</p>
    <p>Trivial Cartesian topology creation</p>
    <p>Cartesian topology initialization</p>
    <p>No Reorder No Reorder Reorder Reorder</p>
    <p>SCOTCH Graph mapping: by constructing Scotch weighted virtual topology from the input graph and mapping it to the extracted physical topology. 1. Initialize and build the Scotch virtual topology graph. 2. Initialize the mapping algorithms strategy in Scotch. 3. Map the virtual topology graph to the extracted physical topology.</p>
    <p>Creating the new MPI communicator</p>
    <p>Creating the new MPI communicator</p>
    <p>IB Subnet manager</p>
    <p>Flow of Functionalities</p>
    <p>Creating equivalent graph topology</p>
    <p>Application profiling</p>
    <p>Input virtual topology graph</p>
    <p>New function added to MPICH</p>
    <p>External library utilized</p>
    <p>Calling a function</p>
    <p>Following a function in the code</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction  Background and Motivation</p>
    <p>MPI Graph and Cartesian Topology Functions  Related Work  Design and Implementation of Topology Functions  Experimental Framework and Performance Results</p>
    <p>Micro-benchmark Results  Applications Results</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Framework</p>
    <p>Cluster A (4 servers, 32-cores total)  Hosts: 2-way quad-core AMD Opteron 2350 servers, with 2MB</p>
    <p>shard L3 cache per processor, and 8GB RAM  Network: QDR InfiniBand, 3 switches at 2 levels  Software: Fedora 12, Kernel 2.6.27, MVAPICH2 1.5, OFED 1.5.2</p>
    <p>Cluster B (16 servers, 192 cores total)  Hosts: 2-way hexa-core Intel Xeon X5670 servers, with a 12MB</p>
    <p>multi-level cache per processor, and 24GB RAM  Network: QDR InfiniBand, 4 switches at 2 levels  Software: RHEL 5, Kernel 2.6.18.94, MVAPICH2 1.5, OFED 1.5.2</p>
  </div>
  <div class="page">
    <p>MPI Applications  Some Statistics</p>
    <p>MPI Application Communication Primitives</p>
    <p>NPB CG - MPI Send/Irecv: ~100% of the calls - MPI Barrier: ~0% of the calls</p>
    <p>NPB MG - MPI Send/Irecv: 98.5% of the calls, ~100% of the volume</p>
    <p>- MPI Allreduce, Reduce, Barrier, Bcast: 1.5% of the calls, ~0.002% of the volume</p>
    <p>LAMMPS - MPI Send/Recv/Irecv/Sendrecv: 95% of the calls, 99% of the volume</p>
    <p>- MPI Allreduce, Reduce, Barrier, Bcast, Scatter, Allgather, Allgatherv: 5% of the calls, 1% of the volume</p>
  </div>
  <div class="page">
    <p>Exchange Micro-benchmark: Topologyaware Mapping Improvement over Block Mapping (%)</p>
    <p>-60</p>
    <p>-40</p>
    <p>-20</p>
    <p>Non-weighted graph Weighted-graph Weighted and network-aware graph</p>
    <p>Exchange Message Size (Byte)</p>
    <p>Non-weighted graph Weighted-graph Weighted and network-aware graph</p>
    <p>Exchange Message Size (Byte)</p>
  </div>
  <div class="page">
    <p>Exchange Micro-benchmark: Topologyaware Mapping Improvement over Block Mapping (%)</p>
    <p>-10</p>
    <p>Non-weighted graph Weighted-graph Weighted and network-aware graph</p>
    <p>Exchange Message Size (Byte)</p>
    <p>Non-weighted graph Weighted-graph Weighted and network-aware graph</p>
    <p>Exchange Message Size (Byte)</p>
  </div>
  <div class="page">
    <p>Collective Micro-benchmark: Topologyaware Mapping Improvement over Block Mapping (%)</p>
    <p>Non-weighted graph Weighted-graph Weighted and network-aware graph</p>
    <p>Exchange Message Size (Byte)</p>
    <p>Non-weighted graph Weighted-graph Weighted and network-aware graph</p>
    <p>Exchange Message Size (Byte)</p>
  </div>
  <div class="page">
    <p>Applications: Topology-aware Mapping Improvement over Cyclic Mapping (%)32-core cluster A</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .A</p>
    <p>CG .B</p>
    <p>CG .C</p>
    <p>M G</p>
    <p>.A .3</p>
    <p>M G</p>
    <p>.B .3</p>
    <p>M G</p>
    <p>.C .3</p>
    <p>-5 0 5</p>
    <p>Communication Time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .A</p>
    <p>CG .B</p>
    <p>CG .C</p>
    <p>M G</p>
    <p>.A .3</p>
    <p>M G</p>
    <p>.B .3</p>
    <p>M G</p>
    <p>.C .3</p>
    <p>-5</p>
    <p>Run-time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
  </div>
  <div class="page">
    <p>Applications: Topology-aware Mapping Improvement over Block Mapping (%)</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .A</p>
    <p>CG .B</p>
    <p>CG .C</p>
    <p>M G</p>
    <p>.A .3</p>
    <p>M G</p>
    <p>.B .3</p>
    <p>M G</p>
    <p>.C .3</p>
    <p>-10</p>
    <p>-5</p>
    <p>Communication Time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .A</p>
    <p>CG .B</p>
    <p>CG .C</p>
    <p>M G</p>
    <p>.A .3</p>
    <p>M G</p>
    <p>.B .3</p>
    <p>M G</p>
    <p>.C .3</p>
    <p>-2</p>
    <p>-1</p>
    <p>Run-time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
  </div>
  <div class="page">
    <p>Applications: Topology-aware Mapping Improvement over Cyclic Mapping (%)</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .D</p>
    <p>M G</p>
    <p>.D .1</p>
    <p>-20</p>
    <p>-10</p>
    <p>Communication Time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .D</p>
    <p>M G</p>
    <p>.D .1</p>
    <p>-5</p>
    <p>Run-time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
  </div>
  <div class="page">
    <p>Applications: Topology-aware Mapping Improvement over Block Mapping (%)</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .D</p>
    <p>M G</p>
    <p>.D .1</p>
    <p>-20</p>
    <p>-15</p>
    <p>-10</p>
    <p>-5</p>
    <p>Communication Time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-F ri</p>
    <p>cti on</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-P ou</p>
    <p>r</p>
    <p>LA M</p>
    <p>M PS</p>
    <p>-C ou</p>
    <p>pl e</p>
    <p>CG .D</p>
    <p>M G</p>
    <p>.D .1</p>
    <p>-6</p>
    <p>-4</p>
    <p>-2</p>
    <p>Run-time Improvement non-weighted graph weighted graph Weighted &amp; network-aware graph</p>
    <p>Applications</p>
  </div>
  <div class="page">
    <p>Communicator Creation time in MPI_Graph_create for LAMMPS</p>
    <p>System # Processes Trivial (ms)</p>
    <p>Non-weighted Graph (ms)</p>
    <p>Weighted Graph (ms)</p>
    <p>Network-aware Graph (ms)</p>
    <p>Cluster A</p>
    <p>Cluster B</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction  Background and Motivation</p>
    <p>MPI Graph and Cartesian Topology Functions  Related Work  Design and Implementation of Topology Functions  Experimental Framework and Performance Results</p>
    <p>Micro-benchmark Results  Applications Results</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Concluding Remarks</p>
    <p>We presented design and implementation of MPI non-distributed graph and Cartesian functions in MVAPICH2 for multi-core nodes connected through multi-level InfiniBand networks.</p>
    <p>The micro-benchmarks showed that the effect of reordering process ranks can be significant, and when the communication is heavier on one dimension the benefits of using weighted and network-aware graphs (instead of non-weighted graph) are considerable.</p>
    <p>We also modified MPI applications with MPI_Graph_create. The evaluation results showed that MPI applications can benefit from topology-aware MPI_Graph_create.</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>We intend to evaluate the effect of topology awareness on other MPI applications.</p>
    <p>We would also like to run our applications on a larger testbed.</p>
    <p>We would like to design a more general communication cost/weight model for graph mapping, and use other libraries.</p>
    <p>We also intend to design and implement MPI distributed topology functions for more scalability in a more distributed, scalable fashion.</p>
  </div>
  <div class="page">
    <p>Acknowledgment</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Contacts:  Mohammad Javad Rashti: mohammad.rashti@queensu.ca  Jonathan Green: jonathan.green@queensu.ca  Pavan Balaji: balaji@mcs.anl.gov  Ahmad Afsahi: ahmad.afsahi@queensu.ca  William D. Gropp: wgropp@illinois.edu</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>MPI_Graph_create</p>
    <p>MPIR_Graph_create_reorder</p>
    <p>MPIU_Get_scotch_arch</p>
    <p>MPIR_Comm_copy_reorder</p>
    <p>SCOTCHHWLOC</p>
    <p>MPI_Cart_create</p>
    <p>MPIR_Graph_createMPIR_Graph_create</p>
    <p>MPIR_Cart_create_reorder</p>
    <p>MPIR_Topo_create</p>
    <p>No Reorder No Reorder Reorder Reorder</p>
    <p>SCOTCH_Graph_build/map</p>
    <p>MPIR_Comm_copy MPIR_Comm_copy</p>
    <p>Scotch mapping</p>
    <p>Legend Existing MPICH function</p>
    <p>New function added to MPICH</p>
    <p>External library utilized</p>
    <p>IB Subnet manager</p>
    <p>Calling a function</p>
    <p>Following a function in the code</p>
    <p>Flow of function calls in MVAPICH code</p>
  </div>
</Presentation>
