<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Neutrino: Revisi&amp;ng Memory Caching for Itera&amp;ve Data Analy&amp;cs</p>
    <p>Erci Xu*, Mohit Saxena, Lawrence Chiu Ohio State University* IBM Research Almaden</p>
  </div>
  <div class="page">
    <p>Background  Itera&amp;ve analy&amp;cs is rapidly gaining popularity  Data Clustering, Log Mining, Graph Processing, Machine Learning</p>
    <p>Dataset is repeatedly accessed across different itera&amp;ons</p>
    <p>In-Memory Caching best fits Itera&amp;ve Analy&amp;cs  In-Memory caching frameworks avoid frequent I/O with underlying storage systems</p>
    <p>Itera&amp;ve Data Analy&amp;cs could have 10x  100x speedup</p>
  </div>
  <div class="page">
    <p>Spark for In-Memory Itera&amp;ve Analy&amp;cs</p>
    <p>Ref: wikibon.org</p>
  </div>
  <div class="page">
    <p>RDD</p>
    <p>Spark RDD</p>
    <p>RDD: Resilient Distributed Datasets</p>
    <p>block 1</p>
    <p>par&amp;&amp;on 3</p>
    <p>par&amp;&amp;on 1</p>
    <p>In Memory</p>
    <p>In HDFS</p>
    <p>par&amp;&amp;on 2</p>
    <p>par&amp;&amp;on 4</p>
    <p>block 3</p>
    <p>block 2</p>
    <p>block 4</p>
    <p>Worker 1 Worker 2</p>
  </div>
  <div class="page">
    <p>RDD Cache Op3ons  Deserialized or Serialized  On Heap or Off Heap  In Memory or Disk</p>
    <p>Memory Caching in Spark</p>
    <p>stripe</p>
    <p>Spark</p>
    <p>Mem</p>
    <p>Disk</p>
    <p>stripe Mem</p>
    <p>Disk</p>
    <p>stripe Mem</p>
    <p>Disk</p>
  </div>
  <div class="page">
    <p>Problems: In-memory Caching for Itera&amp;ve Data Analy&amp;cs</p>
  </div>
  <div class="page">
    <p>Problem 1: Discrete Cache Levels</p>
    <p>Serialized Cache saves 56% to 63% of the space but rela&amp;vely slower 7</p>
    <p>Si ze in M</p>
    <p>em or y( G B)</p>
    <p>DataSize(GB) Serialized Deserialized</p>
  </div>
  <div class="page">
    <p>Problem 1: Discrete Cache Levels</p>
    <p>Deserialized Cache is an order of magnitude faster but become very slow once</p>
    <p>spilled to disk</p>
    <p>Ti m e /S ec</p>
    <p>DataSize /GB</p>
    <p>Deserialized</p>
    <p>Serialized</p>
    <p>Cluster Memory: 100GB</p>
  </div>
  <div class="page">
    <p>Problem 1: Discrete Cache Levels</p>
    <p>Ti m e /S ec</p>
    <p>DataSize /GB</p>
    <p>Deserialized</p>
    <p>Serialized</p>
    <p>Op&amp;mal</p>
    <p>Goal for Neutrino</p>
  </div>
  <div class="page">
    <p>Problems: In-memory Caching for Itera&amp;ve Data Analy&amp;cs</p>
  </div>
  <div class="page">
    <p>Problem 2: Manual Management rdd_1 = sc.textfile(HDFS://file1) rdd_2 = sc.textfile(HDFS://file2)</p>
    <p>rdd_1.persist(Cache_Level) rdd_2.persist(Cache_Level)</p>
    <p>rdd_1.tranformations().action() rdd_2.tranformations().action()</p>
    <p>dataset size?</p>
    <p>de/serialized?</p>
    <p>access order ? 11</p>
  </div>
  <div class="page">
    <p>Problems: In-memory Caching for Itera&amp;ve Data Analy&amp;cs</p>
  </div>
  <div class="page">
    <p>Not Adap&amp;ve to Run&amp;me Changes</p>
    <p>Cache levels are sta&amp;cally assigned to RDD and such programmer decisions may not adapt to: 1. Changing memory u&amp;liza&amp;on on each worker node</p>
  </div>
  <div class="page">
    <p>Our Solu&amp;on: Neutrino Less Manual Management</p>
    <p>Adap&amp;ve to Run&amp;me Changes</p>
    <p>Fine-grained Cache Levels</p>
  </div>
  <div class="page">
    <p>Goal: To understand RDD access order between jobs</p>
    <p>Solu&amp;on: Preliminary run on small workloads to extract RDD access order</p>
    <p>Example: K Nearest Neighbors Classifica&amp;on  Classical ML classifica&amp;on algorithm  1 train dataset, 3 test dataset</p>
  </div>
  <div class="page">
    <p>KNN Example: Job Execu&amp;on</p>
    <p>Spark Applica&amp;on</p>
    <p>Job 1</p>
    <p>Split Split</p>
    <p>KNNjoin</p>
    <p>Train Test_1</p>
    <p>Master</p>
    <p>Executor</p>
    <p>Scheduling Tasks</p>
    <p>Execu&amp;ng Task</p>
    <p>Map Map</p>
    <p>Job 2</p>
    <p>Split Split</p>
    <p>KNNjoin</p>
    <p>Train Test_2</p>
    <p>Scheduling Tasks</p>
    <p>Execu&amp;ng Task</p>
    <p>Map Map</p>
    <p>Job 3</p>
    <p>Split Split</p>
    <p>KNNjoin</p>
    <p>Train Test_3</p>
    <p>Scheduling Tasks</p>
    <p>Execu&amp;ng Task</p>
    <p>Map Map</p>
  </div>
  <div class="page">
    <p>KNN Data Flow Graph</p>
    <p>Goal: Understand the RDD access order between jobs.</p>
    <p>Train SplitRDD</p>
    <p>KNNjoin (ac&amp;on)</p>
    <p>TrainRDD</p>
    <p>Result</p>
    <p>Train MappedRDD</p>
    <p>Test SplitRDD</p>
    <p>Test_1RDD Test</p>
    <p>MappedRDD</p>
    <p>Job 1</p>
    <p>RDD_seq[1]={TrainRDD, Test_1RDD}</p>
    <p>RDD_seq[2]={TrainRDD, Test_2RDD} RDD_seq[3]={TrainRDD, Test_3RDD}</p>
    <p>Job 1 Job 2 Job 3</p>
  </div>
  <div class="page">
    <p>Goal: Fine-grained cache management at RDD par&amp;&amp;on level</p>
    <p>Solu&amp;on: New cache level: Adap+ve. It can move RDD par&amp;&amp;ons between cache levels at run&amp;me</p>
    <p>Par&amp;&amp;on-level Opera&amp;ons: cache, discard and convert</p>
  </div>
  <div class="page">
    <p>Cache Opera&amp;ons in Spark</p>
    <p>Deserialized</p>
    <p>Serialized</p>
    <p>Uncached</p>
    <p>Cache (Deserialize)</p>
    <p>Cache (Serialize)</p>
    <p>Discard</p>
    <p>Discard</p>
    <p>Caching Granularity: RDD 19</p>
  </div>
  <div class="page">
    <p>Deserialized</p>
    <p>Serialized</p>
    <p>Uncached Convert</p>
    <p>Cache (Deserialize)</p>
    <p>Cache (Serialize)</p>
    <p>Discard</p>
    <p>Discard</p>
    <p>Adap&amp;ve</p>
    <p>RDD Caching Granularity: Par&amp;&amp;on</p>
    <p>Adap&amp;ve Cache Opera&amp;ons in Neutrino</p>
  </div>
  <div class="page">
    <p>Solu&amp;ons: Explore cache decisions on all par&amp;&amp;ons by dynamic programming each &amp;me before scheduling</p>
    <p>Dynamic Programming Model  Inputs: RDD access order, par&amp;&amp;on status  Output: Cache decision for each par&amp;&amp;on in the next job  Cost Model: Overall execu&amp;on &amp;me</p>
  </div>
  <div class="page">
    <p>Execu&amp;on of Dynamic Cache Scheduling</p>
    <p>KNNjoin</p>
    <p>Master</p>
    <p>Executor</p>
    <p>Scheduling Tasks</p>
    <p>Execu&amp;ng Task</p>
    <p>DP Scheduling</p>
    <p>RDD Access Order</p>
    <p>Par&amp;&amp;on Status</p>
    <p>Execu&amp;ng Caching Decisions</p>
    <p>Split Split</p>
    <p>Map Map</p>
    <p>Update</p>
  </div>
  <div class="page">
    <p>Dynamic Cache Scheduling: Caching Decisions</p>
    <p>RDD#</p>
    <p>DP Job 1</p>
    <p>Part# Node Status 0 1 worker1 uncached</p>
    <p>Job# Part# 1 RDD0, RDD1</p>
    <p>Path 1 Decision_1: RDD_0_Part_1  Deser_Cache RDD_1_Part_1  Deser_Cache</p>
    <p>Par33on Status Table RDD Access Order</p>
    <p>DP Job 2</p>
    <p>Decision_2: RDD_0_Part_1  Do_Nothing RDD_1_Part_1  Discard RDD_2_Part_1  Deser_Cache</p>
    <p>DP Job 3(Final) Return Path 2: best caching decisions</p>
    <p>DP Job 2</p>
    <p>DP Job 3(Final)</p>
    <p>Path 2 Decision_1: RDD_0_Part_1  Deser_Cache RDD_1_Part_1  Do_Nothing</p>
    <p>Decision_2: RDD_0_Part_1  Do_Nothing RDD_2_Part_1 Do_Nothing</p>
  </div>
  <div class="page">
    <p>Evalua&amp;on</p>
    <p>Neutrino Implementa&amp;on - Extension to Apache Spark</p>
    <p>Methodology  6 nodes of 4 cores, 8GB memory each  Itera&amp;ve machine learning workloads:</p>
    <p>Classifica&amp;on: KNN, Logis&amp;c Regression  Clustering: K-Means  Inference: LDA</p>
    <p>Systems Compared:  Neutrino with Adap&amp;ve Caching  Spark with Serialized and Deserialized Caching</p>
  </div>
  <div class="page">
    <p>Scenario 1: Abundant Memory Deserialized data size &lt; Cluster Memory</p>
    <p>Neutrino deserialize all par&amp;&amp;ons and make efficient use of unused memory 25</p>
    <p>LDA K-Means KNN LogReg</p>
    <p>Rela3ve Job Execu3on Time</p>
    <p>Neutrino Deserialized Serialized</p>
    <p>Neutrino has extra computa&amp;on overhead for dynamic scheduling and addi&amp;onal opera&amp;ons</p>
    <p>-7%</p>
  </div>
  <div class="page">
    <p>Scenario 2: Sufficient Memory Deserialized data size &gt; Cluster Memory</p>
    <p>LDA K-Means KNN LogReg</p>
    <p>Rela3ve Job Execu3on Time</p>
    <p>Neutrino Deserialized Serialized</p>
    <p>Serialized level has extra overhead on deserializa&amp;on. Neutrino cache par&amp;ally in deserialized level and par&amp;ally in serialized level</p>
    <p>Deserialized level starts to hit disk and hence require re-computa&amp;on from HDFS</p>
  </div>
  <div class="page">
    <p>With more frequent cache misses occurred for Deserialized level</p>
    <p>Scenario 3: Just Enough Memory Serialized data size = Cluster Memory</p>
    <p>LDA K-Means KNN LogReg</p>
    <p>Rela3ve Job Execu3on Time</p>
    <p>Neutrino Deserialized Serialized</p>
  </div>
  <div class="page">
    <p>Conclusions  Discrete Cache Levels for In-Memory Caching</p>
    <p>Inefficient memory usage  not op&amp;mal performance</p>
    <p>Neutrino:  Par&amp;&amp;on level adap&amp;ve caching  Dataflow graph genera&amp;on  Dynamic cache scheduling</p>
    <p>Neutrino improves average job execu&amp;on &amp;me by up to 70% over na&amp;ve Spark caching</p>
    <p>Major Problems</p>
    <p>Cached RDD is deserialized in Spark Memory - Checkpointed RDD is serialized and persistent  Cached RDD is computed from HDFS - Checkpointed RDD is re-computed from cached RDD  Cached RDD reading is locality aware - Sta&amp;c Delay scheduling limits Checkpoin&amp;ng locality</p>
  </div>
  <div class="page">
    <p>Thanks Q&amp;A</p>
    <p>Neutrino: Revisi&amp;ng Memory Caching</p>
    <p>for Itera&amp;ve Data Analy&amp;cs</p>
    <p>Erci Xu*, Mohit Saxena, Lawrence Chiu Ohio State University* IBM Research Almaden</p>
  </div>
</Presentation>
