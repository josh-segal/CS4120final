<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>On Sequentially Normalized Maximum Likelihood Models</p>
    <p>Teemu Roos and Jorma Rissanen</p>
    <p>Complex Systems Computation Group Helsinki Institute for Information Technology HIIT</p>
    <p>FINLAND</p>
    <p>WITMSE-08, Tampere, Finland, August 18, 2008</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Universal Models</p>
    <p>Given a sequence, x n = (x1, . . . , xn), the best fitting model in a model class, M, is the maximum likelihood model</p>
    <p>sup</p>
    <p>p(x n ; ) = p(x n ; (x n)) .</p>
    <p>A universal model q() achieves almost as short a code-length as the ML model:</p>
    <p>lim n</p>
    <p>n ln</p>
    <p>p(x n ; (x n))</p>
    <p>q(x n) = 0 ,</p>
    <p>i.e., the log-likelihood ratio (regret) is allowed to grow sublinearly.</p>
    <p>The minimax optimal (NML) model (Shtarkov, 1987):</p>
    <p>pNML(x n) =</p>
    <p>p(x n ; (x n))</p>
    <p>Cn , Cn =</p>
    <p>xnXn</p>
    <p>p(x n ; (x n)) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Universal Models</p>
    <p>Given a sequence, x n = (x1, . . . , xn), the best fitting model in a model class, M, is the maximum likelihood model</p>
    <p>sup</p>
    <p>p(x n ; ) = p(x n ; (x n)) .</p>
    <p>A universal model q() achieves almost as short a code-length as the ML model:</p>
    <p>lim n</p>
    <p>n ln</p>
    <p>p(x n ; (x n))</p>
    <p>q(x n) = 0 ,</p>
    <p>i.e., the log-likelihood ratio (regret) is allowed to grow sublinearly.</p>
    <p>The minimax optimal (NML) model (Shtarkov, 1987):</p>
    <p>pNML(x n) =</p>
    <p>p(x n ; (x n))</p>
    <p>Cn , Cn =</p>
    <p>xnXn</p>
    <p>p(x n ; (x n)) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Universal Models</p>
    <p>Given a sequence, x n = (x1, . . . , xn), the best fitting model in a model class, M, is the maximum likelihood model</p>
    <p>sup</p>
    <p>p(x n ; ) = p(x n ; (x n)) .</p>
    <p>A universal model q() achieves almost as short a code-length as the ML model:</p>
    <p>lim n</p>
    <p>n ln</p>
    <p>p(x n ; (x n))</p>
    <p>q(x n) = 0 ,</p>
    <p>i.e., the log-likelihood ratio (regret) is allowed to grow sublinearly.</p>
    <p>The minimax optimal (NML) model (Shtarkov, 1987):</p>
    <p>pNML(x n) =</p>
    <p>p(x n ; (x n))</p>
    <p>Cn , Cn =</p>
    <p>xnXn</p>
    <p>p(x n ; (x n)) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part plug-in (predictive least squares (PLS), predictive MDL) mixtures (Bayes) sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part plug-in (predictive least squares (PLS), predictive MDL) mixtures (Bayes) sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part plug-in (predictive least squares (PLS), predictive MDL) mixtures (Bayes) sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part</p>
    <p>plug-in (predictive least squares (PLS), predictive MDL) mixtures (Bayes) sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part plug-in (predictive least squares (PLS), predictive MDL)</p>
    <p>mixtures (Bayes) sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part plug-in (predictive least squares (PLS), predictive MDL) mixtures (Bayes)</p>
    <p>sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Approximations and Alternatives to NML</p>
    <p>BIC: k</p>
    <p>Fisher information: k</p>
    <p>n</p>
    <p>det I () d</p>
    <p>two-part plug-in (predictive least squares (PLS), predictive MDL) mixtures (Bayes) sequential NML</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>Basic Idea</p>
    <p>Always gives a stochastic process (unlike NML).</p>
    <p>Each conditional is locally minimax optimal.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>Basic Idea</p>
    <p>Always gives a stochastic process (unlike NML).</p>
    <p>Each conditional is locally minimax optimal.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>Basic Idea</p>
    <p>Always gives a stochastic process (unlike NML).</p>
    <p>Each conditional is locally minimax optimal.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>The sNML (variant 1) model is defined as</p>
    <p>psNML1(x n) =</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x i )) Ki (x i1)</p>
    <p>Ki (x i1) =</p>
    <p>xi</p>
    <p>p(xi | x i1 ; (x i ))</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>The sNML (variant 1) model is defined as</p>
    <p>psNML1(x n) =</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x i )) Ki (x i1)</p>
    <p>Ki (x i1) =</p>
    <p>xi</p>
    <p>p(xi | x i1 ; (x i ))</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>The sNML (variant 1) model is defined as</p>
    <p>psNML1(x n) =</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x i )) Ki (x i1)</p>
    <p>Ki (x i1) =</p>
    <p>xi</p>
    <p>p(xi | x i1 ; (x i ))</p>
    <p>Compare to the plug-in model:</p>
    <p>pplugin(x n) =</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x i1))</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>The sNML (variant 1) model is defined as</p>
    <p>psNML1(x n) =</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x i )) Ki (x i1)</p>
    <p>Ki (x i1) =</p>
    <p>xi</p>
    <p>p(xi | x i1 ; (x i ))</p>
    <p>Compare to the ordinary NML model:</p>
    <p>pNML(x n) =</p>
    <p>p(x n ; (x n))</p>
    <p>Cn</p>
    <p>Cn =</p>
    <p>xnXn p(x n ; (x n))</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequential NML</p>
    <p>The sNML (variant 1) model is defined as</p>
    <p>psNML1(x n) =</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x i )) Ki (x i1)</p>
    <p>Ki (x i1) =</p>
    <p>xi</p>
    <p>p(xi | x i1 ; (x i ))</p>
    <p>The second variant of sNML is defined as</p>
    <p>psNML2(x n) =</p>
    <p>n i=1</p>
    <p>p(x i ; (x i ))</p>
    <p>K i (x i1)</p>
    <p>K i (x i1) =</p>
    <p>xi</p>
    <p>p(x i ; (x i ))</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Computational Complexity</p>
    <p>The only computational issue in applying NML/sNML in the discrete (multinomial) case is the normalization factor.</p>
    <p>In NML, we have a sum of products:</p>
    <p>Cn =  xn</p>
    <p>p(x n ; (x n)) =  xn</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x n)).</p>
    <p>In sNML, we have a product of sums:</p>
    <p>Zn(x n) =</p>
    <p>n i=1</p>
    <p>Ki (x i1) =</p>
    <p>n i=1</p>
    <p>x i</p>
    <p>p(xi | x i1 ; (x i1, xi )).</p>
    <p>Remarkably, we can evaluate both in O(n) time (Kontkanen &amp; Myllymaki, 2007). In general, NML is hard but sNML is easy.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Computational Complexity</p>
    <p>The only computational issue in applying NML/sNML in the discrete (multinomial) case is the normalization factor.</p>
    <p>In NML, we have a sum of products:</p>
    <p>Cn =  xn</p>
    <p>p(x n ; (x n)) =  xn</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x n)).</p>
    <p>In sNML, we have a product of sums:</p>
    <p>Zn(x n) =</p>
    <p>n i=1</p>
    <p>Ki (x i1) =</p>
    <p>n i=1</p>
    <p>x i</p>
    <p>p(xi | x i1 ; (x i1, xi )).</p>
    <p>Remarkably, we can evaluate both in O(n) time (Kontkanen &amp; Myllymaki, 2007). In general, NML is hard but sNML is easy.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Computational Complexity</p>
    <p>The only computational issue in applying NML/sNML in the discrete (multinomial) case is the normalization factor.</p>
    <p>In NML, we have a sum of products:</p>
    <p>Cn =  xn</p>
    <p>p(x n ; (x n)) =  xn</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x n)).</p>
    <p>In sNML, we have a product of sums:</p>
    <p>Zn(x n) =</p>
    <p>n i=1</p>
    <p>Ki (x i1) =</p>
    <p>n i=1</p>
    <p>x i</p>
    <p>p(xi | x i1 ; (x i1, xi )).</p>
    <p>Remarkably, we can evaluate both in O(n) time (Kontkanen &amp; Myllymaki, 2007). In general, NML is hard but sNML is easy.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Computational Complexity</p>
    <p>The only computational issue in applying NML/sNML in the discrete (multinomial) case is the normalization factor.</p>
    <p>In NML, we have a sum of products:</p>
    <p>Cn =  xn</p>
    <p>p(x n ; (x n)) =  xn</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x n)).</p>
    <p>In sNML, we have a product of sums:</p>
    <p>Zn(x n) =</p>
    <p>n i=1</p>
    <p>Ki (x i1) =</p>
    <p>n i=1</p>
    <p>x i</p>
    <p>p(xi | x i1 ; (x i1, xi )).</p>
    <p>Remarkably, we can evaluate both in O(n) time (Kontkanen &amp; Myllymaki, 2007).</p>
    <p>In general, NML is hard but sNML is easy.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Computational Complexity</p>
    <p>The only computational issue in applying NML/sNML in the discrete (multinomial) case is the normalization factor.</p>
    <p>In NML, we have a sum of products:</p>
    <p>Cn =  xn</p>
    <p>p(x n ; (x n)) =  xn</p>
    <p>n i=1</p>
    <p>p(xi | x i1 ; (x n)).</p>
    <p>In sNML, we have a product of sums:</p>
    <p>Zn(x n) =</p>
    <p>n i=1</p>
    <p>Ki (x i1) =</p>
    <p>n i=1</p>
    <p>x i</p>
    <p>p(xi | x i1 ; (x i1, xi )).</p>
    <p>Remarkably, we can evaluate both in O(n) time (Kontkanen &amp; Myllymaki, 2007). In general, NML is hard but sNML is easy.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Properties of sNML: Bernoulli case</p>
    <p>Both variants of sNML are universal:</p>
    <p>sNML1 is identical to Laplaces add one rule:</p>
    <p>PsNML1(1 | x n) = PLap(1 | x n) = n1 + 1</p>
    <p>n + 2 .</p>
    <p>(Takimoto and Warmuth, 2000): The worst-case regret of sNML2 is bounded by</p>
    <p>sup xn</p>
    <p>ln p(x n ; (x n))</p>
    <p>psNML2(x n)</p>
    <p>Is the sun going to rise? x n = 111 . . . 1.</p>
    <p>(PLap(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>(PsNML2(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Properties of sNML: Bernoulli case</p>
    <p>Both variants of sNML are universal:</p>
    <p>sNML1 is identical to Laplaces add one rule:</p>
    <p>PsNML1(1 | x n) = PLap(1 | x n) = n1 + 1</p>
    <p>n + 2 .</p>
    <p>(Takimoto and Warmuth, 2000): The worst-case regret of sNML2 is bounded by</p>
    <p>sup xn</p>
    <p>ln p(x n ; (x n))</p>
    <p>psNML2(x n)</p>
    <p>Is the sun going to rise? x n = 111 . . . 1.</p>
    <p>(PLap(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>(PsNML2(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Properties of sNML: Bernoulli case</p>
    <p>Both variants of sNML are universal:</p>
    <p>sNML1 is identical to Laplaces add one rule:</p>
    <p>PsNML1(1 | x n) = PLap(1 | x n) = n1 + 1</p>
    <p>n + 2 .</p>
    <p>(Takimoto and Warmuth, 2000): The worst-case regret of sNML2 is bounded by</p>
    <p>sup xn</p>
    <p>ln p(x n ; (x n))</p>
    <p>psNML2(x n)</p>
    <p>Is the sun going to rise? x n = 111 . . . 1.</p>
    <p>(PLap(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>(PsNML2(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Properties of sNML: Bernoulli case</p>
    <p>Both variants of sNML are universal:</p>
    <p>sNML1 is identical to Laplaces add one rule:</p>
    <p>PsNML1(1 | x n) = PLap(1 | x n) = n1 + 1</p>
    <p>n + 2 .</p>
    <p>(Takimoto and Warmuth, 2000): The worst-case regret of sNML2 is bounded by</p>
    <p>sup xn</p>
    <p>ln p(x n ; (x n))</p>
    <p>psNML2(x n)</p>
    <p>Is the sun going to rise? x n = 111 . . . 1.</p>
    <p>(PLap(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>(PsNML2(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Properties of sNML: Bernoulli case</p>
    <p>Both variants of sNML are universal:</p>
    <p>sNML1 is identical to Laplaces add one rule:</p>
    <p>PsNML1(1 | x n) = PLap(1 | x n) = n1 + 1</p>
    <p>n + 2 .</p>
    <p>(Takimoto and Warmuth, 2000): The worst-case regret of sNML2 is bounded by</p>
    <p>sup xn</p>
    <p>ln p(x n ; (x n))</p>
    <p>psNML2(x n)</p>
    <p>Is the sun going to rise? x n = 111 . . . 1.</p>
    <p>(PLap(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>(PsNML2(1 | x n))n=0 = (</p>
    <p>) .</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Regrets Visualized</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Regrets Visualized</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Linear-Quadratic Models</p>
    <p>Linear model yt =  xt + t with Gaussian errors t  N(0, 2).</p>
    <p>Least squares parameters bt = arg min Xt  y t2.</p>
    <p>Consider the following three representations:</p>
    <p>yt = b  t1xt + et (1) plug-in</p>
    <p>yt = b  nxt + t (n) (2) least-squares</p>
    <p>yt = b  t xt + et (3) sNML</p>
    <p>Representation (1) corresponds to the predictive least squares (PLS) model selection criterion:</p>
    <p>n i=m+1 e</p>
    <p>Representation (2) leads to the AIC, BIC, and NML criteria.</p>
    <p>Representation (3) is new.  sequentially normalized least squares (SNLS)</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Linear-Quadratic Models</p>
    <p>Linear model yt =  xt + t with Gaussian errors t  N(0, 2).</p>
    <p>Least squares parameters bt = arg min Xt  y t2.</p>
    <p>Consider the following three representations:</p>
    <p>yt = b  t1xt + et (1) plug-in</p>
    <p>yt = b  nxt + t (n) (2) least-squares</p>
    <p>yt = b  t xt + et (3) sNML</p>
    <p>Representation (1) corresponds to the predictive least squares (PLS) model selection criterion:</p>
    <p>n i=m+1 e</p>
    <p>Representation (2) leads to the AIC, BIC, and NML criteria.</p>
    <p>Representation (3) is new.  sequentially normalized least squares (SNLS)</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Linear-Quadratic Models</p>
    <p>Linear model yt =  xt + t with Gaussian errors t  N(0, 2).</p>
    <p>Least squares parameters bt = arg min Xt  y t2.</p>
    <p>Consider the following three representations:</p>
    <p>yt = b  t1xt + et (1) plug-in</p>
    <p>yt = b  nxt + t (n) (2) least-squares</p>
    <p>yt = b  t xt + et (3) sNML</p>
    <p>Representation (1) corresponds to the predictive least squares (PLS) model selection criterion:</p>
    <p>n i=m+1 e</p>
    <p>Representation (2) leads to the AIC, BIC, and NML criteria.</p>
    <p>Representation (3) is new.  sequentially normalized least squares (SNLS)</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Linear-Quadratic Models</p>
    <p>Linear model yt =  xt + t with Gaussian errors t  N(0, 2).</p>
    <p>Least squares parameters bt = arg min Xt  y t2.</p>
    <p>Consider the following three representations:</p>
    <p>yt = b  t1xt + et (1) plug-in</p>
    <p>yt = b  nxt + t (n) (2) least-squares</p>
    <p>yt = b  t xt + et (3) sNML</p>
    <p>Representation (1) corresponds to the predictive least squares (PLS) model selection criterion:</p>
    <p>n i=m+1 e</p>
    <p>Representation (2) leads to the AIC, BIC, and NML criteria.</p>
    <p>Representation (3) is new.  sequentially normalized least squares (SNLS)</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Linear-Quadratic Models</p>
    <p>Linear model yt =  xt + t with Gaussian errors t  N(0, 2).</p>
    <p>Least squares parameters bt = arg min Xt  y t2.</p>
    <p>Consider the following three representations:</p>
    <p>yt = b  t1xt + et (1) plug-in</p>
    <p>yt = b  nxt + t (n) (2) least-squares</p>
    <p>yt = b  t xt + et (3) sNML</p>
    <p>Representation (1) corresponds to the predictive least squares (PLS) model selection criterion:</p>
    <p>n i=m+1 e</p>
    <p>Representation (2) leads to the AIC, BIC, and NML criteria.</p>
    <p>Representation (3) is new.  sequentially normalized least squares (SNLS)</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Linear-Quadratic Models</p>
    <p>Linear model yt =  xt + t with Gaussian errors t  N(0, 2).</p>
    <p>Least squares parameters bt = arg min Xt  y t2.</p>
    <p>Consider the following three representations:</p>
    <p>yt = b  t1xt + et (1) plug-in</p>
    <p>yt = b  nxt + t (n) (2) least-squares</p>
    <p>yt = b  t xt + et (3) sNML</p>
    <p>Representation (1) corresponds to the predictive least squares (PLS) model selection criterion:</p>
    <p>n i=m+1 e</p>
    <p>Representation (2) leads to the AIC, BIC, and NML criteria.</p>
    <p>Representation (3) is new.  sequentially normalized least squares (SNLS)</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Fixed variance 2t =  2 case:</p>
    <p>Non-normalized conditional:</p>
    <p>f (yt | y t1, Xt ; 2, bt ) = 1</p>
    <p>22</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  yt )2</p>
    <p>) ,</p>
    <p>where yt = b  t xt .</p>
    <p>Normalized conditional:</p>
    <p>fSNLS(yt | y t1, Xt ; 2) = 1</p>
    <p>2</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  bt1xt ) 2</p>
    <p>) ,</p>
    <p>where  = (1 + ct ) 22, ct = x</p>
    <p>t (Xt X</p>
    <p>t ) 1xt = O(1/t).</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Fixed variance 2t =  2 case:</p>
    <p>Non-normalized conditional:</p>
    <p>f (yt | y t1, Xt ; 2, bt ) = 1</p>
    <p>22</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  yt )2</p>
    <p>) ,</p>
    <p>where yt = b  t xt .</p>
    <p>Normalized conditional:</p>
    <p>fSNLS(yt | y t1, Xt ; 2) = 1</p>
    <p>2</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  bt1xt ) 2</p>
    <p>) ,</p>
    <p>where  = (1 + ct ) 22, ct = x</p>
    <p>t (Xt X</p>
    <p>t ) 1xt = O(1/t).</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Fixed variance 2t =  2 case:</p>
    <p>Non-normalized conditional:</p>
    <p>f (yt | y t1, Xt ; 2, bt ) = 1</p>
    <p>22</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  yt )2</p>
    <p>) ,</p>
    <p>where yt = b  t xt .</p>
    <p>Normalized conditional:</p>
    <p>fSNLS(yt | y t1, Xt ; 2) = 1</p>
    <p>2</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  bt1xt ) 2</p>
    <p>) ,</p>
    <p>where  = (1 + ct ) 22, ct = x</p>
    <p>t (Xt X</p>
    <p>t ) 1xt = O(1/t).</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Fixed variance 2t =  2 case:</p>
    <p>Non-normalized conditional:</p>
    <p>f (yt | y t1, Xt ; 2, bt ) = 1</p>
    <p>22</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  yt )2</p>
    <p>) ,</p>
    <p>where yt = b  t xt .</p>
    <p>Normalized conditional:</p>
    <p>fSNLS(yt | y t1, Xt ; 2) = 1</p>
    <p>2</p>
    <p>exp</p>
    <p>(</p>
    <p>(yt  bt1xt ) 2</p>
    <p>) ,</p>
    <p>where  = (1 + ct ) 22, ct = x</p>
    <p>t (Xt X</p>
    <p>t ) 1xt = O(1/t).</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Free variance case:</p>
    <p>Consider the maximization problem</p>
    <p>sup 2</p>
    <p>n t=m+1</p>
    <p>f (yt | y t1, Xt ; 2, bt ).</p>
    <p>The maximizing variance is given by n = 1</p>
    <p>nm n</p>
    <p>t=m+1(yt  yt ) 2,</p>
    <p>and the resulting non-normalized joint density is</p>
    <p>(2en) (nm)/2.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Free variance case:</p>
    <p>Consider the maximization problem</p>
    <p>sup 2</p>
    <p>n t=m+1</p>
    <p>f (yt | y t1, Xt ; 2, bt ).</p>
    <p>The maximizing variance is given by n = 1</p>
    <p>nm n</p>
    <p>t=m+1(yt  yt ) 2,</p>
    <p>and the resulting non-normalized joint density is</p>
    <p>(2en) (nm)/2.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>The SNLS criterion is given by</p>
    <p>SNLS(n, k)</p>
    <p>= n  m</p>
    <p>( n  m</p>
    <p>) (1/2)</p>
    <p>+ ln n</p>
    <p>t=m+2</p>
    <p>= n  m</p>
    <p>n t=m+1</p>
    <p>ln(1 + ct ) + Rn,</p>
    <p>where the remainder term Rn is insignificant.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Theorem: If the data is generated by a k-parameter linear-quadratic model (either non-random Xn, or AR model), then we have</p>
    <p>SNLS(n, k) = n  m</p>
    <p>and</p>
    <p>SNLS(n, k) = n  m</p>
    <p>k + 1</p>
    <p>almost surely for almost all  and 2.</p>
    <p>Note that the effective number of parameters is doubled.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Theorem: If the data is generated by a k-parameter linear-quadratic model (either non-random Xn, or AR model), then we have</p>
    <p>SNLS(n, k) = n  m</p>
    <p>and</p>
    <p>SNLS(n, k) = n  m</p>
    <p>k + 1</p>
    <p>almost surely for almost all  and 2.</p>
    <p>Note that the effective number of parameters is doubled.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Sequentially Normalized Least Squares</p>
    <p>Theorem: If the data is generated by a k-parameter linear-quadratic model (either non-random Xn, or AR model), then we have</p>
    <p>SNLS(n, k) = n  m</p>
    <p>and</p>
    <p>SNLS(n, k) = n  m</p>
    <p>k + 1</p>
    <p>almost surely for almost all  and 2.</p>
    <p>Note that the effective number of parameters is doubled.</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
  <div class="page">
    <p>Experiment: AR Model Order Estimation</p>
    <p>sample size, n 50 100 200 400 800 1600 3200</p>
    <p>k = 1 AIC 70.5 71.3 72.0 70.0 71.4 70.8 70.9 BIC 93.5 96.9 97.9 98.0 99.4 99.5 99.4 PLS 75.8 86.3 91.1 93.5 96.7 97.8 98.1</p>
    <p>NML 82.5 88.3 89.7 91.5 94.3 95.9 96.6 SNLS 78.5 87.5 92.2 93.9 97.0 98.1 98.3</p>
    <p>k = 4 AIC 42.8 52.5 60.1 63.3 65.4 66.5 67.5 BIC 45.7 59.6 67.8 76.5 82.6 88.3 91.4 PLS 42.1 58.3 68.5 77.0 82.5 88.3 91.9</p>
    <p>NML 45.0 60.2 68.0 76.7 82.5 88.0 91.6 SNLS 42.4 59.2 69.4 77.0 82.4 88.5 92.0</p>
    <p>k = 7 AIC 33.7 45.4 55.3 59.6 63.6 65.7 67.3 BIC 29.2 43.4 59.1 69.5 77.9 82.8 88.6 PLS 30.0 44.7 60.5 70.0 78.5 82.9 88.6</p>
    <p>NML 28.8 44.2 59.8 69.8 78.3 83.0 88.4 SNLS 30.1 46.5 61.2 70.6 79.4 83.2 88.9</p>
    <p>k = 10 AIC 28.5 43.9 51.5 59.3 64.2 67.1 67.7 BIC 20.6 35.7 51.0 66.1 74.4 81.4 85.5 PLS 20.1 35.7 50.7 65.0 73.4 80.8 84.8</p>
    <p>NML 20.2 37.1 51.9 66.8 74.6 81.4 85.8 SNLS 21.4 37.9 52.3 66.5 74.8 81.8 85.6</p>
    <p>T. Roos and J. Rissanen On Sequentially Normalized Maximum Likelihood Models</p>
  </div>
</Presentation>
