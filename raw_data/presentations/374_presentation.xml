<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Rethinking Deduplication Scalability</p>
    <p>Petros Efstathopoulos, Fanglu Guo</p>
    <p>Symantec Research Labs</p>
    <p>June 2010</p>
  </div>
  <div class="page">
    <p>State of Deduplication</p>
    <p>Deduplication is maturing</p>
    <p>Standard feature of backup/archive systems</p>
    <p>Many approaches, algorithms, techniques</p>
    <p>Inline, offline, file level, block level, variable/fixed sized blocks, global/local deduplication,</p>
    <p>High performance deduplication solutions available</p>
    <p>Near-optimal usage of raw storage Near-optimal usage of raw storage</p>
  </div>
  <div class="page">
    <p>State of Deduplication</p>
    <p>Deduplication is maturing</p>
    <p>Standard feature of backup/archive systems</p>
    <p>Many approaches, algorithms, techniques</p>
    <p>Inline, offline, file level, block level, variable/fixed sized blocks, global/local deduplication,</p>
    <p>High performance deduplication solutions available</p>
    <p>Near-optimal usage of raw storage Near-optimal usage of raw storage</p>
    <p>However: scalability still a problem</p>
    <p>Only a few tens of TBs supported per node</p>
    <p>Not enough for next-generation storage systems</p>
    <p>Deduplication systems optimized for deduplication efficiency</p>
    <p>but scalability (i.e., raw supported capacity) requirements</p>
    <p>are increasing dramatically</p>
  </div>
  <div class="page">
    <p>Challenges Scaling Up</p>
    <p>++++</p>
    <p>Deduplication</p>
    <p>Metadata</p>
    <p>Have I seen this block before? (if yes, then where is it?)</p>
  </div>
  <div class="page">
    <p>Challenges Scaling Up</p>
    <p>Have I seen this block before?</p>
    <p>Item Scale Remarks</p>
    <p>Raw Capacity C = 400 TB</p>
    <p>Segment Size S = 4 KB</p>
    <p>Num of Segments N = 100 * 109 N = C / S</p>
    <p>Fingerprint size F &gt;= 20 B</p>
    <p>Block Index M = 2000 GB M = N * F</p>
    <p>Disk Speed Z = 300 MB/sec  Indexing Scalability Challenges</p>
    <p>Disk Speed Z = 300 MB/sec</p>
    <p>Lookup speed</p>
    <p>requirement</p>
    <p>Resorting to larger segment sizes</p>
    <p>&gt;75 K ops/sec</p>
    <p>Reference Management Challenges</p>
    <p>Scalability bound by resource reference management and reclamation mechanism</p>
    <p>Deletion errors not acceptable</p>
    <p>Crashes possible</p>
    <p>&gt;75 K reference updates</p>
  </div>
  <div class="page">
    <p>Goals</p>
    <p>Scalability</p>
    <p>100+ billion objects with high throughput</p>
    <p>Willing to sacrifice some deduplication efficiency</p>
    <p>Performance  Performance</p>
    <p>Near raw disk throughput for backup and restore</p>
    <p>Near raw disk throughput for data deletion</p>
    <p>Reasonable deduplication efficiency</p>
  </div>
  <div class="page">
    <p>Next Generation Dedup - Indexing</p>
    <p>Increase supported capacity  Bigger segment size</p>
    <p>Lowers dedup efficiency/granularity  too coarse!</p>
    <p>Supported capacity still low (a few tens of TBs)</p>
    <p>Progressive Sampled Indexing</p>
    <p>Cant fit all FPs in memory  keep 1 out of N segment FPs</p>
    <p>Sampling rate R = function (RAM, storage capacity, desired  Sampling rate R = function (RAM, storage capacity, desired segment size)</p>
    <p>Estimate of sampling rate: R = (S * M) / (E * C) = 1 / N</p>
    <p>R = sampling rate</p>
    <p>E = bytes/entry</p>
    <p>C = total raw storage in TBs</p>
    <p>S = segment size in KBs</p>
    <p>M = GBs of main memory used for indexing</p>
  </div>
  <div class="page">
    <p>Sampling Rate</p>
    <p>Example: 32 bytes/entry, 4KB segments and 32GB of RAM</p>
    <p>No sampling (R=1)  4 TB storage</p>
    <p>400TBs of storage  R ~= 1/100  i.e., keep 1 out of 100 FPs</p>
    <p>Supports infinite storage  capacity VS dedup efficiency</p>
    <p>Progressive sampling: used storage VS available storage</p>
    <p>Sampling rate = function(used storage, available RAM)</p>
    <p>Start with no sampling (R=1), progressively lower R, down to Rmin = (S * M) / (E * C)</p>
    <p>Straight sampling  poor deduplication efficiency</p>
    <p>1 out of N segments deduplicatable (i.e., hit on index)</p>
    <p>Fingerprint cache: take advantage of spatial locality</p>
    <p>Small part of index memory: index hit  pre-fetch &amp; cache all FPs in container</p>
  </div>
  <div class="page">
    <p>Dedup Indexing: Hitting the Disk</p>
    <p>SRL prototype  Sampled index in memory</p>
    <p>Index recovery after reboot/crash  Disk Index</p>
    <p>Checkpointed index state  minimum necessary to resume indexing</p>
    <p>What if we store the index directly on disk? What if we store the index directly on disk?</p>
    <p>More indexing capacity  Full(er) disk index  Better deduplication</p>
    <p>Disk operations too slow, bad performance</p>
    <p>SSDs provide interesting alternative</p>
  </div>
  <div class="page">
    <p>SSD Indexing</p>
    <p>Example: 32GB RAM  128GB of SSD  x4 indexing capacity</p>
    <p>32 bytes/entry ~= 4 Gentries  4KB segment  only 16 TB storage</p>
    <p>With a much larger segment size of 128KB  ~512 TB</p>
    <p>SSD holding full index could help scalability in the short term</p>
    <p>SSD-based sampled indexing: SSD-based sampled indexing:</p>
    <p>Better sampling rates based on SSD capacity</p>
    <p>x4 capacity  R = 4 * R = 1 / 25</p>
    <p>Sorted index on SSD</p>
    <p>All available memory used for speedup mechanisms:</p>
    <p>Memory catalogue summarizing SSD guarantees that every lookup will take at most one SSD access (~4.8 MB of memory / GB of storage, for 20-byte FPs, 4KB SSD blocks)</p>
    <p>Container caching/pre-fetching</p>
    <p>Bloom filter</p>
  </div>
  <div class="page">
    <p>Reference Management &amp; Resource Reclamation:Resource Reclamation:</p>
    <p>Grouped Mark-and-Sweep</p>
  </div>
  <div class="page">
    <p>The Problem and Challenges</p>
    <p>Problem: Can I delete a segment/object? Is it still in use/pending to be used?</p>
    <p>Challenges:</p>
    <p>Scale: 100 billion</p>
    <p>Reliability: No deletion error in a distributed</p>
    <p>system - crashes possible</p>
    <p>Speed: 75k/s reference updates</p>
  </div>
  <div class="page">
    <p>Next generation Dedup - Deletion</p>
    <p>Reference count</p>
    <p>Simple but hard to make it crash-resilient (especially in a distributed system)</p>
    <p>E.g., lost or repeated count update</p>
    <p>Corrupted Object -&gt; Who is using it??</p>
    <p>Reference list Reference list</p>
    <p>Resilient to repeated updates but not lost updates</p>
    <p>Poor scalability &amp; performance, variable size list needs to be stored on disk</p>
    <p>Mark-and-sweep (aka garbage collection)</p>
    <p>Workload proportional to system capacity</p>
    <p>400 TB system, 4 KB segment, 20 byte FP  read 2 TB of data during mark phase  1.85 hours to go through once at 300 MB/second</p>
    <p>x10 deduplication factor ~= 20 TB  18.5 hours</p>
  </div>
  <div class="page">
    <p>Next Generation Dedup</p>
    <p>Grouped Mark-and-Sweep</p>
    <p>Mark: Divide and save</p>
    <p>Divide backups to groups</p>
    <p>Track changes to the groups</p>
    <p>Only re-mark changed groups</p>
    <p>Mark results are saved and reused if not changed</p>
    <p>Sweep: Track affected containers</p>
    <p>Only sweep containers that may have deleted objects</p>
    <p>Net result:</p>
    <p>Workload = f (nature of work)</p>
    <p>Instead of system capacity</p>
    <p>SO Container1 SO Container2 SO Container3 SO Container4 SO Container5</p>
    <p>G1</p>
    <p>G2</p>
    <p>G1</p>
    <p>G3</p>
    <p>G2 G2</p>
    <p>G3</p>
    <p>G2</p>
    <p>Containers to sweep</p>
    <p>Group1 Group2 Group3</p>
    <p>Backup1 Backup2 Backup3 Backup4 Backup5</p>
    <p>Some DOs</p>
    <p>are deleted</p>
    <p>Some DOs</p>
    <p>are added</p>
    <p>Instead of system capacity</p>
  </div>
  <div class="page">
    <p>Preliminary Prototype EvaluationPrototype Evaluation</p>
  </div>
  <div class="page">
    <p>Prototype Implementation &amp;</p>
    <p>Preliminary Throughput Results</p>
    <p>Full prototype implemented in C++</p>
    <p>Sampled indexing, Grouped M&amp;S, Backup management logic, etc</p>
    <p>Multithreaded implementation for max throughput:</p>
    <p>Multiple hash calculation threads, multiple backup/disk writer threads</p>
    <p>3GHz Intel Xeon multicore system, 8 TB Clariion FC storage 3GHz Intel Xeon multicore system, 8 TB Clariion FC storage</p>
    <p>Hardware read/write baseline: ~380/330 MB/sec</p>
    <p>Single thread backup: ~310 MB/sec (94% of baseline)</p>
    <p>CPU-bound hash calculation</p>
    <p>Single thread restore throughput: ~ 350 MB/sec (92% of baseline)</p>
    <p>Deduplication performance: up to 1180 MB/sec</p>
    <p>Limited by disk read performance (reading pre-fetched container indexes)</p>
  </div>
  <div class="page">
    <p>Preliminary Index Results</p>
    <p>Example: drop rate 2%, 4 KB segments, 1KB buckets, 149 Mentries capacity</p>
    <p>X Load Drop % Insert Lookup Remove Throughput (4KB segments, 3GHz Xeon)</p>
    <p>4 VMWare image versions (10.2 GB, 2621440 * 4 KB segments):</p>
    <p>Unique segs Total unique Optimal space Space used Achievement Diff</p>
    <p>Preliminary SSD index implementation test results:</p>
    <p>SATA SSD ioDrive</p>
    <p>Seek time 0,24 msec 0,06 msec</p>
    <p>Read Throughput 90 MB/sec 454 MB/sec</p>
    <p>Cycles/lookup 138,871 55,590</p>
    <p>Ops/sec 17,323 53,646</p>
  </div>
  <div class="page">
    <p>GMS Scalability Test</p>
    <p>Measurements: ~empty and ~full (8 TB, 2 billion objects, 4 KB segments)</p>
    <p>Preliminary evaluation:</p>
    <p>Process time is proportional to workload</p>
    <p>Process throughput is stable and fast regardless of capacity</p>
    <p>Results vary depending on underlying filesystem</p>
    <p>Data Ext3 Add (time/throughput) Ext3 Remove (time/throughput)</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Future Work</p>
    <p>Scalable deduplication is possible</p>
    <p>And necessary for system management/administration</p>
    <p>Sampled indexing + pre-fetching works</p>
    <p>Scalability and throughput goals achieved</p>
    <p>Grouped Mark-and-Sweep: scalable reference management Grouped Mark-and-Sweep: scalable reference management</p>
    <p>Work in progress and next goals:</p>
    <p>Comprehensive end-to-end testing with real workloads</p>
    <p>Testing of concurrent backups</p>
    <p>Distributed operation</p>
    <p>Complete SSD index implementation and testing</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Petros Efstathopoulos</p>
    <p>Petros_Efstathopoulos@symantec.com</p>
    <p>Copyright  2008 Symantec Corporation. All rights reserved. Symantec and the Symantec Logo are trademarks or registered trademarks of Symantec Corporation or its</p>
    <p>affiliates in the U.S. and other countries. Other names may be trademarks of their respective owners.</p>
    <p>This document is provided for informational purposes only and is not intended as advertising. All warranties relating to the information in this document, either express or implied,</p>
    <p>are disclaimed to the maximum extent allowed by law. The information in this document is subject to change without notice.</p>
  </div>
</Presentation>
