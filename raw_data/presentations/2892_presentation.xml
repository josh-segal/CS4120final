<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Bayesian Approach  to Unsupervised Semantic Role Induction</p>
    <p>Ivan Titov and Alex Klementiev</p>
  </div>
  <div class="page">
    <p>From Syntax to Semantics</p>
    <p>} Emergence of robust syntactic parsers [Collins 1999, Charniak 2001, Petrov and Klein 2006, McDonald 2005, Titov and Henderson 2007] for many languages has been one of the key successes of statistical NLP in recent years</p>
    <p>} However, syntactic analyses are a long way from representing the meaning of sentences</p>
    <p>} In other words, they do not specify the underlying predicate argument structure</p>
    <p>Specifically, they do not define Who did What to Whom (and How, Where, When, Why, )</p>
    <p>openedJack the lock with a paper clip</p>
    <p>subj det</p>
    <p>dobj</p>
    <p>prep</p>
    <p>det</p>
    <p>nmod</p>
    <p>pobj</p>
  </div>
  <div class="page">
    <p>Semantic Role Labeling (SRL)</p>
    <p>} Identification of arguments and their semantic roles } Example: predicate open</p>
    <p>Jack opened the lock with a paper clip</p>
    <p>Semantic Roles (PropBank-style): PROTO-AGENT (A0)  an initiator/doer in the event [Who?]</p>
    <p>PROTO-PATIENT (A1) - an affected entity [to Whom / to What?]</p>
    <p>INSTRUMENT (A3)  the entity manipulated to accomplish the goal</p>
  </div>
  <div class="page">
    <p>Syntactic-Semantic Interface</p>
    <p>} Though syntactic and lexical representations are often predictive of the predicate argument structure, this relation is far from trivial, consider alternations:</p>
    <p>(1) John broke the window</p>
    <p>(2) The window broke</p>
    <p>(3) The window was broken by John</p>
    <p>Semantic Roles:</p>
    <p>AGENT  an initiator/doer in the event [Who?]</p>
    <p>PATIENT - an affected entity [to Whom / to What?]</p>
  </div>
  <div class="page">
    <p>Approaches to SRL</p>
    <p>} Supervised learning approaches (e.g., [Gildea and Jurafsky, 2002; Johansson, 2008])</p>
    <p>} Rely on large expert-annotated datasets (e.g., PropBank ~40k sentences)</p>
    <p>} Even then they provide very low coverage and are domain dependent</p>
    <p>} Annotated data is not available for many languages</p>
    <p>} Semi-supervised methods  combine labeled and unlabeled data</p>
    <p>} Have relatively limited success so far (e.g., Furstenau and Lapata [09]; Deschacht and Moens [09] )</p>
    <p>} Unsupervised methods</p>
    <p>} This work, also Lang and Lapata [2010, 2011] and Grenager and Manning [2006]</p>
    <p>Our main contributions: - a Bayesian model of unsupervised SRL, substantially outperforming previous</p>
    <p>work - Induction of a representation encoding alternation patterns shared across</p>
    <p>predicates</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Task and Approach Overview</p>
    <p>} Semantic role induction without labeled data</p>
    <p>} Model and Inference</p>
    <p>} Overview of the distance-dependent CRPs</p>
    <p>} A hierarchical Bayesian model defining the process of joint generation of semantic, syntactic and lexical representations</p>
    <p>} Evaluation</p>
    <p>} Results on a human-annotated corpus</p>
  </div>
  <div class="page">
    <p>Mary the door for Peteropened</p>
    <p>window by the windopenedThe was</p>
    <p>Mary the door for Peteropened</p>
    <p>window by the windopenedThe was</p>
    <p>Mary the door</p>
    <p>A0 A1</p>
    <p>for Peteropened</p>
    <p>A3</p>
    <p>window</p>
    <p>A0A1</p>
    <p>by the windopenedThe was</p>
    <p>Our task</p>
    <p>Our goal: induce semantic roles automatically from unannotated texts</p>
    <p>} Equivalent to clustering of argument occurrences (or coloring them)</p>
    <p>Our goal: induce semantic roles automatically from unannotated texts</p>
    <p>} Semantic role labeling involves 2 sub-tasks:</p>
    <p>} Identification: identification of predicate arguments</p>
    <p>} Labeling: assignment of their sematic roles</p>
    <p>Mary the door</p>
    <p>Role 3 Role 12</p>
    <p>for Peteropened</p>
    <p>Role 4</p>
    <p>window</p>
    <p>Role 3Role 12</p>
    <p>by the windopenedThe was</p>
    <p>Can be handled with heuristics (e.g. [Lang and Lapata, 2010])</p>
    <p>Focus of this work</p>
    <p>} Assume that sentences are (auto-) annotated with syntactic trees</p>
  </div>
  <div class="page">
    <p>Argument Keys</p>
    <p>We identify arg occurrences with syntactic signatures (argument keys)</p>
    <p>(as in Lang and Lapata [2011])</p>
    <p>E.g., some simple alternations like locative preposition drop</p>
    <p>Argument keys are designed so that to map mostly to a single role</p>
    <p>Instead of clustering occurrences we cluster argument keys</p>
    <p>Here, we would cluster ACTIVE:RIGHT:OBJ and ACTIVE:RIGHT:PMOD_up together</p>
    <p>More complex alternations require multiples pairs of arg keys clustered</p>
    <p>ACTIVE:RIGHT:PMOD_up</p>
    <p>ACTIVE:RIGHT:OBJ</p>
    <p>climbedMary Mont Ventouxup</p>
    <p>climbedMary Mont Ventoux</p>
    <p>Role 1</p>
    <p>Role 1</p>
    <p>Role 2</p>
    <p>Role 2</p>
  </div>
  <div class="page">
    <p>Factored Model</p>
    <p>} Our first model (Factored) clusters argument keys for every predicate in isolation.</p>
    <p>} These clusterings</p>
    <p>} are different as verbs admit different alternations</p>
    <p>} but expected to be similar: many alternations are common and licensed by many predicates (passivization, dativization, etc)</p>
  </div>
  <div class="page">
    <p>Coupled Model</p>
    <p>} Consequently, propose an extension (Coupled) to induce the clusterings jointly</p>
    <p>} Do not split the learning data</p>
    <p>} The task is easier for some predicate than others</p>
    <p>} E.g., predicates change and defrost admit similar alternations but inducing it for defrost is easier: the set of possible argument fillers is more restricted</p>
    <p>} This is done by inducing a similarity score for every pairs of argument keys</p>
    <p>} Similarities are learned, rather than specified by hand, as part of the inference process</p>
  </div>
  <div class="page">
    <p>Signals for Semantic Role Induction</p>
    <p>} Selection preferences:</p>
    <p>} Two argument signatures are likely to correspond to the same role if the corresponding sets of arguments are similar.</p>
    <p>} Duplicate roles are unlikely to occur. E.g. this coloring is a bad idea:</p>
    <p>John taught students math</p>
    <p>} Predicates admit similar alternation patterns (reuse them)</p>
    <p>How to encode this in a statistical model?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Task and Approach Overview</p>
    <p>} Semantic role induction without labeled data</p>
    <p>} Model and Inference</p>
    <p>} Overview of the distance-dependent CRPs</p>
    <p>} A hierarchical Bayesian model defining the process of joint generation of semantic, syntactic and lexical representations</p>
    <p>} Evaluation</p>
    <p>} Results on a human-annotated corpus</p>
  </div>
  <div class="page">
    <p>A Prior on the Partition of Argument Keys</p>
    <p>} Can use CRP to define a prior on the partition of argument keys: } The first customer (argument key) sits the first table (role) } m-th customer sits at a table according to:</p>
    <p>} An extension is distance-dependent CRP (dd-CRP): } m-th customer chooses a customer to sit with according to:</p>
    <p>. . .</p>
    <p>p(previously occupied table k|Fm1, )  nk p(next unoccupied table|Fm1, )</p>
    <p>State of the restaurant once m-1 customers are seated</p>
    <p>Entire similarity graph</p>
    <p>Similarity between customers m and j</p>
    <p>p(different customer j|D, )  dm,j p(itself|D, )</p>
    <p>Encodes rich-get-richer dynamics but not much more than that</p>
  </div>
  <div class="page">
    <p>A Prior on the Partition of Argument Keys</p>
    <p>} Similarity graph D to couples distinct but similar clusterings of argument keys across predicates } Vertices are argument keys } Weights are similarity scores for each pair of argument keys</p>
    <p>} We treat D as a latent random variable drawn from a prior over weighted graphs } First drawn from a prior } Used to generate each of the clusterings for every predicate</p>
    <p>} We induce D automatically within the model } This is in contrast to all the previous work on dd-CRP where similarities were used to encode</p>
    <p>prior knowledge</p>
  </div>
  <div class="page">
    <p>} Given a (large) collection of sentences annotated with (transformed) syntactic dependencies</p>
    <p>} We want to induce semantic roles</p>
    <p>} Define a family of generative models encoding our assumptions</p>
    <p>} In the prior probability over parameters , we encode our beliefs</p>
    <p>} We incorporate latent variables (our latent weighted graph D)</p>
    <p>} We want to find the maximum-a-posteriori clustering given the observable data</p>
    <p>Bayesian Induction of Semantic Roles</p>
    <p>{xi}ni=1 {mi}ni=1</p>
    <p>Mary the door for PeteropenedMary the door</p>
    <p>Role 3 Role 12</p>
    <p>for Peteropened</p>
    <p>Role 4</p>
    <p>P(m, x|) P()</p>
    <p>z</p>
    <p>{mi}ni=1 = arg max  n</p>
    <p>i=1</p>
    <p>P(mi, xi, zi|)P()ddz</p>
  </div>
  <div class="page">
    <p>Model parameters</p>
    <p>(1) For roles, the distribution over argument fillers is sparse</p>
    <p>} We use a sparse prior, Hierarchical Dirichlet Processes [Teh et al, 05]</p>
    <p>(2) Each predicate undergoes a small number of alternations</p>
    <p>} We use sparse Dirichlet priors to encode the linking</p>
    <p>(3) The same semantic role rarely appears twice</p>
    <p>} Use a non-symmetric Dirchlet prior for the corresponding geom. distrib</p>
    <p>(4) Argument key clusterings for different predicates are related</p>
    <p>} Induce a shared weighted graph used in a (distance-dependent) Chinese Restaurant Process [Blei and Frazer 11] prior for each clustering</p>
  </div>
  <div class="page">
    <p>while [n  p,r] = 1 : GenArgument(p, r)</p>
    <p>GenArgument(p, r)</p>
    <p>if [n  Unif(0, 1)] = 1 : GenArgument(p, r)</p>
    <p>kp,r  Unif(1, . . . , |r|) xp,r  p,r</p>
    <p>for each predicate p = 1, 2,    : for each occurrence l of p :</p>
    <p>for every role r  Bp :</p>
    <p>for each predicate p = 1, 2, . . . : for each role r  Bp:</p>
    <p>p,r  DP(, H(A)) p,r  Beta(0, 1)</p>
    <p>Coupled model: D  NonInform for each predicate p = 1, 2, . . . :</p>
    <p>Bp  dd-CRP(, D)</p>
    <p>Factored model: for each predicate p = 1, 2, . . . :</p>
    <p>Bp  CRP()</p>
    <p>Generative Stories for Factored and Coupled Models</p>
    <p>At least one argument</p>
    <p>Draw first argument</p>
    <p>Continue generation</p>
    <p>Draw more arguments</p>
    <p>Draw argument key</p>
    <p>Draw argument filler</p>
    <p>openedwas</p>
    <p>Role 1</p>
    <p>openedwas</p>
    <p>Role 1</p>
    <p>openedwas</p>
    <p>PASSIVE:LEFT:OBJ</p>
    <p>window</p>
    <p>Role 1</p>
    <p>openedThe was</p>
    <p>PASSIVE:LEFT:OBJ</p>
    <p>window</p>
    <p>Role 3Role 1</p>
    <p>by the windopenedThe was</p>
    <p>PASSIVE:RIGHT:SBJPASSIVE:LEFT:OBJ</p>
  </div>
  <div class="page">
    <p>Inference</p>
    <p>} We use approximate maximum a-posteriori (MAP) decoding to induce semantic representations</p>
    <p>} Similar techniques has been used in the context of Dirichlet process mixtures</p>
    <p>} An EM-like inference algorithm for the Coupled model:</p>
    <p>} Start with uniform similarities</p>
    <p>} Iterate between</p>
    <p>} Inducing new clusterings m of argument keys for each predicates given the similarity graph D</p>
    <p>} Reestimate the similarity graph D</p>
    <p>{mi}ni=1 = arg max {mi}ni=1</p>
    <p>n</p>
    <p>i=1</p>
    <p>P(mi, xi|)P()d</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Task and Approach Overview</p>
    <p>} Semantic role induction without labeled data</p>
    <p>} Model and Inference</p>
    <p>} Overview of the distance-dependent CRPs</p>
    <p>} A hierarchical Bayesian model defining the process of joint generation of semantic, syntactic and lexical representations</p>
    <p>} Evaluation</p>
    <p>} Results on a human-annotated corpus</p>
  </div>
  <div class="page">
    <p>} Evaluation of semantic role induction</p>
    <p>} Purity measures the degree to which each induced role contains arguments sharing the same gold (true) role</p>
    <p>} Collocation evaluates the degree to which arguments with the same gold roles are assigned to a single induced role</p>
    <p>} Report F1, harmonic mean of PU and CO</p>
    <p>Benchmark Dataset: PropBank (CoNLL 08)</p>
    <p>PU = 1</p>
    <p>N</p>
    <p>i</p>
    <p>max j</p>
    <p>|Gj  Ci|</p>
    <p>CO = 1</p>
    <p>N</p>
    <p>j</p>
    <p>max i</p>
    <p>|Gj  Ci|</p>
    <p>Gold role Induced role</p>
  </div>
  <div class="page">
    <p>PropBank (CoNLL 08) with Gold Argument ID</p>
    <p>State-of-the-art methods Our models</p>
    <p>Deterministic mapping from syntactic relations</p>
    <p>Our models</p>
    <p>Gold syntax Predicted syntax</p>
  </div>
  <div class="page">
    <p>PropBank (CoNLL 08) with Predicted Argument ID</p>
    <p>Our models Our models</p>
    <p>Gold syntax Predicted syntax</p>
  </div>
  <div class="page">
    <p>Benchmark Dataset: PropBank (CoNLL 08)</p>
    <p>Looking into induced graph encoding priors over clustering arguments keys, the most highly ranked pairs encode (or partially encode)</p>
    <p>} Passivization } Near-equivalence of subordinating conjunctions and prepositions</p>
    <p>} E.g., whether and if</p>
    <p>} Benefactive alternation Martha carved a doll for the baby Martha carved the baby a doll</p>
    <p>} Dativization I gave the book to Mary I gave Mary the book</p>
    <p>} Recovery of unnecessary splits introduced by argument keys</p>
    <p>Encoded as (ACTIVE:RIGHT:OBJ_if, ACTIVE:RIGHT:OBJ_whether)</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>} We proposed a Bayesian model for unsupervised SRL } Best reported scores on PropBank } First to induce alternation patterns shared across predicates</p>
    <p>} The proposed multi-task clustering approach is a general method } Can be used as a component in many Bayesian models for NLP and beyond</p>
    <p>} The data, code and evaluation scripts will be available on our web-pages within a week or two.</p>
  </div>
</Presentation>
