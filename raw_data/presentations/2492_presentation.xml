<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Multi-Agent Learning Approach to Online Distributed Resource Allocation</p>
    <p>Chongjie Zhang Victor Lesser Prashant Shenoy Computer Science Department</p>
    <p>University of Massachusetts Amherst</p>
  </div>
  <div class="page">
    <p>Focus</p>
    <p>This paper presents a multi-agent learning (MAL) approach to address resource sharing in cluster networks.  Exploit unknown task arrival patterns</p>
    <p>Problem characteristics:  Realistic  Multiple agents  Partial observability  No global reward signal  Communication delay  Two interacting learning problems</p>
  </div>
  <div class="page">
    <p>Increasing Computing Demands</p>
    <p>Software as a service is becomeing a popular IT business model.</p>
    <p>Challenging to build large computing infrastructure to host such wide-spread online services.</p>
  </div>
  <div class="page">
    <p>A Potentially Cost-Effective Solution</p>
    <p>Shared clusters  Built using commodity PCs or workstations.  Running the number of applications significantly</p>
    <p>larger than the number of nodes</p>
    <p>Resource manager</p>
    <p>A dedicated cluster</p>
    <p>Resource manager</p>
    <p>A shared cluster</p>
    <p>[Arpacidusseau and Culler, 1997; Aron et al., 2000; Urgaonkar and Shenoy 2003]</p>
  </div>
  <div class="page">
    <p>Building Larger, Scalable Computing Infrastructures</p>
    <p>Centralized resource management limits the size of shared clusters.</p>
    <p>Organizing shared clusters into a network and sharing resource across clusters.</p>
    <p>How to efficiently share resources within a cluster network?</p>
    <p>Shared Cluster</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem Formulation  Fair Action Learning Algorithm  Learning Distributed Resource Allocation</p>
    <p>Local Allocation Decision  Task Routing Decision</p>
    <p>Experimental Results  Summary</p>
  </div>
  <div class="page">
    <p>Problem Formulation</p>
    <p>A distributed sequential resource allocation problem (DSRAP) is denoted as a tuple &lt;C, A, T, R, B&gt;:  C = {C1, , Cm} is a set of agents (or clusters)  A = {aij} m x m is the adjacent matrix of agents and aij is the</p>
    <p>task transfer time from Ci to Cj  T = {t1, , tl} is a set of task types  R = {R1, , Rq} is a set of resource types  B = {Dij} l x m is the task arrival pattern and Dij is the</p>
    <p>arrival distribution of tasks of type ti at Cj</p>
  </div>
  <div class="page">
    <p>Problem Description: Cluster Network</p>
    <p>C1</p>
    <p>C6 C2</p>
    <p>C4</p>
    <p>C3</p>
    <p>C5</p>
    <p>C7</p>
    <p>C9</p>
    <p>C8</p>
    <p>C10</p>
    <p>a12</p>
    <p>a13</p>
    <p>a24a46</p>
    <p>a25</p>
    <p>a35</p>
    <p>a56</p>
    <p>a37</p>
    <p>a79</p>
    <p>a69</p>
    <p>a68</p>
    <p>a8, 10</p>
    <p>a9, 10</p>
    <p>Computing nodeComputing node</p>
    <p>ResourceResource</p>
    <p>ClusterCluster</p>
    <p>R1 R2 R3</p>
  </div>
  <div class="page">
    <p>Problem Description: Task</p>
    <p>A task is denoted as a tuple &lt;t, u, w, d1,  dq&gt;, where  t is the task type  u is the utility rate of the task  w is the maximum waiting time before being</p>
    <p>allocated  di is the demand for resource i = 1, , q.</p>
  </div>
  <div class="page">
    <p>Problem Description: Task Type</p>
    <p>A task type characterizes a set of tasks, each of whose feature components follows a common distribution.</p>
    <p>A task type t is denoted as a tuple &lt;Dts, Dtu, Dtw, Dtd1,  Dtdq&gt;, where  Dts is the task service time distribution  Dtu is the distribution of utility rate  Dtw is the distribution of the maximum waiting time  Dtdi is the distribution of the demand for resource i = 1,</p>
    <p>, q.</p>
  </div>
  <div class="page">
    <p>Local Task Allocation Decision-Making</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Individual Agents Decision-Makings</p>
    <p>Task Routing Decision-Making</p>
    <p>Task Routing Decision-Making</p>
    <p>Local Resource Scheduling</p>
    <p>Local Resource Scheduling</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Task Routing Decision-Making</p>
    <p>Task Routing Decision-Making</p>
    <p>Existing cluster resource scheduling algorithm</p>
    <p>Tasks to be allocated locally Tasks not allocated locally</p>
    <p>Task Set T</p>
    <p>T2 T3 T4</p>
  </div>
  <div class="page">
    <p>Problem Goal</p>
    <p>The main goal is to derive decision policies for each agent that maximize the average utility rate (AUR) of the whole system.</p>
    <p>Note that, due to its partial view of the system, each individual cluster can only observe its local utility rate, but not the system's utility rate.</p>
  </div>
  <div class="page">
    <p>Multi-Agent Reinforcement Learning (MARL)</p>
    <p>In a multi-agent setting, all agents are concurrently learning their policies.</p>
    <p>The environment becomes non-stationary from the perspective of an individual agent.</p>
    <p>Single-agent reinforcement learning algorithms may diverge due to lack of synchronization.</p>
    <p>Several MARL algorithms are proposed.  GIGA, GIGA-Wolf, WPL, etc.</p>
  </div>
  <div class="page">
    <p>Fair Action Learning (FAL) Algorithm</p>
    <p>We usually dont know the exact policy gradient used by GIGA in practical problems.</p>
    <p>FAL is a direct policy search technique.  FAL is a variant of GIGA, using an easily-calculable, approximate policy</p>
    <p>gradient.</p>
    <p>Policy gradient</p>
    <p>GIGAs normalization function</p>
  </div>
  <div class="page">
    <p>Local Task Allocation Decision-Making</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Individual Agents Decision-Makings</p>
    <p>Task Routing Decision-Making</p>
    <p>Task Routing Decision-Making</p>
    <p>Local Resource Scheduling</p>
    <p>Local Resource Scheduling</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Tasks to be allocated locally Tasks not allocated locally</p>
    <p>Task Set T</p>
    <p>T2 T3 T4</p>
  </div>
  <div class="page">
    <p>Local Task Allocation Decision-Making</p>
    <p>Select a subset of received tasks to be allocated locally to maximize its local utility rate.</p>
    <p>Potentially improve the global utility rate.</p>
    <p>Use an incrementally selecting algorithm.</p>
    <p>selected :=  selected :=</p>
    <p>allocable := getAllocable(tasks) allocable := getAllocable(tasks)</p>
    <p>t := selectTask(Allocable) t := selectTask(Allocable)</p>
    <p>t = nilt = nil</p>
    <p>selected := selected  {t} tasks := tasks \ {t}</p>
    <p>selected := selected  {t} tasks := tasks \ {t}</p>
    <p>learn()learn()</p>
    <p>Return selectedReturn selected</p>
    <p>Yes</p>
    <p>No</p>
  </div>
  <div class="page">
    <p>Learning Local Task Allocation</p>
    <p>Learning model:  State: features describing both tasks to be allocated and availability of</p>
    <p>local resources  Action: selecting a task  Reward for selecting task a at state s</p>
    <p>Due to partial observation, each agent uses FAL to learn a stochastic policy.</p>
    <p>Q-learning is used to update the value function</p>
  </div>
  <div class="page">
    <p>Accelerating the Learning Process</p>
    <p>Reasons  Extremely large policy search space  Non-stationary learning environment  Avoid poor initial policies in practical systems</p>
    <p>Techniques  Initialize policies with a greedy allocation algorithm  Set utilization threshold for conducting -greedy</p>
    <p>exploration  Limit the exploration rate for selecting nil task</p>
  </div>
  <div class="page">
    <p>Local Task Allocation Decision-Making</p>
    <p>Local Task Allocation Decision-Making</p>
    <p>Individual Agents Decision-Makings</p>
    <p>Task Routing Decision-Making</p>
    <p>Task Routing Decision-Making</p>
    <p>Local Resource Scheduling</p>
    <p>Local Resource Scheduling</p>
    <p>Tasks to be allocated locally Tasks not allocated locally</p>
    <p>Task Set T</p>
    <p>T2 T3 T4</p>
  </div>
  <div class="page">
    <p>Task Routing Decision-Making</p>
    <p>To which neighbor should an agent forward an unallocated task to get it to an unsaturated cluster before it expires?</p>
    <p>Learn to route tasks via interacting with its neighbors  The learning objective is to maximize the probability of</p>
    <p>each task to be allocated in the system.</p>
    <p>C1</p>
    <p>C6 C2C4</p>
    <p>C3</p>
    <p>C5</p>
    <p>Task</p>
  </div>
  <div class="page">
    <p>Learning Task Routing</p>
    <p>State sx is defined by the characteristics of the current task x that an agent is forwarding.</p>
    <p>An action j corresponds to choosing neighbor j for forwarding a task.</p>
    <p>Reward is the allocation probability of task x forwarded to neighbor j:</p>
    <p>The probability that j allocates x locally The allocation probability of x forwarded by j</p>
    <p>Routing policy of j</p>
  </div>
  <div class="page">
    <p>Learning Task Routing (cont.)</p>
    <p>The local allocation probability</p>
    <p>Qi(sx, j) is the expected probability that the task x will be allocated if an agent i forwards it to its neighbor j.</p>
    <p>Q-learning is used to update the value function.  FAL is used to learn the task routing policy.</p>
  </div>
  <div class="page">
    <p>Dual Exploration</p>
    <p>ii jj</p>
    <p>ss dd</p>
    <p>r(sx, i)</p>
    <p>r(sx, j) Qi(sx, j)</p>
    <p>Qj(sx, i) x</p>
    <p>Forward Exploration Backward Exploration</p>
    <p>[Kumer and Miikkulainen, 1999]</p>
  </div>
  <div class="page">
    <p>Experiments: Compared Approaches</p>
    <p>Distributed approaches:</p>
    <p>A centralized approach  using best-first algorithm with a global view  ignore the communication delay  sometimes generating optimal allocation</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>Cluster network with heterogeneous clusters and heterogeneous computing nodes (total 1024 nodes)</p>
    <p>Four types of tasks: ordinary, IO-intensive, computeintensive, and demanding</p>
    <p>Two task arrival patterns: light load and heavy load</p>
  </div>
  <div class="page">
    <p>Experimental Result: Light Load</p>
  </div>
  <div class="page">
    <p>Experimental Result: Heavy Load</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>This paper presents a multi-agent learning (MAL) approach to address resource sharing in cluster networks for building large computing infrastructure.</p>
    <p>Experimental results are encouraging.  This work plausibly suggests that MAL may be</p>
    <p>a promising approach to online optimization problems in distributed systems.</p>
  </div>
</Presentation>
