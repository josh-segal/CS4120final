<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Accelerating Large Scale Deep Learning Inference through DeepCPU at Microsoft</p>
    <p>Minjia Zhang, Samyam Rajbandari, Wenhan Wang, Elton Zheng, Olatunji Ruwase, Jeff Rasley, Jason Li, Junhua Wang, Yuxiong He</p>
    <p>Microsoft AI and Research</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>DeepCPU, the fastest deep learning serving library for recurrent neural networks (RNNs) on CPUs</p>
    <p>SLT (Scenario, Library, Technique) driven methodology</p>
    <p>10x lower latency and cost than existing framework</p>
    <p>Ship DL models with great latency/cost reduction in Microsoft</p>
  </div>
  <div class="page">
    <p>Deep Learning Serving Challenges</p>
    <p>Long serving latency blocks deployment</p>
    <p>Support advance models while meeting latency SLA and saving cost</p>
    <p>DL Scenarios Original Latency Latency Target</p>
    <p>MRC Model A ~100ms &lt; 10ms</p>
    <p>MRC Model B ~107ms &lt; 10ms</p>
    <p>Ranking Model 10ms for [query, 1 passage]</p>
    <p>x 150 passages &lt; 5ms</p>
    <p>Query rewriting ~51ms &lt; 5ms</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Improve existing general-purpose DL frameworks?</p>
    <p>Customized optimization with effective reuse</p>
    <p>Co-development of Scenario, Library, and Technique (SLT)</p>
    <p>Scenario  Apply customized optimization, striking for best performance  Think out of box, not limited by existing framework</p>
    <p>Library  Collection of generic building blocks that speed up customized optimization  Framework independent -- can benefit multiple DL frameworks</p>
    <p>Technique  One technique could benefit multiple library components and many scenarios  Parallelism, scheduling, and locality optimization on CPU at no cost in accuracy</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Real-World Scenarios with DeepCPU-Powered RNN-Based Models</p>
    <p>Library Features</p>
    <p>Optimization Techniques</p>
    <p>How is DeepCPU Utilized?</p>
  </div>
  <div class="page">
    <p>Scenario 1: Question Answering</p>
    <p>Bidirectional Attention Flow Model (BiDAF)</p>
  </div>
  <div class="page">
    <p>Performance Critical Factors Implications</p>
    <p>Limited Parallelism (small batch size )</p>
    <p>Poor Scalability</p>
    <p>Poor Data Locality Poor Scalability and Performance due to reading data from slow memory</p>
    <p>RNN Performance Bottleneck</p>
    <p>W1W1W1</p>
    <p>What is Atom?</p>
    <p>Step 1 Step 2 Step 3</p>
    <p>W2 W2 W2</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>Optimization Results</p>
    <p>Bidirectional Attention Flow Model (BiDAF)</p>
    <p>Our Optimization</p>
    <p>DeepCPU implementation for BiDAF</p>
    <p>Same accuracy Latency: 107ms to 4.1ms (&gt;20 times speedup) Non-shippable -&gt; Shippable</p>
  </div>
  <div class="page">
    <p>Scenario 2: Text Similarity Ranking</p>
    <p>Generate text similarities using deep learning model</p>
    <p>Model: word embedding + encoding with GRUs + conv + max-pool</p>
    <p>Latency SLA: 5ms for &lt;query, top 150 passages&gt;</p>
    <p>Tensorflow serving latency</p>
    <p>single &lt;query, passage&gt; pair: 10ms</p>
    <p>&lt;query, 150 passages&gt;: fan-out to 150 machines</p>
    <p>Our optimizations</p>
    <p>&lt;query, 150 passages&gt;: 5ms, one machine (&gt;100x throughput gain)</p>
    <p>Reduce thousands of machines and millions of infrastructure costs</p>
    <p>non-shippable</p>
    <p>shippable save machines</p>
  </div>
  <div class="page">
    <p>Optimization Results</p>
    <p>Scenarios Original Latency Latency</p>
    <p>Target Optimized Latency</p>
    <p>Latency</p>
    <p>reduction</p>
    <p>Throughput</p>
    <p>improvement</p>
    <p>MRC Model A ~100ms 10ms 9ms &gt;10X &gt; 10X</p>
    <p>MRC Model B ~107ms 10ms 4.1ms &gt;20X &gt; 50X</p>
    <p>Neural Ranking</p>
    <p>Model A</p>
    <p>[query, 1 doc]</p>
    <p>x 33 docs</p>
    <p>&lt;6ms for [query,</p>
    <p>&gt;6X &gt; 30X</p>
    <p>Neural Ranking</p>
    <p>Model B</p>
    <p>[query, 1 passage]</p>
    <p>x 150 passages</p>
    <p>&lt;1ms for [query,</p>
    <p>&lt;5ms for [query,</p>
    <p>&gt;10X &gt; 100X</p>
    <p>Query rewriting 51ms 5ms 4ms &gt;10X &gt; 3X</p>
  </div>
  <div class="page">
    <p>Optimization Results Continued</p>
    <p>Scenarios Original Latency Latency</p>
    <p>Target Optimized Latency</p>
    <p>Latency</p>
    <p>reduction</p>
    <p>Throughput</p>
    <p>improvement</p>
    <p>Encoder Model A ~29ms 10ms 5.4ms 5X 5X</p>
    <p>MRC Model C ~45ms for 1</p>
    <p>[query, passage] 10ms</p>
    <p>passage];</p>
    <p>&lt;8.5ms for 20 [query,</p>
    <p>passage]</p>
    <p>Query tagging 9~16ms 3ms 0.95ms 10X &gt; 10X</p>
    <p>Encoder Model B ~25ms for [query,</p>
    <p>size of 33</p>
    <p>Classifier A 60ms 3ms 3ms 20X 20X</p>
    <p>Classifier B 8ms 3ms 1ms 8X 8X</p>
    <p>Latency: 5x  20x faster, from impossible to ship to well fitting SLA Capacity: serving 5x  20x bigger models under the same latency SLA</p>
    <p>Throughput: 5x  100x higher Cost: reduced to 1% - 20% of original cost</p>
  </div>
  <div class="page">
    <p>DeepCPU: Fast DL Serving Library on CPUs</p>
    <p>RNN family  GRU cell and GRU sequence  LSTM cell and LSTM sequence  Bidirectional and stacked RNN networks</p>
    <p>Fundamental building blocks and common DL Layers  Matrix multiplication kernels, activation functions  high-way network, max pool layer, MLP layer</p>
    <p>DL layers for MRC and conversation models  Variety of attention layers  seq2seq decoding with beam search</p>
  </div>
  <div class="page">
    <p>Optimization Techniques</p>
    <p>Optimization Our optimized library on CPU</p>
    <p>Matrix computation Cache-aware matrix kernels + Intel MKL</p>
    <p>Activation functions Vectorization + parallelization</p>
    <p>Operation Fusing Fuse operations to reduce data read/write</p>
    <p>Affinity Bind app thread to hardware thread</p>
    <p>cross-socket awareness</p>
    <p>Locality Private-cache-aware partitioning + weight-centric streamlining</p>
    <p>Parallelism Judicious parallelism considering workload, parallelism efficiency and load</p>
    <p>balancing</p>
    <p>Task Scheduling Priority over critical path</p>
    <p>Global optimization of DAG</p>
  </div>
  <div class="page">
    <p>How is DeepCPU Utilized?</p>
    <p>Customized Model Client</p>
    <p>Performance Hyperparameter</p>
    <p>Tuning</p>
    <p>Customized Serving Runtime</p>
    <p>Even Faster Latency</p>
    <p>Optimized DeepCPU Operators</p>
    <p>Replace Nodes in Model Graph</p>
    <p>Existing Framework</p>
    <p>Serving Engine Faster Latency</p>
    <p>Optimization Techniques</p>
    <p>DeepCPU Library</p>
    <p>Framework Integration (TensorFlow, WinML, ONNX)</p>
    <p>Customized Optimization</p>
    <p>More Development Work</p>
    <p>Less Development Work</p>
    <p>Critical Scenario Owners</p>
    <p>Framework Users</p>
  </div>
  <div class="page">
    <p>DeepCPU: Make DL Serving Faster &amp; More Efficient</p>
    <p>Scenarios</p>
    <p>Question Answering</p>
    <p>Machine Reading Comprehension</p>
    <p>Ranking</p>
    <p>Query Rewriting</p>
    <p>Query Tagging</p>
    <p>Models</p>
    <p>GRU/LSTM</p>
    <p>Stacked RNN</p>
    <p>Seq2Seq</p>
    <p>Attention layers</p>
    <p>Convolution</p>
    <p>Highway network</p>
    <p>MLP</p>
    <p>Usage</p>
    <p>Customized optimization</p>
    <p>Framework integration</p>
    <p>Impact</p>
    <p>10x faster</p>
    <p>10x larger models</p>
    <p>10x - 100x more throughput</p>
    <p>10x - 100x less cost</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Questions?</p>
  </div>
</Presentation>
