<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Improving Robustness of ML Classifiers</p>
    <p>against Realizable Evasion Attacks Using</p>
    <p>Conserved Features</p>
    <p>Liang Tong1, Bo Li2, Chen Hajaj3, Chaowei Xiao4, Ning Zhang1, Yevgeniy</p>
    <p>Vorobeychik1</p>
    <p>August 13, 2019</p>
  </div>
  <div class="page">
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>Machine Learning in Security</p>
    <p>Detection is a fundamental problem in cybersecurity.  e.g. Malware, intrusion, spam, phish</p>
    <p>Natural to use Machine Learning (ML) for these applications.</p>
  </div>
  <div class="page">
    <p>Adversarial Evasion Problem</p>
    <p>ML-based techniques are often susceptible to adversarial examples at test time.</p>
    <p>Attackers can manipulate malicious samples to look benign and fool a classifier.</p>
  </div>
  <div class="page">
    <p>Realizable Attacks</p>
    <p>Modify the actual entity.  e.g., produce a valid PDF file or executable file.  Features are subsequently extracted for ML.</p>
    <p>Have actual malicious effect (e.g., verified by a sandbox) but the feature vector is classifed as benign.</p>
  </div>
  <div class="page">
    <p>Feature Space Attacks</p>
    <p>An abstraction of realizable attacks.  Directly work on features instead of entities. May not be realizable.  Use an `p norm to measure the cost of modifying original examples.</p>
  </div>
  <div class="page">
    <p>Robust ML</p>
    <p>Essentially most approaches for robust ML leverage feature-space attack models. e.g., robust optimization, adversarial training.</p>
  </div>
  <div class="page">
    <p>Motivation: Is Robust ML really robust?</p>
    <p>Suppose we learn a Robust ML against a feature space attack model. Is it robust against realizable attacks?</p>
  </div>
  <div class="page">
    <p>Contribution</p>
    <p>Model Validation: evalute the robustness of Robust ML against realizable attacks.</p>
    <p>Robust ML using feature-space models may fail to provide adequate robustness against realizable attacks.</p>
    <p>Model Refinement: fix the feature-space attack models by using conserved features.</p>
    <p>Generalized Robustness: explore to which extent ML robustness can be generalized to multiple distinct realizable attacks.</p>
  </div>
  <div class="page">
    <p>Methodology and Experiments</p>
  </div>
  <div class="page">
    <p>A Case Study on PDF Malware Detectors</p>
    <p>Content-based detectors: use features based on content information (e.g. size of a PDF file)</p>
    <p>PDFRate-R: 135 normalized features (real-valued)  PDFRate-B: 135 binarized features</p>
    <p>Structure-based detectors: use binary features based on existence of a collection of object paths</p>
    <p>SL2013: 6,087 paths  Hidost: 961 paths</p>
  </div>
  <div class="page">
    <p>Attacks and Defense</p>
    <p>Realizable attack: EvadeML (Xu et al., NDSS).  Automatically evades a PDF classifier by using genetic programming.  Works on both structure- and content-based detectors.</p>
    <p>Feature-space attack model: multi-objective optimization.  The modified feature vector is predicted as benign as possible.  The modification cost (measured with an `p norm) is minimized.</p>
    <p>General defense: iterative retraining.  Iteratively uses an attack to produce adversarial examples, then adds</p>
    <p>them into training data and retrain.</p>
    <p>Works for both realizable and feature-space attacks.</p>
  </div>
  <div class="page">
    <p>Model Validation: Framework</p>
    <p>Evaluation Metrics  Adversarial data: robustness = 1 - success rate of EvadeML  Clean data: ROC (receiver operating characteristic) curve.</p>
  </div>
  <div class="page">
    <p>Model Validation: Real-valued and Content-based</p>
    <p>no defense</p>
    <p>using EvadeML</p>
    <p>Robust ML</p>
    <p>as io</p>
    <p>n Ro</p>
    <p>bu st</p>
    <p>ne ss</p>
    <p>gap</p>
    <p>Figure 1: Left: evasion robustness. Right: ROC curve.</p>
    <p>Original: 2% evasion robustness.  After defense: 100% evasion robustness.</p>
    <p>Robust ML with feature-space model works but degrades performance on clean data!</p>
  </div>
  <div class="page">
    <p>Model Validation: Structure-based</p>
    <p>gap</p>
    <p>gap</p>
    <p>Figure 2: Left: evasion robustness. Right: ROC curve.</p>
    <p>Original: 2% evasion robustness.  Defense using EvadeML: 98% evasion robustness.  Feature-space Robust ML: 70% evasion robustness and degradation</p>
    <p>on clean data.</p>
    <p>Robust ML using feature-space models is not perfect. Can we fix it by creating a minimal anchoring?</p>
  </div>
  <div class="page">
    <p>Model Refinement: Conserved Features</p>
    <p>Conserved features: a subset of features which compromise malicious functionality if they are removed.</p>
    <p>Paths to objects which contain malicious codes.  Paths objects which break the PDF if they are removed.</p>
    <p>Identifying conserved features: systematically manipulating each object in a PDF file and checking the maliciousness.</p>
    <p>Existence of conserved features: we identified 48 conserved features for each detector.</p>
    <p>Feature-space attacks with conserved features: conserved features are preserved in evasive instances.</p>
  </div>
  <div class="page">
    <p>Model Refinement: Binarized Content-based</p>
    <p>using EvadeML</p>
    <p>Robust ML</p>
    <p>Robust ML with CF</p>
    <p>as io</p>
    <p>n Ro</p>
    <p>bu st</p>
    <p>ne ss</p>
    <p>impr ovement</p>
    <p>Figure 3: Left: evasion robustness. Right: ROC curve.</p>
    <p>Defense using EvadeML: 100% evasion robustness.  Feature-space Robust ML: 100% evasion robustness and</p>
    <p>performance degradation on clean data.</p>
    <p>Feature-space Robust ML with conserved features: 100% evasion robustness and improves ROC.</p>
  </div>
  <div class="page">
    <p>Model Refinement: Structure-based</p>
    <p>impr ovement</p>
    <p>impr ovement</p>
    <p>Figure 4: Left: evasion robustness. Right: ROC curve.</p>
    <p>Defense using EvadeML: 98% evasion robustness.  Feature-space Robust ML: 70% evasion robustness and performance</p>
    <p>degradation on clean data.</p>
    <p>Feature-space Robust ML with conserved features: 100% evasion robustness and significant improvement on clean data.</p>
  </div>
  <div class="page">
    <p>Generalized Robustness</p>
    <p>So far, evaluation and baseline defense used EvadeML.</p>
    <p>Is ML hardened with EvadeML effective against other realizable attacks?</p>
    <p>Is ML hardened with a feature-space model of attacks (using conserved features) generally effective against realizable attacks?</p>
  </div>
  <div class="page">
    <p>Generalized Robustness: Mimicry+</p>
    <p>Realizable attack on content-based classifiers</p>
    <p>An improvement of Mimicry Attack (Srndic &amp; Laskov, Oakland).</p>
    <p>using EvadML</p>
    <p>Robust ML</p>
    <p>Ev as</p>
    <p>io n</p>
    <p>Ro bu</p>
    <p>st ne</p>
    <p>ss</p>
    <p>using EvadeML</p>
    <p>Robust ML</p>
    <p>Robust ML with CF</p>
    <p>Ev as</p>
    <p>io n</p>
    <p>Ro bu</p>
    <p>st ne</p>
    <p>ss 0.84</p>
    <p>Figure 5: Left: real-valued. Right: binarized.</p>
    <p>Hardening against EvadeML may fail to be robust to Mimicry+.  Robust ML (w/o conserved features) is still robust to Mimicry+.</p>
  </div>
  <div class="page">
    <p>Generalized Robustness: Reverse Mimicry</p>
    <p>Realizable attack that requires zero knowledge of target classifier</p>
    <p>(Maiorca et al., ASIACCS)</p>
    <p>using EvadeML</p>
    <p>Robust ML</p>
    <p>Ev as</p>
    <p>io n</p>
    <p>Ro bu</p>
    <p>st ne</p>
    <p>ss</p>
    <p>using EvadeML</p>
    <p>Robust ML</p>
    <p>Robust ML with CF</p>
    <p>Ev as</p>
    <p>io n</p>
    <p>Ro bu</p>
    <p>st ne</p>
    <p>ss</p>
    <p>using EvadeML</p>
    <p>Robust ML Robust ML with CF</p>
    <p>Ev as</p>
    <p>io n</p>
    <p>Ro bu</p>
    <p>st ne</p>
    <p>ss</p>
    <p>Figure 6: Left: real-valued content-based. Middle: binarized content-based.</p>
    <p>Right: structure-based</p>
    <p>Hardening against EvadeML may fail to be robust to Reverse Mimicry.</p>
    <p>Robust ML w/ conserved features is still robust to Reverse Mimicry. 18</p>
  </div>
  <div class="page">
    <p>Generalized Robustness: Custom Attack</p>
    <p>Exploitation of a feature extraction bug of the content-based</p>
    <p>classifiers.</p>
    <p>using EvadeML</p>
    <p>Robust ML</p>
    <p>Ev as</p>
    <p>io n</p>
    <p>Ro bu</p>
    <p>st ne</p>
    <p>ss</p>
    <p>Figure 7: Left: real-valued. Right: binarized.</p>
    <p>Defeats detector hardened using EvadeML.  Defeats conserved features of binarized content-based detector.  All feature-space approaches remain robust.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Robust ML methods which assume direct modification of features and measures cost of adversarial noise as norm are sometimes, but</p>
    <p>not always fully effective against real attacks.</p>
    <p>We can fix the model by identifying and using conserved features to anchor the abstract attack model in the problem domain.</p>
    <p>Robust ML using feature space models (after the fix) exhibit more general robustness than methods hardened only against a particular</p>
    <p>(strong, adaptive) realizable attack.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
