<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning</p>
    <p>Jinyuan Jia, Neil Zhenqiang Gong Department of Electrical and Computer Engineering</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Motivation</p>
    <p>Algorithm</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Motivation</p>
    <p>Algorithm</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Attribute Inference Attacks</p>
    <p>Input: Users public data</p>
    <p>Output: Users private attributes</p>
    <p>E.g. In social media, attacker can use machine learning classifier to infer users private attributes.</p>
    <p>qCambridge Analytica</p>
    <p>Private attributes and public data are statistically correlated</p>
    <p>Machine learning classifier</p>
    <p>Public data Private attributes</p>
    <p>(Public data, Private attribute)</p>
  </div>
  <div class="page">
    <p>Attribute Inference Attacks are Pervasive Recommender systems</p>
    <p>qPublic: Rating scores qPrivate: Gender</p>
    <p>Mobile apps qPublic: Users smartphones aggregate power consumption qPrivate: Locations</p>
    <p>Website fingerprinting qPublic: Network traffic qPrivate: Websites</p>
    <p>Side-channel attacks qPublic: Power consumption, processing time qPrivate: Cryptographic keys</p>
  </div>
  <div class="page">
    <p>Existing Defenses Game-theoretic methods:</p>
    <p>q Pros: Defend against optimal inference attacks q Cons: Computationally intractable</p>
    <p>Heuristic methods:</p>
    <p>q Pros: Computationally tractable q Cons:</p>
    <p>q Large utility loss q Direct access to users private attribute value</p>
    <p>Local Differential Privacy (LDP)</p>
    <p>q Pros: Rigorous privacy guarantee q Cons: Large utility loss 6</p>
  </div>
  <div class="page">
    <p>Our Defense: AttriGuard</p>
    <p>Computationally tractable</p>
    <p>Small utility loss</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Motivation</p>
    <p>Algorithm</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>Policy A: Modify_Exist  Policy B: Add_New  Policy C: Modify_Add</p>
    <p>True public data DefenderUser Attacker</p>
    <p>Noisy public data Private attributes</p>
    <p>policy</p>
  </div>
  <div class="page">
    <p>Challenges The defender doesnt know the attackers classifier</p>
    <p>qThe defender itself learn a classifier</p>
    <p>qTransferability: similar classification boundaries</p>
    <p>Defender has no access to users true private attribute value</p>
    <p>qFind a mechanism to add random noise</p>
    <p>qOutput distribution of defenders classifier approaches certain target probability distribution that defender desires</p>
  </div>
  <div class="page">
    <p>Metric</p>
    <p>Difference between output distribution of defenders classifier and target probability distribution</p>
    <p>qKL-divergence:</p>
    <p>Utility loss: q norm:</p>
    <p>q p</p>
    <p>( || ) log iii i</p>
    <p>p KL p</p>
    <p>q =p q</p>
    <p>noise vectorusers true public data vector</p>
    <p>users noisy public data vector</p>
  </div>
  <div class="page">
    <p>Attribute-inference-attack Defense Problem</p>
    <p>Input: q noise-type-policy q utility-loss-budget q target probability distribution q defenders classifier q users true public data.</p>
    <p>Output: Mechanism that adds random noise q is the conditional probability that defender will add noise to users</p>
    <p>true public data qSample from to add noise</p>
    <p>M *( | )M r x r</p>
    <p>x</p>
    <p>M</p>
  </div>
  <div class="page">
    <p>Attribute-inference-attack Defense Problem</p>
    <p>:output distribution of defenders classifier subject to q</p>
    <p>| ( ) Pr( ( ) ) ( | )i</p>
    <p>C i q C i M</p>
    <p>+ =</p>
    <p>= + = = r x r</p>
    <p>x r r x</p>
  </div>
  <div class="page">
    <p>Overview of AttriGuard Challenge to solve the optimization problem:</p>
    <p>qThe probabilistic mapping is exponential to the dimensionality of</p>
    <p>qCategorize noise space into groups to solve the challenge</p>
    <p>ix+r 1i+x+r</p>
    <p>mapping</p>
    <p>Class 1</p>
    <p>Class 2</p>
    <p>Class m</p>
    <p>Output of</p>
    <p>Output of</p>
    <p>Output of</p>
  </div>
  <div class="page">
    <p>Two-Phase Framework</p>
    <p>Phase I: For each noise group, find a minimum noise as representative noise</p>
    <p>Phase II: Simplify the mechanism to be a probability distribution over representative noise</p>
  </div>
  <div class="page">
    <p>Phase I</p>
    <p>Find minimum noise for each group such that defenders classifier outputs class given noisy public data input</p>
    <p>subject to</p>
  </div>
  <div class="page">
    <p>Phase I</p>
    <p>The optimization problem can be viewed as evasion attacks to the defenders classifier</p>
    <p>Existing evasion attacks are insufficient  Not consider different noise-type-policy</p>
    <p>We propose PANDA based on Jacobian-based Saliency Map Attack (JSMA)</p>
    <p>q Consider noise-type-policy</p>
    <p>q Some entries in users public data can be decreased while other entries can be increased in PANDA while all entries can either be increased or decreased in JSMA</p>
  </div>
  <div class="page">
    <p>Phase II</p>
    <p>Transform original optimization problem into following convex optimization problem:</p>
    <p>subject to is a probability distribution,</p>
    <p>and denote the probability select noise</p>
    <p>M iM</p>
    <p>ir</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Motivation</p>
    <p>Algorithm</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Evaluation Dataset A review dataset from Gong and Liu (USENIX Security16)</p>
    <p>Attributes considered: 25 cities</p>
    <p>Basic statistics</p>
    <p>Training and Testing: qTraining: 90% of users qTesting: the remaining users</p>
    <p>#Users #apps #ave. apps</p>
  </div>
  <div class="page">
    <p>Attribute Inference Attacks</p>
    <p>Defense unaware attack q Baseline attack (BA-A) q Logistic regression (LR-A) q Random forest (RF-A) q Neural network (NN-A)</p>
    <p>Robust classifier q Adversarial training (AT-A) q Defensive distillation (DD-A) q Region-based classification (RC-A)</p>
    <p>Detect noise q Detect noise via low-rank approximation (LRA-A)</p>
  </div>
  <div class="page">
    <p>Inference Accuracy without Defense</p>
  </div>
  <div class="page">
    <p>Defenders Classifier</p>
    <p>Neural Network (NN-D)</p>
    <p>qUse a different neural network architecture from attacker</p>
    <p>Logistic Regression (LR-D)</p>
  </div>
  <div class="page">
    <p>Comparing PANDA with Existing Evasion Attack Methods</p>
    <p>Fast Gradient Sign Method (FGSM)</p>
    <p>Jacobian-based Saliency Map Attack (JSMA)</p>
    <p>Carlini and Wagner Attack (CW)</p>
  </div>
  <div class="page">
    <p>Average Noise</p>
    <p>FGSM adds orders of magnitude larger noise PANDA adds smaller noise than JSMA PANDA is comparable to CW</p>
  </div>
  <div class="page">
    <p>Success Rate and Running Time</p>
    <p>PANDA is slightly faster than JSMA PANDA is around 800 times and 4,000 times faster than CW for the LR-D and NN-D, respectively</p>
  </div>
  <div class="page">
    <p>AttriGuard is Effective</p>
  </div>
  <div class="page">
    <p>Impact of the Target Probability Distribution</p>
    <p>Target probability distribution outperforms :Estimated target probability distribution using training dataset :Uniform probability distribution</p>
  </div>
  <div class="page">
    <p>Impact of the Defenders Classifier</p>
    <p>AttriGuard is better when attacker and defender use the same classifier</p>
    <p>Attackers classifer: Neural Network(NN-A)</p>
  </div>
  <div class="page">
    <p>Impact of Different noise-type-policies</p>
    <p>Modify_Add outperforms Add_New, which outperforms Modify_Exist</p>
  </div>
  <div class="page">
    <p>Comparing AttriGuard with Existing Defenses</p>
    <p>Correlation-based Methods qBlurMe qChiSquare</p>
    <p>Approximate game-theoretic method qQuantization Probabilistic Mapping(QPM)</p>
    <p>Local Differential Privacy qLDP-SH</p>
  </div>
  <div class="page">
    <p>Comparing AttriGuard with Existing Defenses</p>
    <p>AttriGuard incurs smaller utility-loss 32</p>
  </div>
  <div class="page">
    <p>Comparing AttriGuard with Existing Defenses</p>
    <p>AttriGuard incurs smaller relative recommendation precision loss</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Motivation</p>
    <p>Algorithm</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>AttriGuard can defend against attribute inference attacks with a small utility loss</p>
    <p>Evasion attacks/Adversarial examples can be used as defensive techniques for privacy protection</p>
    <p>AttriGuard significantly outperforms existing defenses</p>
  </div>
</Presentation>
