<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Case for Unifying Data Loading in Machine Learning Clusters</p>
    <p>Aarati Kakaraparthy*, Abhay Venkatesh*, Amar Phanishayee, Shivaram Venkataraman* University of Wisconsin, Madison* &amp; Microsoft Research</p>
  </div>
  <div class="page">
    <p>Machine Learning Frameworks  Two Parts Machine Learning (ML) frameworks like PyTorch, Tensorflow, etc. typically consist of two sub-systems:</p>
    <p>Data Loading, to generate batches of data, and</p>
    <p>Training, to compute gradients and update the model</p>
    <p>Fetch Data</p>
    <p>Preprocess</p>
    <p>Data Loading</p>
    <p>Compute Gradients</p>
    <p>Update Model</p>
    <p>Batches</p>
    <p>Training</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Data Loading can be multi-threaded or multi-process  Runs asynchronously from training  Requires random accesses to generate batches</p>
    <p>Randomness leads to faster convergence of the training process  Undesirable for HDDs, SSDs</p>
    <p>Data Loading for Machine Learning</p>
    <p>Fetch Data</p>
    <p>Preprocess</p>
    <p>Data Loading</p>
    <p>Compute Gradients</p>
    <p>Update Model</p>
    <p>Batches</p>
    <p>Training</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>The Case for Unifying Data Loading</p>
    <p>Typically, each ML job performs data loading independently</p>
    <p>Inefficient in the cloud  Multiple concurrent jobs could be accessing the same dataset</p>
    <p>We propose unifying data loading in a single system: OneAccess</p>
    <p>File System</p>
    <p>Job 2 ...</p>
    <p>File System</p>
    <p>Job 1 Job 2 ...</p>
    <p>OneAccess</p>
    <p>vs. Data Loading</p>
    <p>Fetch Pre</p>
    <p>process</p>
    <p>Job 1</p>
    <p>Job 2</p>
    <p>Data Loading</p>
    <p>Fetch Pre</p>
    <p>process</p>
    <p>Job 2</p>
    <p>Fetch Pre</p>
    <p>process Unified Data Loading</p>
  </div>
  <div class="page">
    <p>We study Hyperparameter Tuning at Microsoft for one month  Demonstrates the potential of a unified data loading system in the cloud</p>
    <p>What are Hyperparameters?  Hyperparameters determine the configuration of a model</p>
    <p>The number of hidden layers in a neural network, learning rate for SGD, etc.</p>
    <p>What is Hyperparameter Tuning?  Training multiple configurations of a model with different hyperparameters</p>
    <p>Choose the best configuration</p>
    <p>Case Study: Potential of Unified Data Loading</p>
  </div>
  <div class="page">
    <p>Hyperparameter Tuning using HyperDrive  Each launched experiment consists of multiple jobs</p>
    <p>Each job trains a different configuration of the model on the same dataset</p>
    <p>On an average, each experiment has 35 jobs</p>
    <p>Case Study: Hyperparameter Tuning</p>
  </div>
  <div class="page">
    <p>Case Study: Concurrent Tuning Jobs  Not all of the jobs of an experiment are launched concurrently</p>
    <p>We observe that HyperDrive frequently launches 5, 10, or 20 jobs</p>
    <p>simultaneously, and 9 concurrent jobs on an average</p>
    <p>I/O time to fetch data can be reduced by (1-) = 89% if concurrent</p>
    <p>jobs share batches</p>
  </div>
  <div class="page">
    <p>Unified Data Loading Using OneAccess  Two types of processes for data</p>
    <p>loading:  Sample Creator(s), for generating</p>
    <p>reservoir samples</p>
    <p>Batch Creator, for generating batches for training</p>
    <p>More details in the paper</p>
    <p>The distinguishing aspects of OneAccess are:</p>
    <p>Unified Data Loading</p>
    <p>Sequential Accesses through Reservoir Sampling</p>
    <p>Dataset 1,000,000</p>
  </div>
  <div class="page">
    <p>Sequential Accesses with Reservoir Sampling</p>
    <p>D1 D2 D3 D4 D5 D6 ...</p>
    <p>Random Sampling without replacement</p>
    <p>D5 D2 D6 D1 ...</p>
    <p>vs. Reservoir Sampling</p>
    <p>Random</p>
    <p>Sample</p>
    <p>D1 D2 D3 D4 D5 D6 ...</p>
    <p>D1 D2 D5 D6 ... Random Sample</p>
    <p>(Reservoir)</p>
    <p>? ? ? ? ? ?Is part of current</p>
    <p>sample?</p>
  </div>
  <div class="page">
    <p>Evaluation: The Benefit of Unified Data Loading  We compare the</p>
    <p>conventional approach of separate data loading, to unified data loading with OneAccess.</p>
    <p>As fetching data is amortized between the two jobs, we find that the total I/O time reduces by 47.3%.</p>
    <p>File System - Cifar10</p>
    <p>Job 1 Job 2</p>
    <p>OneAccess OneAccess</p>
    <p>File System - Cifar10</p>
    <p>Job 1 Job 2</p>
    <p>OneAccess vs.</p>
    <p>Training Jobs/ Configuration</p>
    <p>Total IOPS (1 epoch)</p>
    <p>I/O Time (seconds)</p>
    <p>Job 1 Job 2</p>
    <p>Separate Data Access 50k  2 14.3 15.1</p>
    <p>Unified Data Access 50k 15.4</p>
    <p>SSD (Samsung 960 EVO) SSD (Samsung 960 EVO)</p>
  </div>
  <div class="page">
    <p>Evaluation: The Benefit of Sequential Access  We compare the</p>
    <p>batch creation time of PyTorch against OneAccess for the MS-COCO dataset during one epoch.</p>
    <p>We find that OneAccess is 3.6x and 1.9x faster compared to PyTorch with 1 and 2 workers respectively.</p>
    <p>File System - MS COCO File System - MS COCO</p>
    <p>OneAccess</p>
    <p>vs.</p>
    <p>PyTorch</p>
    <p>Framework Total Time (min)</p>
    <p>PyTorch (1 I/O worker) 23</p>
    <p>PyTorch (2 I/O workers) 12</p>
    <p>PyTorch (4 I/O workers) 6.8</p>
    <p>OneAccess (1 Sample Creator and</p>
    <p>SSD (Samsung 960 EVO) SSD (Samsung 960 EVO)</p>
    <p>W W ... SC BC</p>
  </div>
  <div class="page">
    <p>Open Challenges  Unifying data pre-processing across frameworks</p>
    <p>Sharing pre-processed data across frameworks will require a standard format</p>
    <p>Synchronizing data access across jobs  The training speed of different training jobs can be different</p>
    <p>Jobs can start at different times</p>
    <p>Jobs can have different batch sizes</p>
    <p>Importance of locality and parallelism  It remains to be seen how effective is reservoir sampling across machines in a cluster</p>
    <p>Extend reservoir sampling to use multiple workers</p>
  </div>
  <div class="page">
    <p>We study data loading in popular ML frameworks</p>
    <p>Make a case for unifying data loading for machine learning in clusters</p>
    <p>We present a case study on Hyperparameter Tuning to demonstrate</p>
    <p>the potential of unified data loading in the cloud</p>
    <p>We develop a prototype system called OneAccess, which has  Unified data loading</p>
    <p>Sequential accesses through reservoir sampling</p>
    <p>Conclusions</p>
    <p>Thank You! Aarati Kakaraparthy | aaratik@cs.wisc.edu</p>
  </div>
  <div class="page">
    <p>Machine Learning Frameworks  Two Parts Machine Learning (ML) frameworks like PyTorch, Tensorflow, etc. typically consist of two sub-systems:</p>
    <p>Data Loading, to generate batches of data, and</p>
    <p>Training, to compute gradients and update the model</p>
    <p>Fetch Data</p>
    <p>Preprocess</p>
    <p>Data Loading</p>
    <p>Batches</p>
  </div>
  <div class="page">
    <p>To demonstrate the cost of data loading, we train Resnet-18 on</p>
    <p>MS-COCO Dataset using PyTorch on an Nvidia GTX 1050 GPU</p>
    <p>Increasing the number of workers alleviates the bottleneck of data</p>
    <p>loading</p>
    <p>The Cost of Data Loading</p>
    <p>Number of PyTorch Workers Training Time Data Loading Time Total Time</p>
  </div>
  <div class="page">
    <p>The Case for Unifying Data Loading  Typically, each ML job performs data loading independently</p>
    <p>Inefficient in the cloud  Multiple concurrent jobs could be accessing the same dataset</p>
    <p>We propose unifying data access in a single system: OneAccess</p>
    <p>File System</p>
    <p>Job 1 Job 2</p>
    <p>Data Loading</p>
    <p>Data Loading</p>
    <p>...</p>
    <p>File System</p>
    <p>Job 1 Job 2 ...</p>
    <p>Unified Data Loading with OneAccessvs.</p>
  </div>
  <div class="page">
    <p>Case Study: Hyperparameter Tuning Jobs  The potential benefit of unified data loading</p>
    <p>If concurrent jobs reuse batches, the I/O cost of fetching data can be reduced by (1</p>
    <p>) = 89%</p>
    <p>If pre-processed data is persisted, the pre-processing cost can be reduced by</p>
    <p>(1-1/35) = 97%</p>
  </div>
</Presentation>
