<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Tapani Raiko and Harri Valpola</p>
    <p>Oscillatory Neural Network for Image Segmentation with Biased Competition</p>
    <p>for Attention</p>
    <p>School of Science and Technology</p>
    <p>Aalto University</p>
    <p>(formerly Helsinki University of Technology)</p>
  </div>
  <div class="page">
    <p>Tapani Raiko and Harri Valpola</p>
    <p>Oscillatory Neural Network for Image Segmentation with Biased Competition</p>
    <p>for Attention</p>
    <p>ZenRobotics Ltd.</p>
    <p>www.zenrobotics.com</p>
  </div>
  <div class="page">
    <p>Intact cerebellum</p>
    <p>and sober</p>
    <p>Cerebellar lesion or</p>
    <p>drunk</p>
    <p>What: Figuring out how the brain works. How: Building brains for robots = system-level modelling,</p>
    <p>implementing a whole vertebrate/mammalian brain. Why: Because we can.</p>
    <p>Computational neuroscience group</p>
  </div>
  <div class="page">
    <p>Our main strengths in research</p>
    <p>Neuroscience  machine learning / AI / neural nets  robotics in unstructured environments</p>
    <p>Cognitive architecture: the organisation of the whole brain</p>
    <p>Cerebral cortex  Basal ganglia  Cerebellum  Hippocampus</p>
  </div>
  <div class="page">
    <p>ZenRobotics Ltd. Sorting waste with intelligent robots</p>
  </div>
  <div class="page">
    <p>ZenRobotics Ltd. Waste recycling lab</p>
  </div>
  <div class="page">
    <p>ZenRobotics Ltd. Robots viewpoint</p>
  </div>
  <div class="page">
    <p>Cerebral cortex</p>
    <p>Modelling the world (and yourself as part of it)  Forward modelling: prediction and simulation  Inverse modelling: figuring out which actions</p>
    <p>lead to desired consequences; planning</p>
  </div>
  <div class="page">
    <p>Different types of inputs and outputs</p>
    <p>Primary input usually from bottom-up (from the senses)  Numerous feedback connections (order of 10 x)</p>
  </div>
  <div class="page">
    <p>From the presentation of Salvador Dura, BICS 2010, 14.7.2010</p>
    <p>Introduction: Bayesian brain</p>
    <p>Top-down priors (context)</p>
    <p>Perception = Probabilistic integration of information using Bayesian inference</p>
    <p>Perception = Probabilistic integration of information using Bayesian inference</p>
    <p>Bottom-up sensory data</p>
    <p>Hierarchical</p>
    <p>Cortical</p>
    <p>structure</p>
    <p>Generative model / Bayesian brain hypothesis</p>
    <p>The generative model allows to infer the causes of sensations and to predict inputs</p>
  </div>
  <div class="page">
    <p>Bayesian theory says:</p>
    <p>Decisions are based on 1. Beliefs (measured by probability) 2. Utilities</p>
    <p>The recipe: 1. Evaluate the probabilities of all possible states of the</p>
    <p>world (probabilistic inference) 2. Evaluate the probabilities of all outcomes for each and</p>
    <p>every potential action (probabilistic inference) 3. Choose the action which maximises the expected</p>
    <p>utility</p>
    <p>This is optimal if there are no restrictions on the available computational resources</p>
  </div>
  <div class="page">
    <p>Selection of information</p>
    <p>But computational resources are restricted   It is impossible to consider all the states and</p>
    <p>actions   It is necessary to select information in order to</p>
    <p>make decisions</p>
  </div>
  <div class="page">
    <p>Selection of information</p>
    <p>In practice, it has turned out to be impossible to learn complex abstractions from real data bottom-up</p>
    <p>There is too much structure  it is necessary to select which abstractions (groupings of elementary features) are meaningful</p>
    <p>Information will be lost  Example: learning the phonemes  results in inability</p>
    <p>to distinguish between foreign phonemes</p>
  </div>
  <div class="page">
    <p>Cerebral cortex</p>
    <p>Modelling the world (and yourself as part of it)  Forward modelling: prediction and simulation  Inverse modelling: figuring which actions lead to</p>
    <p>desired consequences; planning Selection of useful information  Selective learning of features: selecting useful</p>
    <p>high-level abstractions (sensory and motor)  Selective attention Modelling objects and their relations  Segmentation of objects</p>
  </div>
  <div class="page">
    <p>Key problem: How to select useful information?</p>
    <p>It is necessary to select information in order to make decisions</p>
    <p>Selection is a type of decision, in other words:  In order to decide we need to decide Infinite</p>
    <p>regress!</p>
  </div>
  <div class="page">
    <p>Distributed selection on cortex</p>
    <p>Primary input usually from bottom-up (from the senses)  Context (top-down or lateral) guides selection both in</p>
    <p>learning and on behavioural timescale</p>
  </div>
  <div class="page">
    <p>Invariant features</p>
    <p>Group simple features into complex ones in a hierarchical model</p>
    <p>What is the criterion?  Slow feature analysis: features that are activated</p>
    <p>close-by in time  Subspace ICA: features that are activated together  Canonical correlation analysis: features that are</p>
    <p>activated in the same context</p>
  </div>
  <div class="page">
    <p>Canonical correlation analysis</p>
    <p>A statistical technique which finds what is in common between two sets of data</p>
    <p>Find two projections (one from each dataset) such that their correlation is maximized</p>
    <p>Generalizes to several datasets, nonlinearities  E.g., find visual features which are most relevant</p>
    <p>for motor control  On behavioural timescale, activations are determined</p>
    <p>mainly by visual bottom-up inputs  Motor context guides learning</p>
  </div>
  <div class="page">
    <p>Cortical long-range connections are specific</p>
    <p>Inhibitory connections (white dots) are local and symmetric</p>
    <p>Long-range excitatory connections (black dots) adapt through experience</p>
    <p>Kevan Martin, Current Opinion in Neurobiology, 2002</p>
  </div>
  <div class="page">
    <p>Context-guided learning of features</p>
    <p>Inputs are whitened  Context (top-down and lateral) biases bottom-up activations  Competitive learning  invariant features emerge (e.g.,</p>
    <p>complex cells)</p>
  </div>
  <div class="page">
    <p>Attentional modulation of competition</p>
    <p>A V4 neuron is recorded  Weak activity for house,</p>
    <p>strong activity for face  Intermediate activity for a</p>
    <p>combination  Excitation adds up but so</p>
    <p>does inhibition</p>
    <p>Selective attention can mask the effect of the distractor</p>
    <p>Reynolds and Chelazzi, Annual Review of Neuroscience, 2004</p>
  </div>
  <div class="page">
    <p>Biased competition model</p>
    <p>Local competition on each cortical area  Context (top-down and lateral) biases the competition  Selective attention emerges from the dynamics</p>
    <p>primary input</p>
    <p>contextual input</p>
    <p>local inhibition</p>
  </div>
  <div class="page">
    <p>Visual search</p>
    <p>Top-down biasing from working memory  implements visual search</p>
    <p>May look like sequential search (time increases with distractors) but the mechanism is fully parallel</p>
    <p>Both bottom-up and topdown phenomena related to (covert) attention can be explained</p>
    <p>Deco and Rolls, Vision Research, 2004</p>
  </div>
  <div class="page">
    <p>Attention and learning: selection on different timescales</p>
    <p>Within the Bayesian framework, the only difference between perceptual inference and learning is the timescale</p>
    <p>In adults, perceptual learning is very strongly dependent on attention</p>
    <p>E.g., the same bottom-up input but different tasks  learn to perceive different aspects</p>
    <p>Selection in both attention and learning, only timescales differ</p>
  </div>
  <div class="page">
    <p>Biased competitive learning</p>
    <p>Biased competition + competitive learning (Masters thesis of Antti Yli-Krekola, 2007)</p>
    <p>Add adaptation (habituation, getting tired)  attention switches between objects</p>
    <p>Learning can be dramatically improved by switching attention (our paper at ICANN 2009)</p>
  </div>
  <div class="page">
    <p>Problems with engineering solutions to segmentation</p>
    <p>Many engineering solutions suffer from a chicken-or-egg problem:  Recognition is usually successful only after</p>
    <p>segmentation  Segmentation is often successful only after recognition</p>
    <p>Iterative bottom-up / top-down message passing solves this problem in biased competition model</p>
  </div>
  <div class="page">
    <p>Segmentation still often remains poor</p>
    <p>Although biased competition will select one object at a time, segmentation can be poor</p>
    <p>From the viewpoint of an individual neuron/feature, we are effectively asking: Do you belong to the currently active object or not?</p>
    <p>An easier question would be: Do you belong to object1 or object2 or  or something else?</p>
    <p>In Bayesian terms: explaining away</p>
  </div>
  <div class="page">
    <p>Segmentation and synchrony</p>
    <p>Engel, Fries and Singer, Nature Reviews Neuroscience, 2001</p>
    <p>Hypothesis: neurons encoding the same object synchronize</p>
  </div>
  <div class="page">
    <p>Segmentation with weakly coupled oscillators: LEGION model</p>
    <p>http://www.scholarpedia.org/article/LEGION:_locally_excitatory_globally_inhibitory_oscillator_networks</p>
  </div>
  <div class="page">
    <p>Biased competitive learning + coupled oscillators</p>
    <p>Our goal in BICS 2010 paper is to study how biased competitive learning could be combined with coupled oscillators for better segmentation</p>
    <p>Abstraction level:  The behaviour of one area is what we design</p>
    <p>try to keep it as simple as possible</p>
    <p>Try to come up with emergent oscillatory segmentation in a network of interconnected areas</p>
  </div>
  <div class="page">
    <p>Desired emergent properties</p>
    <p>objects) 4. Objects desynchronise between each other</p>
    <p>(when their representations overlap)</p>
  </div>
  <div class="page">
    <p>Properties of a single area  part 1</p>
    <p>Pretty much like biased competitive learning but:  Intrinsic oscillators built from excitatory and</p>
    <p>inhibitory neurons  Low-pass filtering is needed somewhere to build</p>
    <p>oscillators. Low-pass filter inhibition to avoid distorting the signal carried by excitatory neurons.</p>
    <p>Keep activations in check with gain control</p>
  </div>
  <div class="page">
    <p>Properties of a single area  part 2</p>
    <p>If no bottom-up input  no activation  If bottom-up input</p>
    <p>If no support  little oscillation, little activation  If constant support  some oscillation, more activation  If oscillating support  phase-locked oscillation,</p>
    <p>strong activation</p>
    <p>Competing features push each others phases further away from each other</p>
    <p>Details are not crucial, there are many ways to implement these properties</p>
  </div>
  <div class="page">
    <p>Results  part 1</p>
    <p>First we checked that our implementation got the properties for a single area right (using externally generated bottom-up and lateral input)</p>
    <p>Seemed to work ok  What happens now when we connect many</p>
    <p>areas together?</p>
  </div>
  <div class="page">
    <p>Results  part 2</p>
    <p>Use simple visual inputs  A network of 55 inputs  Learn bottom-up features and their lateral connections</p>
    <p>(between excitatory neurons)</p>
  </div>
  <div class="page">
    <p>Results  part 3 50 55 60 65 70 75 80</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Looks promising  There was a problem with multiple activations</p>
    <p>from the same object within one area  Hierarchy and top-down support would help tie them</p>
    <p>together? Complex cells? Synchronize inputs?</p>
    <p>Obvious next step: test a hierarchical model But I want to see it work in motor control:  Is this really useful?  Even context-guided learning and biased</p>
    <p>competition havent been properly integrated with motor learning  test them separately first</p>
  </div>
  <div class="page">
    <p>Future / ongoing work</p>
    <p>Sensory and motor abstractions  Development of sensory abstractions guided by motor</p>
    <p>context and vice versa (akin to canonical correlation analysis)</p>
    <p>Predictive power as a measure of value of information (needed for selection of information)</p>
    <p>Decision-making as biased competition on motor representations</p>
    <p>Better mappings  A hierarchical model of correlations at lower levels;</p>
    <p>latent variables describing the operating points of the system</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The experiments with artificial data have not proven that the system works in real life</p>
    <p>Nevertheless, looks promising  I expect to revisit this work: once we need</p>
    <p>sophisticated segmentation, synchrony really might be a useful ingredient</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>www.zenrobotics.com</p>
  </div>
</Presentation>
