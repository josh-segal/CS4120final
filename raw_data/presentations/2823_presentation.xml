<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>dShark: A General, Easy to Program and Scalable Framework for Analyzing</p>
    <p>In-network Packet Traces</p>
    <p>Da Yu , Yibo Zhu , Behnaz Arzani , Rodrigo Fonseca , Tianrong Zhang , Karl Deng , Lihua Yuan</p>
    <p>Brown University Microsoft</p>
  </div>
  <div class="page">
    <p>Network reliability is critical</p>
    <p>Cloudflare: A bad config (router rule) caused all of their edge routers to crash, taking down all of Cloudflare.</p>
    <p>Etsy: Sending multicast traffic without properly configuring switches caused an Etsy global outage.</p>
    <p>Stack Overflow: A bad firewall config</p>
    <p>blocked stackexchange/stackoverflow.</p>
  </div>
  <div class="page">
    <p>End-host based</p>
    <p>Topology or hardware specific</p>
    <p>Target on specific problems</p>
    <p>Trumpet[SIGCOMM16] Sonata[SIGCOMM18]</p>
    <p>PathDump[OSDI16]</p>
    <p>Pingmesh[SIGCOMM15]</p>
    <p>INT</p>
    <p>Existing tools are the first attempts</p>
  </div>
  <div class="page">
    <p>EverFlow[SIGCOMM15]</p>
    <p>In-network packet capture is the last resort</p>
    <p>Analyzing the in-network packet traces is challenging!</p>
    <p>pkt</p>
    <p>NetSight[NSDI14]</p>
  </div>
  <div class="page">
    <p>In-network analysis: challenges</p>
    <p>Analysis logic varies  Logic is different case by case</p>
    <p>Volume  3.33 Mpps line-speed (10 Gbps, 1500 Bytes)</p>
  </div>
  <div class="page">
    <p>Example: Route error checker</p>
    <p>hasRouteError(path) -&gt; true / false</p>
    <p>pkt</p>
  </div>
  <div class="page">
    <p>Example: Route error checker</p>
    <p>hasRouteError(path) -&gt; true / false</p>
    <p>pkt</p>
  </div>
  <div class="page">
    <p>Example: Route error checker</p>
    <p>hasRouteError(path) -&gt; true / false</p>
    <p>pkt</p>
  </div>
  <div class="page">
    <p>Example: Route error checker</p>
    <p>hasRouteError(path) -&gt; true / false</p>
    <p>pkt</p>
    <p>Streaming Processor</p>
    <p>Result aggregator</p>
  </div>
  <div class="page">
    <p>In-network analysis: challenges</p>
    <p>Difficult to get robust analysis  Header transformation</p>
    <p>Headers are modified by the middleboxes</p>
    <p>Analysis logic varies  Logic is different case by case</p>
    <p>Volume  3.33 Mpps line-speed (10 Gbps, 1500 Bytes)</p>
  </div>
  <div class="page">
    <p>Packet headers are modified by middleboxes</p>
    <p>X</p>
    <p>GW ISP</p>
    <p>ISP-Y Switch</p>
    <p>Cloud Edge</p>
    <p>Cloud WAN</p>
    <p>Y(MSFT)</p>
    <p>Server</p>
    <p>Datacenter</p>
    <p>Gateway</p>
    <p>SLB</p>
    <p>Server</p>
    <p>T2</p>
    <p>T1 T0</p>
    <p>Outside our networks</p>
    <p>Ingress flow Egress flow</p>
    <p>switch/router mirror w/ERSPAN</p>
    <p>VLAN VXLAN GRE</p>
    <p>IP-in-IP</p>
    <p>Outside flow switch/router mirror w/GRE</p>
    <p>Header format</p>
    <p>Headers added after mirroring Mirrored headers</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET 802.1Q IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 GRE ETHERNET IPV4 TCP</p>
  </div>
  <div class="page">
    <p>Header format</p>
    <p>Headers added after mirroring Mirrored headers</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET 802.1Q IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>Same protocol headers bring ambiguity</p>
  </div>
  <div class="page">
    <p>dShark: three goals</p>
    <p>Broadly applicable</p>
    <p>Robust in the wild  Header transformation</p>
    <p>Scalable</p>
  </div>
  <div class="page">
    <p>dShark: three goals</p>
    <p>Broadly applicable</p>
    <p>Robust in the wild  Header transformation</p>
    <p>Scalable  Components work independently and in parallel.</p>
  </div>
  <div class="page">
    <p>dShark: three goals</p>
    <p>Broadly applicable</p>
    <p>Robust in the wild  Header transformation</p>
    <p>Scalable  Components work independently and in parallel.</p>
  </div>
  <div class="page">
    <p>How operators manually process traces</p>
    <p>Observation #1:  Four diagnosis steps: parse, filter, aggregate and analyze</p>
    <p>Need to tightly integrated with the collecting infrastructure!</p>
    <p>pkt</p>
    <p>Filter, Partition (SQL) Map (MapReduce)</p>
    <p>Group by (SQL) Shuffle (MapReduce)</p>
    <p>User-defined Func. (SQL) Reduce (MapReduce)</p>
    <p>Parsers</p>
    <p>Groupers</p>
    <p>Query Processors</p>
  </div>
  <div class="page">
    <p>How operators manually process traces</p>
    <p>One-Hop Multi-Hop</p>
    <p>check appearance of a packet</p>
    <p>show full path of each packet</p>
    <p>in the network</p>
    <p>diagnose middlebox behaviors</p>
    <p>complicated cases that requires end</p>
    <p>information</p>
    <p>One-Packet</p>
    <p>Multi-Packet</p>
    <p>Observation #2:  Diagnosis logic always run on top of 4 aggregation types</p>
  </div>
  <div class="page">
    <p>Declarative: how to parse, summarize and group pkts</p>
    <p>Imperative: how to process groups of packets</p>
    <p>Query function: hasRouteError(path)</p>
    <p>Pkt spec.: All instances of the same packet</p>
    <p>dSharks programming model</p>
    <p>pkt</p>
    <p>Parsers</p>
    <p>Groupers</p>
    <p>Query Processors</p>
    <p>Decl. packet spec.:  Familiar to the operators Imp. query function:  Flexible for analysis logic</p>
  </div>
  <div class="page">
    <p>{ Summary: { Key: [ipId, seqNum], Additional: [ttl]</p>
    <p>},</p>
    <p>Name: { ipId: ipv4.id, seqNum: tcp.seq, ttl: ipv4.ttl</p>
    <p>} }</p>
    <p>Declarative spec. in parsers and groupers</p>
    <p>Definition of a packet summary</p>
    <p>How to extract the values in the header</p>
    <p>A packet summary is a byte array that only contains fields that the operators are interested in.</p>
  </div>
  <div class="page">
    <p>Header format</p>
    <p>Headers added after mirroring Mirrored headers</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET 802.1Q IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 IPV4 UDP VXLAN ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 GRE IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>ETHERNET IPV4 ERSPAN ETHERNET IPV4 GRE ETHERNET IPV4 TCP</p>
    <p>Same protocol headers bring ambiguity</p>
    <p>ipv4[0] ipv4[3] ipv4[-1]</p>
  </div>
  <div class="page">
    <p>{ Summary: { Key: [ipId, seqNum], Additional: [ttl]</p>
    <p>},</p>
    <p>Name: { ipId: ipv4[-1].id, seqNum: tcp[-1].seq, ttl: ipv4[:].ttl</p>
    <p>} }</p>
    <p>Declarative spec. in parsers and groupers</p>
    <p>Definition of a packet summary</p>
    <p>How to extract the values in the header</p>
  </div>
  <div class="page">
    <p>Pair&lt;Object, Object&gt; query (const vector&lt;Summary&gt;&amp; group) {</p>
    <p>// ReconstructS the path based on TTL constructPath(group);</p>
    <p>// Checks path bool result = hasRouteError(group);</p>
    <p>return make_pair(result, 1); }</p>
    <p>Imperative diagnosis logic in the query processors</p>
    <p>In practice, this is implemented in 49 lines of code for the query function!</p>
  </div>
  <div class="page">
    <p>dShark overview</p>
    <p>pkt</p>
    <p>parse</p>
    <p>eth ip.addr</p>
    <p>ip.id</p>
    <p>ip.ttl</p>
    <p>tcp.seq</p>
    <p>data</p>
    <p>extract shuffle query abnormal: X</p>
    <p>normal: Y</p>
    <p>Parser Grouper Query Processor</p>
    <p>read</p>
    <p>Reads packets  Parses, extracts fields for</p>
    <p>summaries  Sends summaries based on</p>
    <p>the keys</p>
    <p>Receives summaries  Aggregates summaries</p>
    <p>based on the keys  Analyze summary</p>
    <p>groups  Returns the output</p>
    <p>summary 2 7</p>
    <p>group</p>
  </div>
  <div class="page">
    <p>{ Summary: { Key: [ipId, seqNum], Additional: [vip, pip]</p>
    <p>},</p>
    <p>Name: { ipId: ipv4[-1].id, seqNum: tcp[-1].seq, vip: ipv4[-1].dst, pip: ipv4[0].dst</p>
    <p>} }</p>
    <p>Another example: load balancer profiler</p>
    <p>Spec:</p>
    <p>Func.:</p>
    <p>Pair&lt;Pair&lt;IP, IP&gt;, int&gt; query(group) { // Validate data ...</p>
    <p>return Pair((Pair(vip, pip), 1); }</p>
    <p>Innermost ipId and tcp seq # to identify a packet  Virtual IP and the physical IP</p>
    <p>Returns the map</p>
    <p>In practice, this is implemented in 18 lines of code for the query function!</p>
  </div>
  <div class="page">
    <p>Group pattern Application Analysis logic</p>
    <p>In-nw ck. only</p>
    <p>Header transf.</p>
    <p>Query LOC</p>
    <p>One packet on one</p>
    <p>hop</p>
    <p>Loop-free detection [21] Detect forwarding loop</p>
    <p>Group: same packet(ipv4[0].ipid, tcp[0].seq) on one hop Query: does the same packet appear multiple times on the same hop No No 8</p>
    <p>Overloop-free detection [69] Detect forwarding loop involving tunnels</p>
    <p>Group: same packet(ipv4[0].ipid, tcp[0].seq) on tunnel endpoints Query: does the same packet appear multiple times on the same endpoint Yes Yes 8</p>
    <p>One packet on multiple</p>
    <p>hops</p>
    <p>Route detour checker Check packets route in failure case</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) on all switches Query: is valid detour in the recovered path(ipv4[:].ttl) No Yes* 49</p>
    <p>Route error Detect wrong packet forwarding</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) on all switches Query: get last correct hop in the recovered path(ipv4[:].ttl) No* Yes* 49</p>
    <p>Netsight [21] Log packets in-network lifecycle</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) on all switches Query: recover path(ipv4[:].ttl) No* Yes* 47</p>
    <p>Hop counter [21] Count packets hop</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) on all switches Query: count record No* Yes* 6</p>
    <p>Multiple packets</p>
    <p>on one hop</p>
    <p>Traffic isolation checker [21] Check whether hosts are allowed to talk</p>
    <p>Group: all packets at dst ToR(SWITCH=dst ToR) Query: have prohibited host(ipv4[0].src) No No 11</p>
    <p>Middlebox(SLB, GW, etc) profiler Check correctness/performance of middleboxes</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) pre/post middlebox Query: is middlebox correct(related fields) Yes Yes 18</p>
    <p>Packet drops on middleboxes Check packet drops in middleboxes</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) pre/post middlebox Query: exist ingress and egress trace Yes Yes 8</p>
    <p>Protocol bugs checker(BGP, RDMA, etc) [69] Identify wrong implementation of protocols</p>
    <p>Group: all BGP packets at target switch(SWITCH=tar SW) Query: correctness(related fields) of BGP(FLTR: tcp[-1].src|dst=179) Yes Yes* 23</p>
    <p>Incorrect packet modification [21] Check packets header modification</p>
    <p>Group: same packet(ipv4[-1].ipid, tcp[-1].seq) pre/post the modifier Query: is modification correct (related fields) Yes Yes* 4</p>
    <p>Waypoint routing checker [21, 43] Make sure packets (not) pass a waypoint</p>
    <p>Group: all packets at waypoint switch(SWITCH=waypoint) Query: contain flow(ipv4[-1].src+dst, tcp[-1].src+dst) should (not) pass Yes No 11</p>
    <p>DDoS diagnosis [43] Localize DDoS attack based on statistics</p>
    <p>Group: all packets at victims ToR(SWITCH=vic ToR) Query: statistics of flow(ipv4[-1].src+dst, tcp[-1].src+dst) No Yes* 18</p>
    <p>Multiple packets</p>
    <p>on multiple</p>
    <p>hops</p>
    <p>Congested link diagestion [43] Find flows using congested links</p>
    <p>Group: all packets(ipv4[-1].ipid, tcp[-1].seq) pass congested link Query: list of flows(ipv4[-1].src+dst, tcp[-1].src+dst) No* Yes* 14</p>
    <p>Silent black hole localizer [43, 69] Localize switches that drop all packets</p>
    <p>Group: packets with duplicate TCP(ipv4[-1].ipid, tcp[-1].seq) Query: get dropped hop in the recovered path(ipv4[:].ttl) No* Yes* 52</p>
    <p>Silent packet drop localizer [69] Localize random packet drops</p>
    <p>Group: packets with duplicate TCP(ipv4[-1].ipid, tcp[-1].seq) Query: get dropped hop in the recovered path(ipv4[:].ttl) No* Yes* 52</p>
    <p>ECMP profiler [69] Profile flow distribution on ECMP paths</p>
    <p>Group: all packets at ECMP ingress switches(SWITCH in ECMP) Query: statistics of flow(ipv4[-1].src+dst, tcp[-1].src+dst) No* No 18</p>
    <p>Traffic matrix [43] Traffic volume between given switch pairs</p>
    <p>Group: all packets at given two switches(SWITCH in tar SW) Query: total volume of overlapped flow(ipv4[-1].src+dst, tcp[-1].src+dst) No* Yes* 21</p>
    <p>Table 2: We implemented 18 typical diagnosis applications in dShark. No* in column in-network checking only means this application can also be done with end-host checking with some assumptions. Yes* in column header transformation needs to be robust to header transformation in our network, but, in other environments, it might not. ipv4[:].ttl in the analysis logic means dShark concatenates all ivp4s TTLs in the header. It preserves order information even with header transformation. Sorting it makes path recovery possible. We profiled SLB. We focused on BGP route filter. We focused on packet encapsulation.</p>
    <p>dShark is designed to allow for the analysis of distributed packet traces in near real time. Our goal in its design has been to allow for scalability, ease of use, generality, and robustness. In this section, we outline dSharks design and how it allows us to achieve these goals. At a high level, dShark provides a domain-specific language for expressing distributed network monitoring tasks, which runs atop a map-reduce-like infrastructure that is tightly coupled, for efficiency, with a packet capture infrastructure. The DSL primitives are designed to</p>
    <p>enable flexible filtering and grouping of packets across the network, while being robust to header transformations and capture noise that we observe in practice.</p>
    <p>To diagnose a problem with dShark, an operator has to write two related pieces: a declarative set of trace specifications indicating relevant fields for grouping and summarizing packets; and an imperative callback function to process groups of packet summaries.</p>
    <p>Here we show a basic example  detecting forwarding loops in the network with dShark. This means dShark must check whether or not any packets appear more than once at any switch. First, network operators can write the trace specifications as follows, in JSON: 1 {</p>
    <p>Please check the paper for details!</p>
  </div>
  <div class="page">
    <p>dShark: three goals</p>
    <p>Robust in the wild  Header transformation</p>
    <p>Define packet signature in the summary.  Leverage the index of protocol.</p>
    <p>Capture noise</p>
    <p>Scalable  Components work independently and in parallel.</p>
    <p>Broadly applicable  dSharks programming model is general.  It supports 4 types of aggregation that covers 18 typical analysis apps.</p>
  </div>
  <div class="page">
    <p>switch Dswitch Cswitch B</p>
    <p>Tolerate capture noise</p>
    <p>switch A</p>
    <p>collectors</p>
    <p>packet</p>
    <p>External network</p>
    <p>Network Boundary</p>
    <p>X X</p>
  </div>
  <div class="page">
    <p>Performance of dShark</p>
    <p>8 VMs from a public cloud</p>
    <p>Each has:  16-core 2.4GHz vCPU  56GB memory  10Gbps virtual network</p>
    <p>Feed with real traces from production</p>
  </div>
  <div class="page">
    <p>dShark scales nearly linearly</p>
    <p>Overall Figure 6: Single parser performance with different packet headers.</p>
    <p>Figure 7: Single grouper performance with different average group sizes.</p>
    <p>Figure 8: Single query processor performance with different query functions.</p>
    <p>(6 S9</p>
    <p>g)</p>
    <p>S2 4g</p>
    <p>)</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>Su t(</p>
    <p>s)</p>
    <p>ideal(linear) d6harN</p>
    <p>Figure 9: dShark performance scales near linearly. from CPU for better performance. However, dShark already delivers sufficient throughput for analyzing 40Gbps online packet captures per server (6) in a practical setting. Meanwhile, dShark, as a pure software solution, is more flexible, has lower hardware cost, and provides operators a programming interface they are familiar with. Thus, we believe that dShark satisfies the current demand of our operators. That said, in an environment that is fully deployed with highly programmable switches,6 it is promising to explore hardwarebased trace analysis like Marple [42].</p>
    <p>dShark, to the best of our knowledge, is the first framework that allows for the analysis of distributed packet traces in the face of noise, complex packet transformations, and large network traces. Perhaps the closest to dShark are PathDump [56] and SwitchPointer [57]. They diagnose problems by adding metadata to packets at each switch and analyzing them at the destination. However, this requires switch hardware modification that is not widely available in todays networks. Also, in-band data shares fate with the packets, making it hard to diagnose problems where packets do not reach the destination.</p>
    <p>Other related work that has been devoted to detection and diagnosis of network failures includes: Switch hardware design for telemetry [21, 28, 32, 36, 42].</p>
    <p>While effective, these work require infrastructure changes that are challenging or even not possible due to various practical reasons. Therefore, until these capabilities are mainstream, the need to for distributed packet traces remains. Our summaries may resemble NetSights postcards [21], but postcards are fixed, while our summaries are flexible, can handle transformations, and are tailored to the queries they serve. Algorithms based on inference [3, 8, 19, 20, 22, 38, 40, 53,</p>
    <p>source of failures within networks. Some attempt to cover the full topology using periodic probes [20]. However, such probing results in loss of information that often complicates detecting certain types of problems which could be easily detected using packet traces from the network itself. Other such approaches, e.g., [38, 40, 53, 54], either rely on the packet arriving endpoints and thus cannot localize packet drops, or assume specific topology. Work such as EverFlow [68] is complementary to dShark. Specifically, dSharks goal is to analyze distributed packet captures fed by Everflow. Finally, [7] can only identify the general type of a problem (network, client, server) rather than the responsible device. Work on detecting packet drops. [11, 16, 17, 2325, 29, 33,</p>
    <p>We present dShark, a general and scalable framework for analyzing in-network packet traces collected from distributed devices. dShark provides a programming model for operators to specify trace analysis logic. With this programming model, dShark can easily address complicated artifacts in real world traces, including header transformations and packet capturing noise. Our experience in implementing 18 typical diagnosis tasks shows that dShark is general and easy to use. dShark can analyze line rate packet captures and scale out to multiple servers with near-linear speedup.</p>
    <p>Acknowledgments</p>
    <p>We thank our shepherd, Anja Feldmann, and the anonymous reviewers for their insightful comments. Da Yu was partly funded by NSF grant CNS-1320397.</p>
    <p>References</p>
    <p>[1] Data plane development kit (DPDK). http://dpdk.org/, 2018. Accessed on 2018-01-25.</p>
  </div>
  <div class="page">
    <p>Some findings</p>
    <p>until which timestamp they should continue processing packets. The parsers will report their progress once they reach the target timestamp and wait for the next instruction. Once all parsers report completion, the coordinator sends out the next target timestamp. This guarantees that the progress of different parsers will never differ too much. To avoid stragglers, the coordinator may drop parsers that are consistently slower. Over-provision the number of instances. Although it may be hard to accurately estimate the minimum number of instances needed (see 6) due to the different CPU overhead of various packet headers and queries, we use conservative estimation and over-provision instances. It only wastes negligible CPU cycles because we implement all components to spend CPU cycles only on demand.</p>
    <p>We used dShark for analyzing the in-network traces collected from our production networks4. In this section, we first present a few examples where we use dShark to check some typical network properties and invariants. Then, we evaluate the performance of dShark.</p>
    <p>We implement 18 typical analysis tasks using dShark (Table 2). We explain three of them in detail below. Loop detection. To show the correctness of dShark, we perform a controlled experiment using loop detection analysis as an example. We first collected in-network packet traces (more than 10M packets) from one of our networks and verified that there is no looping packet in the trace. Then, we developed a script to inject looping packets by repeating some of the original packets with different TTLs. The script can inject with different probabilities.</p>
    <p>We use the same code as in 4.1. Figure 4 illustrates the number of looping packets that are injected and the number of packets caught by dShark. dShark has zero false negative or false positive in this controlled experiment. Profiling load balancers. In our data center, layer-4 software load balancers (SLB) are widely deployed under ToR switches. They receive packets with a virtual IP (VIP) as the destination and forward them to different servers (called DIP) using IP-in-IP encapsulation, based on flow-level hashing. Traffic distribution analysis of SLBs is handy for network operators to check whether the traffic is indeed balanced.</p>
    <p>To demonstrate that dShark can easily provide this, we randomly picked a ToR switch that has an SLB under it. We deployed a rule on that switch that mirrors all packets that go towards a specific VIP and come out. In one hour, our collectors captured more than 30M packets in total.5</p>
    <p>Our query function generates both flow counters and packet</p>
    <p>et 1</p>
    <p>uP be</p>
    <p>r</p>
    <p>Figure 4: Injected loops are all detected.</p>
    <p>Figure 5: Traffic to an SLB VIP has been distributed to destination IPs.</p>
    <p>counters of each DIP. Figure 5 shows the result  among the total six DIPs, DIP5 receives the least packets whereas DIP6 gets the most. Flow-level counters show a similar distribution. After discussing with operators, we conclude that for this VIP, load imbalance does exist due to imbalanced hashing, while it is still in an acceptable range. Packet drop localizer. Noise can affect the packet drop localizer. Here we briefly evaluate the effectiveness of using transport-level retransmission information to reduce false positives (4.5). We implemented the packet drop localizer as shown in Table 2, and used the noise mitigation mechanism described in 4.5. In a production data center, we deployed a mirroring rule on all switches to mirror all packets that originate from or go towards all servers, and fed the captured packets to dShark. We first compare our approach, which takes into account gaps in the sequence of switches, and uses retransmissions as evidence of actual drops, with a nave approach, that just looks at the whether the last captured hop is the expected hop. Since the nave approach does not work for drops at the last switch (including ToR and the data center boundary Tier-2 spine switches), for this comparison we only considered packets whose last recorded switch were leaf (Tier-1) switches. The nave approach reports 5,599 suspected drops while dShark detects 7. The reason for the difference is drops of mirrored packets, which we estimated in our log to be approximately 2.2%. The drops detected by dShark are real, because they generated retransmissions with the same TCP sequence number.</p>
    <p>Looking at all packets (and not only the ones whose traces terminate at the Tier-1 switches), we replayed the trace while randomly dropping capture packets with increasing probabilities. dShark reported 5,802, 5,801, 5,801 and 5,784 packet drops under 0%, 1%, 2% and 5% probabilities respectively. There is still a possibility that we miss the retransmitted packet, but, from the result, it is very low (0.3%).</p>
    <p>Next, we evaluate the performance of dShark components individually. For stress tests, we feed offline traces to dShark as fast as possible. To represent commodity servers, we use eight VMs from our public cloud platform, each has a Xeon 16-core 2.4GHz vCPU, 56GB memory and 10Gbps virtual network. Each experiment is repeated for at least five times and we report the average. We verify the speed difference between the fastest run and slowest run is within 5%. Parser. The overhead of the parser varies based on the layers</p>
    <p>until which timestamp they should continue processing packets. The parsers will report their progress once they reach the target timestamp and wait for the next instruction. Once all parsers report completion, the coordinator sends out the next target timestamp. This guarantees that the progress of different parsers will never differ too much. To avoid stragglers, the coordinator may drop parsers that are consistently slower. Over-provision the number of instances. Although it may be hard to accurately estimate the minimum number of instances needed (see 6) due to the different CPU overhead of various packet headers and queries, we use conservative estimation and over-provision instances. It only wastes negligible CPU cycles because we implement all components to spend CPU cycles only on demand.</p>
    <p>We used dShark for analyzing the in-network traces collected from our production networks4. In this section, we first present a few examples where we use dShark to check some typical network properties and invariants. Then, we evaluate the performance of dShark.</p>
    <p>We implement 18 typical analysis tasks using dShark (Table 2). We explain three of them in detail below. Loop detection. To show the correctness of dShark, we perform a controlled experiment using loop detection analysis as an example. We first collected in-network packet traces (more than 10M packets) from one of our networks and verified that there is no looping packet in the trace. Then, we developed a script to inject looping packets by repeating some of the original packets with different TTLs. The script can inject with different probabilities.</p>
    <p>We use the same code as in 4.1. Figure 4 illustrates the number of looping packets that are injected and the number of packets caught by dShark. dShark has zero false negative or false positive in this controlled experiment. Profiling load balancers. In our data center, layer-4 software load balancers (SLB) are widely deployed under ToR switches. They receive packets with a virtual IP (VIP) as the destination and forward them to different servers (called DIP) using IP-in-IP encapsulation, based on flow-level hashing. Traffic distribution analysis of SLBs is handy for network operators to check whether the traffic is indeed balanced.</p>
    <p>To demonstrate that dShark can easily provide this, we randomly picked a ToR switch that has an SLB under it. We deployed a rule on that switch that mirrors all packets that go towards a specific VIP and come out. In one hour, our collectors captured more than 30M packets in total.5</p>
    <p>Our query function generates both flow counters and packet</p>
    <p>Figure 4: Injected loops are all detected.</p>
    <p>DI31 DI32 DI33 DI34 DI35 DI36 0 5 10 15 20 25 30</p>
    <p>pkt Ilow</p>
    <p>Figure 5: Traffic to an SLB VIP has been distributed to destination IPs.</p>
    <p>counters of each DIP. Figure 5 shows the result  among the total six DIPs, DIP5 receives the least packets whereas DIP6 gets the most. Flow-level counters show a similar distribution. After discussing with operators, we conclude that for this VIP, load imbalance does exist due to imbalanced hashing, while it is still in an acceptable range. Packet drop localizer. Noise can affect the packet drop localizer. Here we briefly evaluate the effectiveness of using transport-level retransmission information to reduce false positives (4.5). We implemented the packet drop localizer as shown in Table 2, and used the noise mitigation mechanism described in 4.5. In a production data center, we deployed a mirroring rule on all switches to mirror all packets that originate from or go towards all servers, and fed the captured packets to dShark. We first compare our approach, which takes into account gaps in the sequence of switches, and uses retransmissions as evidence of actual drops, with a nave approach, that just looks at the whether the last captured hop is the expected hop. Since the nave approach does not work for drops at the last switch (including ToR and the data center boundary Tier-2 spine switches), for this comparison we only considered packets whose last recorded switch were leaf (Tier-1) switches. The nave approach reports 5,599 suspected drops while dShark detects 7. The reason for the difference is drops of mirrored packets, which we estimated in our log to be approximately 2.2%. The drops detected by dShark are real, because they generated retransmissions with the same TCP sequence number.</p>
    <p>Looking at all packets (and not only the ones whose traces terminate at the Tier-1 switches), we replayed the trace while randomly dropping capture packets with increasing probabilities. dShark reported 5,802, 5,801, 5,801 and 5,784 packet drops under 0%, 1%, 2% and 5% probabilities respectively. There is still a possibility that we miss the retransmitted packet, but, from the result, it is very low (0.3%).</p>
    <p>Next, we evaluate the performance of dShark components individually. For stress tests, we feed offline traces to dShark as fast as possible. To represent commodity servers, we use eight VMs from our public cloud platform, each has a Xeon 16-core 2.4GHz vCPU, 56GB memory and 10Gbps virtual network. Each experiment is repeated for at least five times and we report the average. We verify the speed difference between the fastest run and slowest run is within 5%. Parser. The overhead of the parser varies based on the layers</p>
    <p>Please check the paper for details!!</p>
    <p>Case 1: Profile an SLB</p>
    <p>Case 2: Detect loops</p>
    <p>Nave 5,588 dShark 7</p>
    <p>Case 3: Detect packet drops on T1</p>
    <p>Retran. as evidence of actual</p>
    <p>drops</p>
    <p>Diff. caused by noise</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>dShark is a general, easy-to-program, scalable and highperformance in-network packet trace analyzer.</p>
    <p>Takeaways:  dSharks programming model is broadly applicable  We use this model to implement 18 different typical diagnosis apps</p>
    <p>Operators focus on the logic without worrying about:  Header transformation, capture noise, scalability</p>
    <p>dShark is fast and can scale linearly</p>
  </div>
</Presentation>
