<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-Label Transfer Learning for Multi-Relational</p>
    <p>Semantic Similarity Li Harry Zhang, Steven R. Wilson, Rada Mihalcea</p>
    <p>University of Michigan</p>
    <p>*SEM 2019</p>
    <p>Minneapolis, USA</p>
  </div>
  <div class="page">
    <p>Semantic Similarity Task</p>
    <p>Given two texts, rate the degree of equivalence in meaning</p>
    <p>Dataset: pairs of text &amp; human annotated similarity, e.g. 0  5 scale</p>
    <p>Example  I will give her a ride to work.  I will drive her to the company.  Similarity: 5</p>
    <p>Output: A machine predicts similarity scores for all pairs</p>
    <p>Evaluation: Pearson/Spearmans correlation</p>
    <p>Existing datasets: Finkelstein et al. 2012, Agirre et al. 2012-2016, Cer et al. 2017, Hill et al. 2015, Leviant et al. 2015, etc.</p>
  </div>
  <div class="page">
    <p>Multi-Relational Semantic Similarity Task</p>
    <p>Similarity can be defined in different ways, i.e. relations</p>
    <p>Some datasets are annotated in multiple relations of similarity</p>
    <p>Human Activity: similarity, relatedness, motivation, actor (Wilson et al. 2017)</p>
    <p>SICK: relatedness, entailment (Marelli et al. 2014)</p>
    <p>Typed Similarity: general, author, people, time, location, event, action, subject, description (Agirre et al. 2013)</p>
  </div>
  <div class="page">
    <p>Human Activity</p>
    <p>Similarity: do the two activities describe the same thing?</p>
    <p>Relatedness: are the two activities related to one another?</p>
    <p>Motivation: are the two activities done with the same motivation?</p>
    <p>Actor: are the two activities likely to done by the same person?</p>
    <p>Check email vs. write email (scale of 0-4):</p>
    <p>Similarity Relatedness Motivation Actor</p>
  </div>
  <div class="page">
    <p>SICK</p>
    <p>Sentences Involving Compositional Knowledge</p>
    <p>Relatedness: are the two texts related to one another? (scale 1-5)</p>
    <p>Entailment: does one text entail the other? (three-way)</p>
    <p>Two dogs are wrestling and hugging vs. There is no dog wrestling and hugging</p>
    <p>Relatedness Entailment</p>
  </div>
  <div class="page">
    <p>Typed Similarity</p>
    <p>A collection of meta-data describing books, paintings, films, museum objects and archival records (scale of 0-5)</p>
    <p>Title: London Bridge, City of London Creator: not known Description: A view of London Bridge which is packed with horse-drawn traffic and pedestrians. This bridge replaced the earlier medieval bridge upstream. It was built by John Rennie in 1823-31. A new bridge, built in the late 1960s now stands on this site today.</p>
    <p>Title: Serpentine Bridge, Hyde Park, Westminster, Greater London Creator: de Mare, Eric Subject: Waterscape Animals Bridge Gardens And Parks Description: The Serpentine Bridge in Hyde Park seen from the bank. It was built by George and John Rennie, the sons of the geat architect John Rennie, in 1825-8.</p>
    <p>general author people time location event subject description</p>
  </div>
  <div class="page">
    <p>Existing Model: Single Task</p>
    <p>Fine-tuning with pre-trained sentence encoder / sentence embeddings</p>
    <p>InferSent: Bi-LSTM with max pooling (Conneau et al. 2017)</p>
    <p>A logistic regression layer is used as the output layer</p>
    <p>All parameters are being tuned during transfer learning</p>
  </div>
  <div class="page">
    <p>Existing Model: Single Task</p>
    <p>Treats each relation as a single separate task</p>
    <p>No parameter or information is shared among relations of similarity</p>
    <p>The Single-Task baseline</p>
    <p>Question: can we learn across different relations, by sharing parameters?</p>
    <p>LSTM OutRelation A:</p>
    <p>LSTM OutRelation B:</p>
  </div>
  <div class="page">
    <p>Proposed Multi-Label Model</p>
    <p>Same sentence encoder model</p>
    <p>All relations share the lower-level parameters in the LSTM</p>
    <p>Each relation has its own output layers</p>
    <p>Each output layer makes a prediction at the same time</p>
  </div>
  <div class="page">
    <p>Proposed Multi-Label Model</p>
    <p>Assuming 2 relations (A and B)</p>
    <p>One output layer per relation</p>
    <p>The rest of the parameters are shared between the 2 relations</p>
    <p>The 2 losses are summed as the final loss</p>
    <p>All parameters in the model are updated</p>
    <p>The Multi-Label model</p>
    <p>LSTM</p>
    <p>OutRelation A:</p>
    <p>OutRelation B:</p>
  </div>
  <div class="page">
    <p>Alternative Multi-Task Model</p>
    <p>Same sentence encoder model</p>
    <p>Alternate between batches of different relations</p>
    <p>Update the related parameters each time</p>
  </div>
  <div class="page">
    <p>Alternative Multi-Task Model</p>
    <p>Same sentence encoder model</p>
    <p>Alternate between batches of different relations</p>
    <p>Update the related parameters each time</p>
  </div>
  <div class="page">
    <p>Alternative Multi-Task Model</p>
    <p>Same sentence encoder model</p>
    <p>Assuming 2 relations (A and B)</p>
    <p>Still 2 output layers</p>
    <p>Take a batch of pairs, predict relation A</p>
    <p>Update parameters</p>
    <p>Take a batch of pairs, predict relation B</p>
    <p>Update parameters</p>
    <p>The Multi-Task model</p>
    <p>LSTM</p>
    <p>OutRelation A:</p>
    <p>OutRelation B:</p>
    <p>LSTM</p>
    <p>OutRelation A:</p>
    <p>OutRelation B:</p>
  </div>
  <div class="page">
    <p>Comparison Between the Models</p>
    <p>Multi-Label Learning (MLL)</p>
    <p>Single-Task Learning (Single)</p>
    <p>Multi-Task Learning</p>
    <p>LSTM OutRelation A:</p>
    <p>LSTM OutRelation B:</p>
    <p>LSTM</p>
    <p>OutRelation A:</p>
    <p>OutRelation B:</p>
    <p>LSTM</p>
    <p>OutA:</p>
    <p>OutB:</p>
    <p>LSTM</p>
    <p>OutA:</p>
    <p>OutB:</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>means MLL outperforms by a statistically significant margin</p>
    <p>means MLL underperforms by a statistically significant margin</p>
    <p>Multi-Label Learning (MLL) setting has the best performance mostly</p>
    <p>Human Activity dataset (Spearmans correlation)</p>
    <p>SICK dataset (Pearsons correlation)</p>
    <p>Typed-Similarity dataset (Pearsons correlation)</p>
  </div>
  <div class="page">
    <p>Discussion and Conclusion</p>
    <p>Multi-Label Learning is a simple but effective way to approach multirelational semantic similarity tasks</p>
    <p>Learning from one similarity relation helps with learning another</p>
    <p>The idea can be applied to any kind of fine-tuning setting (e.g. graph encoder, language model) used in any multi-label datasets</p>
    <p>Further questions and discussions can be directed to Li Zhang (zharry@umich.edu)</p>
  </div>
</Presentation>
