<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Paralleliza(on Primi(ves for Dynamic Sparse Computa(ons</p>
    <p>Tsung-Han Lin, Steve Tarsa, and H.T. Kung! HotPar13!</p>
    <p>June 24, 2013</p>
  </div>
  <div class="page">
    <p>Dynamic Sparse Computa9ons</p>
    <p>Problem: we want to parallelize sparse computa9ons where the nonzero variables are iden9fied only at run (me</p>
    <p>Important for many applica9ons in signal processing and machine learning</p>
  </div>
  <div class="page">
    <p>Dynamic Sparse Computa9ons Example 1: Compressive Sensing Recovery</p>
    <p>Recover sparse signals from dense compressed measurements</p>
    <p>Sparse signal Compressed measurement</p>
    <p>Random linear combina(ons</p>
    <p>Recovery</p>
  </div>
  <div class="page">
    <p>Compressive Sensing Recovery Example: Dynamic MRI</p>
    <p>3D signal to be recovered is large  E.g., with 40 100x100 MRI 9me frames; signal size is 400K</p>
    <p>[Lus9g 2008]</p>
  </div>
  <div class="page">
    <p>Dynamic Sparse Computa9ons Example 2: Sparse Coding</p>
    <p>Extract sparse representa9ons based on predefined/learned dic9onaries</p>
    <p>Represen(ng image patches as linear combina(ons of a few low-level features</p>
  </div>
  <div class="page">
    <p>Sparse Coding Example: Convolu9onal Neural Networks</p>
    <p>Learning high-level features requires huge datasets for training  E.g., Google trains face detector using 10 million 200x200 images</p>
    <p>Coding Pooling Coding Pooling</p>
    <p>[Yu 2012]</p>
  </div>
  <div class="page">
    <p>A Canonical Example of Dynamic Sparse Computa9on: Solving Under-constrained Linear Systems</p>
    <p>Given x and D, infinitely many solu9ons for z  Suppose D is well-behaved (e.g., sa9sfies RIP), we can recover the correct z by minimizing</p>
    <p>Efficient itera9ve algorithms, such as orthogonal matching pursuit (OMP), are available for recovering z</p>
    <p>x D z</p>
    <p>=</p>
    <p>z 1</p>
    <p>m n</p>
    <p>Measurement vector Data vector</p>
    <p>Sensing matrix Dic(onary</p>
    <p>Target signal Sparse code</p>
  </div>
  <div class="page">
    <p>OMP for Sparse Recovery</p>
    <p>OMP is an itera9ve algorithm, which is greedy, simple, fast  It itera9vely refines the sparse solu9on vector</p>
    <p>Outer loop iden(fies nonzero unknowns and reduces the problem to be over-constrained</p>
    <p>Inner loop es(mates values by solving the over-constrained system via, e.g., least squares</p>
    <p>x D D z z</p>
    <p>= Es(mated nonzero unknowns</p>
    <p>Columns corresponding to nonzero unknowns</p>
  </div>
  <div class="page">
    <p>Parallelizing OMP: Ping-Ponging on a Bipar9te Graph</p>
    <p>DTx Dz</p>
    <p>x1 x2 x3 xm</p>
    <p>z1 z2 z3 zn zj</p>
    <p>x&quot; D&quot; z&quot;</p>
    <p>=&quot;</p>
    <p>Components of z</p>
    <p>Components of x D is the adjacent matrix of the graph</p>
  </div>
  <div class="page">
    <p>Parallelizing OMP: Splibng Graph to Mul9ple Machines</p>
    <p>x1 x2 x3 xm</p>
    <p>z1 z2 z3 zn zj</p>
  </div>
  <div class="page">
    <p>x1 x2 x3 xm</p>
    <p>z1 z2 z3 zn zj</p>
    <p>Ping-Ponging in Outer Loop</p>
    <p>Compute a score for every zi (highlighted) to iden9fy nonzeros</p>
    <p>x1 x2 x3 xm</p>
    <p>z1 z2 z3 zn zj</p>
  </div>
  <div class="page">
    <p>Ping-Ponging in Inner Loop</p>
    <p>x1 x2 x3 xm</p>
    <p>z2</p>
    <p>Select the nonzeros and compute their values using the corresponding subgraph</p>
    <p>(highlighted)</p>
    <p>zj z1 z3 zn</p>
  </div>
  <div class="page">
    <p>Two Paralleliza(on Primi(ves for efficient Parallel Execu9on of Dynamic Sparse Computa9on</p>
    <p>We propose the following two primi9ves for the efficient iden9fica9on of these nonzeros in parallel:  Sta(s(cal barrier to iden9fy nonzero unknowns without having to wait for stragglers</p>
    <p>Selec(ve push-pull to focus computa9ons only on the selected subgraph</p>
    <p>Challenge: For efficiency we must limit computa9on only to the subgraph corresponding to nonzero unknowns, but we dont know them at the outset; they are determined at run 9me</p>
  </div>
  <div class="page">
    <p>Sta9s9cal Barrier</p>
    <p>Con9nue computa9on without wai9ng for the last straggler</p>
    <p>x1 x2 x3 xm</p>
    <p>z1 z2 z3 zn zj</p>
    <p>Finishing a large frac9on of zi is likely to capture sparse nonzeros</p>
    <p>Algorithm is robust to missing values, which can be fixed in the next itera9on E.g. Leave the barrier</p>
    <p>when 80% of zi complete</p>
  </div>
  <div class="page">
    <p>Selec9ve Push-Pull</p>
    <p>Support computa9on on dynamically selected subgraph</p>
    <p>x1 x2 x3 xm</p>
    <p>z2 zj z1 z3 zn</p>
    <p>(1) Select a subset of ac9ve zi (2) zi compute and push update to xi (3) xi compute using incoming updates (4) zi pull updates and con9nue computa9on</p>
    <p>No edge selec(on needed!</p>
    <p>Push Pull</p>
  </div>
  <div class="page">
    <p>Performance Gains of Selec9ve Push-Pull on EC2</p>
    <p>Built selec(ve push- pull on GraphLab</p>
    <p>Vary sparse recovery problem size but with constant sparsity</p>
    <p>O M</p>
    <p>P in</p>
    <p>ne r l</p>
    <p>oo p</p>
    <p>co m</p>
    <p>pu ta</p>
    <p>tio n</p>
    <p>tim e</p>
    <p>(s ec</p>
    <p>)</p>
    <p>Problem size of sparse recovery (N)</p>
    <p>In ne</p>
    <p>r lo op</p>
    <p>co m pu</p>
    <p>ta 9o</p>
    <p>n 9m</p>
    <p>e (s ec )</p>
    <p>Sparse Recovery Problem size</p>
    <p>Selec(ve push-pull</p>
    <p>O M</p>
    <p>P in</p>
    <p>ne r l</p>
    <p>oo p</p>
    <p>co m</p>
    <p>pu ta</p>
    <p>tio n</p>
    <p>tim e</p>
    <p>(s ec</p>
    <p>)</p>
    <p>Problem size of sparse recovery (N)</p>
    <p>Sparse Recovery Problem size</p>
    <p>In ne</p>
    <p>r lo op</p>
    <p>co m pu</p>
    <p>ta 9o</p>
    <p>n 9m</p>
    <p>e (s ec )</p>
    <p>Sta(c graph</p>
    <p>Computa(on (me grows due to unnecessary computa(on</p>
    <p>Computa(on (me remains constant for constant subgraph size</p>
  </div>
  <div class="page">
    <p>Performance Gains of Sta9s9cal Barrier in Simula9on</p>
    <p>Straggler stats from MSs Bing cluster: 25% jobs see high prop. of stragglers, up to 10x median comple9on 9me</p>
    <p>95% Barrier trims worst stragglers  improves average (me by 2.5x, and worst case by 4x over rigid</p>
    <p>75% is too aggressive, and extra itera(ons hurt 0</p>
    <p>C D</p>
    <p>F</p>
    <p>OMP Completion Time (sec)</p>
    <p>p=100% p=95% p=75%</p>
    <p>CD F</p>
  </div>
  <div class="page">
    <p>Parallel Ping-Pong Applicable to Other Applica9ons, e.g., Dic9onary Learning</p>
    <p>for Feature Extrac9on</p>
    <p>Learn an overcomplete dic9onary that can represent data vectors using only a few atoms</p>
    <p>K-SVD alternates between op9mizing Z and D</p>
    <p>X D Z</p>
    <p>m n</p>
    <p>p</p>
  </div>
  <div class="page">
    <p>Express Dic9onary Learning Using Graphical Model</p>
    <p>d1 d2 d3 dm</p>
    <p>z1 z2 z3 zp zj</p>
    <p>z1 z2 zp z3 ..</p>
    <p>d1 d2</p>
    <p>dm</p>
    <p>X</p>
    <p>Sparse representa(on for every data vector</p>
    <p>Dic(onary atoms</p>
  </div>
  <div class="page">
    <p>Express Dic9onary Learning Using Graphical Model</p>
    <p>Update Z: compute sparse code for every data vector  Update D: for a given atom, op9mize using associated data vectors (has nonzero coefficient for the atom)</p>
    <p>d1 d2 d3 dm</p>
    <p>z1 z2 z3 zp zj</p>
    <p>z1 z2 zp z3 ..</p>
    <p>d1 d2</p>
    <p>dm</p>
    <p>X</p>
    <p>Sparse representa(on for every data vector</p>
    <p>Dic(onary atoms</p>
  </div>
  <div class="page">
    <p>Express Dic9onary Learning Using Graphical Model</p>
    <p>Update Z: compute sparse code for every data vector  Update D: for a given atom, op9mize using associated data vectors (has nonzero coefficient for the atom)</p>
    <p>d1 d2 d3 dm</p>
    <p>z2 zj z1 z3 zp</p>
    <p>z1 z2 zp z3 ..</p>
    <p>d1 d2</p>
    <p>dm</p>
    <p>X</p>
    <p>Sparse representa(on for every data vector</p>
    <p>Dic(onary atoms</p>
    <p>Sta(s(cal barrier to skip stragglers</p>
    <p>Selec(ve push-pull to ac(vate only associated data ver(ces</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We have iden9fied an important class of dynamic sparse computa9ons on bipar9te graphs</p>
    <p>This class of computa9ons can benefit from a flexible execu9on model supported by two new primi9ves  Sta9s9cal barrier  Selec9ve push-pull</p>
    <p>There are important applica9ons for machine learning and signal processing</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Express Dic9onary Learning Using Graphical Model</p>
    <p>Update z: compute sparse code for every data vector  Update d: for a given atom, op9mize using associated data vectors (has nonzero coefficient for the atom)</p>
    <p>d1 d2 d3 dm</p>
    <p>z1 z2 z3 zp zj</p>
    <p>z1 z2 zp z3 ..</p>
    <p>d1 d2</p>
    <p>dm</p>
    <p>X =</p>
    <p>Sparse representa(on for every data vector</p>
    <p>Dic(onary atoms</p>
    <p>Sta(s(cal barrier to skip stragglers</p>
    <p>Selec(ve push-pull to ac(vate only associated data ver(ces</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>for every edge gather(); end apply(); for every edge scaaer(); end</p>
    <p>Vertex-based program Scheduler</p>
    <p>Task-level parallelism</p>
    <p>Pipelined Execu9on Model</p>
    <p>for every edge gather(); end</p>
    <p>for every edge gather(); end</p>
    <p>apply_1();</p>
    <p>apply_1();</p>
    <p>apply_2();</p>
    <p>apply_2();</p>
    <p>Vertex 1</p>
    <p>Vertex 2</p>
    <p>Vertex 1</p>
    <p>Vertex 2</p>
    <p>Vertex 1</p>
    <p>Vertex 2</p>
    <p>Fine-grain execu(on to facilitate memory reuse, data-parallel SIMD opera(ons, etc.</p>
  </div>
</Presentation>
