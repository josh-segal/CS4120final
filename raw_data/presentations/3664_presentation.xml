<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Stolen Memories Leveraging Model Memorization for</p>
    <p>Calibrated White-Box Membership Inference Klas Leino* &amp; Matt Fredrikson | Carnegie Mellon University</p>
  </div>
  <div class="page">
    <p>Membership Inference</p>
    <p>Basic Idea: adversary tries to predict whether a given point was part of a target models training set</p>
    <p>Why do we care about MI?  Membership may itself be sensitive/private  Serves as a practical measure of information leakage  Directly related to differential privacy</p>
    <p>auxiliary data even prior training algorithm</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Shokri et al. 2016 Membership Inference Attacks Against Machine Learning Models</p>
    <p>Yeom et al. 2017 The Unintended Consequences of Overfitting: Training Data Inference Attacks</p>
    <p>Long et al. 2017 Towards Measuring Membership Privacy</p>
    <p>Long et al. 2018 Understanding Membership Inferences on Well-generalized Learning Models</p>
    <p>Salem et al. 2019 Model and Data Independent Membership Inference Attacks on Machine Learning Models</p>
    <p>Nasr et al. 2018 Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning Under Passive and Active White-box Inference Attacks</p>
    <p>Bl ac</p>
    <p>kbo</p>
    <p>x W</p>
    <p>hi te</p>
    <p>-b ox</p>
  </div>
  <div class="page">
    <p>Can We do Better with White-box Access?</p>
    <p>Model parameters might leak significantly more information; can we leverage them?</p>
    <p>Yes!  We show how to explicitly identify memorization in deep networks, and weaponize it for</p>
    <p>membership inference  We find our attacks can be calibrated for increased confidence in positive inferences  We evaluate the practicality of popular defenses against our attack</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Privacy in Deep Learning  Understanding Overfitting  White-box Membership Inference  Key Results</p>
  </div>
  <div class="page">
    <p>Hypothesis: idiosyncratic feature use provides evidence of membership</p>
    <p>How Does Overfitting Manifest Itself?</p>
    <p>Celebrity A</p>
    <p>Celebrity B</p>
    <p>training set</p>
    <p>training set</p>
    <p>both celebrities A and B have sunglasses in 25% of instances</p>
    <p>celebrity B has sunglasses in 25% of training instances</p>
    <p>celebrity A has sunglasses in 50% of training instances sunglasses are predictive in</p>
    <p>training set, so the model may learn to encode them</p>
    <p>A</p>
    <p>because sunglasses influence prediction of celebrity A, we infer sunglasses were more common in the training set than average for celebrity A</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Sample of LFW training instances</p>
    <p>Typical explanations on test instances of Tony Blair</p>
    <p>Explanation on training instance of Tony Blair with distinctive pink background. The model uses the background to classify the instance as Tony Blair.</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>Membership information is leaked through the target models idiosyncratic use of features. Features that are distributed differently in the training data from how they are distributed in the general population provide evidence for or against membership.</p>
    <p>Next we would like to formalize this intuition</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Privacy in Deep Learning  Understanding Overfitting  White-box Membership Inference  Bayes-optimal Membership Inference  Extending to Deep Models</p>
    <p>Key Results</p>
  </div>
  <div class="page">
    <p>Formal Intuition</p>
    <p>Consider two normal distributions,  and    is more likely to have been drawn from</p>
    <p>than from  if Pr &quot; [] &lt; Pr</p>
    <p>#&quot; []</p>
    <p>We can make a classifier that predicts whether  was drawn from  or</p>
    <p>( )</p>
    <p>general population training set</p>
    <p>weights from target model convey relevant information about training distribution</p>
    <p>An attacker wont be able to know or</p>
  </div>
  <div class="page">
    <p>Bayes-optimal Membership Inference</p>
    <p>Assume the data is Gaussian and satisfies the nave Bayes assumption  Model the training set as a Gaussian distribution using its empirical mean and covariance  Assume the target model is a linear model</p>
    <p>we can analytically derive the Bayes-optimal MI attack from the distribution parameters</p>
    <p>we can analytically derive the optimal weights of a linear classifier from the distribution parameters</p>
    <p>combining these, we can get the Bayes-optimal MI attack in terms of only the weights of the model</p>
  </div>
  <div class="page">
    <p>Summary of Attack on Linear Models</p>
    <p>Train a proxy model on the auxiliary data  Compare the weights of the proxy model to those of the target model to create an</p>
    <p>attack model  Features that are used differently in the target model from in the proxy model are</p>
    <p>used to determine membership</p>
  </div>
  <div class="page">
    <p>Layer-wise Attacks</p>
    <p>We can apply the same principle to a layer of a deep network  We take a slice of the network, : two functions,  and ,</p>
    <p>such that  =      computes features   classifies using these features</p>
    <p>target model and proxy model must share the same fixed internal representation, i.e., ! = $</p>
    <p>we can use a local linear approximation of ! and # to apply the linear attack described previously to the remainder of the network</p>
    <p>()</p>
    <p>!</p>
  </div>
  <div class="page">
    <p>Combining Layers</p>
    <p>We can train one such attack for each layer of the model and combine the results using a meta attack model*</p>
    <p>*see paper for mor details</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Privacy in Deep Learning  Understanding Overfitting  White-box Membership Inference  Key Results  Performance  Differential Privacy</p>
  </div>
  <div class="page">
    <p>Key Results</p>
    <p>Outperforms black-box attacks  Ability to calibrate for precision  Capable of high-precision inferences even on models that generalize well</p>
    <p>Dataset Generalization Error Our Precision MNIST 1% 75%</p>
  </div>
  <div class="page">
    <p>Differential Privacy</p>
    <p>Formal Guarantee*: -differential privacy guarantees that no adversary can achieve an accuracy greater than &amp;/2</p>
    <p>note that this implies DP gives no meaningful guarantee if   ln 2  0.69</p>
    <p>*result from Yeom et al. 2017</p>
  </div>
  <div class="page">
    <p>Is Differential Privacy a Good Defense?</p>
    <p>Small  reduces attack accuracy, but is costly in terms of model utility  Large  may reduce attack accuracy, but not always</p>
    <p>At ta</p>
    <p>ck A</p>
    <p>cc ur</p>
    <p>ac y</p>
    <p>Labeled Faces in the Wild</p>
    <p>no de</p>
    <p>fen se</p>
    <p>= 0.</p>
    <p>= 4.</p>
    <p>(m od</p>
    <p>el ac</p>
    <p>c.)</p>
    <p>% 20 %</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>What else is in the paper?  Detail on analysis  More sophisticated variants of our attack  Additional experimental results  Evaluation of other defenses, e.g., regularization</p>
    <p>Key Takeaways  Membership information is leaked through sensitive</p>
    <p>feature usage  Using white-box information we can improve upon the</p>
    <p>state-of-the art MI attacks  Large--DP may not confer significant privacy protection</p>
  </div>
  <div class="page">
    <p>Thank You! contact kleino@cs.cmu.edu with questions</p>
  </div>
</Presentation>
