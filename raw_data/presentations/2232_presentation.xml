<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Respecting the block interface  computational storage using virtual objects</p>
    <p>Ian F. Adams, John Keys, Michael P. Mesnier</p>
    <p>Intel Labs</p>
  </div>
  <div class="page">
    <p>A brief history of computational storage</p>
    <p>Intel Labs 2</p>
    <p>Active Disks Seagate Kinetic</p>
    <p>HadoopAssociative Memory (1950s)</p>
    <p>Database Machines</p>
    <p>Simple concept with a long history  Move the compute to the data</p>
    <p>Associative memory, database machines, active disks, key-value HDD</p>
    <p>Why didnt it gain widespread adoption?  Short version: wasnt quite worth it until now</p>
  </div>
  <div class="page">
    <p>Whats changed?</p>
    <p>Very high density, high-performance storage is here  16-32 TB drives are here, 100+TB SSDs are coming</p>
    <p>1PB in a 1U server</p>
    <p>All this behind NICs, I/O controllers, devices, etc.</p>
    <p>Large scale disaggregated block storage is here (NVMeoF)  Enables diskless storage stacks  Greater flexibility, but yet more I/O traffic</p>
    <p>Devices and targets are more powerful  More flexibility and headroom to work with</p>
    <p>(also, were Intel and like hardware )</p>
    <p>Intel Labs 3</p>
    <p>I/O</p>
    <p>Fast server</p>
    <p>Block management SW (maps data objects to blocks)</p>
    <p>Storage application (DB, FS, object store, KV, ...)</p>
    <p>READ block WRITE block</p>
  </div>
  <div class="page">
    <p>Moving compute into storage</p>
    <p>(to avoid an I/O bottleneck)</p>
    <p>Intel Labs 4Intel confidential</p>
  </div>
  <div class="page">
    <p>Moving compute into storage</p>
    <p>Step 1. Teach the storage about data objects</p>
    <p>Files, objects, DB records, key-value pairs,</p>
    <p>Step 2. Provide a way to program storage (API)</p>
    <p>Step 3. Implement compute methods in storage</p>
    <p>E.g., search, compress, checksum, resize,</p>
    <p>Intel Labs 5</p>
    <p>Object or file-based storage makes this process straightforward</p>
    <p>BUT, storage is fundamentally *still* built on blocks!</p>
  </div>
  <div class="page">
    <p>Challenge 1: Moving compute into storage</p>
    <p>Intel Labs 6</p>
    <p>block ^</p>
    <p>Intel confidential</p>
  </div>
  <div class="page">
    <p>Object Awareness</p>
    <p>Recall Step 1: Teach storage about objects  Constraint: we need to talk block storage</p>
    <p>Prior experience makes us leery of changing low-level storage interfaces</p>
    <p>E.g., uphill battle for KV drives</p>
    <p>Can we make block storage object aware without  Changing the interface  Adding a lot of state and complexity</p>
    <p>We need to consider  Host and target data consistency, input vs output, non</p>
    <p>sector aligned data, transport considerations (bidirectional transfers), chained operations, permissions</p>
    <p>Intel Labs 7</p>
    <p>Server</p>
    <p>foo.txt Hello, world!</p>
    <p>Sector 13 Hello,</p>
    <p>Sector 42 world!</p>
    <p>File system (foo.,txt  13 + 42)</p>
  </div>
  <div class="page">
    <p>Introducing virtual objects (step 1 of 3)</p>
    <p>Virtual object:  An ephemeral mapping of blocks to make block storage object aware</p>
    <p>Dont have to turn block storage into object storage  Stateless: mapping is only valid for duration of an operation  Can be used for both input and output</p>
    <p>Complementary to existing stacks built on block storage  Object, KV store, file, etc.</p>
    <p>Intel Labs 8</p>
    <p>VIRTUAL_OBJ: EXT 1: LBA 2008 LEN 4096 EXT 2: LBA 4104 LEN 123 TOTAL_LEN: 4219</p>
    <p>FIEMAP + Stat</p>
    <p>/home/user/foo.txt</p>
    <p>This is step 1: teach the block storage about objects</p>
  </div>
  <div class="page">
    <p>Programmability (step 2 of 3)</p>
    <p>Virtual objects are embedded in compute descriptors  Add arguments and operations for computing inside block storage  Can have multiple input and output virtual objects</p>
    <p>Descriptors are block-protocol compatible!  For SCSI and NVME, works as a vendor specific EXEC command  Small results can be returned as a payload, larger results written to output objects</p>
    <p>Intel Labs 9</p>
    <p>VIRTUAL_OBJ: EXT 1: LBA 2008 LEN 4096 EXT 2: LBA 4104 LEN 123 TOTAL_LEN: 4219</p>
    <p>OPCODE: search ARG: baz</p>
    <p>Compute Descriptor</p>
    <p>This is step 2: provides a way to program storage</p>
  </div>
  <div class="page">
    <p>Implementing offloads (step 3of 3)</p>
    <p>Object Aware Storage (OAS) Library handles host/app interactions  Cache consistency  Creating and allocating virtual objects  Building and transporting compute descriptors</p>
    <p>Offload Engine: interprets EXEC command an descriptors  Implement our methods like checksum, search, etc.</p>
    <p>Intel Labs 10</p>
    <p>OAS Library Offload Engine</p>
    <p>Storage Transport (SCSI or NVMe)</p>
    <p>Host Target</p>
    <p>This is step 3: provides a way to implement operations</p>
  </div>
  <div class="page">
    <p>Prototype Architecture + Flow</p>
    <p>Built using iSCSI and NVMeoF initiators and targets</p>
    <p>Intel Labs 11</p>
    <p>Host</p>
    <p>Application</p>
    <p>File System</p>
    <p>Initiator</p>
    <p>File I/O Offload request</p>
    <p>Page Cache</p>
    <p>EXEC</p>
    <p>fiemap stat</p>
    <p>fsync</p>
    <p>OAS Library</p>
    <p>Offload Engine</p>
    <p>Block Storage Stack</p>
    <p>Target</p>
    <p>Transport (NVMeoF, iSCSI)</p>
    <p>Virtual object creation, request issuing, cache consistency</p>
    <p>Unmodified initiator stack</p>
    <p>EXEC command &amp; operation handling</p>
  </div>
  <div class="page">
    <p>Intel Labs 12</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Intel Labs 13</p>
    <p>Experimental setup</p>
    <p>2 servers connected via 40 GbE  Target and Host: Dual Xeon Gold 6140s, Dual Xeon E5-2699 v3s</p>
    <p>Runs NVMeoF stack, handles offloads</p>
    <p>8 P4600 NVMe SSDs (~3 GB/s per drive)  Benchmark:</p>
    <p>OASBench (in-house benchmarking utility)  100 16 MB files per SSD, 48 worker threads</p>
    <p>Focused on checksum offload  Bitrot detection for object storage  Modern hashes are I/O bound</p>
    <p>Intel Labs 13</p>
    <p>Host (OASBench)</p>
    <p>Target40 GbE NVMeoF</p>
  </div>
  <div class="page">
    <p>Experiment 1: Conventional Access</p>
    <p>Read file/object data from target to host, and compute checksum  Expect to be bottlenecked by the 40 GbE link</p>
    <p>Intel Labs 14</p>
    <p>Host (OASBench) Target</p>
    <p>Checksum</p>
    <p>Read</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>Conventional operations</p>
    <p>Conventional operations: data is pulled to the host before computation  Quickly bottlenecked by 40 GbE network  &lt;2 SSDs worth of throughput</p>
    <p>Intel Labs 15</p>
    <p>C h</p>
    <p>e ck</p>
    <p>su m</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t M</p>
    <p>B /S</p>
    <p>e c</p>
    <p>Num. SSDs Read In Parallel</p>
    <p>Conv. Hhash Conv. CRC Conv. ISAL-CRC 40 GbE Max Drive Throughput</p>
  </div>
  <div class="page">
    <p>Experiment 2: Offloaded Access</p>
    <p>Issue EXEC command with virtual objects  Target computes checksum in-situ and returns digest  Network bottlenecks should go away</p>
    <p>Intel Labs 16</p>
    <p>Host (OASBench) Target</p>
    <p>Checksum</p>
    <p>EXEC</p>
    <p>Checksum Digest</p>
  </div>
  <div class="page">
    <p>Offloaded operations</p>
    <p>Offloaded operations are run in the storage target  Bypasses the 40 GbE bottleneck and scales with the number of SSDs being hit  40 GbE link bypass even what could be provided from 100 GbE!</p>
    <p>No longer transport bound!</p>
    <p>&gt;99% reduction in network traffic, along with up to 3x speedups (Not shown)  Implemented in Ceph, Swift and MinIO</p>
    <p>Intel Labs 17</p>
    <p>C h</p>
    <p>e ck</p>
    <p>su m</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t M</p>
    <p>B /S</p>
    <p>e c</p>
    <p>Num. SSDs Read In Parallel</p>
    <p>OffLd. Hhash OffLd. CRC OffLd. ISAL-CRC 40 GbE Max Drive Throughput 100 GbE</p>
  </div>
  <div class="page">
    <p>Intel Labs 18</p>
    <p>Challenge 2: Handling Distributed, Striped Data</p>
  </div>
  <div class="page">
    <p>Computational Storage and EC</p>
    <p>Trends in Data Striping</p>
    <p>Erasure coded (EC) deployments have exploded beyond traditional RAID  RAID chunks in low bytes to KiB ranges</p>
    <p>Very difficult to offload computations</p>
    <p>EC chunks in hundreds of KiB to low MiB</p>
    <p>Individual elements easily found</p>
    <p>Large volumes of data have well defined structure and elements  E.g., CSVs, JSONs, dense matrices, etc.</p>
    <p>Intel Labs 19</p>
    <p>H E R E I S D A T A</p>
    <p>H E R E I S D A T A</p>
  </div>
  <div class="page">
    <p>Our Solution</p>
    <p>Our solution is to leverage data structure and large stripe pieces  Most work still done inside target  Ambiguous border elements returned as residuals handled host-side</p>
    <p>Intel Labs 20</p>
    <p>The quick brown fox jumped over the lazy dog</p>
    <p>Match: 0-2 No Match Partial: 32 t Partial: 33-34 he</p>
    <p>the==the Match!</p>
    <p>Results: Match: 0-2</p>
    <p>Match: 32-34</p>
  </div>
  <div class="page">
    <p>Ongoing and Future Work</p>
    <p>Lots of other offloads (not enough time to cover)</p>
    <p>Image preprocessing for ML pipelines  &gt;90% data movement reduction</p>
    <p>Merge, Sort, Search, LSM Compaction, CSV queries, microclassifiers</p>
    <p>Were not just for fabrics targets</p>
    <p>Methodology is compatible with devices as well</p>
    <p>Industry involvement and engagement</p>
    <p>Intel Labs 21</p>
  </div>
  <div class="page">
    <p>Wrapping it Up!</p>
    <p>Introduced virtual objects for computational block storage  Prototypes in iSCSI and NVMeoF with a variety of offloads</p>
    <p>Showed that handling distributed, striped data can be straightforward with large EC shards and (semi) structured data</p>
    <p>We want collaborators!  Working on open sourcing</p>
    <p>Stay tuned for more updates from Intel</p>
    <p>Intel Labs 22</p>
  </div>
  <div class="page">
    <p>Thanks for your attention!</p>
    <p>Questions? Comments? ian.f.adams@intel.com</p>
    <p>john.keys@intel.com</p>
    <p>michael.mesnier@intel.com</p>
    <p>Intel Labs 23</p>
  </div>
  <div class="page">
    <p>Extras/Backups</p>
    <p>Intel Labs 24</p>
  </div>
  <div class="page">
    <p>Applications are easy to adapt and enable</p>
    <p>Application integration isnt difficult</p>
    <p>Example with our Golang bindings using iSCSI</p>
    <p>Client library is small  (&lt; 500 LOC)</p>
    <p>New offloads are straightforward</p>
    <p>Currently a combination of C libraries and kernel modules</p>
    <p>Currently porting to full userspace implementations</p>
    <p>Intel Labs 25</p>
    <p>/*path to talk to the scsi device*/</p>
    <p>sgpath := &quot;/dev/bsg/20:0:0:0&quot;</p>
    <p>/*Target file for operating on*/</p>
    <p>fpath := /mnt/oas_dev/test.txt&quot;</p>
    <p>/*Create the OAS Context*/</p>
    <p>ctx := oas_client.OasCtx{sgpath}</p>
    <p>/*Call MD5 method*/</p>
    <p>oas_md5_resp := ctx.MD5(fpath)</p>
  </div>
</Presentation>
