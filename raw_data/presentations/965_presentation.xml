<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multicore Locks: The Case is not Closed Yet</p>
    <p>Hugo Guiroux, Renaud Lachaize, Vivien Quema</p>
    <p>June 24, 2016</p>
    <p>Universite Grenoble Alpes</p>
    <p>Grenoble INP</p>
  </div>
  <div class="page">
    <p>Synchronization on Modern</p>
    <p>Multicore Machines</p>
  </div>
  <div class="page">
    <p>Synchronization</p>
    <p>Most multi-threaded applications require synchronization.  As the number of cores increases, the synchronization</p>
    <p>primitives become a bottleneck.</p>
    <p>The design of efficient multicore locks is still a hot research topic: (e.g., [ASPLOS10], [ATC12], [OLS12], [PPoPPP12],</p>
    <p>[SOSP13], [OOPSLA14], [PPoPP15], [PPoPP16]).</p>
  </div>
  <div class="page">
    <p>Pthread Mutex Lock</p>
    <p>Lock-based synchronization:</p>
    <p>pthread mutex lock(&amp;mutex);</p>
    <p>// Critical section:</p>
    <p>// at most 1 thread here at</p>
    <p>// a time</p>
    <p>...</p>
    <p>pthread mutex unlock(&amp;mutex);</p>
    <p>Plethora of locking algorithms.</p>
    <p>Goals:</p>
    <p>Performance  Throughput: at high</p>
    <p>contention</p>
    <p>Latency: at low contention</p>
    <p>Fairness  Energy efficiency</p>
  </div>
  <div class="page">
    <p>Pthread Mutex Lock</p>
    <p>Lock-based synchronization:</p>
    <p>pthread mutex lock(&amp;mutex);</p>
    <p>// Critical section:</p>
    <p>// at most 1 thread here at</p>
    <p>// a time</p>
    <p>...</p>
    <p>pthread mutex unlock(&amp;mutex);</p>
    <p>Plethora of locking algorithms.</p>
    <p>Goals:</p>
    <p>Performance  Throughput: at high</p>
    <p>contention</p>
    <p>Latency: at low contention</p>
    <p>Fairness  Energy efficiency</p>
  </div>
  <div class="page">
    <p>Problem Statement I</p>
    <p>Applications suffer from lock contention</p>
    <p>Plethora of locks algorithms</p>
    <p>Developers are puzzled:  Does it really matters for my application/my setup?  How to choose?  Will the chosen lock perform reasonably well on most setups?  Should we simply discard old/simple locks?</p>
  </div>
  <div class="page">
    <p>Problem Statement II</p>
    <p>Previous studies:  Are mostly based on microbenchmarks . . .  . . . or on workloads for which a new lock was specifically</p>
    <p>designed</p>
    <p>Do not consider state-of-the-art algorithms that are known to perform well (e.g., recent hierarchical locks) or important</p>
    <p>parameters (e.g., the choice of waiting policy)</p>
  </div>
  <div class="page">
    <p>Contributions</p>
  </div>
  <div class="page">
    <p>Contributions</p>
  </div>
  <div class="page">
    <p>Contributions</p>
  </div>
  <div class="page">
    <p>Contributions</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>With/without pinning</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>With/without pinning</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Locks Algorithms</p>
    <p>LiTL: Library for Transparent Lock Interposition</p>
    <p>Study of lock/application behavior</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Taxonomy of Multicore Locks Algorithms I</p>
    <p>Flat Algorithms</p>
    <p>e.g., Spinlock, Backoff  Principle:</p>
    <p>Loop on a single memory address  Use atomic instruction</p>
    <p>Pros:  Very fast under low lock contention</p>
    <p>Cons:  Collapse under high contention due to cache coherence traffic</p>
  </div>
  <div class="page">
    <p>Taxonomy of Multicore Locks Algorithms II</p>
    <p>Queue-Based Algorithms</p>
    <p>e.g., MCS, CLH  Principle:</p>
    <p>List of waiting threads  Each thread spins on a private variable</p>
    <p>Pros:  Mitigation of cache invalidations  Fairness</p>
    <p>Cons:  Inefficient lock handover if successor has been descheduled  Memory locality issue (lack of NUMA awareness)</p>
  </div>
  <div class="page">
    <p>Taxonomy of Multicore Locks Algorithms III</p>
    <p>Hierarchical Algorithms</p>
    <p>e.g., Cohort locks, AHMCS  Principle:</p>
    <p>One local lock per NUMA node + one global lock  Per-node batching</p>
    <p>Pros:  Good behavior on NUMA hierarchies under high contention</p>
    <p>Cons:  Short-time unfairness  High costs under low lock contention</p>
  </div>
  <div class="page">
    <p>Taxonomy of Multicore Locks Algorithms IV</p>
    <p>Load-control Algorithms</p>
    <p>e.g., MCS-TimePub, Malthusian locks  Principle:</p>
    <p>Bypass threads in the waiting list  Reduce the number of threads trying to acquire the lock</p>
    <p>Pros:  Better resilience under resource contention</p>
    <p>Cons:  Fairness</p>
  </div>
  <div class="page">
    <p>Taxonomy of Multicore Locks Algorithms V</p>
    <p>Delegation-Based Algorithms</p>
    <p>e.g., RCL, CC-Synch  Principle:</p>
    <p>One thread executes the critical section on behalf of the others  Not general purpose, designed for highly contended locks</p>
    <p>Not considered here:  Need to rewrite the code application  Does not support thread-local data, nested locking, . . .</p>
  </div>
  <div class="page">
    <p>Waiting Policy</p>
    <p>Another design dimension (for most locks)  What should a thread do while waiting for a lock?</p>
    <p>Park: sleep (default Pthread policy)  Spin: busy-wait (active)  Spin-Then-Park: spin a little, then go to sleep</p>
  </div>
  <div class="page">
    <p>LiTL: Library for Transparent Lock</p>
    <p>Interposition</p>
  </div>
  <div class="page">
    <p>LiTL: Overview</p>
    <p>Motivation  Implementing all existing locks into all applications is laborious  No existing library to try a lock implementation easily</p>
    <p>LiTL: lock library on top of Pthread Mutex lock API  Support unmodified application via library interposition  Supports condition variables  Supports nested critical sections  27 locks (easy to add new ones)</p>
    <p>https://github.com/multicore-locks/litl</p>
  </div>
  <div class="page">
    <p>LiTL Design Challenges: Lock Context</p>
    <p>Many lock algorithms rely on contexts  The Pthread Mutex lock API does not consider contexts</p>
    <p>Solution:  Each lock instance comes with an array of contexts, with one</p>
    <p>entry per thread to support nested critical sections</p>
    <p>Pthread Mutex lock  custom lock via hash table (CLHT)</p>
  </div>
  <div class="page">
    <p>LiTL Design Challenges: Supporting Condition Variables</p>
    <p>Approach: reuse Pthread Condition variable 1. Take an uncontended Pthread lock with the optimized lock</p>
  </div>
  <div class="page">
    <p>Overhead Evaluation</p>
    <p>Comparison with manual implementation of all locks on 3 lock-intensive applications</p>
    <p>General trends are preserved  Average performance difference is below 5%</p>
  </div>
  <div class="page">
    <p>Study of lock/application behavior</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>5% tolerance margin to take into account deviation  Optimal lock: best or at most 5% of performance degradation</p>
    <p>of the best</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>5% tolerance margin to take into account deviation  Optimal lock: best or at most 5% of performance degradation</p>
    <p>of the best</p>
    <p>Linux (3.17.6)  3 machines</p>
    <p>A-64: AMD 64 cores, 8 nodes  A-48: AMD 48 cores, 8 nodes  I-48: Intel 48 cores, 4 nodes (no hyperthreading)</p>
    <p>Most results presented here are from the A-64 machine  We vary the number of threads used to launch the</p>
    <p>applications.</p>
  </div>
  <div class="page">
    <p>Lock-Sensitive Applications</p>
    <p>60% of the studied applications are lock sensitive</p>
    <p>s ra</p>
    <p>yt ra</p>
    <p>ce ll</p>
    <p>fa ce</p>
    <p>sim</p>
    <p>ra di</p>
    <p>os ity</p>
    <p>ll</p>
    <p>ss l p</p>
    <p>ro xy</p>
    <p>s ra</p>
    <p>yt ra</p>
    <p>ce</p>
    <p>fe rr</p>
    <p>et</p>
    <p>st re</p>
    <p>am cl</p>
    <p>us te</p>
    <p>r</p>
    <p>st re</p>
    <p>am cl</p>
    <p>us te</p>
    <p>r ll</p>
    <p>de du</p>
    <p>p</p>
    <p>m at</p>
    <p>rix m</p>
    <p>ul tip</p>
    <p>ly</p>
    <p>pc a</p>
    <p>ll</p>
    <p>m ys</p>
    <p>ql d vi</p>
    <p>ps pc a</p>
    <p>flu id</p>
    <p>an im</p>
    <p>at e</p>
    <p>lin ea</p>
    <p>r re</p>
    <p>gr es</p>
    <p>sio n</p>
    <p>vo lre</p>
    <p>nd</p>
    <p>w at</p>
    <p>er sp</p>
    <p>at ia</p>
    <p>l</p>
    <p>oc ea</p>
    <p>n cp</p>
    <p>oc ea</p>
    <p>n nc</p>
    <p>p</p>
    <p>w at</p>
    <p>er ns</p>
    <p>qu ar</p>
    <p>ed</p>
    <p>ra di</p>
    <p>os ity</p>
    <p>fm m</p>
    <p>ba rn</p>
    <p>es</p>
    <p>hi st</p>
    <p>og ra</p>
    <p>m</p>
    <p>w or</p>
    <p>d co</p>
    <p>un t fft</p>
    <p>st rin</p>
    <p>g m</p>
    <p>at ch</p>
    <p>bo dy</p>
    <p>tr ac</p>
    <p>k</p>
    <p>km ea</p>
    <p>ns</p>
    <p>sw ap</p>
    <p>tio ns</p>
    <p>lu nc</p>
    <p>b ra</p>
    <p>di x x2</p>
    <p>ca nn</p>
    <p>ea l</p>
    <p>fr eq</p>
    <p>m in</p>
    <p>e lu</p>
    <p>cb</p>
    <p>bl ac</p>
    <p>ks ch</p>
    <p>ol es</p>
    <p>p ra</p>
    <p>yt ra</p>
    <p>ce</p>
    <p>R el</p>
    <p>a ti</p>
    <p>v e</p>
    <p>S ta</p>
    <p>n d</p>
    <p>ar d</p>
    <p>D ev</p>
    <p>ia ti</p>
    <p>o n</p>
    <p>a t</p>
    <p>M a</p>
    <p>x N</p>
    <p>o d</p>
    <p>es</p>
    <p>Locks impact application performance 18</p>
  </div>
  <div class="page">
    <p>Impact of the Number of Nodes</p>
    <p>We consider 2 configurations per application:</p>
    <p>Maximum number of nodes: use all cores of the machine  Optimized number of nodes: take the number of nodes for a</p>
    <p>given lock/application maximizing performance</p>
    <p>not all locks have the same optimized number of nodes  avoid performance collapse  take the best of each lock</p>
    <p>Number of nodes impacts lock performance</p>
  </div>
  <div class="page">
    <p>How Much do Locks Impact Applications?</p>
    <p>At 1 node, reduced impact  From 2% to 683%:</p>
    <p>avg. 4%, med. 7%</p>
    <p>At max nodes, huge impact  From 42% to 3343%:</p>
    <p>avg. 727%, med. 91%</p>
    <p>At opt nodes, significant impact  From 6% to 683%:</p>
    <p>avg. 105%, med. 17%</p>
  </div>
  <div class="page">
    <p>Are Some Locks Always Among the Best? I</p>
    <p>ah m</p>
    <p>cs</p>
    <p>al oc</p>
    <p>kls</p>
    <p>ba ck</p>
    <p>off</p>
    <p>cb om</p>
    <p>cs sp</p>
    <p>in</p>
    <p>cb om</p>
    <p>cs st</p>
    <p>p cl</p>
    <p>hls</p>
    <p>cl h</p>
    <p>sp in</p>
    <p>cl h</p>
    <p>st p</p>
    <p>cpt</p>
    <p>l-t kt</p>
    <p>ctk</p>
    <p>ttk</p>
    <p>t hm</p>
    <p>cs</p>
    <p>ht ic</p>
    <p>ke t</p>
    <p>ls</p>
    <p>m al</p>
    <p>th sp</p>
    <p>in</p>
    <p>m al</p>
    <p>th st</p>
    <p>p</p>
    <p>m cs</p>
    <p>-ls</p>
    <p>m cs</p>
    <p>sp in</p>
    <p>m cs</p>
    <p>st p</p>
    <p>m cs</p>
    <p>-t im</p>
    <p>ep ub</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>ed</p>
    <p>pt hr</p>
    <p>ea d</p>
    <p>pt hr</p>
    <p>ea da</p>
    <p>da pt</p>
    <p>sp in</p>
    <p>lo ck</p>
    <p>sp in</p>
    <p>lo ck</p>
    <p>-ls</p>
    <p>tic ke</p>
    <p>t</p>
    <p>tic ke</p>
    <p>tls</p>
    <p>tt as</p>
    <p>tt as</p>
    <p>-ls</p>
    <p>Fraction of lock-sensitive applications for which a given lock is optimal</p>
    <p>At 1 node, no always-winning lock 80% coverage</p>
  </div>
  <div class="page">
    <p>Are Some Locks Always Among the Best? II</p>
    <p>ah m</p>
    <p>cs</p>
    <p>al oc</p>
    <p>kls</p>
    <p>ba ck</p>
    <p>off</p>
    <p>cb om</p>
    <p>cs sp</p>
    <p>in</p>
    <p>cb om</p>
    <p>cs st</p>
    <p>p cl</p>
    <p>hls</p>
    <p>cl h</p>
    <p>sp in</p>
    <p>cl h</p>
    <p>st p</p>
    <p>cpt</p>
    <p>l-t kt</p>
    <p>ctk</p>
    <p>ttk</p>
    <p>t hm</p>
    <p>cs</p>
    <p>ht ic</p>
    <p>ke t</p>
    <p>ls</p>
    <p>m al</p>
    <p>th sp</p>
    <p>in</p>
    <p>m al</p>
    <p>th st</p>
    <p>p</p>
    <p>m cs</p>
    <p>-ls</p>
    <p>m cs</p>
    <p>sp in</p>
    <p>m cs</p>
    <p>st p</p>
    <p>m cs</p>
    <p>-t im</p>
    <p>ep ub</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>ed</p>
    <p>pt hr</p>
    <p>ea d</p>
    <p>pt hr</p>
    <p>ea da</p>
    <p>da pt</p>
    <p>sp in</p>
    <p>lo ck</p>
    <p>sp in</p>
    <p>lo ck</p>
    <p>-ls</p>
    <p>tic ke</p>
    <p>t</p>
    <p>tic ke</p>
    <p>tls</p>
    <p>tt as</p>
    <p>tt as</p>
    <p>-ls</p>
    <p>Max Nodes Opt Nodes</p>
    <p>Fraction of lock-sensitive applications for which a given lock is optimal</p>
    <p>At max and opt nodes, even worse 52% coverage</p>
  </div>
  <div class="page">
    <p>Are All Locks Potentially Harmful?</p>
    <p>ah m</p>
    <p>cs</p>
    <p>al oc</p>
    <p>kls</p>
    <p>ba ck</p>
    <p>off</p>
    <p>cb om</p>
    <p>cs sp</p>
    <p>in</p>
    <p>cb om</p>
    <p>cs st</p>
    <p>p cl</p>
    <p>hls</p>
    <p>cl h</p>
    <p>sp in</p>
    <p>cl h</p>
    <p>st p</p>
    <p>cpt</p>
    <p>l-t kt</p>
    <p>ctk</p>
    <p>ttk</p>
    <p>t hm</p>
    <p>cs</p>
    <p>ht ic</p>
    <p>ke t</p>
    <p>ls</p>
    <p>m al</p>
    <p>th sp</p>
    <p>in</p>
    <p>m al</p>
    <p>th st</p>
    <p>p</p>
    <p>m cs</p>
    <p>-ls</p>
    <p>m cs</p>
    <p>sp in</p>
    <p>m cs</p>
    <p>st p</p>
    <p>m cs</p>
    <p>-t im</p>
    <p>ep ub</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>ed</p>
    <p>pt hr</p>
    <p>ea d</p>
    <p>pt hr</p>
    <p>ea da</p>
    <p>da pt</p>
    <p>sp in</p>
    <p>lo ck</p>
    <p>sp in</p>
    <p>lo ck</p>
    <p>-ls</p>
    <p>tic ke</p>
    <p>t</p>
    <p>tic ke</p>
    <p>tls</p>
    <p>tt as</p>
    <p>tt as</p>
    <p>-ls</p>
    <p>Max Nodes Opt Nodes</p>
    <p>Fraction of lock-sensitive applications for which a given lock degrades the performance w.r.t. the best lock (by at least 15%)</p>
    <p>Always several applications for which a given lock hurts performance</p>
  </div>
  <div class="page">
    <p>Is There a Stable Hierarchy Between Locks?</p>
    <p>The lock hierarchy for an application strongly changes with:</p>
    <p>The number of nodes:  On average, only 27% of the pairwise comparisons are</p>
    <p>conserved</p>
    <p>The machine:  On average, only 30% of the pairwise comparisons are</p>
    <p>conserved</p>
  </div>
  <div class="page">
    <p>Additional Remarks</p>
    <p>Using thread pinning does not change the general observations</p>
    <p>Pthread Mutex locks perform relatively well (i.e., are among the best locks) for a significant share of the studied</p>
    <p>applications</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Summary of Observations</p>
    <p>60% of the studied applications are lock sensitive  Lock behavior is strongly impacted by the number of nodes  Locks impact applications both at max and opt nodes  No lock is always among the best  There is no stable hierarchy between locks</p>
    <p>The number of threads impacts the lock hierarchy  The machine impacts the lock hierarchy</p>
    <p>All locks are potentially harmful  Using thread pinning leads to the same conclusions  Pthreads locks perform reasonably well</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Lock algorithms should not be hardwired into the code of applications.</p>
    <p>The observed trends call for further research on  new lock algorithms  runtime support for</p>
    <p>parallel performance  contention management</p>
    <p>Extended version of the paper + Data Sets + Source Code</p>
    <p>https://github.com/multicore-locks/litl/</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Lock algorithms should not be hardwired into the code of applications.  The observed trends call for further research on</p>
    <p>new lock algorithms  runtime support for</p>
    <p>parallel performance  contention management</p>
    <p>Extended version of the paper + Data Sets + Source Code</p>
    <p>https://github.com/multicore-locks/litl/</p>
    <p>Thank you for your attention.</p>
  </div>
</Presentation>
