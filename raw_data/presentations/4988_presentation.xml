<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Unlabeled data: Now it helps, now it doesnt</p>
    <p>Aarti Singh, Robert Nowak, Xiaojin Zhu</p>
    <p>University of WisconsinMadison</p>
    <p>NIPS 2008</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 1 / 13</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Learning under Cluster Assumption</p>
    <p>f (X) is the optimal predictor of Y given PXY</p>
    <p>Data: n labeled points iid PXY , m unlabeled points</p>
    <p>iid PX , m  n Goal: learn f (X) from data</p>
    <p>The cluster assumption: I PX is a mixture of components in d-dim I f (X) smooth on each component I  is the margin (&gt; 0 separation, &lt; 0 overlap), characterizes difficulty</p>
    <p>of learning problem</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 2 / 13</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Learning under Cluster Assumption</p>
    <p>f (X) is the optimal predictor of Y given PXY</p>
    <p>Data: n labeled points iid PXY , m unlabeled points</p>
    <p>iid PX , m  n Goal: learn f (X) from data The cluster assumption:</p>
    <p>I PX is a mixture of components in d-dim I f (X) smooth on each component I  is the margin (&gt; 0 separation, &lt; 0 overlap), characterizes difficulty</p>
    <p>of learning problem</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 2 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Does Unlabeled Data Help? [BB05,BDLP08,BL07,CC95,LW08,Ni08,Ri07]</p>
    <p>Unlabeled data doesnt help</p>
    <p>For any  &gt; 0, given enough labeled data, unlabeled data is superfluous (SSL does not result in faster rates of convergence).</p>
    <p>Unlabeled data helps</p>
    <p>Given a finite labeled data, there are learning problems with small enough  that SL fails, whereas perfect knowledge of components would yield small error.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 3 / 13</p>
  </div>
  <div class="page">
    <p>Our Contributions</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 4 / 13</p>
  </div>
  <div class="page">
    <p>Finite Sample Bounds</p>
    <p>fm,n: predictor learned from m unlabeled and n labeled points I m = 0: supervised I m &gt; 0: semi-supervised I m = : oracle (full knowledge of PX , but not f )</p>
    <p>R(fm,n): Risk under loss function `, e.g., ` = (fm,n(X)  Y )2</p>
    <p>R(fm,n) = E(X,Y )PXY [`(fm,n(X), Y )]</p>
    <p>E(fm,n): Excess Risk, the difference between expected Risk (over random draws of training set) and Bayes Risk</p>
    <p>E(fm,n) = Etraining [R(fm,n)]  inf f</p>
    <p>R(f )</p>
    <p>Minimax error m,n,</p>
    <p>polylog inf fm,n</p>
    <p>sup P ()</p>
    <p>E(fm,n)</p>
    <p>,n,  m,n,  0,n,</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 5 / 13</p>
  </div>
  <div class="page">
    <p>Finite Sample Bounds</p>
    <p>fm,n: predictor learned from m unlabeled and n labeled points I m = 0: supervised I m &gt; 0: semi-supervised I m = : oracle (full knowledge of PX , but not f )</p>
    <p>R(fm,n): Risk under loss function `, e.g., ` = (fm,n(X)  Y )2</p>
    <p>R(fm,n) = E(X,Y )PXY [`(fm,n(X), Y )]</p>
    <p>E(fm,n): Excess Risk, the difference between expected Risk (over random draws of training set) and Bayes Risk</p>
    <p>E(fm,n) = Etraining [R(fm,n)]  inf f</p>
    <p>R(f )</p>
    <p>Minimax error m,n,</p>
    <p>polylog inf fm,n</p>
    <p>sup P ()</p>
    <p>E(fm,n)</p>
    <p>,n,  m,n,  0,n,</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 5 / 13</p>
  </div>
  <div class="page">
    <p>Finite Sample Bounds</p>
    <p>fm,n: predictor learned from m unlabeled and n labeled points I m = 0: supervised I m &gt; 0: semi-supervised I m = : oracle (full knowledge of PX , but not f )</p>
    <p>R(fm,n): Risk under loss function `, e.g., ` = (fm,n(X)  Y )2</p>
    <p>R(fm,n) = E(X,Y )PXY [`(fm,n(X), Y )]</p>
    <p>E(fm,n): Excess Risk, the difference between expected Risk (over random draws of training set) and Bayes Risk</p>
    <p>E(fm,n) = Etraining [R(fm,n)]  inf f</p>
    <p>R(f )</p>
    <p>Minimax error m,n,</p>
    <p>polylog inf fm,n</p>
    <p>sup P ()</p>
    <p>E(fm,n)</p>
    <p>,n,  m,n,  0,n,</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 5 / 13</p>
  </div>
  <div class="page">
    <p>Finite Sample Bounds</p>
    <p>fm,n: predictor learned from m unlabeled and n labeled points I m = 0: supervised I m &gt; 0: semi-supervised I m = : oracle (full knowledge of PX , but not f )</p>
    <p>R(fm,n): Risk under loss function `, e.g., ` = (fm,n(X)  Y )2</p>
    <p>R(fm,n) = E(X,Y )PXY [`(fm,n(X), Y )]</p>
    <p>E(fm,n): Excess Risk, the difference between expected Risk (over random draws of training set) and Bayes Risk</p>
    <p>E(fm,n) = Etraining [R(fm,n)]  inf f</p>
    <p>R(f )</p>
    <p>Minimax error m,n,</p>
    <p>polylog inf fm,n</p>
    <p>sup P ()</p>
    <p>E(fm,n)</p>
    <p>,n,  m,n,  0,n,</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 5 / 13</p>
  </div>
  <div class="page">
    <p>Finite Sample Bounds</p>
    <p>fm,n: predictor learned from m unlabeled and n labeled points I m = 0: supervised I m &gt; 0: semi-supervised I m = : oracle (full knowledge of PX , but not f )</p>
    <p>R(fm,n): Risk under loss function `, e.g., ` = (fm,n(X)  Y )2</p>
    <p>R(fm,n) = E(X,Y )PXY [`(fm,n(X), Y )]</p>
    <p>E(fm,n): Excess Risk, the difference between expected Risk (over random draws of training set) and Bayes Risk</p>
    <p>E(fm,n) = Etraining [R(fm,n)]  inf f</p>
    <p>R(f )</p>
    <p>Minimax error m,n,</p>
    <p>polylog inf fm,n</p>
    <p>sup P ()</p>
    <p>E(fm,n)</p>
    <p>,n,  m,n,  0,n,</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 5 / 13</p>
  </div>
  <div class="page">
    <p>Mathematical Formalization of Cluster Assumption</p>
    <p>Components (compact support, Lipschitz boundary)</p>
    <p>Density bounded from below and above, Holder- smooth</p>
    <p>Decision sets D: all intersections of components Overall density jumps at decision set boundaries</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 6 / 13</p>
  </div>
  <div class="page">
    <p>Mathematical Formalization of Cluster Assumption</p>
    <p>Components (compact support, Lipschitz boundary)</p>
    <p>Density bounded from below and above, Holder- smooth</p>
    <p>Decision sets D: all intersections of components</p>
    <p>Overall density jumps at decision set boundaries</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 6 / 13</p>
  </div>
  <div class="page">
    <p>Mathematical Formalization of Cluster Assumption</p>
    <p>Components (compact support, Lipschitz boundary)</p>
    <p>Density bounded from below and above, Holder- smooth</p>
    <p>Decision sets D: all intersections of components Overall density jumps at decision set boundaries</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 6 / 13</p>
  </div>
  <div class="page">
    <p>SSL Approach</p>
    <p>Oracle knows the shape of decision sets, learns within a decision set.</p>
    <p>SSL mimics Oracle, learns only from connected labeled points</p>
    <p>Connected: x1  x2 if there is a sequence of unlabeled steppingstones: (1) close together, (2) similar local density</p>
    <p>Connectedness is almost as good as knowing the decision sets: Lemma: if || &gt; Cm1/d, then for all pairs x1, x2 not in a small tube around decision set boundaries, with large probability</p>
    <p>x1, x2 in same decision set if and only if x1  x2</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 7 / 13</p>
  </div>
  <div class="page">
    <p>SSL Approach</p>
    <p>Oracle knows the shape of decision sets, learns within a decision set.</p>
    <p>SSL mimics Oracle, learns only from connected labeled points</p>
    <p>Connected: x1  x2 if there is a sequence of unlabeled steppingstones: (1) close together, (2) similar local density</p>
    <p>Connectedness is almost as good as knowing the decision sets: Lemma: if || &gt; Cm1/d, then for all pairs x1, x2 not in a small tube around decision set boundaries, with large probability</p>
    <p>x1, x2 in same decision set if and only if x1  x2</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 7 / 13</p>
  </div>
  <div class="page">
    <p>SSL Approach</p>
    <p>Oracle knows the shape of decision sets, learns within a decision set.</p>
    <p>SSL mimics Oracle, learns only from connected labeled points</p>
    <p>Connected: x1  x2 if there is a sequence of unlabeled steppingstones: (1) close together, (2) similar local density</p>
    <p>Connectedness is almost as good as knowing the decision sets: Lemma: if || &gt; Cm1/d, then for all pairs x1, x2 not in a small tube around decision set boundaries, with large probability</p>
    <p>x1, x2 in same decision set if and only if x1  x2</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 7 / 13</p>
  </div>
  <div class="page">
    <p>SSL Approach</p>
    <p>Oracle knows the shape of decision sets, learns within a decision set.</p>
    <p>SSL mimics Oracle, learns only from connected labeled points</p>
    <p>Connected: x1  x2 if there is a sequence of unlabeled steppingstones: (1) close together, (2) similar local density</p>
    <p>Connectedness is almost as good as knowing the decision sets: Lemma: if || &gt; Cm1/d, then for all pairs x1, x2 not in a small tube around decision set boundaries, with large probability</p>
    <p>x1, x2 in same decision set if and only if x1  x2 Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 7 / 13</p>
  </div>
  <div class="page">
    <p>SSL Error</p>
    <p>Corollary: if || &gt; Cm1/d, then SSL is only a bit worse than oracle:</p>
    <p>m,n,  ,n, + O ( nm1/d</p>
    <p>)</p>
    <p>The value of unlabeled data: if m  n s.t. nm1/d  ,n, , then SSL is as good as Oracle.</p>
    <p>I if ,n, decays polynomially, m must grow polynomially with n I if ,n, decays exponentially, m must grow exponentially with n</p>
    <p>If, in addition, Oracle is better than any ordinary SL</p>
    <p>,n, &lt; 0,n,</p>
    <p>then SSL helps.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 8 / 13</p>
  </div>
  <div class="page">
    <p>SSL Error</p>
    <p>Corollary: if || &gt; Cm1/d, then SSL is only a bit worse than oracle:</p>
    <p>m,n,  ,n, + O ( nm1/d</p>
    <p>)</p>
    <p>The value of unlabeled data: if m  n s.t. nm1/d  ,n, , then SSL is as good as Oracle.</p>
    <p>I if ,n, decays polynomially, m must grow polynomially with n I if ,n, decays exponentially, m must grow exponentially with n</p>
    <p>If, in addition, Oracle is better than any ordinary SL</p>
    <p>,n, &lt; 0,n,</p>
    <p>then SSL helps.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 8 / 13</p>
  </div>
  <div class="page">
    <p>SSL Error</p>
    <p>Corollary: if || &gt; Cm1/d, then SSL is only a bit worse than oracle:</p>
    <p>m,n,  ,n, + O ( nm1/d</p>
    <p>)</p>
    <p>The value of unlabeled data: if m  n s.t. nm1/d  ,n, , then SSL is as good as Oracle.</p>
    <p>I if ,n, decays polynomially, m must grow polynomially with n I if ,n, decays exponentially, m must grow exponentially with n</p>
    <p>If, in addition, Oracle is better than any ordinary SL</p>
    <p>,n, &lt; 0,n,</p>
    <p>then SSL helps.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 8 / 13</p>
  </div>
  <div class="page">
    <p>Application to SSL Regression</p>
    <p>Assumption: target function Holder- smooth within a decision set, but may be discontinuous across decision sets.</p>
    <p>Two possible sources of error: 1 regression error within decision sets n2/(2+d)</p>
    <p>Oracle: learn f on each decision set separately, ,n, = n2/(2+d)</p>
    <p>SL: if  &gt; cn1/d then 0,n, = n2/(2+d), otherwise 0,n, = n1/d</p>
    <p>(worse: blur across decision sets).</p>
    <p>SSL: if || &gt; Cm1/d and m  n2d, then the same as Oracle.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 9 / 13</p>
  </div>
  <div class="page">
    <p>Application to SSL Regression</p>
    <p>Assumption: target function Holder- smooth within a decision set, but may be discontinuous across decision sets.</p>
    <p>Two possible sources of error: 1 regression error within decision sets n2/(2+d)</p>
    <p>Oracle: learn f on each decision set separately, ,n, = n2/(2+d)</p>
    <p>SL: if  &gt; cn1/d then 0,n, = n2/(2+d), otherwise 0,n, = n1/d</p>
    <p>(worse: blur across decision sets).</p>
    <p>SSL: if || &gt; Cm1/d and m  n2d, then the same as Oracle.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 9 / 13</p>
  </div>
  <div class="page">
    <p>Application to SSL Regression</p>
    <p>Assumption: target function Holder- smooth within a decision set, but may be discontinuous across decision sets.</p>
    <p>Two possible sources of error: 1 regression error within decision sets n2/(2+d)</p>
    <p>Oracle: learn f on each decision set separately, ,n, = n2/(2+d)</p>
    <p>SL: if  &gt; cn1/d then 0,n, = n2/(2+d), otherwise 0,n, = n1/d</p>
    <p>(worse: blur across decision sets).</p>
    <p>SSL: if || &gt; Cm1/d and m  n2d, then the same as Oracle.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 9 / 13</p>
  </div>
  <div class="page">
    <p>Application to SSL Regression</p>
    <p>Assumption: target function Holder- smooth within a decision set, but may be discontinuous across decision sets.</p>
    <p>Two possible sources of error: 1 regression error within decision sets n2/(2+d)</p>
    <p>Oracle: learn f on each decision set separately, ,n, = n2/(2+d)</p>
    <p>SL: if  &gt; cn1/d then 0,n, = n2/(2+d), otherwise 0,n, = n1/d</p>
    <p>(worse: blur across decision sets).</p>
    <p>SSL: if || &gt; Cm1/d and m  n2d, then the same as Oracle.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 9 / 13</p>
  </div>
  <div class="page">
    <p>Application to SSL Regression</p>
    <p>Assumption: target function Holder- smooth within a decision set, but may be discontinuous across decision sets.</p>
    <p>Two possible sources of error: 1 regression error within decision sets n2/(2+d)</p>
    <p>Oracle: learn f on each decision set separately, ,n, = n2/(2+d)</p>
    <p>SL: if  &gt; cn1/d then 0,n, = n2/(2+d), otherwise 0,n, = n1/d</p>
    <p>(worse: blur across decision sets).</p>
    <p>SSL: if || &gt; Cm1/d and m  n2d, then the same as Oracle.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 9 / 13</p>
  </div>
  <div class="page">
    <p>Unlabeled data: now it helps, now it doesnt</p>
    <p>margin Oracle SL SSL SSL ,n, 0,n, m,n, helps?</p>
    <p>n 1 d   n</p>
    <p>2 2+d n</p>
    <p>2 2+d no</p>
    <p>m 1 d   &lt; n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>|| &lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>&lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>In particular, with  &lt; 0, SSL has a faster rate of error convergence than SL, provided m  n2d.</p>
    <p>Thank you</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 10 / 13</p>
  </div>
  <div class="page">
    <p>Unlabeled data: now it helps, now it doesnt</p>
    <p>margin Oracle SL SSL SSL ,n, 0,n, m,n, helps?</p>
    <p>n 1 d   n</p>
    <p>2 2+d n</p>
    <p>2 2+d no</p>
    <p>m 1 d   &lt; n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>|| &lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>&lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>In particular, with  &lt; 0, SSL has a faster rate of error convergence than SL, provided m  n2d.</p>
    <p>Thank you</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 10 / 13</p>
  </div>
  <div class="page">
    <p>Unlabeled data: now it helps, now it doesnt</p>
    <p>margin Oracle SL SSL SSL ,n, 0,n, m,n, helps?</p>
    <p>n 1 d   n</p>
    <p>2 2+d n</p>
    <p>2 2+d no</p>
    <p>m 1 d   &lt; n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>|| &lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>&lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>In particular, with  &lt; 0, SSL has a faster rate of error convergence than SL, provided m  n2d.</p>
    <p>Thank you</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 10 / 13</p>
  </div>
  <div class="page">
    <p>Unlabeled data: now it helps, now it doesnt</p>
    <p>margin Oracle SL SSL SSL ,n, 0,n, m,n, helps?</p>
    <p>n 1 d   n</p>
    <p>2 2+d n</p>
    <p>2 2+d no</p>
    <p>m 1 d   &lt; n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>|| &lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>&lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>In particular, with  &lt; 0, SSL has a faster rate of error convergence than SL, provided m  n2d.</p>
    <p>Thank you</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 10 / 13</p>
  </div>
  <div class="page">
    <p>Unlabeled data: now it helps, now it doesnt</p>
    <p>margin Oracle SL SSL SSL ,n, 0,n, m,n, helps?</p>
    <p>n 1 d   n</p>
    <p>2 2+d n</p>
    <p>2 2+d no</p>
    <p>m 1 d   &lt; n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>|| &lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>&lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>In particular, with  &lt; 0, SSL has a faster rate of error convergence than SL, provided m  n2d.</p>
    <p>Thank you</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 10 / 13</p>
  </div>
  <div class="page">
    <p>Unlabeled data: now it helps, now it doesnt</p>
    <p>margin Oracle SL SSL SSL ,n, 0,n, m,n, helps?</p>
    <p>n 1 d   n</p>
    <p>2 2+d n</p>
    <p>2 2+d no</p>
    <p>m 1 d   &lt; n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>|| &lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>&lt; m 1 d n</p>
    <p>2 2+d n</p>
    <p>2 2+d yes</p>
    <p>In particular, with  &lt; 0, SSL has a faster rate of error convergence than SL, provided m  n2d.</p>
    <p>Thank you</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 10 / 13</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 11 / 13</p>
  </div>
  <div class="page">
    <p>Holder Smoothness</p>
    <p>If f is Holder-, then the k = bc Taylor polynomial at x0, pk,f,x0 , yields the approximation error bound:</p>
    <p>|pk,f,x0 (x)  f (x)|  C|x  x0|</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 12 / 13</p>
  </div>
  <div class="page">
    <p>The Corollary</p>
    <p>Even when || &gt; Cm1/d, the Lemma may fail for two reasons: One of the n labeled points or the test point falls in the small uncertain tube.</p>
    <p>I Volume of the tube O(m1/d) I This is the probability that one point falls in the tube I Union bound gives O(nm1/d) I Risk is bounded I The contribution to excess error is O(nm1/d)</p>
    <p>With probability 1/m connectedness does not imply same decision set I The contribution to excess error is O(1/m)</p>
    <p>Overall, O(1/m + nm1/d)  O(nm1/d). The lemma does not apply when ||  Cm1/d.</p>
    <p>Singh, Nowak, Zhu (Wisconsin) Unlabeled data: Now it helps, now it doesnt NIPS 2008 13 / 13</p>
  </div>
</Presentation>
