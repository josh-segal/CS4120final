<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification</p>
    <p>Yizhong Wang1 Kai Liu2 Jing Liu2 Wei He2</p>
    <p>Yajuan Lyu2 Hua Wu2 Sujian Li1 Haifeng Wang2</p>
    <p>ACL, July 17, 2018</p>
  </div>
  <div class="page">
    <p>Background / Motivation  Machine Reading Comprehension (MRC)</p>
    <p>Why Multi-Passage MRC is Challenging?</p>
    <p>Model Architecture  Answer Boundary Prediction</p>
    <p>Answer Content Modeling</p>
    <p>Cross-Passage Answer Verification</p>
    <p>Joint Training and Prediction</p>
    <p>Experiments  Results on MS-MARCO and DuReader</p>
    <p>Ablation Study</p>
    <p>Quantitative Analysis</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Machine Reading Comprehension (MRC)</p>
    <p>Passage:  Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused Morgan was shocked by the reminder of his part in the stock market</p>
    <p>Question: On what did Tesla blame for the loss of the initial money?</p>
    <p>[from SQuAD v1.1[1]]</p>
  </div>
  <div class="page">
    <p>Machine Reading Comprehension (MRC)</p>
    <p>Passage:  Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused Morgan was shocked by the reminder of his part in the stock market</p>
    <p>Question: On what did Tesla blame for the loss of the initial money?</p>
    <p>Answer: Panic of 1901</p>
    <p>[from SQuAD v1.1[1]]</p>
  </div>
  <div class="page">
    <p>Machine Reading Comprehension (MRC)</p>
    <p>Passage:  Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused Morgan was shocked by the reminder of his part in the stock market</p>
    <p>Question: On what did Tesla blame for the loss of the initial money?</p>
    <p>Answer: Panic of 1901</p>
    <p>[from SQuAD v1.1[1]]</p>
    <p>Single-passage MRC</p>
  </div>
  <div class="page">
    <p>Machine Reading Comprehension (MRC)</p>
    <p>Passage:  Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused Morgan was shocked by the reminder of his part in the stock market</p>
    <p>Question: On what did Tesla blame for the loss of the initial money?</p>
    <p>Answer: Panic of 1901</p>
    <p>[from SQuAD v1.1[1]]</p>
    <p>Different types: cloze test, entity extraction, span extraction, multiple-choice</p>
    <p>Various models: Match-LSTM[2], BiDAF[3], R-Net[4], QANet[5]</p>
    <p>Very impressive performance</p>
    <p>Single-passage MRC</p>
  </div>
  <div class="page">
    <p>Reading the Web to Answer Questions?</p>
  </div>
  <div class="page">
    <p>Applying MRC to the Web</p>
    <p>Search engine is employed.</p>
    <p>Multiple passages are retrieved.</p>
  </div>
  <div class="page">
    <p>Applying MRC to the Web</p>
    <p>Search engine is employed.</p>
    <p>Multiple passages are retrieved.</p>
    <p>All of them seem relevant.</p>
  </div>
  <div class="page">
    <p>Applying MRC to the Web</p>
    <p>Search engine is employed.</p>
    <p>Multiple passages are retrieved.</p>
    <p>All of them seem relevant.</p>
    <p>But they give different answers!</p>
  </div>
  <div class="page">
    <p>Applying MRC to the Web</p>
    <p>Search engine is employed.</p>
    <p>Multiple passages are retrieved.</p>
    <p>All of them seem relevant.</p>
    <p>But they give different answers!</p>
    <p>Key challenge :</p>
    <p>Much more misleading candidates</p>
  </div>
  <div class="page">
    <p>An Example from MS-MARCO[6] Dataset</p>
    <p>Question: What is the difference between a mixed and pure culture?</p>
    <p>Passages:</p>
  </div>
  <div class="page">
    <p>An Example from MS-MARCO [6] Dataset</p>
    <p>Question: What is the difference between a mixed and pure culture?</p>
    <p>Passages: Correct</p>
  </div>
  <div class="page">
    <p>An Example from MS-MARCO [6] Dataset</p>
    <p>Question: What is the difference between a mixed and pure culture?</p>
    <p>Passages: Partially Correct</p>
  </div>
  <div class="page">
    <p>An Example from MS-MARCO [6] Dataset</p>
    <p>Question: What is the difference between a mixed and pure culture?</p>
    <p>Passages: Incorrect</p>
  </div>
  <div class="page">
    <p>An Example from MS-MARCO [6] Dataset</p>
    <p>Question: What is the difference between a mixed and pure culture?</p>
    <p>Passages: Incorrect Partially Correct Correct</p>
    <p>Different</p>
    <p>Similar or same</p>
  </div>
  <div class="page">
    <p>An Example from MS-MARCO [6] Dataset</p>
    <p>Question: What is the difference between a mixed and pure culture?</p>
    <p>Passages: Incorrect Partially Correct Correct</p>
    <p>Different</p>
    <p>Correct Answer</p>
    <p>Verify</p>
  </div>
  <div class="page">
    <p>Overview of Our Model</p>
    <p>Encoding</p>
    <p>Q-P Matching</p>
    <p>Answer Boundary</p>
    <p>Prediction</p>
    <p>Answer Content</p>
    <p>Modeling</p>
    <p>Question</p>
    <p>Passage 1</p>
    <p>1</p>
    <p>1</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 1</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1</p>
    <p>Passage 2</p>
    <p>2</p>
    <p>2</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 2</p>
    <p>weighted</p>
    <p>sum</p>
    <p>2</p>
    <p>Passage n</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer</p>
    <p>weighted</p>
    <p>sum</p>
    <p>...</p>
    <p>...</p>
    <p>Answer Verification</p>
    <p>1 1 2 2</p>
    <p>Score 1 Score 2 Score 3</p>
    <p>Attention</p>
    <p>Final</p>
    <p>Answer</p>
  </div>
  <div class="page">
    <p>Overview of Our Model</p>
    <p>Encoding</p>
    <p>Q-P Matching</p>
    <p>Answer Boundary</p>
    <p>Prediction</p>
    <p>Answer Content</p>
    <p>Modeling</p>
    <p>Question</p>
    <p>Passage 1</p>
    <p>1</p>
    <p>1</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 1</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1</p>
    <p>Passage 2</p>
    <p>2</p>
    <p>2</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 2</p>
    <p>weighted</p>
    <p>sum</p>
    <p>2</p>
    <p>Passage n</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer</p>
    <p>weighted</p>
    <p>sum</p>
    <p>...</p>
    <p>...</p>
    <p>Answer Verification</p>
    <p>1 1 2 2</p>
    <p>Score 1 Score 2 Score 3</p>
    <p>Attention</p>
    <p>Final</p>
    <p>Answer</p>
  </div>
  <div class="page">
    <p>Overview of Our Model</p>
    <p>Encoding</p>
    <p>Q-P Matching</p>
    <p>Answer Boundary</p>
    <p>Prediction</p>
    <p>Answer Content</p>
    <p>Modeling</p>
    <p>Question</p>
    <p>Passage 1</p>
    <p>1</p>
    <p>1</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 1</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1</p>
    <p>Passage 2</p>
    <p>2</p>
    <p>2</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 2</p>
    <p>weighted</p>
    <p>sum</p>
    <p>2</p>
    <p>Passage n</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer</p>
    <p>weighted</p>
    <p>sum</p>
    <p>...</p>
    <p>...</p>
    <p>Answer Verification</p>
    <p>1 1 2 2</p>
    <p>Score 1 Score 2 Score 3</p>
    <p>Attention</p>
    <p>Final</p>
    <p>Answer</p>
  </div>
  <div class="page">
    <p>Overview of Our Model</p>
    <p>Encoding</p>
    <p>Q-P Matching</p>
    <p>Answer Boundary</p>
    <p>Prediction</p>
    <p>Answer Content</p>
    <p>Modeling</p>
    <p>Question</p>
    <p>Passage 1</p>
    <p>1</p>
    <p>1</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 1</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1</p>
    <p>Passage 2</p>
    <p>2</p>
    <p>2</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer 2</p>
    <p>weighted</p>
    <p>sum</p>
    <p>2</p>
    <p>Passage n</p>
    <p>() ()</p>
    <p>()</p>
    <p>Answer</p>
    <p>weighted</p>
    <p>sum</p>
    <p>...</p>
    <p>...</p>
    <p>Answer Verification</p>
    <p>1 1 2 2</p>
    <p>Score 1 Score 2 Score 3</p>
    <p>Attention</p>
    <p>Final</p>
    <p>Answer</p>
  </div>
  <div class="page">
    <p>Input Question Passage 1 Passage 2 Passage n...</p>
  </div>
  <div class="page">
    <p>Question and Passage Encoding Question Passage 1 Passage 2 Passage n...</p>
    <p>1 2   Encoding with Bi-LSTM:</p>
  </div>
  <div class="page">
    <p>Question-Passage Matching Question Passage 1 Passage 2 Passage n...</p>
    <p>1 2</p>
    <p>1 2</p>
    <p>Bi-directional Attention Flow (Seo et al., 2016)</p>
    <p>Dot attention matrix:</p>
  </div>
  <div class="page">
    <p>Answer Boundary Prediction Question Passage 1 Passage 2 Passage n...</p>
    <p>1 2</p>
    <p>1 2</p>
    <p>() ()</p>
    <p>Answer 1</p>
    <p>() ()</p>
    <p>Answer 2</p>
    <p>() ()</p>
    <p>Answer</p>
    <p>...</p>
    <p>Start and end pointer:</p>
  </div>
  <div class="page">
    <p>Answer Content Modeling Question Passage 1 Passage 2 Passage n...</p>
    <p>1 2</p>
    <p>1 2</p>
    <p>() ()</p>
    <p>Answer 1</p>
    <p>() ()</p>
    <p>Answer 2</p>
    <p>() ()</p>
    <p>Answer</p>
    <p>...</p>
    <p>()</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1</p>
    <p>()</p>
    <p>weighted</p>
    <p>sum</p>
    <p>2</p>
    <p>()</p>
    <p>weighted</p>
    <p>sum</p>
    <p>Content score for each word:</p>
    <p>Representation for :</p>
  </div>
  <div class="page">
    <p>Cross-Passage Answer Verification Question Passage 1 Passage 2 Passage n...</p>
    <p>1 2</p>
    <p>1 2</p>
    <p>() ()</p>
    <p>Answer 1</p>
    <p>() ()</p>
    <p>Answer 2</p>
    <p>() ()</p>
    <p>Answer</p>
    <p>...</p>
    <p>()</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1</p>
    <p>()</p>
    <p>weighted</p>
    <p>sum</p>
    <p>2</p>
    <p>()</p>
    <p>weighted</p>
    <p>sum</p>
    <p>1 1 2 2</p>
    <p>Score 1 Score 2 Score 3</p>
    <p>Attention</p>
    <p>Ans-to-ans Attention:</p>
    <p>Verification score:</p>
  </div>
  <div class="page">
    <p>Joint Training and Prediction</p>
    <p>Three objectives:</p>
    <p>Finding the boundary of the answer</p>
    <p>Predicting whether each word should be included in the answer</p>
    <p>Selecting the best answer from all the candidates</p>
    <p>Prediction:</p>
    <p>Score =</p>
    <p>Training Loss:</p>
    <p>join =  + 1 + 2</p>
  </div>
  <div class="page">
    <p>Experiments Setup</p>
    <p>Datasets: MS-MARCO[6] and DuReader[7]:</p>
    <p>Language Search Engine</p>
    <p>Size Questions with</p>
    <p>Multi Annotated Answers Questions with</p>
    <p>Multi Answer Spans</p>
    <p>MS-MARCO English Bing 100K+ 9.93% 40.00%</p>
    <p>DuReader Chinese Baidu 200K+ 67.28% 56.38%</p>
  </div>
  <div class="page">
    <p>Experiments Setup</p>
    <p>Datasets: MS-MARCO[6] and DuReader[7]:</p>
    <p>Language Search Engine</p>
    <p>Size Questions with</p>
    <p>Multi Annotated Answers Questions with</p>
    <p>Multi Answer Spans</p>
    <p>MS-MARCO English Bing 100K+ 9.93% 40.00%</p>
    <p>DuReader Chinese Baidu 200K+ 67.28% 56.38%</p>
  </div>
  <div class="page">
    <p>Experiments Setup</p>
    <p>Datasets: MS-MARCO[6] and DuReader[7]:</p>
    <p>Language Search Engine</p>
    <p>Size Questions with</p>
    <p>Multi Annotated Answers Questions with</p>
    <p>Multi Answer Spans</p>
    <p>MS-MARCO English Bing 100K+ 9.93% 40.00%</p>
    <p>DuReader Chinese Baidu 200K+ 67.28% 56.38%</p>
    <p>Hyper-parameters (tuned on the dev set):</p>
    <p>Word Embedding</p>
    <p>Character Embedding</p>
    <p>Hidden Size L2 Optimizer Learning Rate Batch Size</p>
  </div>
  <div class="page">
    <p>Main Results</p>
    <p>Tab 1. Performance on MS-MARCO test set</p>
    <p>Tab 2. Performance on DuReader test set</p>
    <p>Model ROUGE-L BLEU-1 FastQA_Ext 33.67 33.93</p>
    <p>Match-LSTM 37.33 40.72 ReasoNet 38.81 39.86</p>
    <p>R-Net 42.89 42.22 S-Net 45.23 43.78</p>
    <p>Our Model 46.15 44.47 S-Net (Ensemble) 46.65 44.78</p>
    <p>Our Model (Ensemble) 46.66 45.41 Human 47 46</p>
    <p>Model ROUGE-L BLEU-4</p>
    <p>Match-LSTM 39.0 31.8</p>
    <p>BiDAF 39.2 31.9 PR+BiDAF 41.8 37.6</p>
    <p>Our Model 44.2 41.0</p>
    <p>Human 57.4 56.1</p>
  </div>
  <div class="page">
    <p>Ablation Study on MS-MARCO Dev Set</p>
    <p>Model ROUGE-L</p>
    <p>Complete Model 45.65</p>
    <p>- Answer Verification 44.38 -1.27</p>
    <p>- Content Modeling 44.27 -1.38</p>
    <p>- Joint Training 44.12 -1.53</p>
    <p>-Yes/No Classification 41.87 -3.78</p>
    <p>Boundary Baseline 38.95 -6.70</p>
  </div>
  <div class="page">
    <p>Quantitative Analysis: the Predicted Scores</p>
  </div>
  <div class="page">
    <p>Quantitative Analysis: the Predicted Scores</p>
    <p>Boundary / content / verification scores are usually positively relevant</p>
  </div>
  <div class="page">
    <p>Quantitative Analysis: the Predicted Scores</p>
    <p>More commonality --&gt; larger verification score</p>
  </div>
  <div class="page">
    <p>Quantitative Analysis: the Predicted Scores</p>
    <p>Correct answer is selected by considering verification!</p>
  </div>
  <div class="page">
    <p>Necessity of the Content Model</p>
  </div>
  <div class="page">
    <p>Necessity of the Content Model</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>-L R</p>
    <p>B</p>
    <p>n o u</p>
    <p>n</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>T h</p>
    <p>e</p>
    <p>n o u</p>
    <p>n</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>h a s 1</p>
    <p>se n</p>
    <p>se</p>
    <p>: 1 . a</p>
    <p>m e a su</p>
    <p>r e o f</p>
    <p>th e</p>
    <p>q u</p>
    <p>a n</p>
    <p>ti ty o</p>
    <p>f</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic it</p>
    <p>y</p>
    <p>-L R</p>
    <p>B</p>
    <p>d e te</p>
    <p>r m</p>
    <p>in e d</p>
    <p>b y</p>
    <p>th e</p>
    <p>a m</p>
    <p>o u</p>
    <p>n t</p>
    <p>o f</p>
    <p>a n</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic</p>
    <p>c u</p>
    <p>r r e n</p>
    <p>t</p>
    <p>a n</p>
    <p>d</p>
    <p>th e</p>
    <p>ti m</p>
    <p>e</p>
    <p>fo r</p>
    <p>w h</p>
    <p>ic h it</p>
    <p>fl o w</p>
    <p>s</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>fa m</p>
    <p>il ia</p>
    <p>r it</p>
    <p>y</p>
    <p>in fo</p>
    <p>:</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>u se</p>
    <p>d a s a</p>
    <p>n o u</p>
    <p>n is</p>
    <p>v e r y</p>
    <p>r a</p>
    <p>r e .</p>
    <p>start probability</p>
  </div>
  <div class="page">
    <p>Necessity of the Content Model</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>-L R</p>
    <p>B</p>
    <p>n o u</p>
    <p>n</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>T h</p>
    <p>e</p>
    <p>n o u</p>
    <p>n</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>h a s 1</p>
    <p>se n</p>
    <p>se</p>
    <p>: 1 . a</p>
    <p>m e a su</p>
    <p>r e o f</p>
    <p>th e</p>
    <p>q u</p>
    <p>a n</p>
    <p>ti ty o</p>
    <p>f</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic it</p>
    <p>y</p>
    <p>-L R</p>
    <p>B</p>
    <p>d e te</p>
    <p>r m</p>
    <p>in e d</p>
    <p>b y</p>
    <p>th e</p>
    <p>a m</p>
    <p>o u</p>
    <p>n t</p>
    <p>o f</p>
    <p>a n</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic</p>
    <p>c u</p>
    <p>r r e n</p>
    <p>t</p>
    <p>a n</p>
    <p>d</p>
    <p>th e</p>
    <p>ti m</p>
    <p>e</p>
    <p>fo r</p>
    <p>w h</p>
    <p>ic h it</p>
    <p>fl o w</p>
    <p>s</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>fa m</p>
    <p>il ia</p>
    <p>r it</p>
    <p>y</p>
    <p>in fo</p>
    <p>:</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>u se</p>
    <p>d a s a</p>
    <p>n o u</p>
    <p>n is</p>
    <p>v e r y</p>
    <p>r a</p>
    <p>r e .</p>
    <p>start probability end probability</p>
  </div>
  <div class="page">
    <p>Visualization of the Probability Distribution</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>-L R</p>
    <p>B</p>
    <p>n o u</p>
    <p>n</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>T h</p>
    <p>e</p>
    <p>n o u</p>
    <p>n</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>h a s 1</p>
    <p>se n</p>
    <p>se</p>
    <p>: 1 . a</p>
    <p>m e a su</p>
    <p>r e o f</p>
    <p>th e</p>
    <p>q u</p>
    <p>a n</p>
    <p>ti ty o</p>
    <p>f</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic it</p>
    <p>y</p>
    <p>-L R</p>
    <p>B</p>
    <p>d e te</p>
    <p>r m</p>
    <p>in e d</p>
    <p>b y</p>
    <p>th e</p>
    <p>a m</p>
    <p>o u</p>
    <p>n t</p>
    <p>o f</p>
    <p>a n</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic</p>
    <p>c u</p>
    <p>r r e n</p>
    <p>t</p>
    <p>a n</p>
    <p>d</p>
    <p>th e</p>
    <p>ti m</p>
    <p>e</p>
    <p>fo r</p>
    <p>w h</p>
    <p>ic h it</p>
    <p>fl o w</p>
    <p>s</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>fa m</p>
    <p>il ia</p>
    <p>r it</p>
    <p>y</p>
    <p>in fo</p>
    <p>:</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>u se</p>
    <p>d a s a</p>
    <p>n o u</p>
    <p>n is</p>
    <p>v e r y</p>
    <p>r a</p>
    <p>r e .</p>
    <p>start probability end probability content probability</p>
  </div>
  <div class="page">
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>-L R</p>
    <p>B</p>
    <p>n o u</p>
    <p>n</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>T h</p>
    <p>e</p>
    <p>n o u</p>
    <p>n</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>h a s 1</p>
    <p>se n</p>
    <p>se</p>
    <p>: 1 . a</p>
    <p>m e a su</p>
    <p>r e o f</p>
    <p>th e</p>
    <p>q u</p>
    <p>a n</p>
    <p>ti ty o</p>
    <p>f</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic it</p>
    <p>y</p>
    <p>-L R</p>
    <p>B</p>
    <p>d e te</p>
    <p>r m</p>
    <p>in e d</p>
    <p>b y</p>
    <p>th e</p>
    <p>a m</p>
    <p>o u</p>
    <p>n t</p>
    <p>o f</p>
    <p>a n</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic</p>
    <p>c u</p>
    <p>r r e n</p>
    <p>t</p>
    <p>a n</p>
    <p>d</p>
    <p>th e</p>
    <p>ti m</p>
    <p>e</p>
    <p>fo r</p>
    <p>w h</p>
    <p>ic h it</p>
    <p>fl o w</p>
    <p>s</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>fa m</p>
    <p>il ia</p>
    <p>r it</p>
    <p>y</p>
    <p>in fo</p>
    <p>:</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>u se</p>
    <p>d a s a</p>
    <p>n o u</p>
    <p>n is</p>
    <p>v e r y</p>
    <p>r a</p>
    <p>r e .</p>
    <p>start probability end probability content probability</p>
    <p>Necessity of the Content Model</p>
    <p>When the answer is long, boundary words carry little information.</p>
  </div>
  <div class="page">
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>-L R</p>
    <p>B</p>
    <p>n o u</p>
    <p>n</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>T h</p>
    <p>e</p>
    <p>n o u</p>
    <p>n</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>h a s 1</p>
    <p>se n</p>
    <p>se</p>
    <p>: 1 . a</p>
    <p>m e a su</p>
    <p>r e o f</p>
    <p>th e</p>
    <p>q u</p>
    <p>a n</p>
    <p>ti ty o</p>
    <p>f</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic it</p>
    <p>y</p>
    <p>-L R</p>
    <p>B</p>
    <p>d e te</p>
    <p>r m</p>
    <p>in e d</p>
    <p>b y</p>
    <p>th e</p>
    <p>a m</p>
    <p>o u</p>
    <p>n t</p>
    <p>o f</p>
    <p>a n</p>
    <p>e le</p>
    <p>c tr</p>
    <p>ic</p>
    <p>c u</p>
    <p>r r e n</p>
    <p>t</p>
    <p>a n</p>
    <p>d</p>
    <p>th e</p>
    <p>ti m</p>
    <p>e</p>
    <p>fo r</p>
    <p>w h</p>
    <p>ic h it</p>
    <p>fl o w</p>
    <p>s</p>
    <p>-R R</p>
    <p>B - .</p>
    <p>fa m</p>
    <p>il ia</p>
    <p>r it</p>
    <p>y</p>
    <p>in fo</p>
    <p>:</p>
    <p>c h</p>
    <p>a r g e</p>
    <p>u n</p>
    <p>it</p>
    <p>u se</p>
    <p>d a s a</p>
    <p>n o u</p>
    <p>n is</p>
    <p>v e r y</p>
    <p>r a</p>
    <p>r e .</p>
    <p>start probability end probability content probability</p>
    <p>Necessity of the Content Model</p>
    <p>Content words reflect the real semantics of this answer.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Multi-passage MRC: much more misleading answers</p>
    <p>End-to-end model for multi-passage MRC:</p>
    <p>Find the answer boundary</p>
    <p>Model the answer content</p>
    <p>Cross-passage answer verification</p>
    <p>Joint training and prediction</p>
    <p>SOTA performance on two datasets created from real-world web data:</p>
    <p>MS-MARCO (English)</p>
    <p>DuReader (Chinese)</p>
  </div>
  <div class="page">
    <p>References 1) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+</p>
    <p>questions for machine comprehension of text.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Q &amp; A</p>
    <p>Contact: yizhong@pku.edu.cn</p>
  </div>
</Presentation>
