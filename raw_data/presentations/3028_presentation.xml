<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Exploring the Hidden Dimension in Graph Processing</p>
    <p>Mingxing Zhang, Yongwei Wu, Kang Chen, *Xuehai Qian, Xue Li, and Weimin Zheng</p>
    <p>Tsinghua University *University of Shouthern California</p>
  </div>
  <div class="page">
    <p>Graph is Ubiquitous</p>
    <p>Google: &gt; 1 trillion indexed pages Facebook: &gt; 800</p>
    <p>million active users</p>
    <p>Information Network Biological Network</p>
    <p>De Bruijn: 4k nodes (k = 20,  , 40)31 billion RDF triples</p>
    <p>in 2011</p>
    <p>Web Graph Social Graph</p>
    <p>Acknowledgement: Arijit Khan, Sameh Elnikety [VLDB '14]</p>
  </div>
  <div class="page">
    <p>Graph Computing is Even More Ubiquitous</p>
    <p>Traditional Graph Analysis Many graph applications aim at analyzing the property of graphs</p>
    <p>Examples Shortest Path</p>
    <p>Triangle Counting</p>
    <p>PageRank</p>
    <p>Connecting Component</p>
  </div>
  <div class="page">
    <p>Graph Computing is Even More Ubiquitous</p>
    <p>Traditional Graph Analysis Many graph applications aim at analyzing the property of graphs</p>
    <p>Examples Shortest Path</p>
    <p>Triangle Counting</p>
    <p>PageRank</p>
    <p>Connecting Component</p>
    <p>MLDM as Graph Computing Many MLDM applications can be modeled and computed as graph problems</p>
    <p>Examples Collaborative Filtering</p>
    <p>SpMM</p>
    <p>Neural Network</p>
    <p>Matrix Factorization</p>
  </div>
  <div class="page">
    <p>dashed circle)</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>Original Graph V0 V3</p>
    <p>V2</p>
    <p>Node 0</p>
    <p>V3</p>
    <p>Node 1</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>Node 2</p>
    <p>V3</p>
    <p>V2</p>
    <p>Node 3</p>
    <p>Partition into 4 nodes with 1D partitioning</p>
  </div>
  <div class="page">
    <p>Vertex Program Each vertex program can read and modify the neighborhood of a vertex</p>
    <p>Communication by messages (e.g. Pregel)</p>
    <p>by shared state (e.g. GraphLab)</p>
    <p>Vertex Program</p>
    <p>Granularity of task dispatching is vertex, which is the SAME as</p>
    <p>the granularity of data dispatching</p>
    <p>Parallelism by running multiple vertex programs simultaneously</p>
    <p>Problem SKEW!!!</p>
  </div>
  <div class="page">
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>Original Graph Partition into 4 nodes with 2D partitioning</p>
    <p>dashed circle)</p>
    <p>V0 V3</p>
    <p>V2</p>
    <p>Node 0</p>
    <p>V3</p>
    <p>V2</p>
    <p>Node 1</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>Node 2</p>
    <p>V1 V2</p>
    <p>Node 3</p>
  </div>
  <div class="page">
    <p>Gather( ) Y</p>
    <p>1 + 2  3</p>
    <p>Y</p>
    <p>Gather User Defined:</p>
    <p>Apply( , )  YY</p>
    <p>Y Y</p>
    <p>Scaber( ) Y</p>
    <p>Y</p>
    <p>Apply User Defined:</p>
    <p>Scatter User Defined:</p>
    <p>PowerGraph</p>
    <p>Granularity of task dispatching is edge, which is also the SAME as the granularity of data dispatching</p>
  </div>
  <div class="page">
    <p>Is this the END of task partitioning?</p>
    <p>YES</p>
    <p>Vertex can be attached with multiple edges</p>
    <p>But an edge is connected to only two vertices</p>
    <p>Indivisible!</p>
  </div>
  <div class="page">
    <p>Is this the END of task partitioning?</p>
    <p>YES</p>
    <p>Vertex can be attached with multiple edges</p>
    <p>But an edge is connected to only two vertices</p>
    <p>Indivisible!  NOT true for many problems</p>
    <p>NO</p>
    <p>Each vertex/edge can be further partitioned!</p>
  </div>
  <div class="page">
    <p>Example: Collaborative Filtering</p>
    <p>Collaborative Filtering estimating missing ratings based on a given incomplete set of (user, item) ratings</p>
    <p>In graph view Each vertex is attached with a feature vector with size D.</p>
    <p>A typical pattern of modelling MLDM problem as graph problem.</p>
    <p>M: # of items</p>
    <p>N : # o f u</p>
    <p>se rs</p>
    <p>R[u,v] *</p>
    <p>D</p>
    <p>D</p>
    <p>P[u]</p>
    <p>Q[v]R P QT</p>
    <p>R[u,v]  &lt;P[u], Q[v]&gt;</p>
    <p>Matrix View</p>
    <p>Graph View</p>
    <p>P0 P1 P2 Pu PN</p>
    <p>Q0 Q1 Q2 Qv QM R[u,v]</p>
  </div>
  <div class="page">
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>Original Graph Partition into 4 nodes with 3D partitioning</p>
    <p>Node 0,0 Node 0,1</p>
    <p>Node 1,0 Node 1,1</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>We found a NEW dimension, shouldnt we partition it?</p>
    <p>Observation</p>
    <p>These vector properties are usually operated by</p>
    <p>element-wise operators</p>
    <p>Easy to parallelize</p>
    <p>v With no communication!</p>
  </div>
  <div class="page">
    <p>Two Kinds of Communications</p>
    <p>Intra-layer Communication Less sub-graphs</p>
    <p>Less replicas</p>
    <p>Reduced!</p>
    <p>Node 0,0</p>
    <p>Node 1,0</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>Node 0,1</p>
    <p>Node 1,1</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
  </div>
  <div class="page">
    <p>Two Kinds of Communications</p>
    <p>Intra-layer Communication Less sub-graphs</p>
    <p>Less replicas</p>
    <p>Reduced!</p>
    <p>Inter-layer Communication Not exist before</p>
    <p>Increased!</p>
    <p>Node 0,0</p>
    <p>Node 1,0</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>Node 0,1</p>
    <p>Node 1,1</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
  </div>
  <div class="page">
    <p>Two Kinds of Communications</p>
    <p>Intra-layer Communication Less sub-graphs</p>
    <p>Less replicas</p>
    <p>Reduced!</p>
    <p>Inter-layer Communication Not exist before</p>
    <p>Increased!</p>
    <p>Node 0,0</p>
    <p>Node 1,0</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>Node 0,1</p>
    <p>Node 1,1</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
    <p>V0 V3</p>
    <p>V1 V2</p>
  </div>
  <div class="page">
    <p>New System: CUBE</p>
    <p>Existing models does not formalize the inter-layer communication</p>
    <p>CUBE</p>
    <p>Adopts 3D partitioning</p>
    <p>Implements a new programming model UPPS</p>
    <p>Use a matrix backend</p>
  </div>
  <div class="page">
    <p>CUBEData Model of CUBE</p>
    <p>The data graph model is also used in CUBE, but each vertex/edge can be attached with two kinds of data.</p>
    <p>Two classes of data</p>
    <p>DShare :-- an indivisible property</p>
    <p>v represented by a single variable.</p>
    <p>DColle :-- a divisible collection of property vector</p>
    <p>v stored as a vector of variables</p>
    <p>v len(DColle) == collection size SC</p>
  </div>
  <div class="page">
    <p>CUBE3D Partitioner</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>S</p>
    <p>S</p>
    <p>S V0 V3</p>
    <p>V1 V2</p>
    <p>S</p>
    <p>S</p>
    <p>S</p>
    <p>S</p>
    <p>V0 V3</p>
    <p>V1</p>
    <p>S</p>
    <p>S</p>
    <p>S V0 V3</p>
    <p>V1 V2</p>
    <p>S</p>
    <p>S</p>
    <p>S</p>
    <p>S</p>
    <p>Node 0,0 Node 1,0</p>
    <p>Node 0,1 Node 1,1</p>
    <p>L</p>
    <p>=</p>
    <p>DShare is replicated</p>
    <p>DColle is equally partitioned</p>
    <p>Partitioned with P</p>
  </div>
  <div class="page">
    <p>CUBEProgramming Model</p>
    <p>Update :-- inter-layer communication v UpdateVertex: read/write all the data of a vertex</p>
    <p>v UpdateEdge: read/write all the data of an edge</p>
    <p>Push, Pull, Sink :-- intra-layer communication</p>
    <p>Variations of GAS, for an edge edg that connects (src, dst)</p>
    <p>v Push: use src and edg to update dst.</p>
    <p>v Pull: use dst and edg to update src.</p>
    <p>v Sink: use src and dst to update edg.</p>
  </div>
  <div class="page">
    <p>CUBEMatrix Backend</p>
    <p>Vertex Frontend</p>
    <p>Easy to program</p>
    <p>Similar to existing works</p>
    <p>Less efficiency</p>
    <p>Matrix Backend</p>
    <p>Hard to program</p>
    <p>High efficiency</p>
    <p>v Simple indexing</p>
    <p>v Hilbert order</p>
    <p>MAP</p>
    <p>Mapping from vertex frontend to matrix backend can significantly speedup the system; a useful method pioneered by GraphMat.</p>
  </div>
  <div class="page">
    <p>CUBEEvaluation</p>
    <p>Baseline: PowerGraph and PowerLyra  the default partitioner oblivious is used for PowerGraph</p>
    <p>every possible partitioner is tested for PowerLyra and</p>
    <p>the best is used</p>
    <p>P of CUBE is the same as the partitioner used by PowerLyra</p>
    <p>Platform: 8-node system; connected with 1Gb Ethernet</p>
    <p>Dataset</p>
    <p>Name |U| |V| |E| Best 2D Partitioner Libimseti 135,359 168,791 17,359,346 Hybrid-cut Last.fm 359,349 211,067 17,559,530 Bi-cut Netflix 17,770 480,189 100,480,507 Bi-cut</p>
  </div>
  <div class="page">
    <p>Micro Benchmarks: SpMM</p>
    <p>=</p>
    <p>* P[u]</p>
    <p>Q[v]</p>
    <p>N R[w,v] R</p>
    <p>D</p>
    <p>P</p>
    <p>M</p>
    <p>DQ</p>
    <p>Q[v] = R[u,v]*P[u] + R[w,v]*P[w]</p>
    <p>R[u,v]</p>
    <p>P[w]</p>
    <p>SpMM</p>
    <p># of layers</p>
    <p>Execution Time of SpMM</p>
    <p>Libimseti, Sc = 256 Lastfm, Sc = 256 Libimseti, Sc = 1024</p>
    <p># of replicas</p>
    <p># of subgraphs</p>
    <p>Communication cost</p>
    <p># of layers</p>
    <p>proportional to</p>
    <p>negative relation</p>
    <p>inversely proportional to</p>
  </div>
  <div class="page">
    <p>Micro Benchmarks: SumV &amp; SumE</p>
    <p>Execution Time of SumV Libimseti, Sc = 256 Lastfm, Sc = 256 Libimseti, Sc = 1024</p>
    <p>Execution Time of SumE Libimseti, Sc = 256 Lastfm, Sc = 256 Libimseti, Sc = 1024</p>
    <p>SumV: summing up each vertexs vector</p>
    <p>SumE: summing up each edges vector</p>
    <p>!&quot;# ! , L is # of layersCommunication cost</p>
    <p>proportional to</p>
  </div>
  <div class="page">
    <p>CUBECompare to PowerLyra &amp; PowerGraph</p>
    <p>Dataset D</p>
    <p>Execution Time GD ALS</p>
    <p>PowerGraph PowerLyra CUBE PowerGraph PowerLyra CUBE</p>
    <p>Libimseti 64 6.82 6.89 2.59 (4) 87.0 86.8 28 (64)</p>
    <p>Lastfm 64 10.4 9.86 2.48 (4) 158 111 57 (64)</p>
    <p>Netflix 64 18.3 7.42 4.16 (1) 179 66.0 42.5 (8)</p>
    <p>Up to 4.7x speedup (V.S. PowerLyra)</p>
    <p>Up to 7.3x speedup (V.S. PowerGraph)</p>
  </div>
  <div class="page">
    <p>CUBEPiecewise Breakdown Network Reduction on GD Network Reduction on ALS</p>
    <p>About half of the speedup is from 3D partitioning (up to 2x) for Lastfm and Libimseti.</p>
    <p>Not useful for Netflix.</p>
    <p>v Can achieve 2.5x speedup on Netflix if D is set to 2048.</p>
    <p>Almost all the speedup is from network reduction.</p>
    <p>v Computation of ALS is dominated by DSYSV, an O(N3) computation kernel.</p>
  </div>
  <div class="page">
    <p>CUBEMemory Consumption</p>
    <p># of layers</p>
    <p>Total Memory Consumption Memory Consumption</p>
    <p>better than L=1, i.e., 2D only</p>
    <p>best for time is not always best for memory consumption</p>
    <p>Partitioning Time similar curve as memory consumption</p>
    <p>Total memory consumption for running ALS on 64 workers with D=32, which means that SC=D2+D=1056</p>
  </div>
  <div class="page">
    <p>Conclusion  We found that, for certain graph problems,</p>
    <p>vertices and edges are not indivisible.</p>
    <p>We propose CUBE, which</p>
    <p>adopts 3D partitioning;</p>
    <p>uses matrix backend;</p>
    <p>achieves up to 4.7x speedup.</p>
  </div>
  <div class="page">
    <p>Thank You!!</p>
  </div>
</Presentation>
