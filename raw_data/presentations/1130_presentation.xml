<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DeepCPU: Serving RNN-based Deep Learning Models 10x Faster</p>
    <p>Minjia Zhang*, Samyam Rajbhandari*, Wenhan Wang, Yuxiong He</p>
    <p>Microsoft AI and Research</p>
    <p>(*Equal contribution)</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>DeepCPU, the fastest deep learning serving library for recurrent neural networks (RNNs) on CPUs</p>
    <p>10x lower latency and cost than TensorFlow and CNTK</p>
    <p>Empower CPU to beat GPU for RNN serving</p>
    <p>Ship DL models with great latency/cost reduction in Microsoft</p>
  </div>
  <div class="page">
    <p>Deep Learning Serving Challenges</p>
    <p>Long serving latency blocks deployment</p>
    <p>Support advance models while meeting latency SLA and saving cost</p>
    <p>DL Scenarios Original Latency Latency Target</p>
    <p>Attention sum reader ~100ms &lt; 10ms</p>
    <p>Bidirectional attention flow model</p>
    <p>~107ms &lt; 10ms</p>
    <p>Text similarity model 10ms for [query, 1 passage]</p>
    <p>x 150 passages &lt; 5ms</p>
    <p>Seq2seq model ~51ms &lt; 5ms</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Overview of Recurrent Neural Network (RNN)</p>
    <p>DeepCPU-Powered Real-World RNN-Based Models</p>
    <p>Library Features</p>
    <p>Performance Optimizations</p>
    <p>Performance Results</p>
  </div>
  <div class="page">
    <p>RNN Serving Performance Challenges</p>
    <p>Xt-1 Xt Xt+1</p>
    <p>Ot-1 Ot Ot+1</p>
    <p>St-1 St St+1W W W</p>
    <p>Language Modeling</p>
    <p>Machine Translation</p>
    <p>Machine Reading Comprehension</p>
    <p>Conversation Bot</p>
    <p>Speech Recognition</p>
    <p>Limited Parallelism</p>
    <p>Limited Bandwidth</p>
    <p>Small batch size  Sequential dependency</p>
    <p>Vector-matrix multiplication  Low data reuse</p>
    <p>W1</p>
    <p>W2</p>
    <p>Xt</p>
    <p>St-1 St</p>
  </div>
  <div class="page">
    <p>Case 1: Question Answering</p>
    <p>Direct answer</p>
    <p>Good quality</p>
  </div>
  <div class="page">
    <p>Model Complexity and Latency</p>
    <p>Bidirectional Attention Flow Model (BiDAF)</p>
  </div>
  <div class="page">
    <p>Optimization Results</p>
    <p>Bidirectional Attention Flow Model (BiDAF)</p>
    <p>Our Optimization</p>
    <p>DeepCPU implementation for BiDAF</p>
    <p>Same accuracy Latency: 107ms to 4.1ms (&gt;20 times speedup) Non-shippable -&gt; Shippable</p>
  </div>
  <div class="page">
    <p>Case 2: Text Similarity Ranking</p>
    <p>Generate text similarities using deep model</p>
    <p>Model: word embedding + encoding with GRUs + conv + max-pool + scoring</p>
    <p>Latency SLA: 5ms for &lt;query, top 150 passages&gt;</p>
    <p>Tensorflow serving latency</p>
    <p>single &lt;query, passage&gt; pair: 10ms</p>
    <p>&lt;query, 150 passages&gt;: fan-out to 150 machines</p>
    <p>Our optimizations</p>
    <p>&lt;query, 150 passages&gt;: 5ms, one machine (&gt;100x throughput gain)</p>
    <p>Reduce thousands of machines to serve Bing traffic</p>
    <p>non-shippable</p>
    <p>shippable save machines</p>
  </div>
  <div class="page">
    <p>DeepCPU: Fast DL Serving Library on CPUs</p>
    <p>RNN family  GRU cell and GRU sequence  LSTM cell and LSTM sequence  Stacked RNN networks</p>
    <p>Fundamental building blocks and common DL Layers  Matrix multiplication kernels, activation functions  high-way network, max pool layer, MLP layer</p>
    <p>DL layers for MRC and conversation models  Variety of attention layers  seq2seq decoding with beam search</p>
  </div>
  <div class="page">
    <p>Performance Critical Factors Implications</p>
    <p>Limited Parallelism Poor Scalability</p>
    <p>Poor Data Locality Poor Scalability and Performance due to reading data from slow memory</p>
    <p>Deep Dive : RNN Performance Bottleneck</p>
    <p>W1W1W1</p>
    <p>What is Atom?</p>
    <p>Step 1 Step 2 Step 3</p>
    <p>W2 W2 W2</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>Deep Dive : DeepCPU RNN Optimizations</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Theorem</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Deep Dive : Summary Nave Schedule</p>
    <p>Schedule Generator + MM-Fusion + Parallelism</p>
    <p>+ Cache-Aware Partitioning + Weight-Centric Streamlining</p>
  </div>
  <div class="page">
    <p>Performance : DeepCPU vs TF vs CNTK</p>
    <p>Average LSTM speedup  DeepCPU is 23x faster</p>
    <p>than Tensorflow</p>
    <p>DeepCPU is 31x faster than CNTK</p>
    <p>Average GRU speedup  DeepCPU is 16x faster</p>
    <p>than Tensorflow</p>
    <p>DeepCPU is 25x faster than CNTK</p>
  </div>
  <div class="page">
    <p>DeepCPU vs GPU</p>
    <p>G ig</p>
    <p>a fl</p>
    <p>o p</p>
    <p>s</p>
    <p>G ig</p>
    <p>a fl</p>
    <p>o p</p>
    <p>s</p>
    <p>Batch Size = 1, Sequence Length = 100 Batch Size = 1, Input/Hidden Dimension = 256</p>
    <p>Input/Hidden Dimensions Sequence Length</p>
  </div>
  <div class="page">
    <p>Summary of DeepCPU</p>
    <p>DeepCPU, the fastest deep learning serving library for recurrent neural networks (RNNs) on CPUs</p>
    <p>10x lower latency and cost than Tensorflow and CNTK</p>
    <p>Empower CPU to beat GPU for RNN serving</p>
    <p>Ship DL models in Microsoft with great latency/cost reduction</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Questions?</p>
  </div>
</Presentation>
