<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Write Policies for Host-side Flash Caches</p>
    <p>Ricardo Koller, FIU (currently at VMware) Leonardo Marmol, FIU Raju Rangaswami, FIU</p>
    <p>Swaminathan Sundararaman, FusionIO Nisha Talagala, FusionIO</p>
    <p>Ming Zhao, FIU</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Virtualized environments typically use networked storage</p>
    <p>Higher VM consolidation demands</p>
    <p>Flash caches used to alleviate VMs performance demands</p>
    <p>Recent literature has argued in favor of write-through [Byan12, VFCache12, Hensbergen06, Srinivasan12].</p>
  </div>
  <div class="page">
    <p>Architecture</p>
    <p>Networked storage</p>
    <p>App / VM</p>
    <p>OS / hypervisor</p>
    <p>Flash cache</p>
    <p>Host</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Write caching</p>
    <p>The advantage of caching a read is unquestionable  What about writes?</p>
    <p>Application</p>
    <p>Hypervisor</p>
    <p>Solid State Drive</p>
    <p>Networked Storage</p>
    <p>T im</p>
    <p>e</p>
    <p>Write-through Write-back</p>
    <p>Eviction</p>
    <p>Optional Read: Write:</p>
    <p>Figure 2: Write-back and write-through policies with a hypervisor managed flash cache.</p>
    <p>back implementations exist, these are accompanied with appropriate disclaimers about storage-level inconsistencies after a failure [9, 35]. Network storage inconsistency compromises both data availability after a flashcache/host failure and the correctness of network storage level solutions such as replication and backup.</p>
    <p>In this paper, we develop two new write policies for host-side flash caches that provide close to full writeback performance without compromising network storage consistency. Ordered write-back is designed to work seamlessly, requiring no changes to existing storage systems, and outperforms conventional write-through. Journaled write-back relies on a logical disk [10] interface that implements atomic write groups at the storage system and offers significant performance gains over the ordered policy. Ordered write-back ensures that blocks get evicted from the flash cache and written out to the networked storage in the original write order. Journaled write-back allows overwrites in the cache but ensures that the networked storage atomically transitions from one consistent point to the next. Both policies trade-off strict durability of writes in their design and support an eventually consistent model for the network storage. Under host and/or host-flash failures, these policies do result in data staleness (i.e., loss of recent updates) at the network storage. They thus apply only to applications which can tolerate a non-zero RPO. Journaled write-back additionally allows for a straightforward implementation of application-level storage consistency, a stricter form of consistency than the transactional consistency provided by write-through.</p>
    <p>We implemented the new ordered and journaled write back as well as conventional write-through and writeback in Linux and evaluated these policies for the PostMark [19], TPC-C [36], Filebench [26], and YCSB [8] benchmarks. The new policies performed significantly better than write-through, with throughput improvements ranging from 10% to 8x for ordered write-back and 50% to 10x for journaled write-back across the four benchmarks. Our sensitivity analysis illustrates the impact of cache size and file system used on the relative performance of these policies. We find that except under extremely low cache sizes, ordered write-back performs better than write-through and that journaled write</p>
    <p>back can trade-off staleness for performance, approaching, and in some cases, exceeding conventional writeback performance. Finally, our findings were largely preserved across the four Linux filesystems that we evaluated including ext2, btrfs and three variants each of the journaling ext3 and ext4 file systems.</p>
    <p>Persistent host-side flash caches are different than volatile DRAM caches in several respects. In this section, we examine arguments in support of optimizing writes differently in persistent caches and survey the related work in the literature.</p>
    <p>Recent literature has argued in favor of managing persistent host-side flash-based caches as write-through and to optimize exclusively for reads [6, 12, 17, 35]. However, production storage workloads comprise of a significant (often dominant) fraction of writes [5, 21, 29, 32, 37]. Recent studies also report a trend of increasing writes:reads ratios in production workloads [22, 30]. This is a consequence of newer systems absorbing more reads within larger DRAM caches at hosts while all writes get written out to storage for durability. Employing a write-through policy thus, unfortunately, represents a lost opportunity.</p>
    <p>Caches that maintain dirty blocks are referred as writeback and caches that do not as writethrough. In case of write-back, the write is acknowledged immediately after the write to the cache. With write-through, writes are first committed to network storage and then to the cache before completion is acknowledged to the guest. Figure 2 illustrates these policies for a hypervisor-managed flashbased cache. As noted earlier, the I/O latency and peak I/O throughput implication of having one policy versus the other is significant (Figure 1). This translates to significantly reduced network storage provisioning requirements with write-back caching for production workloads that are typically bursty [22, 29, 15]</p>
    <p>A write-back caching policy offers critical performance benefits. First, it significantly lowers write latencies and improves write throughput (per Figure 1); write bursts, if any, get absorbed in the cache, making the best possible use of the high-performance flash layer. Consequently, networked storage can be provisioned for average (instead of peak) write I/O volume. Second, since writeback allows for overwrites (coalescing) in the cache, it reduces the volume of write I/O traffic to the networked storage system for workloads with write locality. Third, since application writes are effectively decoupled from network storage writes, higher levels of I/O parallelism (than available in the application I/O stream) be</p>
  </div>
  <div class="page">
    <p>Write caching</p>
    <p>The advantage of caching a read is unquestionable  What about writes?</p>
    <p>Application</p>
    <p>Hypervisor</p>
    <p>Solid State Drive</p>
    <p>Networked Storage</p>
    <p>T im</p>
    <p>e</p>
    <p>Write-through Write-back</p>
    <p>Eviction</p>
    <p>Optional Read: Write:</p>
    <p>Figure 2: Write-back and write-through policies with a hypervisor managed flash cache.</p>
    <p>back implementations exist, these are accompanied with appropriate disclaimers about storage-level inconsistencies after a failure [9, 35]. Network storage inconsistency compromises both data availability after a flashcache/host failure and the correctness of network storage level solutions such as replication and backup.</p>
    <p>In this paper, we develop two new write policies for host-side flash caches that provide close to full writeback performance without compromising network storage consistency. Ordered write-back is designed to work seamlessly, requiring no changes to existing storage systems, and outperforms conventional write-through. Journaled write-back relies on a logical disk [10] interface that implements atomic write groups at the storage system and offers significant performance gains over the ordered policy. Ordered write-back ensures that blocks get evicted from the flash cache and written out to the networked storage in the original write order. Journaled write-back allows overwrites in the cache but ensures that the networked storage atomically transitions from one consistent point to the next. Both policies trade-off strict durability of writes in their design and support an eventually consistent model for the network storage. Under host and/or host-flash failures, these policies do result in data staleness (i.e., loss of recent updates) at the network storage. They thus apply only to applications which can tolerate a non-zero RPO. Journaled write-back additionally allows for a straightforward implementation of application-level storage consistency, a stricter form of consistency than the transactional consistency provided by write-through.</p>
    <p>We implemented the new ordered and journaled write back as well as conventional write-through and writeback in Linux and evaluated these policies for the PostMark [19], TPC-C [36], Filebench [26], and YCSB [8] benchmarks. The new policies performed significantly better than write-through, with throughput improvements ranging from 10% to 8x for ordered write-back and 50% to 10x for journaled write-back across the four benchmarks. Our sensitivity analysis illustrates the impact of cache size and file system used on the relative performance of these policies. We find that except under extremely low cache sizes, ordered write-back performs better than write-through and that journaled write</p>
    <p>back can trade-off staleness for performance, approaching, and in some cases, exceeding conventional writeback performance. Finally, our findings were largely preserved across the four Linux filesystems that we evaluated including ext2, btrfs and three variants each of the journaling ext3 and ext4 file systems.</p>
    <p>Persistent host-side flash caches are different than volatile DRAM caches in several respects. In this section, we examine arguments in support of optimizing writes differently in persistent caches and survey the related work in the literature.</p>
    <p>Recent literature has argued in favor of managing persistent host-side flash-based caches as write-through and to optimize exclusively for reads [6, 12, 17, 35]. However, production storage workloads comprise of a significant (often dominant) fraction of writes [5, 21, 29, 32, 37]. Recent studies also report a trend of increasing writes:reads ratios in production workloads [22, 30]. This is a consequence of newer systems absorbing more reads within larger DRAM caches at hosts while all writes get written out to storage for durability. Employing a write-through policy thus, unfortunately, represents a lost opportunity.</p>
    <p>Caches that maintain dirty blocks are referred as writeback and caches that do not as writethrough. In case of write-back, the write is acknowledged immediately after the write to the cache. With write-through, writes are first committed to network storage and then to the cache before completion is acknowledged to the guest. Figure 2 illustrates these policies for a hypervisor-managed flashbased cache. As noted earlier, the I/O latency and peak I/O throughput implication of having one policy versus the other is significant (Figure 1). This translates to significantly reduced network storage provisioning requirements with write-back caching for production workloads that are typically bursty [22, 29, 15]</p>
    <p>A write-back caching policy offers critical performance benefits. First, it significantly lowers write latencies and improves write throughput (per Figure 1); write bursts, if any, get absorbed in the cache, making the best possible use of the high-performance flash layer. Consequently, networked storage can be provisioned for average (instead of peak) write I/O volume. Second, since writeback allows for overwrites (coalescing) in the cache, it reduces the volume of write I/O traffic to the networked storage system for workloads with write locality. Third, since application writes are effectively decoupled from network storage writes, higher levels of I/O parallelism (than available in the application I/O stream) be</p>
  </div>
  <div class="page">
    <p>Write caching performance</p>
    <p>The advantage of caching a read is unquestionable  What about writes?</p>
    <p>Application</p>
    <p>Hypervisor</p>
    <p>Solid State Drive</p>
    <p>Networked Storage</p>
    <p>T im</p>
    <p>e</p>
    <p>Write-through Write-back</p>
    <p>Eviction</p>
    <p>Optional Read: Write:</p>
    <p>Figure 2: Write-back and write-through policies with a hypervisor managed flash cache.</p>
    <p>back implementations exist, these are accompanied with appropriate disclaimers about storage-level inconsistencies after a failure [9, 35]. Network storage inconsistency compromises both data availability after a flashcache/host failure and the correctness of network storage level solutions such as replication and backup.</p>
    <p>In this paper, we develop two new write policies for host-side flash caches that provide close to full writeback performance without compromising network storage consistency. Ordered write-back is designed to work seamlessly, requiring no changes to existing storage systems, and outperforms conventional write-through. Journaled write-back relies on a logical disk [10] interface that implements atomic write groups at the storage system and offers significant performance gains over the ordered policy. Ordered write-back ensures that blocks get evicted from the flash cache and written out to the networked storage in the original write order. Journaled write-back allows overwrites in the cache but ensures that the networked storage atomically transitions from one consistent point to the next. Both policies trade-off strict durability of writes in their design and support an eventually consistent model for the network storage. Under host and/or host-flash failures, these policies do result in data staleness (i.e., loss of recent updates) at the network storage. They thus apply only to applications which can tolerate a non-zero RPO. Journaled write-back additionally allows for a straightforward implementation of application-level storage consistency, a stricter form of consistency than the transactional consistency provided by write-through.</p>
    <p>We implemented the new ordered and journaled write back as well as conventional write-through and writeback in Linux and evaluated these policies for the PostMark [19], TPC-C [36], Filebench [26], and YCSB [8] benchmarks. The new policies performed significantly better than write-through, with throughput improvements ranging from 10% to 8x for ordered write-back and 50% to 10x for journaled write-back across the four benchmarks. Our sensitivity analysis illustrates the impact of cache size and file system used on the relative performance of these policies. We find that except under extremely low cache sizes, ordered write-back performs better than write-through and that journaled write</p>
    <p>back can trade-off staleness for performance, approaching, and in some cases, exceeding conventional writeback performance. Finally, our findings were largely preserved across the four Linux filesystems that we evaluated including ext2, btrfs and three variants each of the journaling ext3 and ext4 file systems.</p>
    <p>Persistent host-side flash caches are different than volatile DRAM caches in several respects. In this section, we examine arguments in support of optimizing writes differently in persistent caches and survey the related work in the literature.</p>
    <p>Recent literature has argued in favor of managing persistent host-side flash-based caches as write-through and to optimize exclusively for reads [6, 12, 17, 35]. However, production storage workloads comprise of a significant (often dominant) fraction of writes [5, 21, 29, 32, 37]. Recent studies also report a trend of increasing writes:reads ratios in production workloads [22, 30]. This is a consequence of newer systems absorbing more reads within larger DRAM caches at hosts while all writes get written out to storage for durability. Employing a write-through policy thus, unfortunately, represents a lost opportunity.</p>
    <p>Caches that maintain dirty blocks are referred as writeback and caches that do not as writethrough. In case of write-back, the write is acknowledged immediately after the write to the cache. With write-through, writes are first committed to network storage and then to the cache before completion is acknowledged to the guest. Figure 2 illustrates these policies for a hypervisor-managed flashbased cache. As noted earlier, the I/O latency and peak I/O throughput implication of having one policy versus the other is significant (Figure 1). This translates to significantly reduced network storage provisioning requirements with write-back caching for production workloads that are typically bursty [22, 29, 15]</p>
    <p>A write-back caching policy offers critical performance benefits. First, it significantly lowers write latencies and improves write throughput (per Figure 1); write bursts, if any, get absorbed in the cache, making the best possible use of the high-performance flash layer. Consequently, networked storage can be provisioned for average (instead of peak) write I/O volume. Second, since writeback allows for overwrites (coalescing) in the cache, it reduces the volume of write I/O traffic to the networked storage system for workloads with write locality. Third, since application writes are effectively decoupled from network storage writes, higher levels of I/O parallelism (than available in the application I/O stream) be</p>
    <p>Less latency  Extra read</p>
    <p>Just delaying =</p>
  </div>
  <div class="page">
    <p>Write through vs. write back: Performance</p>
    <p>TPCC results: write-back performs better than write-through</p>
    <p>Figure 3: Transaction response times and throughput for TPC-C with 2GB of RAM and 25 warehouses.</p>
    <p>seq seq rnd rnd</p>
    <p>write read read write</p>
    <p>Local SSD 82 93 177 66 iSCSI RAID HDD 891 593 4813 5285</p>
    <p>Table 1: Access times in microseconds for local PCI-e SSD vs. networked RAID HDD storage.</p>
    <p>come possible when writing to networked storage; storage systems perform better at higher levels of I/O parallelism [4, 7, 11, 16, 33, 34]. Fourth, reads that miss the flash cache experience less I/O contention at the network storage due to write coalescing at the cache layer; the read cache can thus be populated with the changes in the working set more quickly as workload phases change. Finally, since the cache is effective for both reads and writes, cache resizing can potentially serve as a storage QoS control knob (e.g., for I/O latency control) for all workloads including those that are write-intensive.</p>
    <p>To quantify the write-back performance advantage, we ran the TPC-C OLTP benchmark which mimics the operations of a typical web retailer including creating and delivering orders, recording payments, checking order status and monitoring inventory levels configured with 10 warehouses [36]. We configured a RAID5 storage array of 8 7200 RPM disks over iSCSI to be the network store and an OCZ PCI-e flash-based SSD as the storage cache on the host. SSD random writes were 80 times faster than the networked iSCSI store. Other aspects of the performance difference are summarized in Table 1.</p>
    <p>Figure 3 depicts median response times for the 4 types of TPC-C transactions. Having a write-back cache thus reduces the average response time by at least 75X across the transaction types. Throughput measured as TpmC (new order transactions/minute) showed an increase of 3X with write-back compared to the write-through cache. It is important to point out here that the available SSD</p>
    <p>Consistency</p>
    <p>Staleness</p>
    <p>Per f ormance</p>
    <p>Write-back</p>
    <p>Write-through</p>
    <p>Figure 4: Trade-offs in conventional write caching policies.</p>
    <p>throughput was not stressed in write-back mode at the queue depths offered by the workload. For many enterprise applications, lower worst-case performance and lower performance variance are as important as averagecase performance. The new write-back caching policies that we develop in this paper target both these metrics by mimicking the basic behavior of synchronous and lowlatency local flash updates and asynchronous network storage updates in conventional write-back.</p>
    <p>Write policies make different trade-offs with respect to data consistency, data staleness, and performance as indicated in Figure 4. Conventional write-through and writeback represent merely two extreme points in a spectrum of possible trade-offs. Write-through provides strict durability of writes, i.e., does not introduce any data staleness. On the other hand, write-back fundamentally alters the notion of data durability for applications and introduces both data inconsistency and staleness.</p>
    <p>Interestingly, this dilemma has parallels in the remote mirroring for disaster recovery literature. Asynchronous remote mirroring solutions ensure data consistency but introduce data staleness at the target storage system [31, 18, 20, 38]. Key to this argument is the lower cost of data loss after a disaster event when using asynchronous (akin to write-back) mirroring relative to the cost of the high-speed, WAN links necessary to implement fully synchronous (akin to write-through) mirroring to the remote site. While some applications, like financial databases, may require a recovery point objective (RPO) of zero, it has been pointed out that other applications such as non-critical filers and document servers and even online retailers can tolerate non-zero RPO [20]. With such applications, the performance cost of zero data staleness can become prohibitive and trading-off data staleness for performance becomes attractive [20, 31].</p>
  </div>
  <div class="page">
    <p>Reasons for write-back good performance</p>
    <p>Lower latency  Writes do not need to reach networked storage</p>
    <p>Write coalescing  Reduction of writes to networked storage</p>
    <p>Parallel evictions  Batched writes to networked storage</p>
  </div>
  <div class="page">
    <p>Tradeoffs in conventional write caching policies</p>
    <p>Performance is not the only dimension</p>
    <p>Performance</p>
  </div>
  <div class="page">
    <p>Tradeoffs in conventional write caching policies</p>
    <p>Performance is not the only dimension</p>
    <p>Performance</p>
    <p>Consistency Staleness</p>
  </div>
  <div class="page">
    <p>Write through vs. write back: Staleness</p>
    <p>Time</p>
    <p>Flash cache</p>
    <p>Networked storage</p>
    <p>Write-back</p>
    <p>Old</p>
    <p>New</p>
  </div>
  <div class="page">
    <p>Write through vs. write back: Consistency example: File system consistency</p>
    <p>Time</p>
    <p>Flash cache</p>
    <p>Networked storage</p>
    <p>Write-back Delete inode Free block Commit</p>
    <p>Free block Commit</p>
  </div>
  <div class="page">
    <p>Write through vs. write back: Consistency example: File system consistency</p>
    <p>Time</p>
    <p>Networked storage</p>
    <p>Write-back</p>
    <p>Free block Commit</p>
    <p>Used! blocks:! 01010100!</p>
    <p>$ ls! file1! file2!</p>
  </div>
  <div class="page">
    <p>Write through vs. write back: Consistency and no staleness</p>
    <p>Time</p>
    <p>Flash cache</p>
    <p>Networked storage</p>
    <p>Write-through</p>
  </div>
  <div class="page">
    <p>Other options?</p>
    <p>Write-back and write-through are opposites</p>
    <p>Figure 3: Transaction response times and throughput for TPC-C with 2GB of RAM and 25 warehouses.</p>
    <p>seq seq rnd rnd</p>
    <p>write read read write</p>
    <p>Local SSD 82 93 177 66 iSCSI RAID HDD 891 593 4813 5285</p>
    <p>Table 1: Access times in microseconds for local PCI-e SSD vs. networked RAID HDD storage.</p>
    <p>come possible when writing to networked storage; storage systems perform better at higher levels of I/O parallelism [4, 7, 11, 16, 33, 34]. Fourth, reads that miss the flash cache experience less I/O contention at the network storage due to write coalescing at the cache layer; the read cache can thus be populated with the changes in the working set more quickly as workload phases change. Finally, since the cache is effective for both reads and writes, cache resizing can potentially serve as a storage QoS control knob (e.g., for I/O latency control) for all workloads including those that are write-intensive.</p>
    <p>To quantify the write-back performance advantage, we ran the TPC-C OLTP benchmark which mimics the operations of a typical web retailer including creating and delivering orders, recording payments, checking order status and monitoring inventory levels configured with 10 warehouses [36]. We configured a RAID5 storage array of 8 7200 RPM disks over iSCSI to be the network store and an OCZ PCI-e flash-based SSD as the storage cache on the host. SSD random writes were 80 times faster than the networked iSCSI store. Other aspects of the performance difference are summarized in Table 1.</p>
    <p>Figure 3 depicts median response times for the 4 types of TPC-C transactions. Having a write-back cache thus reduces the average response time by at least 75X across the transaction types. Throughput measured as TpmC (new order transactions/minute) showed an increase of 3X with write-back compared to the write-through cache. It is important to point out here that the available SSD</p>
    <p>Consistency</p>
    <p>Staleness</p>
    <p>Per f ormance</p>
    <p>Write-back</p>
    <p>Write-through</p>
    <p>Figure 4: Trade-offs in conventional write caching policies.</p>
    <p>throughput was not stressed in write-back mode at the queue depths offered by the workload. For many enterprise applications, lower worst-case performance and lower performance variance are as important as averagecase performance. The new write-back caching policies that we develop in this paper target both these metrics by mimicking the basic behavior of synchronous and lowlatency local flash updates and asynchronous network storage updates in conventional write-back.</p>
    <p>Write policies make different trade-offs with respect to data consistency, data staleness, and performance as indicated in Figure 4. Conventional write-through and writeback represent merely two extreme points in a spectrum of possible trade-offs. Write-through provides strict durability of writes, i.e., does not introduce any data staleness. On the other hand, write-back fundamentally alters the notion of data durability for applications and introduces both data inconsistency and staleness.</p>
    <p>Interestingly, this dilemma has parallels in the remote mirroring for disaster recovery literature. Asynchronous remote mirroring solutions ensure data consistency but introduce data staleness at the target storage system [31, 18, 20, 38]. Key to this argument is the lower cost of data loss after a disaster event when using asynchronous (akin to write-back) mirroring relative to the cost of the high-speed, WAN links necessary to implement fully synchronous (akin to write-through) mirroring to the remote site. While some applications, like financial databases, may require a recovery point objective (RPO) of zero, it has been pointed out that other applications such as non-critical filers and document servers and even online retailers can tolerate non-zero RPO [20]. With such applications, the performance cost of zero data staleness can become prohibitive and trading-off data staleness for performance becomes attractive [20, 31].</p>
  </div>
  <div class="page">
    <p>Other options?</p>
    <p>Asynchronous remote mirroring ensure consistency but delays writes to the remote server [Ji03, Patterson02]</p>
    <p>Some applications [Keeton04] allow RPO (Recovery Point Objective) &gt; 0</p>
    <p>Figure 3: Transaction response times and throughput for TPC-C with 2GB of RAM and 25 warehouses.</p>
    <p>seq seq rnd rnd</p>
    <p>write read read write</p>
    <p>Local SSD 82 93 177 66 iSCSI RAID HDD 891 593 4813 5285</p>
    <p>Table 1: Access times in microseconds for local PCI-e SSD vs. networked RAID HDD storage.</p>
    <p>come possible when writing to networked storage; storage systems perform better at higher levels of I/O parallelism [4, 7, 11, 16, 33, 34]. Fourth, reads that miss the flash cache experience less I/O contention at the network storage due to write coalescing at the cache layer; the read cache can thus be populated with the changes in the working set more quickly as workload phases change. Finally, since the cache is effective for both reads and writes, cache resizing can potentially serve as a storage QoS control knob (e.g., for I/O latency control) for all workloads including those that are write-intensive.</p>
    <p>To quantify the write-back performance advantage, we ran the TPC-C OLTP benchmark which mimics the operations of a typical web retailer including creating and delivering orders, recording payments, checking order status and monitoring inventory levels configured with 10 warehouses [36]. We configured a RAID5 storage array of 8 7200 RPM disks over iSCSI to be the network store and an OCZ PCI-e flash-based SSD as the storage cache on the host. SSD random writes were 80 times faster than the networked iSCSI store. Other aspects of the performance difference are summarized in Table 1.</p>
    <p>Figure 3 depicts median response times for the 4 types of TPC-C transactions. Having a write-back cache thus reduces the average response time by at least 75X across the transaction types. Throughput measured as TpmC (new order transactions/minute) showed an increase of 3X with write-back compared to the write-through cache. It is important to point out here that the available SSD</p>
    <p>Consistency</p>
    <p>Staleness</p>
    <p>Per f ormance</p>
    <p>Write-back</p>
    <p>Write-through</p>
    <p>Figure 4: Trade-offs in conventional write caching policies.</p>
    <p>throughput was not stressed in write-back mode at the queue depths offered by the workload. For many enterprise applications, lower worst-case performance and lower performance variance are as important as averagecase performance. The new write-back caching policies that we develop in this paper target both these metrics by mimicking the basic behavior of synchronous and lowlatency local flash updates and asynchronous network storage updates in conventional write-back.</p>
    <p>Write policies make different trade-offs with respect to data consistency, data staleness, and performance as indicated in Figure 4. Conventional write-through and writeback represent merely two extreme points in a spectrum of possible trade-offs. Write-through provides strict durability of writes, i.e., does not introduce any data staleness. On the other hand, write-back fundamentally alters the notion of data durability for applications and introduces both data inconsistency and staleness.</p>
    <p>Interestingly, this dilemma has parallels in the remote mirroring for disaster recovery literature. Asynchronous remote mirroring solutions ensure data consistency but introduce data staleness at the target storage system [31, 18, 20, 38]. Key to this argument is the lower cost of data loss after a disaster event when using asynchronous (akin to write-back) mirroring relative to the cost of the high-speed, WAN links necessary to implement fully synchronous (akin to write-through) mirroring to the remote site. While some applications, like financial databases, may require a recovery point objective (RPO) of zero, it has been pointed out that other applications such as non-critical filers and document servers and even online retailers can tolerate non-zero RPO [20]. With such applications, the performance cost of zero data staleness can become prohibitive and trading-off data staleness for performance becomes attractive [20, 31].</p>
    <p>RPO &gt; 0</p>
  </div>
  <div class="page">
    <p>Desired characteristics of new write-back</p>
  </div>
  <div class="page">
    <p>Ordered write-back</p>
    <p>Preserve the original order of writes  Networked storage is always in a valid state</p>
    <p>Time</p>
    <p>Flash cache</p>
    <p>Networked storage</p>
    <p>Delete inode Free block Commit</p>
    <p>Free block Commit</p>
    <p>Used! blocks:! 01010100!</p>
    <p>$ ls! file1! file2!</p>
  </div>
  <div class="page">
    <p>Storing I/O Ordering using Completion-issue invariants</p>
    <p>Completion-issue invariants:</p>
    <p>- 2 issued (I2) after 1 completes (C1) - 3 issued after 1 completes - 4 issued after 1, 2, and 3 complete</p>
    <p>I1 C1 I2</p>
  </div>
  <div class="page">
    <p>Storing I/O Ordering using Completion-issue invariants</p>
    <p>Completion-issue invariants stored in a graph:</p>
    <p>- 2 issued (I2) after 1 completes (C1) - 3 issued after 1 completes - 4 issued after 1, 2, and 3 complete</p>
  </div>
  <div class="page">
    <p>Time</p>
    <p>Flash cache</p>
    <p>Assume that 2 has to be evicted at t</p>
    <p>t</p>
    <p>Ordered Write-back</p>
  </div>
  <div class="page">
    <p>Ordered evictions: problem</p>
    <p>Time</p>
    <p>Flash cache</p>
    <p>Networked storage</p>
    <p>No write coalescing  Block 2 has to be stored and evicted twice</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Journaled Write-back</p>
    <p>Time</p>
    <p>Networked storage</p>
    <p>Evict 1, 3, 2 atomically [deJong1993]</p>
    <p>Flash cache</p>
  </div>
  <div class="page">
    <p>Storage-side journal</p>
    <p>Time</p>
    <p>Networked storage</p>
    <p>Transaction 1</p>
    <p>Flash cache</p>
    <p>Commit</p>
    <p>OK if we crashed at these 3 places</p>
  </div>
  <div class="page">
    <p>Restart the host</p>
    <p>Time</p>
    <p>Networked storage</p>
    <p>Flash cache</p>
    <p>Restart</p>
    <p>???</p>
  </div>
  <div class="page">
    <p>Host-side journal</p>
    <p>Time</p>
    <p>Networked storage</p>
    <p>Flash cache</p>
    <p>Restart</p>
    <p>Commit mapping</p>
    <p>Transaction 1 Current</p>
    <p>transaction</p>
    <p>transaction</p>
    <p>Commit</p>
    <p>using mapping</p>
  </div>
  <div class="page">
    <p>Journaled write-back</p>
    <p>Time</p>
    <p>Networked storage</p>
    <p>Flash cache</p>
    <p>Restart</p>
    <p>Commit mapping</p>
    <p>Transaction 1 Current</p>
    <p>transaction</p>
    <p>transaction</p>
    <p>Commit</p>
    <p>using mapping</p>
  </div>
  <div class="page">
    <p>Dual staleness control</p>
    <p>Staleness at host crash with SSD accessible (d)  Frequency of host-side commits</p>
    <p>Staleness at host crash with SSD un-accessible (D)  Frequency of storage-side checkpoints</p>
    <p>Transaction 3 Current</p>
    <p>transaction Transaction 2 Transaction 1</p>
    <p>checkpoint checkpoint</p>
    <p>D</p>
    <p>d</p>
  </div>
  <div class="page">
    <p>Consistency analysis</p>
    <p>Point-in-time consistency  Ordered write-back  Journaled write-back</p>
    <p>Transactional consistency (block layer)  Write-through</p>
    <p>Application consistency  Journaled write-back with application hints</p>
  </div>
  <div class="page">
    <p>Consistency analysis</p>
    <p>Point-in-time consistency  Ordered write-back  Journaled write-back</p>
    <p>Transactional consistency (block layer)  Write-through</p>
    <p>Application consistency  Journaled write-back with application hints</p>
    <p>Application</p>
    <p>Flash</p>
    <p>Storage</p>
    <p>sync( )</p>
  </div>
  <div class="page">
    <p>Implementation and evaluation setup</p>
    <p>Linux module intercepts I/Os at the block layer  4 KB blocks and ARC as the replacement algorithm</p>
    <p>Local SSD (OCZ Revodrive PCI-e)  Networked storage is Enterprise ISCSI Target, array of 2 disks at 7200</p>
    <p>RPM</p>
    <p>Filebench, Postmark, YCSB (Yahoo Cloud Serving Benchmark), TPC-C</p>
    <p>Performance comparison and analysis</p>
    <p>Controlled staleness</p>
  </div>
  <div class="page">
    <p>Performance comparison</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through Write-back Ordered write-back</p>
    <p>Journaled write-back</p>
  </div>
  <div class="page">
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through Write-back Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Performance comparison</p>
    <p>expected</p>
  </div>
  <div class="page">
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through Write-back Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Performance comparison</p>
    <p>Journaled write-back is better</p>
  </div>
  <div class="page">
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through Write-back Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Performance comparison Ordered write-back is better</p>
  </div>
  <div class="page">
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through Write-back Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Performance comparison Journaled write-back is worse</p>
  </div>
  <div class="page">
    <p>Performance comparison Filebench fileserver</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a ti o n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g e</p>
    <p>w ri te</p>
    <p>v o lu</p>
    <p>m e i n G</p>
    <p>B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e ra</p>
    <p>ti o n s</p>
    <p>p e r</p>
    <p>s e c o n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o lu</p>
    <p>m e</p>
    <p>i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e r</p>
    <p>s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o</p>
    <p>rm a</p>
    <p>liz e</p>
    <p>d T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e</p>
    <p>ra ti o n</p>
    <p>s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o</p>
    <p>rm a</p>
    <p>liz e</p>
    <p>d T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a ti o n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g e</p>
    <p>w ri te</p>
    <p>v o lu</p>
    <p>m e i n G</p>
    <p>B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e ra</p>
    <p>ti o n s</p>
    <p>p e r</p>
    <p>s e c o n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through</p>
    <p>Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Write-back</p>
  </div>
  <div class="page">
    <p>Performance comparison Filebench fileserver</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o</p>
    <p>rm a</p>
    <p>liz e</p>
    <p>d T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e</p>
    <p>ra ti o n</p>
    <p>s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o</p>
    <p>rm a</p>
    <p>liz e</p>
    <p>d T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a ti o n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g e</p>
    <p>w ri te</p>
    <p>v o lu</p>
    <p>m e i n G</p>
    <p>B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e ra</p>
    <p>ti o n s</p>
    <p>p e r</p>
    <p>s e c o n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through</p>
    <p>Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Write-back</p>
    <p>The image cannot be displayed. Your computer may not have enough memory to open the image, or the image may have been corrupted. Restart your computer, and then open the file again. If the red x still appears, you may have to delete the image and then insert it again.</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a ti o n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g e</p>
    <p>w ri te</p>
    <p>v o lu</p>
    <p>m e i n G</p>
    <p>B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e ra</p>
    <p>ti o n s</p>
    <p>p e r</p>
    <p>s e c o n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o lu</p>
    <p>m e</p>
    <p>i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e r</p>
    <p>s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
  </div>
  <div class="page">
    <p>Performance improvements - Filebench</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o</p>
    <p>rm a</p>
    <p>liz e</p>
    <p>d T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e</p>
    <p>tw o</p>
    <p>rk s</p>
    <p>to ra</p>
    <p>g e</p>
    <p>w</p>
    <p>ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e</p>
    <p>ra ti o n</p>
    <p>s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o</p>
    <p>rm a</p>
    <p>liz e</p>
    <p>d T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a ti o n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g e</p>
    <p>w ri te</p>
    <p>v o lu</p>
    <p>m e i n G</p>
    <p>B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e ra</p>
    <p>ti o n s</p>
    <p>p e r</p>
    <p>s e c o n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>Write-through</p>
    <p>Ordered write-back</p>
    <p>Journaled write-back</p>
    <p>Write-back</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a ti o n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g e</p>
    <p>w ri te</p>
    <p>v o lu</p>
    <p>m e i n G</p>
    <p>B</p>
    <p>(b) Network storage write volume</p>
    <p>O p e ra</p>
    <p>ti o n s</p>
    <p>p e r</p>
    <p>s e c o n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
    <p>FB PM TPC-C A B F</p>
    <p>N o rm</p>
    <p>a liz</p>
    <p>e d T</p>
    <p>h ro</p>
    <p>u g h p u t</p>
    <p>WT WB WB-O WB-J</p>
    <p>Figure 12: Normalized throughput for Filebenchfileserver (FB), PostMark (PM), TPC-C, and YCSB workloads-A, B, and F.</p>
    <p>case (YCSB-F), ordered write-back performs the best, even surpassing both conventional and journaled writeback variants. This occurs due to two reasons: (i) unlike conventional write-back, ordered write-back benefits from dependency-induced batching of evictions from the cache, and (ii) journaled write-back was configured with a small journal in this experiment relative to the workloads working-set leading to too many evictions relative to ordered and conventional write-back. Finally, journaled write-back can sometimes perform better than conventional write-back because it benefits from eager, grouped checkpointing for destageing dirty data whereas conventional write-back destages dirty data only on demand. Later in this section, we demonstrate how the performance with the journaled write-back policy can be tuned to meet write-back performance depending on application staleness tolerance.</p>
    <p>The performance of the write policies are sensitive to the size of the cache, the size of the journal, and the differential shaping of application I/O traffic induced by different file systems. We used the Filebench-fileserver and PostMark workloads to better understand the performance sensitivity of the write policies to these factors.</p>
    <p>We ran Filebench-fileserver using each of the write policies under several cache sizes. Journaled write-back was configured to limit the journal size, and as a result, the maximum staleness at the network storage to 100 MB. Figure 13(a) shows fileserver throughput performance in operations per second. Filebench-fileserver is a write intensive workload and therefore insensitive to cache size in the case of reads. We see that that write-through performance is similar across all cache sizes since all writes must be written synchronously to network storage. The other policies have two dominant trends. For cache sizes greater than 1.5GB, we observe the expected</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f w</p>
    <p>ri te</p>
    <p>a</p>
    <p>llo c a</p>
    <p>ti o</p>
    <p>n s</p>
    <p>Cache size in GB</p>
    <p>(c) Write allocations</p>
    <p>e tw</p>
    <p>o rk</p>
    <p>s to</p>
    <p>ra g</p>
    <p>e</p>
    <p>w ri te</p>
    <p>v o</p>
    <p>lu m</p>
    <p>e i n</p>
    <p>G B</p>
    <p>(b) Network storage write volume</p>
    <p>O p</p>
    <p>e ra</p>
    <p>ti o</p>
    <p>n s</p>
    <p>p e</p>
    <p>r s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>(a) Workload performance</p>
    <p>WB WB-J</p>
    <p>WB-O WT</p>
    <p>Figure 13: Filebench-fileserver performance for all four write policies at different cache sizes.</p>
    <p>trend across the write policies. All three write-back policies perform better than write-through. Write-through, as shown in Figure 13(b), has the highest write volume to network storage. The write-back policies coalesce writes, reducing the write volume to network storage. Among the write-back variants, ordered write-back has lower relative performance, 800 compared to 1000 operations per second for conventional write-back. A write-allocation indicates an operation whereby a write requires additional space in the cache. Since all writes in case of ordered write-back need to be made to a new location to avoid overwriting previous versions of the block, they all require write-allocation (Figure 13(c)). Write-allocations can be expensive; if eviction is necessary to allocate and if the block selected to be evicted is dirty, a write to network storage becomes necessary. This affects the performance of ordered write-back negatively the effects of which can also be observed in Figure 13(b): more write traffic to network storage in the case of ordered write-back.</p>
    <p>The second trend occurs at cache sizes less than 1.5GB of cache size where write-through and journaled writeback perform significantly better than the other two policies. For small cache sizes, most cache writes induce write-allocation and the resulting dirty block evictions from the cache in case of the write-back policy variants dominate cache behavior which negatively affects performance with these policies. Interestingly, write-through</p>
  </div>
  <div class="page">
    <p>Controlled staleness Filebench fileserver</p>
    <p>T ra</p>
    <p>n s a</p>
    <p>c ti o</p>
    <p>n s p</p>
    <p>e r</p>
    <p>s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>Staleness (journal size in MB)</p>
    <p>WB-J WT WB</p>
    <p>Figure 14: PostMark performance with 1.5GB of SSD cache for varying staleness set as the maximum number of dirty pages.</p>
    <p>does well since all write-allocations in write-through are free; they do not induce additional writes to network storage as all the cached blocks are always clean. In the case of journaled write-back, the write-allocations are relatively less expensive than the other write-back variants because the eviction of dirty data is batched and therefore more efficiently written to network storage. Next, we evaluate the sensitivity of journaled write-back policy to the host-side journal size.</p>
    <p>Journaled write-back does not need to store copies of old blocks and therefore has minimal overhead relative to conventional write-back. The reason why it performs worse than write-back in some of the previous experiments is that it limits the amount of staleness by restricting the size of the host-side journal. Reducing the journal size increases the probability of cache evictions during transaction commits. To evaluate the sensitivity of journaled write-back performance to the allowable staleness of storage, we conducted an experiment where we fix the cache size to 1.5GB and vary the allowable staleness (host-side journal size). Figure 14 depicts how performance can be tuned to span a significant portion of the range between write-through and write-back by varying the staleness tolerance for the PostMark benchmark. A larger journal allows for greater write coalescing and batching of write traffic to the network storage and thus aids performance, but it also results in greater staleness at the network storage after a host-level failure. The journal size is an ideal knob to achieve an applicationdefined performance/staleness trade-off using the journaled write-back policy.</p>
    <p>File systems alter write ordering and impose additional synchronous write requirements. In this regard, file system designs vary significantly. The host-side cache operates at the block layer (either within the OS or hypervi</p>
    <p>ext3-data</p>
    <p>Cache size in GB</p>
    <p>ext3-ordered ext3-writeback</p>
    <p>ext4-data</p>
    <p>WT WB</p>
    <p>WB-O WB-J</p>
    <p>ext4-ordered ext4-writeback</p>
    <p>ext2Btrfs</p>
    <p>Figure 15: PostMark transactions per second under different file systems. The axis ranges are indicated in the bottom-left plot and are the same across all the sub-plots.</p>
    <p>Notice that ext*-writeback and ext*-ordered are referring to a</p>
    <p>journaling mode and not to the caches write policy.</p>
    <p>sor) and alters the I/O stream created by the file system operating at some layer above it. It is therefore important to evaluate how the performance afforded to applications by individual filesystems are impacted due to this additional layer and policies within it.</p>
    <p>We studied 4 different file systems, ext2, ext3, ext4, and btrfs. Ext2 is a file system based on the traditional Berkeley FFS [27]. Ext3 implements a journaling layer on top of ext2 whereby metadata (and optionally, data) writes are initially directed to a journal, and later checkpointed to their final location within the ext2 on-disk structure. We evaluated all three journaling modes of ext3: writeback and ordered, where only the metadata is journaled, and data, where both data and metadata are journaled. Figure 15 depicts performance for the PostMark benchmark for all write policies as we change the underlying file system at various sizes of the host-side SSD cache. This set of experiments revealed a set of interesting insights about how these file system designs are impacted by an SSD caching layer operating below them. We discuss these next.</p>
    <p>The first broad trend we draw is that across all the file system variants, we notice that the write-back caching policy variants outperform the write-through policy for cache sizes that are sufficiently large so as to accommodate a sizable fraction of the workloads working set size (2GB in this case). Second, the ordered write-back caching policy provides performance superior to writethrough beyond a certain cache size for all file systems,</p>
    <p>Staleness (transaction size in MB)</p>
    <p>Write-back</p>
    <p>Write-through</p>
    <p>Journaled Write-back</p>
  </div>
  <div class="page">
    <p>Summary and conclusions</p>
    <p>Write-through and write-back are extreme opposites in terms of performance, staleness and consistency</p>
    <p>Ordered write-back provides point-in-time consistency at some performance cost</p>
    <p>Journaled write-back provides point-in-time consistency at no performance cost</p>
    <p>A variant of journaled write-back provides application consistency by using hints from the application</p>
  </div>
  <div class="page">
    <p>Thank you Questions?</p>
  </div>
</Presentation>
