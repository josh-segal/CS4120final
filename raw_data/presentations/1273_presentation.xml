<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Offload Annotations: Bringing Heterogeneous Computing to</p>
    <p>Existing Libraries and Workloads Gina Yuan, Shoumik Palkar, Deepak Narayanan, Matei Zaharia</p>
    <p>Stanford University USENIX ATC 2020 (July 15-17)</p>
  </div>
  <div class="page">
    <p>Background: Hardware Commoditization</p>
    <p>NVIDIA GPU</p>
  </div>
  <div class="page">
    <p>Background: CPUs vs. GPUs</p>
    <p>Core Core</p>
    <p>Core Core Control</p>
    <p>Cache</p>
    <p>Memory Memory</p>
    <p>CPUs GPUs</p>
    <p>(PCI-E)</p>
    <p>Costly data transfers!</p>
  </div>
  <div class="page">
    <p>Background: Data Science on the CPU</p>
    <p>+ (CPU)</p>
  </div>
  <div class="page">
    <p>NVIDIA GPU</p>
    <p>Trend: Data Science on the GPU</p>
    <p>+</p>
    <p>(cuDF, cuML, etc.)</p>
    <p>Lots of parallel data!</p>
  </div>
  <div class="page">
    <p>Trend: CPU Libraries vs. GPU Libraries https://github.com/rapidsai/cudf https://cupy.chainer.org/</p>
    <p>https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html</p>
    <p>https://github.com/rapidsai/cuml</p>
  </div>
  <div class="page">
    <p>Trend: CPU Libraries vs. GPU Libraries https://github.com/rapidsai/cudf https://cupy.chainer.org/</p>
    <p>https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html</p>
    <p>https://github.com/rapidsai/cuml</p>
    <p>Are GPU libraries as straightforward to use as they seem?</p>
  </div>
  <div class="page">
    <p>Motivating Example</p>
    <p>cuML</p>
  </div>
  <div class="page">
    <p>cuml cuml</p>
    <p>Motivating Example</p>
    <p>Missing Functions</p>
  </div>
  <div class="page">
    <p>cuml cuml</p>
    <p>Motivating Example</p>
    <p>Missing Functions</p>
    <p>Manual Data Transfers</p>
    <p>X_train = transfer(X_train, GPU)</p>
    <p>Y_train = transfer(Y_train, GPU)</p>
    <p>X_test = transfer(X_test, GPU)</p>
    <p>result = transfer(result, CPU)</p>
  </div>
  <div class="page">
    <p>cuml cuml</p>
    <p>Motivating Example</p>
    <p>Missing Functions</p>
    <p>Manual Data Transfers</p>
    <p>Small GPU Memory</p>
    <p>X_train = transfer(X_train, GPU)</p>
    <p>Y_train = transfer(Y_train, GPU)</p>
    <p>X_test[i,j]=transfer(X_test[i,j], GPU)</p>
    <p>result[i,j]=transfer(result[i,j], CPU)</p>
    <p>for (i,j) in split(X_test):</p>
    <p>[i,j] [i,j]) [i,j] [i,j])</p>
    <p>[i,j] [i,j])</p>
  </div>
  <div class="page">
    <p>cuml cuml</p>
    <p>Motivating Example</p>
    <p>Missing Functions</p>
    <p>Manual Data Transfers</p>
    <p>Small GPU Memory</p>
    <p>Scheduling</p>
    <p>X_train = transfer(X_train, GPU)</p>
    <p>Y_train = transfer(Y_train, GPU)</p>
    <p>X_test[i,j]=transfer(X_test[i,j], GPU)</p>
    <p>result[i,j]=transfer(result[i,j], CPU)</p>
    <p>for (i,j) in split(X_test):</p>
    <p>[i,j] [i,j]) [i,j] [i,j])</p>
    <p>[i,j] [i,j])</p>
    <p>??? ???</p>
  </div>
  <div class="page">
    <p>Solution: Offload Annotations The annotator writes offload annotations (OAs) for CPU libraries. An end user imports the annotated library instead of the CPU library. Our runtime, Bach, automatically schedules data transfers and pages computation.</p>
  </div>
  <div class="page">
    <p>With less developer effort: 1. Match handwritten GPU performance</p>
    <p>Goals</p>
  </div>
  <div class="page">
    <p>With less developer effort: 1. Match handwritten GPU performance 2. Scale to data sizes larger than GPU memory</p>
    <p>Goals</p>
  </div>
  <div class="page">
    <p>With less developer effort: 1. Match handwritten GPU performance 2. Scale to data sizes larger than GPU memory 3. Beat CPU performance</p>
    <p>Goals</p>
  </div>
  <div class="page">
    <p>multiply = @oa(func=torch.mul)(np.multiply) sqrt = @oa(func=torch.sqrt)(np.sqrt)</p>
    <p>Step 1: Annotator  Function Annotations</p>
    <p>GPU library CPU library</p>
  </div>
  <div class="page">
    <p>multiply = @oa(func=torch.mul)(np.multiply) sqrt = @oa(func=torch.sqrt)(np.sqrt)</p>
    <p>Step 1: Annotator  Function Annotations</p>
    <p>GPU library CPU library</p>
    <p>corresponding functions</p>
  </div>
  <div class="page">
    <p>inputs outputs</p>
    <p>arg = (NdArrayType(),) args = (NdArrayType(), NdArrayType()) ret = NdArrayType()</p>
    <p>multiply = @oa(args, ret, func=torch.mul)(np.multiply) sqrt = @oa(arg, ret, func=torch.sqrt)(np.sqrt)</p>
    <p>Step 1: Annotator  Function Annotations</p>
  </div>
  <div class="page">
    <p>arg = (NdArrayType(),) args = (NdArrayType(), NdArrayType()) ret = NdArrayType()</p>
    <p>multiply = @oa(args, ret, func=torch.mul)(np.multiply) sqrt = @oa(arg, ret, func=torch.sqrt)(np.sqrt) ones = @oa_alloc(ret, func=torch.ones)(np.ones)</p>
    <p>Step 1: Annotator  Allocation Annotations</p>
    <p>Allocations only have a return type.</p>
  </div>
  <div class="page">
    <p>arg = (NdArrayType(),) args = (NdArrayType(), NdArrayType()) ret = NdArrayType()</p>
    <p>multiply = @oa(args, ret, func=torch.mul)(np.multiply) sqrt = @oa(arg, ret, func=torch.sqrt)(np.sqrt) ones = @oa_alloc(ret, func=torch.ones)(np.ones)</p>
    <p>Step 1: Annotator  Allocation Annotations</p>
    <p>Whats in an offload split type?</p>
    <p>&quot;offload split type&quot;</p>
  </div>
  <div class="page">
    <p>API Description</p>
    <p>device(value) Which device the value is on.</p>
    <p>to(value, device) Transfers [value] to [device]. offloading</p>
    <p>API</p>
    <p>Step 1: Annotator  Offload Split Type</p>
  </div>
  <div class="page">
    <p>API Description</p>
    <p>device(value) Which device the value is on.</p>
    <p>to(value, device) Transfers [value] to [device]. offloading</p>
    <p>API</p>
    <p>Step 1: Annotator  Offload Split Type</p>
    <p>API Implementation for NdArrayType()</p>
    <p>device(value) ...if isinstance(value, torch.Tensor): ...</p>
    <p>to(value, device) ...value.to(torch.device('cpu')).numpy()</p>
  </div>
  <div class="page">
    <p>API Description size(value) Number of elements in the value. split(start, end, value) Splits a value to enable paging.</p>
    <p>merge(values) Merges split values.</p>
    <p>splitting API [Mozart</p>
    <p>SOSP 19] (optional)</p>
    <p>Step 1: Annotator  Offload Split Type</p>
  </div>
  <div class="page">
    <p>API Description size(value) Number of elements in the value. split(start, end, value) Splits a value to enable paging.</p>
    <p>merge(values) Merges split values.</p>
    <p>splitting API [Mozart</p>
    <p>SOSP 19] (optional)</p>
    <p>Step 1: Annotator  Offload Split Type</p>
    <p>API Implementation for NdArrayType() size(value) return value.shape[-1] split(start, end, value) return value[start, end] merge(values) return np.concatenate(values)</p>
  </div>
  <div class="page">
    <p>Step 1: Annotator  Offload Split Type</p>
    <p>DataFrameType() ModelType()NdArrayType()</p>
  </div>
  <div class="page">
    <p>import numpy as np # Allocate a = np.ones(size, dtype='float64') b = np.ones(size, dtype='float64) # Compute np.arcsin(a, out=a) np.multiply(a, b, out=b) np.sqrt(b, out=b)</p>
    <p>Step 2: End User</p>
    <p>(Simple, somewhat dumb, Python program.)</p>
    <p>End User  Annotator</p>
  </div>
  <div class="page">
    <p>import bach.numpy as np # Allocate a = np.ones(size, dtype='float64') b = np.ones(size, dtype='float64) # Compute np.arcsin(a, out=a) np.multiply(a, b, out=b) np.sqrt(b, out=b)</p>
    <p>Import the annotated library instead of the CPU library.</p>
    <p>Step 2: End User</p>
  </div>
  <div class="page">
    <p>import bach.numpy as np # Allocate a = np.ones(size, dtype='float64') b = np.ones(size, dtype='float64) # Compute np.arcsin(a, out=a) np.multiply(a, b, out=b) np.sqrt(b, out=b)</p>
    <p>np.evaluate()</p>
    <p>Step 2: End User</p>
    <p>Explicitly materialize lazy values with included evaluate() function.</p>
  </div>
  <div class="page">
    <p>Generate a lazy computation graph and do a topological sort.</p>
    <p>Step 3: Runtime - Scheduling</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>np.sqrt(b)</p>
    <p>a = np.ones() np.arcsin(a)</p>
    <p>b = np.ones() np.multiply(a,b)</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>Bach</p>
  </div>
  <div class="page">
    <p>Assign functions to the CPU/GPU based on whether a GPU library implementation is provided in the annotation.</p>
    <p>Step 3: Runtime - Scheduling</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>No.</p>
    <p>GPU library imple</p>
    <p>mentation provided?</p>
    <p>Yes.</p>
    <p>Yes. np.sqrt(b)</p>
    <p>a = np.ones() np.arcsin(a)</p>
    <p>b = np.ones() np.multiply(a,b)</p>
    <p>Bach</p>
  </div>
  <div class="page">
    <p>Assign allocations to the CPU/GPU so they are on the same device as the first function that uses the data.</p>
    <p>Step 3: Runtime - Scheduling</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>np.sqrt(b)</p>
    <p>a = np.ones() np.arcsin(a)</p>
    <p>b = np.ones() np.multiply(a,b)</p>
    <p>Bach</p>
  </div>
  <div class="page">
    <p>Automatically transfer data using the offloading API.</p>
    <p>Step 3: Runtime  Offloading API</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>-- transfer to CPU -</p>
    <p>-- transfer to GPU -</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>np.sqrt(b)</p>
    <p>a = np.ones() np.arcsin(a)</p>
    <p>b = np.ones() np.multiply(a,b)</p>
    <p>Bach</p>
  </div>
  <div class="page">
    <p>np.sqrt(b)</p>
    <p>a = np.ones() np.arcsin(a)</p>
    <p>b = np.ones() np.multiply(a,b)</p>
    <p>Automatically page large datasets using the splitting API.</p>
    <p>Step 3: Runtime  Splitting API</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>Split</p>
    <p>Merge</p>
    <p>-- transfer to CPU -</p>
    <p>-- transfer to GPU -</p>
    <p>Data Size = 2^28</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>Bach</p>
  </div>
  <div class="page">
    <p>Naive cost-benefit analysis between data transfer and computation cost.</p>
    <p>Step 3: Runtime  Scheduling Heuristics</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>GPU Compute + Data Transfer</p>
    <p>CPU Compute</p>
    <p>Data Size = 2^28</p>
    <p>(optional)</p>
  </div>
  <div class="page">
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>= Allocation</p>
    <p>= CPU = GPU</p>
    <p>GPU Compute + Data Transfer</p>
    <p>CPU Compute</p>
    <p>Data Size = 2^10</p>
    <p>Naive cost-benefit analysis between data transfer and computation cost.</p>
    <p>Step 3: Runtime  Scheduling Heuristics (optional)</p>
  </div>
  <div class="page">
    <p>Naive implementations of cost estimators.</p>
    <p>Estimated Cost</p>
    <p>Data Size</p>
    <p>CPU Compute GPU Compute</p>
    <p>+ Data Transfer</p>
    <p>CPU GPU</p>
    <p>Step 3: Runtime  Scheduling Heuristics (optional)</p>
  </div>
  <div class="page">
    <p>Evaluation 4 library integrations and 8 data science and ML workloads.</p>
  </div>
  <div class="page">
    <p>~130 LOC per library including offloading / splitting APIs and function annotations.</p>
    <p>Integration Experience</p>
  </div>
  <div class="page">
    <p>Speedup: max 1200x, median 6.3x.</p>
    <p>Evaluation: Summary</p>
  </div>
  <div class="page">
    <p>Evaluation: Summary</p>
  </div>
  <div class="page">
    <p>With less developer effort, Bach can: 1. Match handwritten GPU</p>
    <p>performance</p>
    <p>Evaluation: Summary</p>
  </div>
  <div class="page">
    <p>With less developer effort, Bach can: 1. Match handwritten GPU</p>
    <p>performance 2. Scale to data sizes larger than</p>
    <p>GPU memory</p>
    <p>Evaluation: Summary</p>
  </div>
  <div class="page">
    <p>With less developer effort, Bach can: 1. Match handwritten GPU</p>
    <p>performance 2. Scale to data sizes larger than</p>
    <p>GPU memory 3. Beat CPU performance</p>
    <p>Evaluation: Summary</p>
  </div>
  <div class="page">
    <p>Crime Index saves time by eliminating the initial data transfer, while the allocation still fits in GPU memory.</p>
    <p>In-Depth Evaluation: Allocations</p>
  </div>
  <div class="page">
    <p>At smaller data sizes, TSVD schedules all computation on the CPU.</p>
    <p>In-Depth Evaluation: Heuristics</p>
  </div>
  <div class="page">
    <p>[Motivating Example] The &quot;fit&quot; phase dominates the runtime until the &quot;predict&quot; phase can split/page data into the GPU.</p>
    <p>In-Depth Evaluation: Splitting/Paging Datasets</p>
  </div>
  <div class="page">
    <p>Evaluation: Summary</p>
    <p>Max Speedup</p>
  </div>
  <div class="page">
    <p>Evaluation: Summary</p>
    <p>Max Speedup</p>
  </div>
  <div class="page">
    <p>OAs enable heterogeneous GPU computing in existing libraries and workloads with little to no code modifications.</p>
    <p>Conclusion</p>
    <p>github.com/stanford-futuredata/offload-annotations gyuan@cs.stanford.edu</p>
    <p>With less developer effort, Bach + OAs can:  Match handwritten GPU performance  Scale to data sizes larger than GPU memory  Beat CPU performance</p>
  </div>
</Presentation>
