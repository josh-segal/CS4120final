<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DLSpec: A Deep Learning Task Exchange Specification</p>
    <p>Abdul Dakkak1*, Cheng Li1*, Jinjun Xiong2, Wen-mei Hwu1</p>
    <p>University of Illinois Urbana-Champaign1, IBM Research2</p>
    <p>OpML2020</p>
  </div>
  <div class="page">
    <p>Deep Learning (DL) innovations are introduced at a fast pace  Current lack of standard specification of DL tasks makes sharing,</p>
    <p>running, reproducing, and comparing DL innovations difficult</p>
    <p>Background</p>
  </div>
  <div class="page">
    <p>Ad-hoc scripts and textual documentation to describe the execution process of DL tasks  Curation of DL tasks in framework model zoo  Model catalogs that can be used through a cloud providers API</p>
    <p>Hard to reproduce the reported accuracy or performance results and have a consistent comparison across DL artifacts</p>
    <p>Current Practice of Publishing DL Artifacts</p>
  </div>
  <div class="page">
    <p>A DL artifact exchange specification with clearly defined model, data, software, and hardware aspects  Model-, dataset-, software-, and hardware agnostic  Works with runtimes built using existing MLOp tools</p>
    <p>We developed a DLSpec runtime for DL inference tasks in the context of benchmarking</p>
    <p>DLSpec Objectives</p>
  </div>
  <div class="page">
    <p>Reproducible  Minimal  Only contains essential information to increase the transparency and</p>
    <p>ease the creation</p>
    <p>Program-/human-readable  Executed by a runtime/easy to introspect and repurpose</p>
    <p>Maximum expressiveness  Describes both training and inference</p>
    <p>DLSpec is Based on a Few Key Principles</p>
  </div>
  <div class="page">
    <p>Decoupling DL task description  Increases the reuse/portability and enables easy of comparison</p>
    <p>Splitting the DL task pipeline stages  Enables consistent comparison and simplifies accuracy and performance</p>
    <p>debugging</p>
    <p>Avoiding serializing intermediate data into files  Avoids high serializing/deserializing overhead  Supports DL tasks that use streaming data</p>
    <p>DLSpec is Based on a Few Key Principles</p>
  </div>
  <div class="page">
    <p>DLSpec Design</p>
  </div>
  <div class="page">
    <p>Defines the hardware requirements for a DL task</p>
    <p>Some hardware settings cannot be specified within a container (E.g. the runtime set Intels turbo-boosting outside the container)</p>
    <p>Hardware Manifest</p>
  </div>
  <div class="page">
    <p>Defines the software environment for a DL task</p>
    <p>All executions occur within the specified container</p>
    <p>Specified environment variables are setup after running the container</p>
    <p>Software Manifest</p>
  </div>
  <div class="page">
    <p>Defines the training, validation, or test dataset</p>
    <p>The source location defines where to download the dataset from</p>
    <p>Dataset Manifest</p>
  </div>
  <div class="page">
    <p>Defines the logic to run a DL task and the required artifact sources</p>
    <p>Model Manifest</p>
    <p>Python functions, executed by the runtime through the Python sub-interpreter</p>
  </div>
  <div class="page">
    <p>Defines the logic to run a DL task and the required artifact sources</p>
    <p>Model Manifest</p>
    <p>Input and output formats</p>
  </div>
  <div class="page">
    <p>Defines the logic to run a DL task and the required artifact sources</p>
    <p>Model Manifest</p>
    <p>Remote resources hosted on FTP, HTTP, or file servers</p>
  </div>
  <div class="page">
    <p>A text file provided by the specification author for others to refer to. It contains:  IDs of the manifests used to create it  Achieved accuracy/performance on</p>
    <p>DL task  Expected outputs  Author-specified information (e.g. hyper-parameters used in</p>
    <p>training)</p>
    <p>Reference Log</p>
  </div>
  <div class="page">
    <p>A DLSpec Runtime Consumes the Manifests</p>
    <p>Selects the hardware</p>
    <p>Runs the setup code</p>
    <p>Launches the container</p>
    <p>Downloads the dataset using the URLs</p>
    <p>The dataset file paths are passed to the pre-processing function and its outputs match the models input format</p>
  </div>
  <div class="page">
    <p>A DLSpec Runtime Consumes the Manifests</p>
    <p>Downloads the model and runs the inference task</p>
    <p>Post-processes the result using the models output format</p>
  </div>
  <div class="page">
    <p>A distributed runtime that consumes the DLSpec for inference  Web and command line UI  Middleware, e.g. registry, database, tracer  Framework agents  Other modular components</p>
    <p>A Runtime for Benchmarking DL Inference MLModelScope</p>
    <p>User Interface Website Command Line</p>
    <p>API RPCREST C Library</p>
    <p>Hardware CPU GPU FPGA ASIC</p>
    <p>Predictors</p>
    <p>Caffe2 TensorFlow MXNet</p>
    <p>TensorRT PyTorch</p>
    <p>Middleware</p>
    <p>Data ManagerRegistry Eval Database</p>
    <p>ML Artifacts Manifests Dockerfiles Assets Repo</p>
    <p>CNTKCaffe</p>
    <p>Tracer Analyser</p>
    <p>The Design and Implementation of a Scalable DL Benchmarking Platform, IEEE CLOUD20</p>
  </div>
  <div class="page">
    <p>An exchange specification, such as DLSpec, enables a streamlined way to share, reproduce, and compare DL tasks</p>
    <p>DLSpec takes the first step in defining a DL task for both training and inference and captures the different aspects of DL model reproducibility</p>
    <p>We are actively working on refining the specifications as new DL tasks are introduced</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Thank you</p>
    <p>Abdul Dakkak1*, Cheng Li1*, Jinjun Xiong2, Wen-mei Hwu1</p>
    <p>University of Illinois Urbana-Champaign1, IBM Research2</p>
  </div>
</Presentation>
