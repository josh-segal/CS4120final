<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DRN: A Deep Reinforcement Learning Framework for News</p>
    <p>Recommendation Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang,</p>
    <p>Nicholas Jing Yuan, Xing Xie, Zhenhui (Jessie) Li</p>
  </div>
  <div class="page">
    <p>Introduction: Why reinforcement recommendation</p>
    <p>Images from: 1. https://www.ohio.com/akron/sports/cavs/cavaliers-j-r-smith-expects-lebron-james-to-keep-opening-night-streak-alive-despite-sprained-ankle 2. https://www.cnbc.com/2018/03/05/kobe-bryant-has-won-an-oscar-heres-what-he-says-it-takes-to-succeed.html 3. https://twitter.com/NBA 4. https://www.stgeorgeutah.com/news/archive/2016/06/11/djg-alert-significant-weather-strong-thunderstorm-rolls-through-washington-county/#.WsqHUdPwb6Y 5. http://www.clipartpanda.com/clipart_images/question-mark-36565633</p>
    <p>Equal rewarding recommendation for current round</p>
    <p>First round</p>
    <p>Second round</p>
  </div>
  <div class="page">
    <p>Introduction: News recommendation is dynamic</p>
    <p>R at</p>
    <p>io of</p>
    <p>cl ic</p>
    <p>k fo</p>
    <p>r d i</p>
    <p>er en</p>
    <p>t ca</p>
    <p>te go</p>
    <p>ri es</p>
    <p>Auto Business Politics Education</p>
    <p>Entertainment Military Real estate Technology</p>
    <p>Society Sports Travel OthersThe life period for news</p>
    <p>is usually very short.</p>
    <p>Users interest may change during time.</p>
  </div>
  <div class="page">
    <p>Introduction: Is there more than click/noclick?</p>
    <p># of</p>
    <p>re q u es</p>
    <p>t  t = 24 hours  t = 48 hours</p>
    <p>Users clicks on news are usually very dense in a short period. Then, user usually leave the app!</p>
    <p>User may return everyday!</p>
  </div>
  <div class="page">
    <p>Introduction: Should we keep recommending similar items?</p>
    <p>Lebron James will be the MVP! Tony Parker has come back from injury!</p>
    <p>Paul Gasol promises to help the Spurs in the playoff.</p>
    <p>Will you get bored if all the recommended news are from NBA when you are browsing the sports news?</p>
    <p>Images from: https://twitter.com/nba11/9/18 5</p>
  </div>
  <div class="page">
    <p>Method: Using reinforcement learning to do recommendation</p>
    <p>Agent</p>
    <p>Environment</p>
    <p>Action</p>
    <p>State</p>
    <p>Reward DQN</p>
    <p>Click / no click</p>
    <p>User activiness</p>
    <p>Action 1 Action 2</p>
    <p>Action mUser News</p>
    <p>Explore</p>
    <p>Memory</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>Push Minor update</p>
    <p>Push Push</p>
    <p>Major updateMinor</p>
    <p>update Push</p>
    <p>Minor update</p>
    <p>Push</p>
    <p>Offline Part</p>
    <p>Online Part</p>
    <p>t1 t2 t3 t4 t5</p>
    <p>Memory</p>
    <p>Explore</p>
    <p>Interaction log Candidates Candidates Candidates Candidates Candidates</p>
    <p>Feedback Feedback Feedback</p>
    <p>Policy Policy Policy Policy Policy</p>
    <p>Replay Mini-batch Feedback Feedback</p>
    <p>Activeness analysis</p>
    <p>Timeline</p>
  </div>
  <div class="page">
    <p>Method: Dueling network structure  value and advantage function</p>
    <p>V(s) A(s, a)</p>
    <p>Q(s, a)</p>
    <p>User features Context features</p>
    <p>User-news features</p>
    <p>News features 11/9/18 8</p>
  </div>
  <div class="page">
    <p>Method: user activeness modeling -- survival analysis</p>
    <p>Time</p>
    <p>U se</p>
    <p>r ac</p>
    <p>ti ve</p>
    <p>n es</p>
    <p>s</p>
    <p>t1 t2 t3 t4 t5 t6 t7 t8 t9 t10</p>
    <p>User activeness decay function</p>
    <p>User activeness</p>
  </div>
  <div class="page">
    <p>Method: Effective exploration</p>
    <p>C</p>
    <p>D</p>
    <p>B</p>
    <p>Step towardsKeep</p>
    <p>Model choice</p>
    <p>List</p>
    <p>Probabilistic Interleave</p>
    <p>Current Network Explore Network</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>List</p>
    <p>Feedback</p>
    <p>A</p>
    <p>C</p>
    <p>D</p>
    <p>A</p>
    <p>C</p>
    <p>D</p>
    <p>List</p>
    <p>Push to user</p>
    <p>Collect feedback</p>
    <p>Step1: get recommendation from ! and &quot;!  Step2: probabilistic interleave these two lists  Step3: get feedback from user and compare the performance of two network  Step4: if &quot;! performs better, update ! towards it</p>
  </div>
  <div class="page">
    <p># of</p>
    <p>n ew</p>
    <p>s</p>
    <p># of</p>
    <p>u se</p>
    <p>rs</p>
    <p>Dataset</p>
  </div>
  <div class="page">
    <p>Results: Offline</p>
    <p>C T</p>
    <p>R</p>
    <p>LR FM W &amp; D LinUCB HLinUCB</p>
    <p>DN DDQN DDQN+U DDQN+U+EG DDQN+U+DBGD</p>
    <p>Accuracy11/9/18 12</p>
  </div>
  <div class="page">
    <p>Results: Online</p>
    <p>Accuracy Diversity11/9/18 13</p>
  </div>
  <div class="page">
    <p>Summary of motivation and solution</p>
    <p>Motivation Solution Long term effect in recommendation Deep reinforcement learning (DRL)  Dynamic nature of news</p>
    <p>recommendation  Consider more measures for long</p>
    <p>term effect  Recommendation diversity</p>
    <p>Online learning feature of DRL</p>
    <p>Reward function design of DRL</p>
    <p>Explore in DRL</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We propose a reinforcement learning framework to do online personalized news recommendation, taking care of both immediate and future reward. Our framework can be generalized to many other recommendation problems.  We consider user activeness to help improve recommendation</p>
    <p>accuracy, which can provide extra information than simply using user click labels.  Our system has been deployed online in a commercial news</p>
    <p>recommendation application. Extensive offline and online experiments have shown the superior performance of our methods.</p>
  </div>
</Presentation>
