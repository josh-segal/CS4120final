<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Page 1</p>
    <p>Parallel Media Processors in the Billion-Transistor Era</p>
    <p>Jason Fritts Princeton University</p>
    <p>Dept. of Electrical Engineering Co-Authors: Zhao Wu and Wayne Wolf</p>
    <p>D E I M V S B N I N</p>
    <p>V I G E T E V</p>
    <p>VET TES EN</p>
    <p>NOV TAM TVM</p>
    <p>Overview</p>
    <p>Why Parallel Media Processors (PMPs)?</p>
    <p>Architecture Issues  datapath  memory hierarchy  potential performance over next decade</p>
    <p>Compiler Issues  extracting data parallelism  compiling to highly parallel architectures  potential parallel performance</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Page 2</p>
    <p>Next Decade of VLSI Technology</p>
    <p>Billion-transistor chips</p>
    <p>Multi-gigahertz processors</p>
    <p>Impact on multimedia processing industry?  still evolving -- strongly dependent upon computing performance  must consider future of multimedia</p>
    <p>Multimedia processing industry evolves with processor performance</p>
    <p>Multimedia processing industry evolves with processor performance</p>
    <p>Future of Multimedia</p>
    <p>Multimedia is moving towards an object-oriented representation</p>
    <p>Multimedia is moving towards an object-oriented representation</p>
    <p>Currently represented as video frames or audio channels  data organized as fixed-sized blocks/groups of continuous data  processing regularity</p>
    <p>Moving towards object-based representation  objects represent real-world objects</p>
    <p>objects have own video, audio, and graphical characteristics  advantages: greater compression, more interactive, content-based processing</p>
    <p>greater processing demands  less processing regularity  requires flexibility of high-level language (HLL) programmability</p>
  </div>
  <div class="page">
    <p>Page 3</p>
    <p>Multimedia Processing Solutions</p>
    <p>Multimedia has distinctive workloads  extensive data parallelism  high computation rates  large amounts of streaming data  small data types</p>
    <p>Current multimedia processing support  application-specific processors  multimedia extensions to general-purpose processors  current media processors</p>
    <p>VLIW or DSP style architectures  performance achieved through specialized functional units  flexibility, but require special programming libraries or paradigms</p>
    <p>Next generation of multimedia requires:  flexibility of high-level language (HLL) programmability  computing power for all forms of multimedia</p>
    <p>Future of Media Processors</p>
    <p>Evolve towards Parallel Media Processors (PMPs)  increased parallelism  achieve throughput from both high parallelism and high frequency</p>
    <p>Larger on-chip memory hierarchies  accommodate large amounts of data  minimize penalties from long external memory latencies</p>
    <p>Greater architecture regularity  greater flexibility over a wide range of applications  better high-level language (HLL) programmability</p>
    <p>no special libraries or programming paradigms</p>
  </div>
  <div class="page">
    <p>Page 4</p>
    <p>Architecture Issues</p>
    <p>What to do with a Billion Transistors?</p>
    <p>More functional units?  extra silicon enables higher degrees of parallelism  however, functional units require minimal area</p>
    <p>Larger memory hierarchies?  many multimedia applications access memory intensively</p>
    <p>especially video and graphics  larger on-chip memory helps reduce increasing processor</p>
    <p>memory gap</p>
    <p>Majority of extra resources likely used for memory</p>
  </div>
  <div class="page">
    <p>Page 5</p>
    <p>Datapath Issues</p>
    <p>Datapath architecture?  variety of architectures exist:</p>
    <p>superscalar, VLIW, single-chip multiprocessors, simultaneous-multithreading, vector processor, array processors, etc.</p>
    <p>uncertain which alternative is most desirable</p>
    <p>Throughput is primary datapath problem  both high parallelism and high frequency necessary for multimedia</p>
    <p>computing demands  unfortunately, high parallelism and high frequency are counter-productive</p>
    <p>high parallelism leads to excessive bypassing, memory ports, and register file ports</p>
    <p>Distributed architecture necessary  one possibility: clustered architecture</p>
    <p>Wide Issue =&gt; Clustered Architecture</p>
    <p>Global register file too large, slow for wide issue</p>
    <p>Separate datapath into disjoint clusters:  functional units (2-4 issue slots)  local register register file  local memories/caches</p>
    <p>Low-latency network between clusters</p>
    <p>Cluster Cluster Cluster Cluster Instruction Cache</p>
    <p>Cluster Cluster Cluster Cluster</p>
    <p>Instruction Cache</p>
    <p>C on</p>
    <p>trol</p>
    <p>Network</p>
    <p>Mult Data RAM</p>
    <p>Register File</p>
    <p>ALU</p>
    <p>FPShift</p>
  </div>
  <div class="page">
    <p>Page 6</p>
    <p>Memory Hierarchy Issues</p>
    <p>Memory hierarchy still undefined  memory? with prefetching?  cache? with prefetching?  hybrids?</p>
    <p>Must support multimedia memory access patterns  high data rates  streaming data  high spatial locality</p>
    <p>Streaming memory structures likely  stream buffer  stride prediction table  hybrids  where in hierarchy will they be most effective?</p>
    <p>Memory Hierarchy Issues, cont.</p>
    <p>Must support parallel memory accesses  large multi-ported memory too slow  require either:</p>
    <p>highly banked memory  distributed memory</p>
    <p>Primary truths about memory hierarchies  need aggressive memory hierarchies for high data rates  increased silicon resources will enable many interesting alternatives</p>
    <p>over next decade</p>
  </div>
  <div class="page">
    <p>Page 7</p>
    <p>Potential Performance of PMPs over Next Decade</p>
    <p>Scaling based on National Semiconductor Roadmap:  scaled processor frequency and number of transistors  additional transistors primarily applied to memory hierarchy</p>
    <p>MPEG-2 evaluated with trace-driven simulator  clustered architecture with 8 clusters of 4-issue slots/cluster  perfect branch prediction and memory disambiguation  scheduling window of one billion operations</p>
    <p>R e</p>
    <p>la ti</p>
    <p>v e</p>
    <p>S p</p>
    <p>e e</p>
    <p>d u</p>
    <p>p</p>
    <p>E n c o d e r</p>
    <p>D e c o d e r</p>
    <p>Additional Architecture Problems</p>
    <p>Dynamic versus static scheduling  static scheduling sufficient for media processing?  dynamic methods desirable?</p>
    <p>out-of-order execution, dynamic branch prediction, dynamic memory disambiguation, etc.</p>
    <p>Larger versus smaller datapath widths  small data types amenable to narrower datapath</p>
    <p>potential for higher frequency  support for pointers?</p>
    <p>larger datapath conducive to subword parallelism (e.g. Intels MMX)</p>
    <p>Supporting mixed-signal multimedia  designing media processors to support multiple types of multimedia  how to support simultaneous processing of multiple media?</p>
    <p>context-switching, simultaneous multi-threading, multiprocessing, etc.</p>
  </div>
  <div class="page">
    <p>Page 8</p>
    <p>Compiler Issues</p>
    <p>Compilers for Media Processors</p>
    <p>Aggressive optimizing compiler  multimedia offers extensive parallelism  throughput requires successful extraction of parallelism by compiler</p>
    <p>Compiler and architecture balanced  achieve high utilization of all architecture resources  allow each to tackle problems too difficult for the other</p>
    <p>compiler: static optimizations  architecture: dynamic optimizations</p>
    <p>Chief requirements  extracting parallelism  scheduling to highly parallel (distributed) architectures</p>
  </div>
  <div class="page">
    <p>Page 9</p>
    <p>Parallelism in Multimedia</p>
    <p>Studies indicate significant parallelism  trace-driven (Oracle-style) studies  parallelism has been utilized in many systems</p>
    <p>Instruction Level Parallelism (ILP)  fine-grained parallelism between individual instructions  multimedia contains no more ILP than general-purpose code</p>
    <p>Data Parallelism  exists between data elements whose processing are independent  coarser level of parallelism than ILP  generally separated by 500+ dynamic operations</p>
    <p>Extracting Data Parallelism</p>
    <p>Conventional compilers cannot extract data parallelism  good at scheduling for ILP  schedule over windows of typically less than 100 sequential</p>
    <p>instructions  scheduling windows too small for data parallelism</p>
    <p>Compilation method  execute separate iterations of outer loops in parallel</p>
    <p>SIMD parallelism  perform any optimization on inner loops</p>
  </div>
  <div class="page">
    <p>Page 10</p>
    <p>Example - Straightforward 2-D DCT</p>
    <p>Two inner loops  dependent on accumulation of y[k][l]</p>
    <p>Two outer loops  independent  minimum distance between data parallel elements</p>
    <p>is 9*64 = 576 instructions</p>
    <p>for (k = 0; k &lt; 8; k++) for (l = 0; l &lt; 8; l++) {</p>
    <p>y[k][l] = 0;</p>
    <p>for (i = 0; i &lt; 8; i++) for (j = 0; j &lt; 8; j++)</p>
    <p>y[k][l] += y[k][l] + x[i][j] * c[i][k] * c[j][l]; }</p>
    <p>for (k = 0; k &lt; 8; k++) for (l = 0; l &lt; 8; l++) {</p>
    <p>y[k][l] = 0;</p>
    <p>for (i = 0; i &lt; 8; i++) for (j = 0; j &lt; 8; j++)</p>
    <p>y[k][l] += y[k][l] + x[i][j] * c[i][k] * c[j][l]; }</p>
    <p>Exploiting Data Parallelism for Straightforward 2-D DCT</p>
    <p>L1</p>
    <p>L0</p>
    <p>L2</p>
    <p>i</p>
    <p>j</p>
    <p>l</p>
    <p>L1</p>
    <p>L0</p>
    <p>L2</p>
    <p>j</p>
    <p>L1</p>
    <p>L0</p>
    <p>L2</p>
    <p>j</p>
    <p>. . . ( loop i</p>
    <p>unrolled )</p>
    <p>( loop l data parallel )</p>
  </div>
  <div class="page">
    <p>Page 11</p>
    <p>Compiling to Highly Parallel (Distributed) Architectures</p>
    <p>Communication in distributed architectures  operation results not uniformly available to all clusters at same time  clusters interconnected by a low latency network</p>
    <p>Cluster scheduling  copy operations scheduled by compiler for inter-cluster communication  compiler attempts to schedule critical operations on same cluster to</p>
    <p>minimize performance degradation</p>
    <p>Cluster scheduling for data parallelism  independence of data parallelism matches well with independence of clusters  data parallel elements processing on separate clusters require little or no</p>
    <p>inter-cluster communication</p>
    <p>Data Partitioning</p>
    <p>Memory for wide-issue architecture requires either:  highly banked cache  separate cache or memory in each cluster</p>
    <p>Evaluate data parallelism at various levels  determine loop levels that offer data parallelism  examine memory requirements at each level  choose level with:</p>
    <p>largest independent data set  acceptable granularity of independent data  least amount of copied data (read sharing)</p>
  </div>
  <div class="page">
    <p>Page 12</p>
    <p>Example - DCT</p>
    <p>b) Single iteration of k loop (1/8 of block)</p>
    <p>c) Full 8x8 block</p>
    <p>a) Single iteration of l loop (1/64 of block)</p>
    <p>C</p>
    <p>C</p>
    <p>C</p>
    <p>X</p>
    <p>X</p>
    <p>X</p>
    <p>Y</p>
    <p>Y</p>
    <p>Y</p>
    <p>Performance of Compilation versus Trace-Driven Simulation</p>
    <p>Trace-driven simulator  clustered VLIW architecture with 8-clusters of 4-issue slots/cluster  assumes perfect branch prediction and memory disambiguation  scheduling window of one billion instructions</p>
    <p>IMPACT compiler  32-issue statically-scheduled unclustered VLIW architecture</p>
    <p>h 2 6 3 d e c h 2 6 3 e n c m p e g 2 d e c m p e g 2 e n c m p e g 4 d e c</p>
    <p>IP C Co mp ile r</p>
    <p>S imu la to r</p>
  </div>
  <div class="page">
    <p>Page 13</p>
    <p>Granularity of Data Parallelism in MPEG-2</p>
    <p>Performance relative to scheduling window size  used trace-driven simulator  varied size of scheduling window  true data parallelism smaller than indicated</p>
    <p>scheduler does not eliminate loop-carried dependencies from loop induction variables</p>
    <p>K</p>
    <p>K</p>
    <p>M</p>
    <p>M</p>
    <p>S c h e d u lin g W in d o w S iz e</p>
    <p>IP C</p>
    <p>En c o d e r</p>
    <p>De c o d e r</p>
    <p>Conclusions</p>
    <p>Much potential for PMPs over next decade  advances in VLSI technology will enable single-chip PMPs  many research issues remain to realize full potential</p>
    <p>Architecture Issues  majority of additional silicon resources applied to memory hierarchy  wider datapaths</p>
    <p>throughput requires high parallelism and high frequency  larger memory hierarchies</p>
    <p>streaming memory structures likely  must support many parallel memory accesses</p>
    <p>Compiling Issues  extracting data parallelism  compiling to highly parallel (distributed) architectures</p>
    <p>cluster scheduling  data partitioning</p>
  </div>
</Presentation>
