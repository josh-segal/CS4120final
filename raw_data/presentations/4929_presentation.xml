<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Mean Field Inference in Dependency Networks: An Empirical Study</p>
    <p>Daniel Lowd and Arash Shamaei University of Oregon</p>
  </div>
  <div class="page">
    <p>Learning and Inference in Graphical Models</p>
    <p>We want to learn a probability distribution from data and use it to answer queries.</p>
    <p>Applications: medical diagnosis, fault diagnosis, web usage analysis, bioinformatics, collaborative filtering, etc.</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Data Model</p>
    <p>Learning Inference</p>
  </div>
  <div class="page">
    <p>One-Slide Summary</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Data Model</p>
    <p>Learning Inference</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Graphical models: Dependency networks vs. others  Representation  Learning  Inference</p>
    <p>Mean field inference in dependency networks  Experiments</p>
  </div>
  <div class="page">
    <p>Dependency Networks Represents a probability distribution over {X1, , Xn} as a</p>
    <p>set of conditional probability distributions.</p>
    <p>Example:</p>
    <p>{P1(X1 | X 3 ),P2 (X 2 | X1, X 3 ), P3 ( X 3 | X1, X 2 )}</p>
    <p>X1</p>
    <p>X2 X3</p>
    <p>[Heckerman et al., 2000]</p>
  </div>
  <div class="page">
    <p>Comparison of Graphical Models</p>
    <p>Bayesian Network</p>
    <p>Markov Network</p>
    <p>Dependency Network</p>
    <p>Allow cycles? N Y Y Easy to learn? Y N Y Consistent distribution?</p>
    <p>Y Y N</p>
    <p>Inference algorithms</p>
    <p>lots lots Gibbs, MF (new!)</p>
  </div>
  <div class="page">
    <p>Learning Dependency Networks</p>
    <p>For each variable Xi, learn conditional distribution,</p>
    <p>Pi ( X i | X i )</p>
    <p>B=?</p>
    <p>&lt;0.2, 0.8&gt;</p>
    <p>&lt;0.5, 0.5&gt; &lt;0.7, 0.3&gt;</p>
    <p>false</p>
    <p>false C=?</p>
    <p>tru e</p>
    <p>tru e</p>
    <p>PA ( A | B,C) =</p>
    <p>C=?</p>
    <p>&lt;0.7, 0.3&gt; &lt;0.4, 0.6&gt;</p>
    <p>falsetr ue</p>
    <p>PB (B | C) =</p>
    <p>[Heckerman et al., 2000]</p>
  </div>
  <div class="page">
    <p>Approximate Inference Methods</p>
    <p>Gibbs sampling: Slow but effective  Mean field: Fast and usually accurate  Belief propagation: Fast and usually accurate</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Model</p>
  </div>
  <div class="page">
    <p>Gibbs Sampling Resample each variable in turn, given its neighbors:</p>
    <p>Use set of samples to answer queries. e.g.,</p>
    <p>Converges to true distribution, given enough samples (assuming positive distribution).</p>
    <p>xi ( t +1) ~ P X i | X i = x i</p>
    <p>( t ) ( )</p>
    <p>P(B = True) = # of samples where B = True</p>
    <p>total # of samples</p>
    <p>Previously, the only method used to compute probabilities in DNs.Previously, the only method used to compute probabilities in DNs.</p>
  </div>
  <div class="page">
    <p>Mean Field Approximate P with simpler distribution Q:</p>
    <p>To find best Q, optimize reverse K-L divergence:</p>
    <p>Mean field updates converge to local optimum:</p>
    <p>P( X1, X 2,..., X n )  Q( X1)Q(X 2 ) L Q( X n )</p>
    <p>KL(Q || P) = Q( X = x) log P( X = x) Q( X = x)x</p>
    <p>Q(t )( X i )  exp E X  i ~Q ( t 1 ) ( X  i ) log P( X i | X i )[ ]( )</p>
    <p>Works for DNs! Never before tested!Works for DNs! Never before tested! 10</p>
  </div>
  <div class="page">
    <p>Mean Field in Dependency Networks</p>
    <p>Q( t )( X i )  exp E X  i ~Q ( t 1 ) ( X  i ) log Pi ( X i | X i )[ ]( )</p>
    <p>If consistent, this is guaranteed to converge. If inconsistent, this always seems to converge in practice.</p>
  </div>
  <div class="page">
    <p>Empirical Questions</p>
    <p>Q1. In DNs, how does MF compare to Gibbs sampling in speed and accuracy?</p>
    <p>Q2. How do DNs compare to BNs in inference speed and accuracy?</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Learned DNs and BNs on 12 datasets  Generated queries from test data</p>
    <p>Varied evidence variables from 10% to 90%  Score using average CMLL per variable</p>
    <p>(conditional marginal log-likelihood):</p>
    <p>CMLL( x,e) = 1</p>
    <p>| X | log P(X i = xi | E = e)</p>
    <p>i</p>
  </div>
  <div class="page">
    <p>Results: Accuracy in DNs</p>
    <p>N eg</p>
    <p>at ve</p>
    <p>C M</p>
    <p>LL N</p>
    <p>eg at</p>
    <p>ve C</p>
    <p>M LL</p>
  </div>
  <div class="page">
    <p>Results: Timing in DNs (log scale)</p>
    <p>In fe</p>
    <p>re nc</p>
    <p>e Ti</p>
    <p>m e</p>
    <p>(s )</p>
  </div>
  <div class="page">
    <p>MF vs. Gibbs in DNs, run for equal time</p>
    <p>Evidence # of MF wins % wins</p>
    <p>Average 10.2 85%</p>
    <p>In DNs, MF usually more accurate, given equal time.In DNs, MF usually more accurate, given equal time. 16</p>
  </div>
  <div class="page">
    <p>Results: Accuracy</p>
    <p>DN.MF DN.Gibbs BN.MF BN.Gibbs BN.BP</p>
    <p>N eg</p>
    <p>at ve</p>
    <p>C M</p>
    <p>LL</p>
  </div>
  <div class="page">
    <p>Gibbs: DN vs. BN Evidence DN wins Percent wins</p>
    <p>Average 5.7 47%</p>
    <p>With more evidence, DNs are more accurate.With more evidence, DNs are more accurate. 18</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>Q1. In DNs, how does MF compare to Gibbs sampling in speed and accuracy?</p>
    <p>A1. MF is consistently faster with similar accuracy, or more accurate with similar speed.</p>
    <p>Q2. How do DNs compare to BNs in inference speed and accuracy?</p>
    <p>A2. DNs are competitive with BNs  better with more evidence, worse with less evidence.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>MF inference in DNs is fast and accurate, especially with more evidence.</p>
    <p>Future work:  Relational dependency networks</p>
    <p>(Neville &amp; Jensen, 2007)</p>
    <p>More powerful approximations</p>
    <p>Source code available: http://libra.cs.uoregon.edu/ Source code available: http://libra.cs.uoregon.edu/</p>
  </div>
  <div class="page">
    <p>Results: Timing (log scale)</p>
    <p>In fe</p>
    <p>re nc</p>
    <p>e Ti</p>
    <p>m e</p>
    <p>(s )</p>
  </div>
  <div class="page">
    <p>Learned Models</p>
  </div>
</Presentation>
