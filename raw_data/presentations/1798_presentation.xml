<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>EyeQ: (An engineers approach to)</p>
    <p>Taming network performance unpredictability in the Cloud</p>
    <p>Vimal Mohammad Alizadeh</p>
    <p>Balaji Prabhakar David Mazires</p>
    <p>Changhoon Kim Albert Greenberg</p>
  </div>
  <div class="page">
    <p>What are we depending on?</p>
    <p>http://techblog.netflix.com/2010/12/5lessons-weve-learned-using-aws.html</p>
    <p>in the Netflix data centers, we have a high capacity, super fast, highly reliable network. This has afforded us the luxury of designing around chatty APIs to remote systems. AWS networking has more variable latency.</p>
    <p>Overhaul apps to deal with variability</p>
    <p>Many customers dont even realise network issues:</p>
    <p>Just spin up more VMs! Makes app more network dep.</p>
  </div>
  <div class="page">
    <p>Cloud: Warehouse Scale Computer Multi-tenancy: To increase cluster utilisation</p>
    <p>http://research.google.com/people/jeff/latency.html</p>
    <p>Provisioning the Warehouse CPU, memory, disk</p>
    <p>Network</p>
  </div>
  <div class="page">
    <p>Sharing the Network</p>
    <p>Policy  Sharing model</p>
    <p>Mechanism  Computing rates  Enforcing rates on entities  Per-VM (multi-tenant)  Per-service (search, map-reduce, etc.)</p>
    <p>Can we achieve this?</p>
    <p>Tenant Xs Virtual Switch</p>
    <p>Tenant Xs Virtual Switch</p>
    <p>VM1VM1 VM2VM2 VMnVMnVM3VM3</p>
    <p>Tenant Ys Virtual Switch</p>
    <p>Tenant Ys Virtual Switch</p>
    <p>VM1VM1 VM2VM2 VMiVMiVM3VM3</p>
    <p>Customer X specifies the thickness of each pipe. No traffic matrix. (Hose Model)</p>
  </div>
  <div class="page">
    <p>Why is it hard? (1)</p>
    <p>Bandwidth demands can be  Random, bursty  Short: few millisecond requests</p>
    <p>Timescales matter!  Need guarantees on the order of few RTTs (ms)</p>
    <p>Default policy insufficient: 1 vs many TCP flows, UDP, etc.  Poor scalability of traditional QoS mechanisms</p>
  </div>
  <div class="page">
    <p>Seconds: Eternity</p>
    <p>Switch</p>
    <p>Bursty UDP session ON: 5ms OFF: 15ms</p>
    <p>Shared 10G pipe</p>
  </div>
  <div class="page">
    <p>Under the hood</p>
    <p>Switch</p>
  </div>
  <div class="page">
    <p>Why is it hard? (2)</p>
    <p>Switch</p>
    <p>Switch sees contention, but lacks VM state  Receiver-host has VM state, but does not see contention</p>
    <p>(1) Drops in network: servers dont see true demand</p>
    <p>(2) Elusive TCP (back-off) makes true demand detection harder</p>
  </div>
  <div class="page">
    <p>Key Idea: Bandwidth Headroom  Bandwidth guarantees: managing congestion  Congestion: link util reaches 100%  At millisecond timescales</p>
    <p>Dont allow 100% util  10% headroom: Early detection at receiver</p>
    <p>N x 10G</p>
    <p>UDP</p>
    <p>TCP</p>
    <p>Shared pipe Limit to 9G</p>
    <p>C C</p>
    <p>C  C  &lt; 1</p>
    <p>Single Switch: Headroom</p>
    <p>What about a network?</p>
  </div>
  <div class="page">
    <p>Network design: the old</p>
    <p>http://bradhedlund.com/2012/04/30/network-that-doesnt-suckfor-cloud-and-big-data-interop-2012-session-teaser/</p>
    <p>Over-subscription</p>
  </div>
  <div class="page">
    <p>Network design: the new</p>
    <p>http://bradhedlund.com/2012/04/30/network-that-doesnt-suckfor-cloud-and-big-data-interop-2012-session-teaser/</p>
    <p>(1) Uniform capacity across racks</p>
    <p>(2) Over-subscription only at Top-of-Rack</p>
  </div>
  <div class="page">
    <p>Mitigating Congestion in a Network</p>
    <p>Load balancing + Admissibility = Hotspot free network core</p>
    <p>[VL2, FatTree, Hedera, MicroTE]</p>
    <p>Aggregate rate &gt; 10Gbps Fabric gets congested</p>
    <p>Server</p>
    <p>VMVM</p>
    <p>Fabric</p>
    <p>Aggregate rate &lt; 10Gbps Congestion free Fabric</p>
    <p>Server</p>
    <p>VMVM</p>
    <p>Fabric Load balancing: ECMP, etc.</p>
    <p>Admissibility: e2e congestion control (EyeQ)</p>
  </div>
  <div class="page">
    <p>EyeQ Platform</p>
    <p>TX packets</p>
    <p>VMVM VMVM</p>
    <p>TX</p>
    <p>VMVM</p>
    <p>Software VSwitchSoftware VSwitch</p>
    <p>Adaptive Rate Limiters</p>
    <p>Adaptive Rate Limiters</p>
    <p>untrusted</p>
    <p>RX</p>
    <p>RX packets</p>
    <p>Software VSwitchSoftware VSwitch</p>
    <p>VMVM</p>
    <p>Congestion Detectors Congestion Detectors</p>
    <p>untrusted VMVM</p>
    <p>RX component detects</p>
    <p>TX component reacts</p>
    <p>End-to-end flow control</p>
    <p>(VSwitchVSwitch)</p>
    <p>DataCentre Fabric</p>
    <p>Congestion Feedback</p>
  </div>
  <div class="page">
    <p>Does it work?</p>
    <p>Without EyeQ With EyeQ</p>
    <p>Improves utilisation</p>
    <p>Provides protection</p>
    <p>TCP: 6Gbps UDP: 3Gbps</p>
  </div>
  <div class="page">
    <p>State: only at edge</p>
    <p>EyeQEyeQ</p>
    <p>One Big SwitchOne Big Switch</p>
  </div>
  <div class="page">
    <p>Thanks! jvimal@stanford.edu</p>
    <p>EyeQ Load balancing + Bandwidth headroom + Admissibility at millisec timescales = Network as one big switch = Bandwidth sharing at edge</p>
    <p>Linux, Windows implementation for 10Gbps ~1700 lines C code http://github.com/jvimal/perfiso_10g (Linux kmod) No documentation, yet.</p>
  </div>
  <div class="page"/>
</Presentation>
