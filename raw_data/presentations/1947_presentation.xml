<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Enabling Concurrent Multithreaded MPI Communication on Multicore Petascale Systems</p>
    <p>Gabor Dozsa1, Sameer Kumar1, Pavan Balaji2, Darius Buntinas2, David Goodell2, William Gropp3, Joe Ratterman4, and Rajeev Thakur2</p>
    <p>Joint Collaboration between Argonne and IBM</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>MPI Semantics for multi-threading</p>
    <p>Blue Gene/P overview</p>
    <p>Deep computing messaging framework (DCMF)</p>
    <p>Optimize MPI thread parallelism</p>
    <p>Performance results</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Multicore architectures with many threads per node  Running MPI processes on each core results in</p>
    <p>Less memory per process  Higher TLB pressure  Problem may not scale to as many processes</p>
    <p>Hybrid Programming  Use MPI across nodes  Use shared memory within nodes (posix threads, OpenMP)  MPI library accessed concurrently from many threads</p>
    <p>Fully concurrent network interfaces that permit concurrent access from multiple threads</p>
    <p>Thread optimized MPI library critical for hybrid programming</p>
  </div>
  <div class="page">
    <p>MPI Semantics for Multithreading</p>
    <p>MPI defines four thread levels  Single</p>
    <p>Only one thread will execute  Funneled</p>
    <p>The process may be multi-threaded, but only the main thread will make MPI calls</p>
    <p>Serialized  The process may be multi-threaded, and multiple threads may</p>
    <p>make MPI calls, but only one at a time: MPI calls are not made concurrently from two distinct threads</p>
    <p>Multiple  Multiple threads may call MPI, with no restrictions</p>
  </div>
  <div class="page">
    <p>Blue Gene/P Overview</p>
  </div>
  <div class="page">
    <p>BlueGene/P Interconnection Networks</p>
    <p>ns  Communications backbone for computations</p>
    <p>Collective Network  core responsible for handling packets  One-to-all broadcast functionality  Reduction operations functionality  6.8 Gb/s (850 MB/s) of bandwidth per link  Latency of one way network traversal 1.3 s</p>
    <p>Low Latency Global Barrier and Interrupt  Latency of one way to reach all 72K nodes 0.65 s,</p>
    <p>MPI 1.2 s</p>
  </div>
  <div class="page">
    <p>BG/P Torus network and the DMA</p>
    <p>Torus network accessed via a direct memory access unit</p>
    <p>DMA unit sends and receives data with physical addresses</p>
    <p>Messages have to be in contiguous buffers</p>
    <p>DMA performs cache injection on sender and receiver</p>
    <p>Resources in the DMA managed by softwareBlue Gene/P Compute Card</p>
  </div>
  <div class="page">
    <p>BG/P DMA</p>
    <p>DMA Resources</p>
    <p>Injection Memory FIFOs</p>
    <p>Reception Memory FIFOs</p>
    <p>Counters</p>
    <p>A collection of DMA resources forms a group</p>
    <p>Each BG/P node has four groups</p>
    <p>Access to a DMA group can be concurrent</p>
    <p>Typically used to support virtual node mode with four processes</p>
    <p>Blue Gene/P Node Card</p>
  </div>
  <div class="page">
    <p>Message Passing on the BG/P DMA</p>
    <p>Sender injects a DMA descriptor</p>
    <p>Descriptor provides detailed information to the DMA on the actions to perform</p>
    <p>DMA initiates intra-node or inter-node data movement</p>
    <p>Memory FIFO send : results in packets in the destination reception memory FIFO</p>
    <p>Direct put : moves local data to a destination buffer  Remote get : pulls data from a source node to a local (or</p>
    <p>even remote) buffer  Access to injection and reception FIFOs in different groups</p>
    <p>can be done in parallel by different processes and threads</p>
  </div>
  <div class="page">
    <p>Deep Computing Messaging Framework (DCMF)</p>
    <p>Low level messaging API on BG/P</p>
    <p>Supporting multiple paradigms on BG/P</p>
    <p>Active message API  Good match for LAPI, Charm++ and other active message</p>
    <p>runtimes</p>
    <p>MPI supported on this active message runtime</p>
    <p>Optimized collectives</p>
  </div>
  <div class="page">
    <p>Extensions to DCMF for concurrency</p>
    <p>Introduce the notion of channels  In SMP mode the four injection/reception DMA groups</p>
    <p>exposed as four DCMF channels</p>
    <p>New DCMF API calls  DCMF_Channel_acquire()  DCMF_Channel_release()  DCMF progress calls enhanced to only advance acquired</p>
    <p>channel  Channel state stored in thread private memory</p>
    <p>Pt-to-pt API unchanged</p>
    <p>Channels are similar to the notion of enpoints proposal in the MPI Forum</p>
  </div>
  <div class="page">
    <p>Optimize MPI thread concurrency</p>
  </div>
  <div class="page">
    <p>MPICH Thread Parallelism</p>
    <p>Coarse Grained</p>
    <p>Each MPI call is guarded by an ALLFUNC critical section macro</p>
    <p>Blocking MPI calls release the critical section enabling all threads to make progress</p>
    <p>Fine grained</p>
    <p>Decrease size of the critical sections</p>
    <p>ALLFUNC macros are disabled</p>
    <p>Each shared resource is locked  For example a MSGQUEUE critical section can guard a message queue  The RECVQUEUE critical section will guard the MPI receive queues  HANDLE mutex for allocating object handles</p>
    <p>Eliminate critical sections  Operations such as reference counting can be optimized via scalable atomics</p>
    <p>(e.g. fetch-add)</p>
  </div>
  <div class="page">
    <p>Enabling Concurrent MPI on BG/P</p>
    <p>Messages to the same destination always use the same channel to preserve MPI ordering</p>
    <p>Messages from different sources arrive on different channels to improve parallelism</p>
    <p>Map each source destination pair to a channel via a hash function</p>
    <p>E.g. (srcrank + dstrank) % numchannels</p>
  </div>
  <div class="page">
    <p>Parallel Receive Queues</p>
    <p>Standard MPICH has two queues for posted receives and unexpected messages</p>
    <p>Extend MPICH to have  Queue of unexpected messages and posted receives for each channel  Additional queue for wild card receives</p>
    <p>Each process posts receives to the channel queue in the absence of wild cards  When there is a wild card all receives are posted to the wild card queue</p>
    <p>When a packet arrives  First the wild card queue is processed after acquiring WC lock  If its empty, the thread that receives the packet</p>
    <p>acquires channel RECVQUEUE lock  Matches packet with posted channel receives or  If no match is found a new entry is created in the channel unexpected queue</p>
  </div>
  <div class="page">
    <p>Performance Results</p>
  </div>
  <div class="page">
    <p>Message Rate Benchmark</p>
    <p>Message rate benchmark where each thread exchanges messages with a different node</p>
  </div>
  <div class="page">
    <p>Message Rate Performance</p>
    <p>Zero threads = MPI_THREAD_SINGLE</p>
  </div>
  <div class="page">
    <p>Thread scaling on MPI vs DCMF</p>
    <p>Absence of receiver matching enables higher concurrency in</p>
    <p>DCMF</p>
  </div>
  <div class="page">
    <p>Summary and Future work</p>
    <p>We presented different techniques to improve throughput of MPI calls in multi-threaded architectures</p>
    <p>Performance improves 3.6x on four threads</p>
    <p>These techniques should be extendible to other architectures where network interfaces permit concurrent access</p>
    <p>Explore lockless techniques to eliminate critical sections for handles and other resources</p>
    <p>Garbage collect request objects</p>
  </div>
</Presentation>
