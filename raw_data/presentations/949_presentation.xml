<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>FastCDC: a Fast and Efficient Content-Defined Chunking Approach</p>
    <p>for Data Deduplication</p>
    <p>Wen Xia Yukun Zhou, Hong Jiang, Dan Feng, Yu Hua,</p>
    <p>Yuchong Hu, Yucheng Zhang, Qing Liu</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and Motivation</p>
    <p>Design and Implementation</p>
    <p>Performance Evaluation</p>
    <p>Conclusion</p>
    <p>2</p>
  </div>
  <div class="page">
    <p>Redundancy Data in the Big Data Era</p>
    <p>Explosive growth of digital data  2007281 EB  20101.2 ZB  20111.8 ZB  202044 ZB  GB,TB, PB, EB, ZB, YB,</p>
    <p>Data Reduction for Backup  Unstructured and Cumbersome  Duplicate or Redundant</p>
    <p>3</p>
  </div>
  <div class="page">
    <p>Data Deduplication</p>
    <p>A global compression approach for storage systems  Identifying the duplicate chunks (size of about 8KB)  Using a hash signature to uniquely identify a data chunk  Secure hash signature: MD5, SHA-1, SHA-256,</p>
    <p>Benefits Reduces the storage space requirement for backup Minimizes the network transmission of redundant data</p>
    <p>4</p>
  </div>
  <div class="page">
    <p>Deduplication Workflow</p>
    <p>We focus designing a fast and efficient chunking approach in this paper</p>
    <p>a7 5d a7</p>
    <p>Data Chunks</p>
    <p>Files</p>
    <p>Unique Chunks File A: File B:</p>
    <p>Metadata</p>
    <p>a7 5d</p>
    <p>Containers:</p>
    <p>5</p>
  </div>
  <div class="page">
    <p>Content-Defined Chunking (CDC)</p>
    <p>CDC declares the chunk boundaries based on the contents instead of on the byte offset</p>
    <p>It resolves the boundary-shift problem facing Fixed-Size Chunking (FSC)</p>
    <p>FF58 6535 8975 3239 4626 4338 3279 5028 89</p>
    <p>File A</p>
    <p>File A boundary-shift</p>
    <p>6</p>
  </div>
  <div class="page">
    <p>FSC vs. CDC</p>
    <p>FSC: No duplicates will be detected</p>
    <p>FF58 6535 8975 3239 4626 4338 3279 5028 89</p>
    <p>File A</p>
    <p>File A</p>
    <p>CDC: Most duplicates will be detected</p>
    <p>FF58 6535 8975 3239 4626 4338 3279 5028 89</p>
    <p>File A</p>
    <p>File A</p>
    <p>7</p>
  </div>
  <div class="page">
    <p>The Classic Rabin-based CDC</p>
    <p>LBFS employs Rabin hash for CDC</p>
    <p>It uses a sliding-window on the contents and computes Rabin hash of the window</p>
    <p>Cut a chunk if fp mod D == r  Otherwise continue sliding to find cut-point</p>
    <p>LBFS: A low-bandwidth network file system @ SOSP01 8</p>
    <p>C1 C2 C3 C4</p>
    <p>fp mod D r fp mod D = r</p>
    <p>File V1 C1 C7 C3 C4 File V2 Modified</p>
  </div>
  <div class="page">
    <p>The Classic Rabin-based CDC</p>
    <p>Rabin is a rolling hash since it is able to be computed from the previous value  Fast and efficient for CDC process</p>
    <p>Influence on chunk size  Variable size (due to the randomness of Rabin hash)  Expected length is approx. N (e.g., 8KB)</p>
    <p>Define Min/Max Chunk Size  To avoid chunks of extremely large or small sizes  Typically 2KB and 64KB when avg. 8KB</p>
    <p>9</p>
  </div>
  <div class="page">
    <p>State of Art</p>
    <p>Challenges: still time-consuming  Compute and judge the hashes byte-by-byte  A potential bottleneck in dedup systems</p>
    <p>Algorithmic-oriented CDC optimizations  SampleByte (NSDI10), Gear (Performance14), AE</p>
    <p>(Infocom15), etc.</p>
    <p>Hardware-oriented CDC optimizations  GPGPU, multicore/manycore  StoreGPU (HPDC08), Shredder (FAST12), P-Dedupe</p>
    <p>(NAS12), etc.</p>
    <p>10</p>
  </div>
  <div class="page">
    <p>Gear: a very fast rolling hash for CDC</p>
    <p>Gear-based CDC</p>
    <p>Implementation: fp = (fp &lt;&lt; 1) + G(b)  One array lookup  One + operation  One left-shift operation</p>
    <p>Ddelta: A dedup-inspired fast delta compression approach @ Performance14</p>
  </div>
  <div class="page">
    <p>Motivations</p>
    <p>Can Gear-based CDC be faster?</p>
    <p>12</p>
  </div>
  <div class="page">
    <p>FastCDC Overview</p>
    <p>Improving chunking performance over Gear-based CDC</p>
    <p>Optimi zing Hash Ju dgment</p>
    <p>Cut-points S kipp ing</p>
    <p>Normalized C hun king</p>
    <p>FastCDC</p>
    <p>HDR: High Deduplication Ratio</p>
    <p>HCS: High Chunking Speed</p>
    <p>LCS: Large Avg. Chunk Si ze</p>
    <p>HCS, HDR</p>
    <p>HCS, LCS</p>
    <p>HDR, HCS</p>
    <p>High Performance</p>
    <p>13</p>
  </div>
  <div class="page">
    <p>Optimizing Hash Judgment</p>
    <p>Zero-padding, to enlarge the sliding window  Gear-based chunking: fp &amp; (2n-1) == r  Small sliding window size results in low dedup ratio  Padding several zero bits into the mask value (i.e., 2n-1)</p>
    <p>enlarges the effective masked bit-width of fp,  The enlarged sliding window size improves the dedup ratio</p>
    <p>Simplified hash judgment speeds up CDC  fp mod D == r  ! (fp &amp; mask)  Speeds up the stage of hash judgment for CDC</p>
    <p>14</p>
  </div>
  <div class="page">
    <p>Cut-point Skipping</p>
    <p>The limit on minimum chunk size  Avoids generating extremely small-sized chunks  Also can be used for cut-point skipping, which</p>
    <p>reduces computation and thus further speeds up CDC But   It results in some chunks not truly divided by contents</p>
    <p>but forced by the cut-point skipping  According to our study, the chunks smaller than 2KB</p>
    <p>account for about 22.1% of the total  Thus this approach will decrease dedup ratio</p>
    <p>15</p>
  </div>
  <div class="page">
    <p>Normalized Chunking (NC)</p>
    <p>A novel chunking approach  To solve the problem of decreased dedupication ratio</p>
    <p>facing cut-point skipping  To generate chunks of sizes normalized to a specified</p>
    <p>region that is always larger than the min chunk size</p>
    <p>A higher level of NC</p>
    <p>exponential distribution normal distribution 16</p>
  </div>
  <div class="page">
    <p>Implementation of Normalized Chunking (NC)</p>
    <p>For the traditional CDC (avg. 8KB chunk size)  fp &amp; 0x1fff==r, 13 effective mask bits</p>
    <p>For the normalized chunking  When the offset is smaller than 8KB</p>
    <p>E.g., fp &amp; 0x7fff==r, 14 effective mask bits   harder to cut chunks (smaller than 8KB)</p>
    <p>When the offset is larger than 8KB  E.g., fp &amp; 0x0fff==r, 12 effective mask bits   easier to cut chunks (larger than 8KB)</p>
    <p>17</p>
  </div>
  <div class="page">
    <p>Chunk-Size Distribution of NC</p>
    <p>NC generates chunks approximately normalized to 8KB  Makes it possible to combine with cut-point skipping  Improves dedup ratio by reducing the larger chunks  No additional computing and comparing operations</p>
    <p>18</p>
  </div>
  <div class="page">
    <p>Performance Evaluation</p>
    <p>Experimental platform  Runs FastCDC on Ubuntu 12.04.2 operating system  Intel i7-4770 (@3.4GHz) and i7-930 (@2.8GHz) processors</p>
    <p>Seven workloads  Source code files, backup tarred files, VM images, database</p>
    <p>files, etc.  Total size of about 5TB</p>
    <p>Three key metrics  Chunking speed (in memory)  Deduplication ratio  Average chunk size (after chunking)</p>
    <p>19</p>
  </div>
  <div class="page">
    <p>Putting It All Together (Chunking Speed)</p>
    <p>FastCDC is about 10X faster than Rabin-based CDC Using Gear for Hashing Optimizing hash judgment Cut-point skipping Normalizing chunking</p>
    <p>FastCDC uses far fewer instructions and CPU cycles</p>
    <p>20</p>
  </div>
  <div class="page">
    <p>Put It All Together (Dedup ratio)</p>
    <p>FastCDC achieves nearly the same (Min8KB) or higher (Min4KB) dedup ratio than Rabin-based CDC</p>
    <p>Rabin-based CDC FastCDC with diff. min chunk size</p>
    <p>21</p>
  </div>
  <div class="page">
    <p>Put It All Together (Avg. chunk size)</p>
    <p>FastCDC generates nearly the same (Min4KB) or much larger (Min8KB) average chunk size, which means less metadata overheads for dedup indexing</p>
    <p>Rabin-based CDC FastCDC with diff. min chunk size</p>
    <p>22</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>FastCDC: improving CDC performance  Optimizing the hash judgment  Cut-point skipping  Normalized chunking</p>
    <p>FastCDC is shown to be 10X faster than Rabin-based CDC and 3X faster than Gear-based CDC while achieving a comparable dedup ratio</p>
    <p>In our future work: Incorporate FastCDC in some other deduplication systems Release the FastCDC source code to be shared with the</p>
    <p>storage systems research community</p>
    <p>23</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>I can be reach at: xia@hust.edu.cn Or wxia.hustbackup.cn</p>
  </div>
</Presentation>
