<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Pegasus: Coordinated Scheduling for Virtualized</p>
    <p>Accelerator-based Systems</p>
    <p>Vishakha Gupta, Karsten Schwan @ Georgia Tech</p>
    <p>Niraj Tolia @ Maginatics</p>
    <p>Vanish Talwar, Parthasarathy Ranganathan @ HP Labs</p>
    <p>USENIX ATC 2011  Portland, OR, USA</p>
  </div>
  <div class="page">
    <p>Increasing Popularity of Accelerators</p>
    <p>IBM CellbasedPlaystation</p>
    <p>IBM Cellbased RoadRunner</p>
    <p>CUDA programmab le GPUs for developers</p>
    <p>Increasing popularity of NVIDIA GPUs powered desktops and laptops</p>
    <p>Amazon EC2 adopts GPUs</p>
    <p>Tianhe-1A and Nebulae supercomput ers in Top500</p>
    <p>Tegras in cellphones</p>
    <p>Keeneland</p>
  </div>
  <div class="page">
    <p>Example x86-GPU System</p>
    <p>PCIe</p>
  </div>
  <div class="page">
    <p>Example x86-GPU System</p>
    <p>PCIe</p>
    <p>Proprietary NVIDIA Driver and CUDA runtime  Memory management  Communication with device  Scheduling logic  Binary translation</p>
  </div>
  <div class="page">
    <p>Example x86-GPU System</p>
    <p>PCIe</p>
    <p>Proprietary NVIDIA Driver and CUDA runtime  Memory management  Communication with device  Scheduling logic  Binary translation</p>
    <p>C-like CUDA-based applications (host portion)</p>
  </div>
  <div class="page">
    <p>Example x86-GPU System</p>
    <p>PCIe</p>
    <p>CUDA Kernels</p>
    <p>Proprietary NVIDIA Driver and CUDA runtime  Memory management  Communication with device  Scheduling logic  Binary translation</p>
    <p>C-like CUDA-based applications (host portion)</p>
  </div>
  <div class="page">
    <p>Example x86-GPU System</p>
    <p>PCIe</p>
    <p>CUDA Kernels</p>
    <p>Proprietary NVIDIA Driver and CUDA runtime  Memory management  Communication with device  Scheduling logic  Binary translation</p>
    <p>C-like CUDA-based applications (host portion)</p>
    <p>Design flaw: Bulk of logic in drivers which were meant to be for simple operations like read, write and handle interrupts Shortcoming: Inaccessibility and one scheduling fits all</p>
  </div>
  <div class="page">
    <p>Sharing Accelerators</p>
    <p>Amazon EC2 adopts GPUs</p>
    <p>Other cloud offerings by AMD, NVIDIA</p>
    <p>Tegras in cellphones</p>
    <p>HPC GPU Cluster (Keeneland )</p>
  </div>
  <div class="page">
    <p>Sharing Accelerators</p>
    <p>Most applications fail to occupy GPUs completely</p>
    <p>With the exception of extensively tuned (e.g. supercomputing) applications</p>
    <p>Amazon EC2 adopts GPUs</p>
    <p>Other cloud offerings by AMD, NVIDIA</p>
    <p>Tegras in cellphones</p>
    <p>HPC GPU Cluster (Keeneland )</p>
  </div>
  <div class="page">
    <p>Sharing Accelerators</p>
    <p>Most applications fail to occupy GPUs completely</p>
    <p>With the exception of extensively tuned (e.g. supercomputing) applications</p>
    <p>Expected utilization of GPUs across applications in some domains may follow patterns to allow sharing</p>
    <p>Amazon EC2 adopts GPUs</p>
    <p>Other cloud offerings by AMD, NVIDIA</p>
    <p>Tegras in cellphones</p>
    <p>HPC GPU Cluster (Keeneland )</p>
  </div>
  <div class="page">
    <p>Sharing Accelerators</p>
    <p>Most applications fail to occupy GPUs completely</p>
    <p>With the exception of extensively tuned (e.g. supercomputing) applications</p>
    <p>Expected utilization of GPUs across applications in some domains may follow patterns to allow sharing</p>
    <p>Amazon EC2 adopts GPUs</p>
    <p>Other cloud offerings by AMD, NVIDIA</p>
    <p>Tegras in cellphones</p>
    <p>HPC GPU Cluster (Keeneland )</p>
    <p>Need for accelerator sharing: resource sharing is now supported in NVIDIAs Fermi architecture Concern: Can driver scheduling do a good job?</p>
  </div>
  <div class="page">
    <p>NVIDIA GPU Sharing  Driver Default  Xeon Quadcore with</p>
    <p>Coulomb Potential [CP] benchmark from parboil benchmark suite</p>
    <p>Result of sharing two GPUs among four instances of the application</p>
    <p>Max</p>
    <p>Min</p>
  </div>
  <div class="page">
    <p>NVIDIA GPU Sharing  Driver Default  Xeon Quadcore with</p>
    <p>Coulomb Potential [CP] benchmark from parboil benchmark suite</p>
    <p>Result of sharing two GPUs among four instances of the application</p>
    <p>Max</p>
    <p>Min</p>
    <p>Driver can: efficiently implement computation and data interactions between host and accelerator Limitations: Call ordering suffers when sharing  any scheme used is static and cannot adapt to different system expectations</p>
  </div>
  <div class="page">
    <p>Re-thinking Accelerator-based Systems</p>
  </div>
  <div class="page">
    <p>Re-thinking Accelerator-based Systems</p>
    <p>Accelerators as first class citizens</p>
    <p>Why treat such powerful processing resources as devices?</p>
    <p>How can such heterogeneous resources be managed especially with evolving programming models, evolving hardware and proprietary software?</p>
  </div>
  <div class="page">
    <p>Re-thinking Accelerator-based Systems</p>
    <p>Accelerators as first class citizens</p>
    <p>Why treat such powerful processing resources as devices?</p>
    <p>How can such heterogeneous resources be managed especially with evolving programming models, evolving hardware and proprietary software?</p>
    <p>Sharing of accelerators</p>
    <p>Are there efficient methods to utilize a heterogeneous pool of resources?</p>
    <p>Can applications share accelerators without a big hit in efficiency?</p>
  </div>
  <div class="page">
    <p>Re-thinking Accelerator-based Systems</p>
    <p>Accelerators as first class citizens</p>
    <p>Why treat such powerful processing resources as devices?</p>
    <p>How can such heterogeneous resources be managed especially with evolving programming models, evolving hardware and proprietary software?</p>
    <p>Sharing of accelerators</p>
    <p>Are there efficient methods to utilize a heterogeneous pool of resources?</p>
    <p>Can applications share accelerators without a big hit in efficiency?</p>
    <p>Coordination across different processor types</p>
    <p>How do you deal with multiple scheduling domains?</p>
    <p>Does coordination obtain any performance gains?</p>
  </div>
  <div class="page">
    <p>Pegasus addresses the urgent need for systems support to smartly manage accelerators.</p>
  </div>
  <div class="page">
    <p>Pegasus addresses the urgent need for systems support to smartly manage accelerators.</p>
    <p>(Demonstrated through x86--NVIDIA GPU-based systems)</p>
  </div>
  <div class="page">
    <p>Pegasus addresses the urgent need for systems support to smartly manage accelerators.</p>
    <p>(Demonstrated through x86--NVIDIA GPU-based systems)</p>
    <p>It leverages new opportunities presented by increased adoption of virtualization technology in commercial, cloud</p>
    <p>computing, and even high performance infrastructures.</p>
  </div>
  <div class="page">
    <p>Pegasus addresses the urgent need for systems support to smartly manage accelerators.</p>
    <p>(Demonstrated through x86--NVIDIA GPU-based systems)</p>
    <p>It leverages new opportunities presented by increased adoption of virtualization technology in commercial, cloud</p>
    <p>computing, and even high performance infrastructures. (Virtualization provided by Xen hypervisor and Dom0</p>
    <p>management domain)</p>
  </div>
  <div class="page">
    <p>ACCELERATORS AS FIRST CLASS CITIZENS</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Compute Accelerators (NVIDIA GPUs) Compute Accelerators (NVIDIA GPUs) Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Compute Accelerators (NVIDIA GPUs) Compute Accelerators (NVIDIA GPUs) Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>Linux Runtime + GPU Driver</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Compute Accelerators (NVIDIA GPUs) Compute Accelerators (NVIDIA GPUs) Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>Linux</p>
    <p>NVIDIAs CUDA  Compute Unified Device Architecture for managing GPUs</p>
    <p>Runtime + GPU Driver</p>
    <p>CUDA API</p>
    <p>GPU Application</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>GPU Backend</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Compute Accelerators (NVIDIA GPUs) Compute Accelerators (NVIDIA GPUs) Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>GPU Frontend</p>
    <p>Linux</p>
    <p>NVIDIAs CUDA  Compute Unified Device Architecture for managing GPUs</p>
    <p>Runtime + GPU Driver</p>
    <p>CUDA API</p>
    <p>GPU Application</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>GPU Backend</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Compute Accelerators (NVIDIA GPUs) Compute Accelerators (NVIDIA GPUs) Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>GPU Frontend</p>
    <p>Linux</p>
    <p>NVIDIAs CUDA  Compute Unified Device Architecture for managing GPUs</p>
    <p>Runtime + GPU Driver</p>
    <p>CUDA API</p>
    <p>GPU Application</p>
    <p>VM</p>
    <p>GPU Frontend</p>
    <p>Linux</p>
    <p>CUDA API</p>
    <p>GPU Application</p>
  </div>
  <div class="page">
    <p>Manageability Extending Xen for Closed NVIDIA GPUs</p>
    <p>Management Domain (Dom0) Management Domain (Dom0)</p>
    <p>Hypervisor (Xen) Hypervisor (Xen)</p>
    <p>Mgmt Extension</p>
    <p>GPU Backend</p>
    <p>Traditional Device Drivers</p>
    <p>General purpose multicores General purpose multicores</p>
    <p>Compute Accelerators (NVIDIA GPUs) Compute Accelerators (NVIDIA GPUs) Traditional Devices Traditional Devices</p>
    <p>VM</p>
    <p>GPU Frontend</p>
    <p>Linux</p>
    <p>NVIDIAs CUDA  Compute Unified Device Architecture for managing GPUs</p>
    <p>Runtime + GPU Driver</p>
    <p>CUDA API</p>
    <p>GPU Application</p>
    <p>VM</p>
    <p>GPU Frontend</p>
    <p>Linux</p>
    <p>CUDA API</p>
    <p>GPU Application</p>
  </div>
  <div class="page">
    <p>Accelerator Virtual CPU (aVCPU) Abstraction</p>
    <p>Pegasus Frontend</p>
    <p>Frontend driver</p>
    <p>Interposer library</p>
    <p>CUDA calls + Responses</p>
    <p>Xen shared ring for CUDA calls (per VM)</p>
    <p>call buffer</p>
    <p>Shared pages for data</p>
    <p>Application data</p>
    <p>VM</p>
  </div>
  <div class="page">
    <p>Accelerator Virtual CPU (aVCPU) Abstraction</p>
    <p>Pegasus Frontend</p>
    <p>Frontend driver</p>
    <p>Interposer library</p>
    <p>CUDA calls + Responses</p>
    <p>Xen shared ring for CUDA calls (per VM)</p>
    <p>call buffer</p>
    <p>Shared pages for data</p>
    <p>Application data</p>
    <p>CUDA calls + Responses</p>
    <p>Polling thread</p>
    <p>Application data</p>
    <p>Pegasus Backend</p>
    <p>VM Dom0</p>
  </div>
  <div class="page">
    <p>Accelerator Virtual CPU (aVCPU) Abstraction</p>
    <p>Pegasus Frontend</p>
    <p>Frontend driver</p>
    <p>Interposer library</p>
    <p>CUDA calls + Responses</p>
    <p>Xen shared ring for CUDA calls (per VM)</p>
    <p>call buffer</p>
    <p>Shared pages for data</p>
    <p>Application data</p>
    <p>CUDA calls + Responses</p>
    <p>Polling thread</p>
    <p>Application data</p>
    <p>CUDA Runtime + Driver</p>
    <p>Pegasus Backend</p>
    <p>VM Dom0</p>
  </div>
  <div class="page">
    <p>Accelerator Virtual CPU (aVCPU) Abstraction</p>
    <p>Pegasus Frontend</p>
    <p>Frontend driver</p>
    <p>Interposer library</p>
    <p>CUDA calls + Responses</p>
    <p>Xen shared ring for CUDA calls (per VM)</p>
    <p>call buffer</p>
    <p>Shared pages for data</p>
    <p>Application data</p>
    <p>CUDA calls + Responses</p>
    <p>Polling thread</p>
    <p>Application data</p>
    <p>CUDA Runtime + Driver</p>
    <p>Polling thread is the VMs representative for call execution It can be queued or scheduled to pick calls and issue them for any amount of time  the accelerator portion of the VM can be scheduled Hence, we define an accelerator virtual CPU or aVCPU</p>
    <p>Pegasus Backend</p>
    <p>VM Dom0</p>
  </div>
  <div class="page">
    <p>First Class Citizens</p>
    <p>The aVCPU has execution context on both, CPU (polling thread, runtime, driver context) and GPU (CUDA kernel)</p>
    <p>It has data used by these calls</p>
    <p>aVCPU</p>
    <p>Runtime and driver context</p>
    <p>CUDA calls + data</p>
    <p>Polling Thread</p>
    <p>VCPU</p>
    <p>Data</p>
    <p>Execution context</p>
  </div>
  <div class="page">
    <p>First Class Citizens</p>
    <p>The aVCPU has execution context on both, CPU (polling thread, runtime, driver context) and GPU (CUDA kernel)</p>
    <p>It has data used by these calls</p>
    <p>aVCPU</p>
    <p>Runtime and driver context</p>
    <p>CUDA calls + data</p>
    <p>Polling Thread</p>
    <p>VCPU</p>
    <p>Data</p>
    <p>Execution context</p>
    <p>VCPU: first class schedulable entity on a physical CPU aVCPU: first class schedulable entity on GPU (with a CPU component due to execution model) Manageable pool of heterogeneous resources</p>
  </div>
  <div class="page">
    <p>SHARING OF ACCELERATORS</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>aVCPUs are given equal time slices and scheduled in a circular fashion</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>XC: Proportional fair share</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Adopt Xen credit scheduling for aVCPU scheduling. E.g. VMs 1, 2 and 3 have 256, 512, 1024 credits, they get 1, 2, 4 time ticks respectively, every scheduling cycle</p>
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>XC: Proportional fair share</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>XC: Proportional fair share</p>
    <p>AccC: Prop. fair share</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>XC: Proportional fair share</p>
    <p>AccC: Prop. fair share</p>
    <p>Instead of using the assigned VCPU credits for scheduling aVCPUs, define new accelerator credits. These could be some fraction of CPU credits</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>XC: Proportional fair share</p>
    <p>AccC: Prop. fair share</p>
    <p>SLAF: Feedbackbased prop. fair share</p>
    <p>Too fine</p>
    <p>Too coarse</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Scheduling aVCPUs</p>
    <p>Per call granularity</p>
    <p>Per application granularity</p>
    <p>RR: Fair share</p>
    <p>XC: Proportional fair share</p>
    <p>AccC: Prop. fair share</p>
    <p>SLAF: Feedbackbased prop. fair share</p>
    <p>Too fine</p>
    <p>Too coarse Periodic scanning can lead to adjustment in the timer ticks assigned to aVCPUs if they do not get or exceed their assigned/expected time quota</p>
    <p>Time slot based methods</p>
  </div>
  <div class="page">
    <p>Performance Improves but Still High Variation</p>
    <p>BlackScholes &lt;2mi,128&gt;  Xen 3.2.1 with 2.6.18</p>
    <p>linux kernel in all domains</p>
    <p>NVIDIA driver 169.09 + SDK 1.1</p>
    <p>Dom1, Dom4 = 256, Dom2 = 512, Dom3 = 1024 credits</p>
    <p>Max</p>
    <p>Min</p>
  </div>
  <div class="page">
    <p>Performance Improves but Still High Variation</p>
    <p>BlackScholes &lt;2mi,128&gt;  Xen 3.2.1 with 2.6.18</p>
    <p>linux kernel in all domains</p>
    <p>NVIDIA driver 169.09 + SDK 1.1</p>
    <p>Dom1, Dom4 = 256, Dom2 = 512, Dom3 = 1024 credits</p>
    <p>Still high variation: due to the hidden driver and runtime Coordination: Can we do better?</p>
    <p>Max</p>
    <p>Min</p>
  </div>
  <div class="page">
    <p>COORDINATION ACROSS SCHEDULING DOMAINS</p>
  </div>
  <div class="page">
    <p>Coordinating CPU-GPU Scheduling</p>
    <p>Hypervisor co-schedule [CoSched]</p>
    <p>Hypervisor scheduling determines which domain should run on a GPU depending on the CPU schedule</p>
    <p>Latency reduction by occasional unfairness</p>
    <p>Possible waste of resources e.g. if domain picked for GPU has no work to do</p>
  </div>
  <div class="page">
    <p>Coordinating CPU-GPU Scheduling</p>
    <p>Hypervisor co-schedule [CoSched]</p>
    <p>Hypervisor scheduling determines which domain should run on a GPU depending on the CPU schedule</p>
    <p>Latency reduction by occasional unfairness</p>
    <p>Possible waste of resources e.g. if domain picked for GPU has no work to do</p>
    <p>Augmented credit [AugC]</p>
    <p>Scan the hypervisor CPU schedule to temporarily boost credits of domains selected for CPUs</p>
    <p>Pick domain(s) for GPU(s) based on GPU credits + remaining CPU credits from hypervisor (augmenting)</p>
    <p>Throughput improvement by temporary credit boost</p>
  </div>
  <div class="page">
    <p>Coordination Further Improves Performance</p>
    <p>BlackScholes &lt;2mi,128&gt;  Xen 3.2.1 with 2.6.18</p>
    <p>linux kernel in all domains</p>
    <p>NVIDIA driver 169.09 + SDK 1.1</p>
    <p>Dom1, Dom4 = 256, Dom2 = 512, Dom3 = 1024 credits</p>
  </div>
  <div class="page">
    <p>Coordination Further Improves Performance</p>
    <p>Coordination: Aligning the CPU and GPU portions of an application to run almost simultaneously, reduces variation and improves performance</p>
    <p>BlackScholes &lt;2mi,128&gt;  Xen 3.2.1 with 2.6.18</p>
    <p>linux kernel in all domains</p>
    <p>NVIDIA driver 169.09 + SDK 1.1</p>
    <p>Dom1, Dom4 = 256, Dom2 = 512, Dom3 = 1024 credits</p>
  </div>
  <div class="page">
    <p>Pegasus Scheduling Policies</p>
    <p>No coordination:</p>
    <p>Default  GPU driver based  base case (None)</p>
    <p>Round Robin (RR)</p>
    <p>AccCredit (AccC)  credits based on static profiling</p>
    <p>Coordination based:</p>
    <p>XenCredit (XC)  use Xen CPU credits</p>
    <p>SLA feedback based (SLAF)</p>
    <p>Augmented Credit based (AugC)  temporarily augment credits for co-scheduling</p>
    <p>Controlled</p>
    <p>HypeControlled or coscheduled (CoSched)</p>
  </div>
  <div class="page">
    <p>Pegasus Scheduling Policies</p>
    <p>No coordination:</p>
    <p>Default  GPU driver based  base case (None)</p>
    <p>Round Robin (RR)</p>
    <p>AccCredit (AccC)  credits based on static profiling</p>
    <p>Coordination based:</p>
    <p>XenCredit (XC)  use Xen CPU credits</p>
    <p>SLA feedback based (SLAF)</p>
    <p>Augmented Credit based (AugC)  temporarily augment credits for co-scheduling</p>
    <p>Controlled</p>
    <p>HypeControlled or coscheduled (CoSched)</p>
  </div>
  <div class="page">
    <p>Pegasus Scheduling Policies</p>
    <p>No coordination:</p>
    <p>Default  GPU driver based  base case (None)</p>
    <p>Round Robin (RR)</p>
    <p>AccCredit (AccC)  credits based on static profiling</p>
    <p>Coordination based:</p>
    <p>XenCredit (XC)  use Xen CPU credits</p>
    <p>SLA feedback based (SLAF)</p>
    <p>Augmented Credit based (AugC)  temporarily augment credits for co-scheduling</p>
    <p>Controlled</p>
    <p>HypeControlled or coscheduled (CoSched)</p>
  </div>
  <div class="page">
    <p>Application Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>Management Domain</p>
    <p>OS OS</p>
    <p>Guest VM</p>
    <p>Hypervisor</p>
  </div>
  <div class="page">
    <p>Application Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>Management Domain</p>
    <p>OS OS</p>
    <p>Guest VM</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>Management Domain</p>
    <p>OS OS</p>
    <p>Guest VM</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>Doms (Credits) Accelerator</p>
    <p>Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>DomA Scheduler DomA Scheduler</p>
    <p>Accelerator Selection Module</p>
    <p>Accelerator Selection Module</p>
    <p>Management Domain</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>Doms (Credits) Accelerator</p>
    <p>Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>DomA Scheduler DomA Scheduler</p>
    <p>Accelerator Selection Module</p>
    <p>Accelerator Selection Module</p>
    <p>Management Domain</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>Host Part</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Picked</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>aVCPU</p>
    <p>Doms (Credits) Accelerator</p>
    <p>Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>DomA Scheduler DomA Scheduler</p>
    <p>Accelerator Selection Module</p>
    <p>Accelerator Selection Module</p>
    <p>Management Domain</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>Accelerator Part Host Part</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Picked</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>aVCPU</p>
    <p>Monitoring/Feedback Monitoring/Feedback</p>
    <p>Doms (Credits) Accelerator</p>
    <p>Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>DomA Scheduler DomA Scheduler</p>
    <p>Accelerator Selection Module</p>
    <p>Accelerator Selection Module</p>
    <p>Management Domain</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>Accelerator Part Host Part</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Picked</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>aVCPU</p>
    <p>Monitoring/Feedback Monitoring/Feedback</p>
    <p>Doms (Credits) Accelerator</p>
    <p>Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>DomA Scheduler DomA Scheduler</p>
    <p>Accelerator Selection Module</p>
    <p>Accelerator Selection Module</p>
    <p>Management Domain</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>Accelerator Part Host Part</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>Domains (Credits)</p>
    <p>Schedule</p>
    <p>Acc. Data</p>
  </div>
  <div class="page">
    <p>Application Application Accelerator Application Accelerator Application</p>
    <p>Logical View of the</p>
    <p>Pegasus Resource</p>
    <p>Management Framework</p>
    <p>Picked</p>
    <p>Acc1 (Compute)</p>
    <p>Acc2 (Compute)</p>
    <p>C1 C3 C2 C4</p>
    <p>Physical Platform</p>
    <p>aVCPU</p>
    <p>Monitoring/Feedback Monitoring/Feedback</p>
    <p>More aVCPUs</p>
    <p>Doms (Credits) Accelerator</p>
    <p>Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>DomA Scheduler DomA Scheduler</p>
    <p>Accelerator Selection Module</p>
    <p>Accelerator Selection Module</p>
    <p>Management Domain</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>Accelerator Part Host Part</p>
    <p>CPU Ready Queues</p>
    <p>Domains to Schedule</p>
    <p>Picked</p>
    <p>VCPU</p>
    <p>CPU Scheduler</p>
    <p>Hypervisor</p>
    <p>More VCPUs</p>
    <p>Domains (Credits)</p>
    <p>OS OS Acc. Frontend</p>
    <p>Guest VM</p>
    <p>Accelerator Application Accelerator Application Accelerator Part Host Part</p>
    <p>Schedule</p>
    <p>Acc. Data</p>
  </div>
  <div class="page">
    <p>Testbed Details</p>
    <p>Xeon 4 core @3GHz, 3GB RAM, 2 NVIDIA GPUs G92-450</p>
    <p>Xen 3.2.1  stable, Fedora 8 Dom0 and DomU running Linux kernel 2.6.18, NVIDIA driver 169.09, SDK 1.1</p>
    <p>Guest domains given 512M memory and 1 core mostly</p>
    <p>Pinned to different physical cores</p>
    <p>Launched almost simultaneously: worst case measurement due to maximum load</p>
    <p>Data currently sampled over 50runs for statistical significance despite driver/runtime variation</p>
    <p>Scheduling plots report h-spread with min-max over 85% readings or total work done over all runs in an experiment</p>
  </div>
  <div class="page">
    <p>Benchmarks</p>
    <p>Category Source Benchmarks</p>
    <p>Financial SDK Binomial (BOp), BlackScholes (BS), MonteCarlo (MC)</p>
    <p>Media processing</p>
    <p>SDK/parboil ProcessImage(PI)=matrix multiply+DXTC, MRIQ, FastWalshTransform(FWT)</p>
    <p>Scientific Parboil CP, TPACF, RPES</p>
  </div>
  <div class="page">
    <p>Benchmarks</p>
    <p>Category Source Benchmarks</p>
    <p>Financial SDK Binomial (BOp), BlackScholes (BS), MonteCarlo (MC)</p>
    <p>Media processing</p>
    <p>SDK/parboil ProcessImage(PI)=matrix multiply+DXTC, MRIQ, FastWalshTransform(FWT)</p>
    <p>Scientific Parboil CP, TPACF, RPES</p>
    <p>Diverse benchmarks: from different application domains show (a) different throughput and latency constraints, (b) varying data and CUDA kernel sizes and (c) different number of CUDA calls</p>
  </div>
  <div class="page">
    <p>Benchmarks</p>
    <p>Category Source Benchmarks</p>
    <p>Financial SDK Binomial (BOp), BlackScholes (BS), MonteCarlo (MC)</p>
    <p>Media processing</p>
    <p>SDK/parboil ProcessImage(PI)=matrix multiply+DXTC, MRIQ, FastWalshTransform(FWT)</p>
    <p>Scientific Parboil CP, TPACF, RPES</p>
    <p>Diverse benchmarks: from different application domains show (a) different throughput and latency constraints, (b) varying data and CUDA kernel sizes and (c) different number of CUDA calls</p>
    <p>BlackScholes worst in the set: Throughput + latency sensitive due to large number of CUDA calls (depending on iteration)</p>
  </div>
  <div class="page">
    <p>Benchmarks</p>
    <p>Category Source Benchmarks</p>
    <p>Financial SDK Binomial (BOp), BlackScholes (BS), MonteCarlo (MC)</p>
    <p>Media processing</p>
    <p>SDK/parboil ProcessImage(PI)=matrix multiply+DXTC, MRIQ, FastWalshTransform(FWT)</p>
    <p>Scientific Parboil CP, TPACF, RPES</p>
    <p>Diverse benchmarks: from different application domains show (a) different throughput and latency constraints, (b) varying data and CUDA kernel sizes and (c) different number of CUDA calls</p>
    <p>BlackScholes worst in the set: Throughput + latency sensitive due to large number of CUDA calls (depending on iteration)</p>
    <p>Latency sensitive FastWalshTransform: multiple computation kernel launches and large data transfer</p>
  </div>
  <div class="page">
    <p>Ability to Achieve Low Virtualization Overhead</p>
    <p>Speed improvement for most benchmarks</p>
    <p>Increased #</p>
    <p>of CUDA</p>
    <p>Calls</p>
    <p>Cuda Time: Time within application to execute CUDA calls Total Time: Total execution time of benchmark from command line</p>
  </div>
  <div class="page">
    <p>Appropriate Scheduling is Important</p>
    <p>Scheduler - RR</p>
  </div>
  <div class="page">
    <p>Appropriate Scheduling is Important</p>
    <p>Scheduler - RR</p>
  </div>
  <div class="page">
    <p>Appropriate Scheduling is Important</p>
    <p>Without resource management, calls can be variably delayed due to interference from other application(s)/domain(s), even in the absence of virtualization</p>
    <p>Scheduler - RR</p>
  </div>
  <div class="page">
    <p>Pegasus Scheduling Black Scholes  Latency and throughput sensitive</p>
    <p>Equal credits for all domains</p>
    <p>Work done =</p>
  </div>
  <div class="page">
    <p>Pegasus Scheduling FWT  Latency sensitive</p>
    <p>Dom1, Dom4</p>
  </div>
  <div class="page">
    <p>Insights</p>
    <p>Pegasus approach efficiently virtualizes GPUs</p>
  </div>
  <div class="page">
    <p>Insights</p>
    <p>Pegasus approach efficiently virtualizes GPUs</p>
    <p>Coordinated scheduling is effective</p>
    <p>Even basic accelerator request scheduling can improve sharing performance</p>
    <p>While co-scheduling is really useful [CoSched], other methods can come close [AugC], keep up utilization and give desirable properties</p>
  </div>
  <div class="page">
    <p>Insights</p>
    <p>Pegasus approach efficiently virtualizes GPUs</p>
    <p>Coordinated scheduling is effective</p>
    <p>Even basic accelerator request scheduling can improve sharing performance</p>
    <p>While co-scheduling is really useful [CoSched], other methods can come close [AugC], keep up utilization and give desirable properties</p>
    <p>Scheduling lowers degree of variability caused by uncoordinated use of the NVIDIA driver.</p>
  </div>
  <div class="page">
    <p>Insights</p>
    <p>Pegasus approach efficiently virtualizes GPUs</p>
    <p>Coordinated scheduling is effective</p>
    <p>Even basic accelerator request scheduling can improve sharing performance</p>
    <p>While co-scheduling is really useful [CoSched], other methods can come close [AugC], keep up utilization and give desirable properties</p>
    <p>Scheduling lowers degree of variability caused by uncoordinated use of the NVIDIA driver.</p>
    <p>No single `best' scheduling policy Clear need for diverse policies geared to match different system goals and to account for different application characteristics</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We successfully virtualize GPUs to convert them into first class citizens</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We successfully virtualize GPUs to convert them into first class citizens</p>
    <p>Pegasus approach abstracts accelerator interfaces through CUDA-level virtualization</p>
    <p>Devise scheduling methods that coordinate accelerator use with that of general purpose host cores</p>
    <p>Performance evaluated on x86-GPU Xen-based prototype</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We successfully virtualize GPUs to convert them into first class citizens</p>
    <p>Pegasus approach abstracts accelerator interfaces through CUDA-level virtualization</p>
    <p>Devise scheduling methods that coordinate accelerator use with that of general purpose host cores</p>
    <p>Performance evaluated on x86-GPU Xen-based prototype</p>
    <p>Evaluation with a variety of benchmarks shows</p>
    <p>Need for coordination when sharing accelerator resources, especially for applications with high CPU-GPU coupling</p>
    <p>Need for diverse policies when coordinating resource management decisions made for general purpose vs. accelerator core</p>
  </div>
  <div class="page">
    <p>Future Work: Generalizing Pegasus</p>
    <p>Applicability: concepts applicable to open as well as close accelerators due lack of integration with runtimes</p>
    <p>Past experience with IBM Cell accelerator [Cellule]</p>
    <p>Open architecture allows finer grained control of resources</p>
  </div>
  <div class="page">
    <p>Future Work: Generalizing Pegasus</p>
    <p>Toolchains: sophistication through integration</p>
    <p>Instrumentation support from Ocelot [GTOcelot]</p>
    <p>Improve admission control, load balancing and scheduling</p>
  </div>
  <div class="page">
    <p>Future Work: Generalizing Pegasus</p>
    <p>Heterogeneous platforms: Scheduling different personalities for a virtual machine [Poster session]</p>
    <p>More generic problem where even processing resources on the same chip can be asymmetric</p>
  </div>
  <div class="page">
    <p>Future Work: Generalizing Pegasus</p>
    <p>Scale: Extensions to cluster-based systems with Shadowfax [VTDC`11]</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Heterogeneous and larger-scale systems  [Helios], [MultiKernel]</p>
    <p>Scheduling extension  [Cypress], [Xen Credit Scheduling], [QoS Adaptive Communication], [Intel Shared ISA Heterogeneity], [Cellular Disco]</p>
    <p>GPU Virtualization: [OpenGL], [VMWare DirectX], [VMGL], [vCUDA], [gVirtuS]</p>
    <p>Other related work</p>
    <p>Accelerator Frontend or multi-core programming models: [CUDA], [Georgia Tech Harmony], [Georgia Tech Cellule], [OpenCL]</p>
    <p>Some examples: [Intel Tolapai], [AMD Fusion], [LANL Roadrunner]</p>
    <p>Application domains: [NSF Keeneland], [Amazon Cloud]</p>
    <p>Interaction with higher levels: [PerformancePointsOSR]</p>
    <p>Cluster level: [rCUDA], [Shadowfax]</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
