<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>To Zip or not to Zip  Effective Resource Usage for Real-Time Compression Danny Harnik, Oded Margalit, Ronen Kat, Dmitry Sotnikov, Avishay Traeger IBM Research - Haifa</p>
  </div>
  <div class="page">
    <p>Our scope  Real-Time Compression</p>
    <p>Compression for primary data in enterprise storage systems  Many benefits to compression. Reduces:</p>
    <p>Costs  Rack space  Cooling  Can delay need for additional purchases to existing systems</p>
    <p>The challenge: Add seamless compression to a storage system with little effect on performance</p>
    <p>A bit about compression techniques:  We focused on Zlib  a popular compression engine (zip). Combines :</p>
    <p>Lempel Ziv [LZ77] compression  pointers instead of repetitions  Huffman Encoding  use shorter encoding to popular characters</p>
  </div>
  <div class="page">
    <p>Estimating compression ratios  some motivation</p>
    <p>To Zip or not to Zip? Compression does not come for free Incurs overheads, sometimes significant Not always worth the effort  depends on the actual data Goal: Avoid compressing incompressible data</p>
    <p>Other potential benefits:  Evaluation and sizing</p>
    <p>Compression ratio  number of disks  money! Evaluation: should I invest in a storage system with compression Sizing: How many disks should I buy</p>
    <p>Especially In a storage system with a high disk to</p>
    <p>server ratio</p>
  </div>
  <div class="page">
    <p>Existing solutions</p>
    <p>Rules of thumb  Deduce from past experience with similar applications  By file extension</p>
    <p>Not always accurate  Not always available</p>
    <p>Look at the actual data</p>
    <p>Scan and compress everything  Takes too long</p>
    <p>Look at a prefix (of a file/chunk) and deduce about the rest  No guarantees on the outcome</p>
    <p>No established method for estimating compression ratio other than compressing  Raskhodnikova et al. [RRRS 2007]  Cannot accurately estimate LZ77</p>
    <p>compression without reading essentially all of the data!</p>
    <p>.jpg .doc</p>
    <p>.ppt .vmdk</p>
    <p>.zip</p>
  </div>
  <div class="page">
    <p>Macro scale</p>
    <p>Large volumes  GigaBytes, TeraBytes  Time to compress is large (hours)  Estimation can run a short time</p>
    <p>(minutes)  Can obtain accuracy guarantees</p>
    <p>Good for:  Avoiding compression (and associated</p>
    <p>overheads) for incompressible volumes.</p>
    <p>Evaluation and sizing</p>
    <p>Micro scale</p>
    <p>Single write  KiloBytes  Time to compress is small</p>
    <p>(miliseconds)  Estimation has to be ultra quick  Heuristic</p>
    <p>Good for:  On the fly decisions  No prior information about the data  Data with varying compression ratio</p>
    <p>Our Work Two granularities</p>
  </div>
  <div class="page">
    <p>Input: Large volume of data  Block volume, file system, etc..</p>
    <p>Goal: Estimate the overall compression ratio with accuracy guarantee.</p>
    <p>Part I The Macro</p>
    <p>The framework:  Choose m random locations  Compute an average of the</p>
    <p>compression ratio of these locations</p>
    <p>What is location?  What is compression ratio of a</p>
    <p>location?  How do we get a guarantee?</p>
    <p>Input</p>
  </div>
  <div class="page">
    <p>The Macro-scale  what to sample?</p>
    <p>Solution is straightforward if compression is done independently on fixed size chuncks</p>
    <p>Uniformly sample input chuncks</p>
    <p>But in general this is not the case  For example, Zlib can output one zip file for</p>
    <p>very large files/volumes.</p>
    <p>What about the impossibility result for LZ77?  [RRRS07]</p>
    <p>In theory, theory and practice are the same. In practice, they are not</p>
    <p>Albert Einstein</p>
    <p>Input Output</p>
    <p>Input</p>
  </div>
  <div class="page">
    <p>The Macro-scale  what to sample?</p>
    <p>Real life implementations of compression algorithms are subject to locality limits</p>
    <p>Dont want to hold long back pointers  Memory management, need to flush their</p>
    <p>buffers</p>
    <p>We sample single bytes  Define the contribution of a byte as the</p>
    <p>compression ratio of its locality</p>
    <p>Analysis sketch:  Prove that the overall ratio is an average</p>
    <p>of the contributions of all bytes in the input  Use known statistical analysis for</p>
    <p>estimating averages via sampling.</p>
    <p>Input Output</p>
  </div>
  <div class="page">
    <p>The Macro-scale  Sample size and accuracy</p>
    <p>Analysis yields:</p>
    <p>Confidence  2e-2mAccuracy</p>
    <p>Accuracy is a bound on the additive error  Plug desired confidence and accuracy into</p>
    <p>equation to get the required sample size  Sample size independent of Volume size!</p>
    <p>Results of an estimator run are normally distributed around the actual compression ratio</p>
    <p>Width of Gaussian dictated by sample size.</p>
  </div>
  <div class="page">
    <p>The Macro-scale  the Actual Tool</p>
    <p>Written in C  Multi-threaded</p>
    <p>Two implementations: 1. IBM Real-Time compression 2. Zlib compression on full objects</p>
    <p>Tested on real life data  Example of a run: 73 seconds on a 3.2 TB volume  Error ~0.5%</p>
    <p>Exhaustive run took almost 4 hours</p>
    <p>IBM Comprestimator  the macro-scale for IBM Real-time compression on Storewize V7000 and SAN Volume controller: Downloadable at:</p>
    <p>http://www-01.ibm.com/support/docview.wss?uid=ssg1S4001012</p>
  </div>
  <div class="page">
    <p>Input: A single write  For example: 8KB, 16KB, 32KB, 128KB</p>
    <p>Goal: Quickly recommend to zip or not to zip.  Has to be much faster than actual compression!</p>
    <p>Dont want to read the entire chunk  Impossible to get guarantees  locality is the entire chunk</p>
    <p>Part II The Micro</p>
    <p>Option 1: Prefix estimation  Start compressing the input chunk and stop early</p>
    <p>E.g., in a 8KB chunk stop after 1KB  Evaluate compression ratio thus far:</p>
    <p>Compressed well  Continue  Incompressible  abort and copy the uncompressed chunk</p>
    <p>Good for compressible data  zero overhead   Not so much for incompressible data</p>
    <p>Problematic for data that changes in the middle   E.g. A text document with an embedded photo</p>
  </div>
  <div class="page">
    <p>Option 2: The Heuristics Method</p>
    <p>Collect a set of basic indicators about the chunk</p>
    <p>Based on the indicators make one of 3 recommendations on the chunk: 1. Dont compress 2. Compress 3. Huffman - Skip the LZ compression</p>
    <p>and use only Huffman encoding</p>
    <p>Indicators  Core-set size  The character set that</p>
    <p>makes up most of the data  Byte-Entropy  Symbol-pairs distribution indicator</p>
  </div>
  <div class="page">
    <p>Heuristics Method: Speeding it up</p>
    <p>Employed a number of techniques to improve performance:</p>
    <p>Sampling  Indicators are computed on a sample of the data</p>
    <p>Adaptive sampling  Add more samples until the desired statistical significance is reached.</p>
    <p>Lazy evaluation  Calculate the heuristics sequentially, in progressing order of difficulty (light to heavy)</p>
    <p>Attempt to reach a decision as early as possible</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Time vs. compression trade-off:  For example, on real life data with 32KB writes, the methods exhibit:</p>
    <p>Prefix: 74% CPU utilization at the price 2.2% capacity increase  Heuristics: 65% CPU utilization at the price 2.3% capacity increase</p>
    <p>Running time: of heuristics, prefix and full compression</p>
    <p>Measured on a benchmark collection of real life data  Tested on mixed data types  Over 300 GB  17790 files</p>
  </div>
  <div class="page">
    <p>Putting it all together</p>
    <p>Use an a combination of the different methods:  Whenever applicable  use the macro-scale  After this, the operation mode depends on the observed data: compressible: incompressible:</p>
    <p>When most (or all) is compressible  use prefix estimation</p>
    <p>When significant percent is incompressible  use heuristics method</p>
    <p>When most is incompressible  Turn compression off, run macro-scale off-line to detect if a change in tendencies occured</p>
  </div>
  <div class="page">
    <p>Thank You !</p>
  </div>
</Presentation>
