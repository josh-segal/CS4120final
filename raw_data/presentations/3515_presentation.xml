<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>With Great Training Comes Great Vulnerability: Practical Attacks against Transfer Learning</p>
    <p>Bolun Wang*, Yuanshun Yao, Bimal Viswanath</p>
    <p>Haitao Zheng, Ben Y. Zhao</p>
    <p>University of Chicago, * UC Santa Barbara,  Virginia Tech</p>
    <p>bolunwang@cs.ucsb.edu</p>
  </div>
  <div class="page">
    <p>Deep Learning is Data Hungry</p>
    <p>High-quality models are trained using large labeled datasets  Vision domain: ImageNet contains over 14 million labeled images</p>
    <p>Where do small companies get such large datasets?</p>
  </div>
  <div class="page">
    <p>A Prevailing Solution: Transfer Learning</p>
    <p>Company X Limited Training Data Highly-trained Model</p>
    <p>+</p>
    <p>High-quality Model Student A Student B Student CRecommended by Google, Microsoft, and Facebook DL frameworks</p>
    <p>Teacher</p>
    <p>Student Transfer and re-use pre-trained model</p>
  </div>
  <div class="page">
    <p>Deep Learning 101</p>
  </div>
  <div class="page">
    <p>Transfer Learning: Details</p>
    <p>Output</p>
    <p>Input</p>
    <p>! Layers !  1 Layers</p>
    <p>In general, first $ layers can be directly transferred</p>
    <p>($ = !  1)</p>
    <p>Insight: high-quality features can be re-used</p>
    <p>Teacher Teacher-specific classification layer</p>
    <p>Student OutputStudent-specific</p>
    <p>classification layer</p>
    <p>Input</p>
  </div>
  <div class="page">
    <p>Transfer Learning: Example</p>
    <p>Face recognition: recognize faces of 65 people</p>
    <p>Teacher (VGG-Face)</p>
    <p>Student Transfer 15 out of 16 layers</p>
    <p>Classification Accuracy Without Transfer Learning With Transfer Learning</p>
    <p>Company X</p>
  </div>
  <div class="page">
    <p>Is Transfer Learning Safe?</p>
    <p>Transfer Learning lacks diversity  Users have very limited choices of Teacher models</p>
    <p>Same Teacher</p>
    <p>Company A Attacker</p>
    <p>Help attacker exploit all Student models</p>
    <p>Company B</p>
  </div>
  <div class="page">
    <p>In This Talk</p>
    <p>Adversarial attack in the context of Transfer Learning</p>
    <p>Impact on real DL services</p>
    <p>Defense solutions</p>
  </div>
  <div class="page">
    <p>Background: Adversarial Attack</p>
    <p>Adversarial attack  Misclassify inputs by adding carefully engineered perturbation</p>
    <p>+ &quot; # Misclassified as</p>
    <p>Imperceptible perturbation</p>
  </div>
  <div class="page">
    <p>Attack Models of Prior Adversarial Attacks</p>
    <p>White-box attack: assumes full access to model internals  Find the optimal perturbation offline</p>
    <p>Black-box attack: assumes no access to model internals  Repeated query to reverse engineer the victim  Test intermediate result and improve</p>
    <p>Not practical</p>
    <p>Easily detected</p>
  </div>
  <div class="page">
    <p>Our Attack Model</p>
    <p>We propose a new adversarial attack targeting Transfer Learning</p>
    <p>Attack model</p>
    <p>StudentTeacher</p>
    <p>Black-box</p>
    <p>Model internals are hidden and kept secure</p>
    <p>White-box</p>
    <p>Model internals are known to the attacker</p>
    <p>Default access model today  Teachers are made public by popular DL services  Students are trained offline and kept secret</p>
  </div>
  <div class="page">
    <p>Attack Methodology: Neuron Mimicry</p>
    <p>+ Source Image</p>
    <p>Target Image</p>
    <p>Perturbation</p>
    <p>&quot;($)</p>
    <p>&quot;($)</p>
    <p>&amp;($)</p>
    <p>&amp;($)</p>
    <p>'( '(</p>
    <p>If two inputs match at layer ), then they produce the same result regardless of changes above layer ).</p>
    <p>&quot;('() &quot;('()</p>
    <p>&amp;(&quot; '( ) &amp;(&quot; '( )</p>
    <p>Same as Teacher</p>
  </div>
  <div class="page">
    <p>How to Compute Perturbation?</p>
    <p>Compute perturbation () by solving an optimization problem  Goal: mimic hidden-layer representation  Constraint: perturbation should be indistinguishable by humans</p>
    <p>&quot;#: source image &quot;$: target image</p>
    <p>min ()*+,-./(12 &quot;# +  ,12(&quot;$)) *.+. 7/8+98:_&lt;,=-)+9&gt;/ &quot;# + ,&quot;# &lt; @ABCDE$</p>
    <p>Minimize L2 distance between internal representations</p>
    <p>Constrain perturbation</p>
    <p>DSSIM: an objective measure for image distortion</p>
  </div>
  <div class="page">
    <p>Attack Effectiveness</p>
    <p>Targeted attack: randomly select 1,000 source, target image pairs  Attack success rate: percentage of images successfully misclassified into the target</p>
    <p>Source Adversarial Target</p>
    <p>Face recognition 92.6% attack success rate</p>
    <p>Source Adversarial Target</p>
    <p>Iris recognition 95.9% attack success rate</p>
  </div>
  <div class="page">
    <p>Attack in the Wild</p>
    <p>Q1: given Student, how to determine Teacher?  Craft fingerprint input for each Teacher candidate  Query Student to identify Teacher among candidates</p>
    <p>Q2: would attack work on Students trained by real DL services?  Follow tutorials to build Student using following services</p>
    <p>Attack achieves &gt;88.0% success rate for all three services</p>
    <p>Student</p>
    <p>Teacher A</p>
    <p>Teacher B</p>
    <p>Which Teacher is used?</p>
    <p>Fingerprint input</p>
  </div>
  <div class="page">
    <p>In This Talk</p>
    <p>Adversarial attack in the context of Transfer Learning</p>
    <p>Impact on real DL services</p>
    <p>Defense solutions</p>
  </div>
  <div class="page">
    <p>Intuition: Make Student Unpredictable</p>
    <p>Modify Student to make internal representation deviate from Teacher  Modification should be unpredictable by the attacker  No countermeasure  Without impacting classification accuracy</p>
    <p>min $%&amp;''()*%&amp;+,(,./01,,3/14) '.*. 78 9  ;8(9) &lt; &gt; &gt;.? for 9  D./EFG</p>
    <p>Maintain classification accuracyTeacher</p>
    <p>Robust Student</p>
    <p>Transfer using an updated objective function</p>
    <p>Updated objective function</p>
    <p>Guarantee difference between Teacher and Student</p>
  </div>
  <div class="page">
    <p>Effectiveness of Defense</p>
    <p>Model Face Recognition Iris Recognition</p>
    <p>Before Patching Attack Success</p>
    <p>Rate 92.6% 100%</p>
    <p>After Patching</p>
    <p>Attack Success Rate</p>
    <p>Change of Classification</p>
    <p>Accuracy  2.86%  2.73%</p>
  </div>
  <div class="page">
    <p>One More Thing</p>
    <p>Findings disclosed to Google, Microsoft, and Facebook</p>
    <p>Whats not included in the talk  Impact of Transfer Learning approaches  Impact of attack configurations  Fingerprinting Teacher</p>
  </div>
  <div class="page">
    <p>Code, models, and datasets are available at https://github.com/bolunwang/translearn</p>
    <p>Thank you!</p>
  </div>
</Presentation>
