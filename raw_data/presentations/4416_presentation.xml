<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Scoring Lexical Entailment with a Supervised Directional Similarity Network</p>
    <p>Marek Rei, Daniela Gerz and Ivan Vuli</p>
  </div>
  <div class="page">
    <p>Lexical Relations</p>
    <p>Task: Graded lexical entailment To what degree is X a type of Y?</p>
    <p>girl  person 9.85 / 10 guest  person 7.22 / 10 person  guest 2.88 / 10</p>
    <p>Useful for query expansion, natural language inference, paraphrasing, machine translation, etc.</p>
  </div>
  <div class="page">
    <p>Lexical Relations</p>
    <p>Distributional vectors are not great for directional lexical relations</p>
    <p>carrot ~ vegetable new ~ old</p>
    <p>Retro-fitting (Faruqui et al., 2015) Counter-fitting (Mrki et al., 2016)</p>
    <p>BUT these mostly affect words that are in the training data</p>
  </div>
  <div class="page">
    <p>Main Idea</p>
    <p>Specialized network for directional lexical relations01</p>
    <p>Off-the-shelf pre-trained embeddings</p>
  </div>
  <div class="page">
    <p>Supervised Directional Similarity Network</p>
    <p>Fixed pre-trained word embeddings as input</p>
    <p>Predict a score indicating the strength of a specific lexical relation</p>
  </div>
  <div class="page">
    <p>SDSN: Gating</p>
    <p>Conditioning each word based on the other</p>
  </div>
  <div class="page">
    <p>SDSN: Mapping</p>
    <p>Mapping the representations to new spaces</p>
  </div>
  <div class="page">
    <p>SDSN: Sparse Features</p>
    <p>Features based on sparse distributional representations</p>
    <p>cosine  weighted cosine</p>
    <p>(Rei &amp; Briscoe, 2014)  ratio of shared contexts</p>
  </div>
  <div class="page">
    <p>SDSN: Scoring</p>
    <p>Mapping the representations to a score</p>
    <p>Optimize the network with labeled examples</p>
  </div>
  <div class="page">
    <p>HyperLex: Graded Lexical Entailment</p>
    <p>Sp ea</p>
    <p>rm an</p>
    <p>s</p>
  </div>
  <div class="page">
    <p>HypeNet: Hyponym Detection</p>
    <p>F 1</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Can train a neural network to find specific regularities in off-the-shelf word embeddings</p>
    <p>Traditional sparse embeddings still provide complementary information</p>
  </div>
  <div class="page">
    <p>Thank you! Any questions?</p>
  </div>
  <div class="page">
    <p>Examples Premise Hypothesis Gold Predicted</p>
    <p>captain officer 8.22 8.17</p>
    <p>celery food 9.3 9.43</p>
    <p>horn bull 1.12 0.94</p>
    <p>wing airplane 1.03 0.84</p>
    <p>prince royalty 9.85 4.71</p>
    <p>autumn season 9.77 3.69</p>
    <p>kid parent 0.52 8.00</p>
    <p>discipline punishment 7.7 3.2</p>
  </div>
</Presentation>
