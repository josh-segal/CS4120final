<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>In-situ MapReduce for Log Processing</p>
    <p>Dionysios Logothe9s, Kevin Webb, Kenneth Yocum UC San Diego</p>
    <p>Chris Trezzo Salesforce Inc.</p>
    <p>USENIX Annual Technical Conference June 2011</p>
  </div>
  <div class="page">
    <p>Log analy9cs</p>
    <p>Data centers with 1000s of servers  Genera9ng logs with valuable informa9on</p>
    <p>Data-intensive compu9ng: Store and analyze TBs of logs</p>
    <p>Examples:</p>
    <p>Click logs: ad-targe9ng, personaliza9on  Social media feeds: brand monitoring  Purchase logs: fraud detec9on  System logs: anomaly detec9on, debugging</p>
  </div>
  <div class="page">
    <p>Log analy9cs today  Store-first-query-later</p>
    <p>Migrate logs to dedicated clusters</p>
    <p>Problems:  Scale</p>
    <p>e.g. Facebook collects 100TB a day!  Data migra9on stresses network and disks</p>
    <p>Failures  e.g. server is unreachable  Delay analysis or process incomplete data</p>
    <p>Timeliness  e.g. long data migra9on 9mes  Hinders real-9me apps: ad-targe9ng, fraud detec9on</p>
    <p>Store first</p>
    <p>query later</p>
    <p>MapReduce Dedicated cluster</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>In-situ MapReduce (iMR)</p>
    <p>Idea:  Move analysis to the servers  MapReduce for con9nuous data  Ability to trade fidelity for latency</p>
    <p>Op9mized for:  Highly selec9ve workloads</p>
    <p>e.g. up to 80% data filtered or summarized!  Online analy9cs</p>
    <p>e.g. Ad re-targe9ng based on most recent clicks</p>
    <p>MapReduce</p>
    <p>Dedicated cluster</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>An iMR query</p>
    <p>The same:</p>
    <p>MapReduce API  map(r)  {k,v} : extract/filter data  reduce( {k, v[]} )  v : data aggrega9on  combine( {k, v[]} )  v : early, par9al aggrega9on</p>
    <p>The new:  Provides con9nuous results  Because logs are con9nuous</p>
  </div>
  <div class="page">
    <p>Con9nuous MapReduce</p>
    <p>iMR input is an infinite stream of logs</p>
    <p>Bound input with sliding windows:  Range of data  Update frequency  e.g. Process user clicks over the last 60</p>
    <p>and update analysis every 15</p>
    <p>Nodes output stream of results, one for each window</p>
    <p>Analysis con9nuously updated with new data</p>
    <p>Map</p>
    <p>Combine</p>
    <p>0 60 30 90</p>
    <p>Log entries</p>
    <p>Time</p>
    <p>Reduce</p>
  </div>
  <div class="page">
    <p>Processing windows in-network</p>
    <p>Aggrega9on trees for efficiency  Distribute processing load  Reduce network traffic</p>
    <p>Problem:  Overlapping data</p>
    <p>Processed mul9ple 9mes: wastes CPU  Sent to the root mul9ple 9mes: wastes network</p>
    <p>Reduce Query root</p>
    <p>Map</p>
    <p>Combine</p>
    <p>log entries</p>
    <p>Map</p>
    <p>Combine</p>
    <p>log entries</p>
    <p>Combine windows in-network</p>
    <p>Map</p>
    <p>Combine</p>
    <p>Overlapping data</p>
  </div>
  <div class="page">
    <p>Efficient processing with panes</p>
    <p>Eliminate redundant work</p>
    <p>Divide window into panes (sub-windows)  Each pane is processed and sent only once  Root combines panes to produce window</p>
    <p>Saves CPU &amp; network resources, faster analysis</p>
    <p>Map</p>
    <p>Combine</p>
    <p>0 60 30 90</p>
    <p>Time</p>
    <p>P1 P2 P3</p>
    <p>P1 P2 P3</p>
    <p>P4</p>
    <p>P4</p>
    <p>Reduce</p>
  </div>
  <div class="page">
    <p>Impact of data loss on analysis</p>
    <p>Servers may get overloaded or fail  Apps may have latency requirements  Data loss is unavoidable to ensure 9meliness</p>
    <p>Challenges:</p>
    <p>Characterize incomplete results  Allow users to trade fidelity for latency</p>
    <p>Map</p>
    <p>Combine</p>
    <p>P1 P2 P3 P4</p>
    <p>Reduce</p>
    <p>X</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Quan9fying data fidelity</p>
    <p>Data are naturally distributed across:  Space (server nodes)  Time (processing window)</p>
    <p>Panes describe temporal and spa9al nature of data</p>
    <p>C2 metric: annotates result windows with a scoreboard  Marks successfully received panes</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
    <p>Time</p>
    <p>Sp ac e</p>
  </div>
  <div class="page">
    <p>Trading fidelity for latency</p>
    <p>Use C2 spec to trade fidelity for latency</p>
    <p>Users may specify:</p>
    <p>Maximum latency requirement  e.g. process window within 60sec</p>
    <p>Minimum fidelity  e.g. at least 50% of the total data</p>
    <p>Different ways to meet minimum fidelity  Impact latency and accuracy of analysis</p>
    <p>We iden9fied 4 useful classes of C2 specifica9ons 11</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
    <p>Time</p>
    <p>Sp ac e</p>
    <p>Time</p>
    <p>Sp ac e</p>
  </div>
  <div class="page">
    <p>Minimizing result latency</p>
    <p>Minimum fidelity with earlier results  e.g. 50% of the data</p>
    <p>Gives freedom to decrease latency  Returns the earliest data available  e.g. data from the fastest servers</p>
    <p>Appropriate for uniformly distributed events  Accurately summarizes rela9ve event frequencies</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
  </div>
  <div class="page">
    <p>Sampling non-uniform events</p>
    <p>Minimum fidelity with random sampling  e.g. random 50% of the data</p>
    <p>Less freedom to decrease latency  Included data may not be the first available</p>
    <p>Appropriate even for non-uniform data  Reproduces rela9ve occurrence of events</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
  </div>
  <div class="page">
    <p>Correla9ng events across 9me and space  Leverage knowledge about data distribu9on</p>
    <p>Temporal completeness:</p>
    <p>Include all data from a node or no data at all  e.g. all data from 50% of the nodes</p>
    <p>Useful when events are local to a node  e.g. coun9ng events on a per node basis</p>
    <p>Spa9al completeness:</p>
    <p>Each pane contains data from all nodes  Useful for correla9ng events across servers</p>
    <p>e.g click sessioniza9on</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
    <p>N1 N2 N3 N4</p>
    <p>P1 P2 P3 P4</p>
  </div>
  <div class="page">
    <p>Prototype</p>
    <p>Builds upon Mortar distributed stream processor [Logothe9s et al., USENIX08]</p>
    <p>Sliding windows  In-network aggrega9on trees</p>
    <p>Extended to support:  MapReduce API  Paned-based processing  Fault tolerance mechanisms: operator restart, adap9ve data rou9ng</p>
  </div>
  <div class="page">
    <p>Processing data in-situ</p>
    <p>Analysis co-located with client-facing services  Limited CPU resources for log analysis</p>
    <p>Goal: use available resources intelligently</p>
    <p>Load shedding mechanism  Nodes monitor local processing rate  Shed panes that cannot be processed on 9me</p>
    <p>Increases result fidelity under 9me and resource constraints</p>
  </div>
  <div class="page">
    <p>Evalua9on</p>
    <p>System scalability</p>
    <p>Usefulness of C2 metric  Understanding incomplete results  Trading fidelity for latency  Applica9ons:</p>
    <p>Click-stream sessioniza9on  HDFS failure detec9on</p>
    <p>Processing data in-situ  Improving fidelity under load with load shedding  Minimize impact on services</p>
  </div>
  <div class="page">
    <p>Exploring fidelity-latency tradeoffs</p>
    <p>Hadoop DFS anomaly detec9on algorithm [Tan et al. WASL08]</p>
    <p>Query: compute distribu9on of service 9mes for every HDFS server, to detect outliers</p>
    <p>Data: HDFS log trace from 30-node cluster</p>
  </div>
  <div class="page">
    <p>Exploring fidelity-latency tradeoffs  Data loss affects accuracy of distribu9on  Report: probability observed distribu9on is</p>
    <p>incorrect</p>
    <p>Temporal completeness  Distribu9ons are 100% accurate  Computed on per server basis</p>
    <p>Spa9al completeness &amp; random sampling  Poor results if more than 20% data lost  Reduce latency by &gt;25%</p>
    <p>C2 allows to trade fidelity for lower latency</p>
    <p>P ro b ab</p>
    <p>ili ty</p>
    <p>Data volume (%)</p>
    <p>Random Spa9al completeness Temporal completeness</p>
    <p>La te n cy ( se c)</p>
    <p>Data volume (%)</p>
    <p>Temporal completeness Random Spa9al completeness</p>
  </div>
  <div class="page">
    <p>In-situ performance  iMR side-by-side with a real service (Hadoop)</p>
    <p>on a 10-node cluster  iMR executes a word count query  Latency requirement set to 60sec.</p>
    <p>Vary CPU allocated to iMR (niceness)  Report:</p>
    <p>Result fidelity  Hadoop performance (job throughput)</p>
    <p>Shedding improves fidelity by 560% !  Hadoop performance drops by &lt;11%</p>
    <p>Liwle impact on Hadoop, while s9ll delivering useful results</p>
    <p>Fi d el it y (%</p>
    <p>)</p>
    <p>Niceness</p>
    <p>Shedding No shedding</p>
    <p>ve p er fo rm</p>
    <p>an ce ( % )</p>
    <p>Niceness</p>
    <p>&lt;11% overhead</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>In-situ architecture processes logs at the sources, avoids bulk data transfers, reduces analysis 9me</p>
    <p>Model allows incomplete data under failures or server load, provides 9mely analysis</p>
    <p>C2 metric helps understand incomplete data and trade fidelity for latency</p>
    <p>Pro-ac9vely sheds load, improves data fidelity under resource and 9me constraints</p>
  </div>
</Presentation>
