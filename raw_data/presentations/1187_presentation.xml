<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>IASO: A Fail-Slow Detection and Mitigation Framework for Distributed Storage Services</p>
    <p>Biswaranjan Panda (Nutanix) Deepthi Srinivasan (Nutanix) Huan Ke (University of Chicago) Karan Gupta (Nutanix) Vinayak Khot (Nutanix) Haryadi S. Gunawi (University of Chicago)</p>
  </div>
  <div class="page">
    <p>Stopped node</p>
    <p>Slow node</p>
    <p>Fail Stop Fail Slow</p>
  </div>
  <div class="page">
    <p>We present a study of 101 reports of fail-slow hardware incidents, collected from large-scale cluster deployments in 12 institutions.</p>
    <p>Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems. In Proceedings of the 16th USENIX Symposium on File and Storage Technologies (FAST), 2018.</p>
    <p>The dataset contains 232 validated cases collected from the deployment of 39,000 nodes throughout a period of 7 months We found that the fail-slow annual failure rate in our field is 1.02%</p>
    <p>IASO: A Fail-Slow Detection and Mitigation Framework for Distributed Storage Services</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>We benchmark five cloud systems (Hadoop, HDFS, ZooKeeper, Cassandra, and HBase) and find that limpware can severely impact distributed operations, nodes, and an entire cluster.</p>
    <p>Limplock: Understanding the Impact of Limpware on Scale-Out Cloud Systems. In Proceedings of the 4th ACM Symposium on Cloud Computing (SoCC), 2013.</p>
  </div>
  <div class="page">
    <p>IASO</p>
    <p>A peer based non intrusive</p>
    <p>fail-slow detection &amp; mitigation framework</p>
    <p>Deployment</p>
    <p>Scale</p>
    <p>Effectiveness</p>
  </div>
  <div class="page">
    <p>ATC 2019, 12:20 PM, Track 1 on July 10, 2019</p>
  </div>
</Presentation>
