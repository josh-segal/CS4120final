<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>NetChain: Scale-Free Sub-RTT Coordination</p>
    <p>Xin Jin Xiaozhou Li, Haoyu Zhang, Robert Soul, Jeongkeun Lee,</p>
    <p>Nate Foster, Changhoon Kim, Ion Stoica</p>
  </div>
  <div class="page">
    <p>Conventional wisdom: avoid coordination</p>
    <p>NetChain: lightning fast coordination enabled by programmable switches</p>
    <p>Open the door to rethink distributed systems design</p>
  </div>
  <div class="page">
    <p>Applications</p>
    <p>Coordination services: fundamental building block of the cloud</p>
    <p>Coordination Service Chubby</p>
  </div>
  <div class="page">
    <p>Configuration Management</p>
    <p>Distributed Locking</p>
    <p>Group Membership Barrier</p>
    <p>Applications</p>
    <p>Coordination Service</p>
    <p>Provide critical coordination functionalities</p>
  </div>
  <div class="page">
    <p>Configuration Management</p>
    <p>Distributed Locking</p>
    <p>Group Membership Barrier</p>
    <p>Applications</p>
    <p>Coordination Service</p>
    <p>Servers</p>
    <p>Strongly-Consistent, Fault-Tolerant Key-Value Store</p>
    <p>The core is a strongly-consistent, fault-tolerant key-value store</p>
    <p>This Talk</p>
  </div>
  <div class="page">
    <p>client coordination servers</p>
    <p>running a consensus protocol</p>
    <p>request</p>
    <p>reply</p>
    <p>Workflow of coordination services</p>
    <p>Can we do better?</p>
    <p>Throughput: at most server NIC throughput  Latency: at least one RTT, typically a few RTTs</p>
  </div>
  <div class="page">
    <p>client coordination servers</p>
    <p>running a consensus protocol</p>
    <p>request</p>
    <p>reply</p>
    <p>Opportunity: in-network coordination</p>
    <p>Server Switch Example [NetBricks, OSDI16] Barefoot Tofino Packets per second 30 million A few billion Bandwidth 10-100 Gbps 6.5 Tbps Processing delay 10-100 us &lt; 1 us</p>
    <p>Distributed coordination is communication-heavy, not computation-heavy.</p>
  </div>
  <div class="page">
    <p>client coordination switches</p>
    <p>running a consensus protocol</p>
    <p>request</p>
    <p>reply</p>
    <p>Opportunity: in-network coordination</p>
    <p>Throughput: switch throughput  Latency: half of an RTT</p>
  </div>
  <div class="page">
    <p>Design goals for coordination services</p>
    <p>High throughput</p>
    <p>Low latency</p>
    <p>Strong consistency</p>
    <p>Fault tolerance</p>
    <p>Directly from high-performance switches</p>
    <p>How?</p>
  </div>
  <div class="page">
    <p>Design goals for coordination services</p>
    <p>High throughput</p>
    <p>Low latency</p>
    <p>Strong consistency</p>
    <p>Fault tolerance</p>
    <p>Directly from high-performance switches</p>
    <p>Chain replication in the network</p>
  </div>
  <div class="page">
    <p>What is chain replication</p>
    <p>S0 S1 S2</p>
    <p>Head Replica Tail</p>
    <p>Read Request</p>
    <p>Read Reply</p>
    <p>Storage nodes are organized in a chain structure  Handle operations</p>
    <p>Read from the tail</p>
  </div>
  <div class="page">
    <p>What is chain replication</p>
    <p>Storage nodes are organized in a chain structure  Handle operations</p>
    <p>Read from the tail  Write from head to tail</p>
    <p>Provide strong consistency and fault tolerance  Tolerate f failures with f+1 nodes</p>
    <p>S0 S1 S2</p>
    <p>Head Replica Tail</p>
    <p>Write Request</p>
    <p>Read Request</p>
    <p>Read/Write Reply</p>
  </div>
  <div class="page">
    <p>Division of labor in chain replication: a perfect match to network architecture</p>
    <p>Optimize for high-performance to handle read &amp; write requests</p>
    <p>Provide strong consistency</p>
    <p>Storage Nodes</p>
    <p>Handle less frequent reconfiguration  Provide fault tolerance</p>
    <p>Auxiliary Master</p>
    <p>Handle packets at line rate</p>
    <p>Network Data Plane</p>
    <p>Handle network reconfiguration</p>
    <p>Network Control Plane</p>
    <p>Chain Replication</p>
    <p>Network Architecture</p>
  </div>
  <div class="page">
    <p>NetChain</p>
    <p>NetChain overview</p>
    <p>Host Racks</p>
    <p>S2 S3 S4 S5</p>
    <p>S0 S1 Network</p>
    <p>Controller</p>
    <p>Handle reconfigurations (e.g., switch failures)</p>
    <p>Handle read &amp; write requests at line rate</p>
  </div>
  <div class="page">
    <p>How to build a strongly-consistent, fault-tolerant, in-network key-value store</p>
    <p>How to store and serve key-value items?</p>
    <p>How to route queries according to chain structure?</p>
    <p>How to handle out-of-order delivery in network?</p>
    <p>How to handle switch failures?</p>
    <p>Data Plane</p>
    <p>Control Plane</p>
  </div>
  <div class="page">
    <p>PISA: Protocol Independent Switch Architecture</p>
    <p>Programmable Parser  Convert packet data into metadata</p>
    <p>Programmable Mach-Action Pipeline  Operate on metadata and update memory state</p>
    <p>Match + Action</p>
    <p>Programmable Parser Programmable Match-Action Pipeline</p>
    <p>Memory ALU</p>
  </div>
  <div class="page">
    <p>PISA: Protocol Independent Switch Architecture</p>
    <p>Programmable Parser  Parse custom key-value fields in the packet</p>
    <p>Programmable Mach-Action Pipeline  Read and update key-value data at line rate</p>
    <p>Match + Action</p>
    <p>Programmable Parser Programmable Match-Action Pipeline</p>
    <p>Memory ALU</p>
  </div>
  <div class="page">
    <p>Match + Action</p>
    <p>Programmable Parser Programmable Match-Action Pipeline</p>
    <p>Memory ALU</p>
    <p>Data plane (ASIC)</p>
    <p>Control plane (CPU)</p>
    <p>Network Functions</p>
    <p>Network Management</p>
    <p>Run-time API</p>
    <p>P C</p>
    <p>Ie</p>
    <p>NetChain Switch Agent</p>
    <p>Key-Value Store</p>
    <p>NetChain Controller</p>
  </div>
  <div class="page">
    <p>How to build a strongly-consistent, fault-tolerant, in-network key-value store</p>
    <p>How to store and serve key-value items?</p>
    <p>How to route queries according to chain structure?</p>
    <p>How to handle out-of-order delivery in network?</p>
    <p>How to handle switch failures?</p>
    <p>Data Plane</p>
    <p>Control Plane</p>
  </div>
  <div class="page">
    <p>NetChain packet format</p>
    <p>Application-layer protocol: compatible with existing L2-L4 layers</p>
    <p>Invoke NetChain with a reserved UDP port</p>
    <p>ETH IP UDP OP KEY VALUES0 SEQS1  Sk</p>
    <p>NetChain routingL2/L3 routing inserted by head switchread, write, delete, etc. reserved port #</p>
    <p>SC</p>
    <p>Existing Protocols NetChain Protocol</p>
  </div>
  <div class="page">
    <p>In-network key-value storage</p>
    <p>Key-value store in a single switch  Store and serve key-value items using register arrays [SOSP17, NetCache]</p>
    <p>Key-value store in the network  Data partitioning with consistent hashing and virtual nodes</p>
    <p>Match Action Key = X Read/Write RA[0] Key = Y Read/Write RA[5] Key = Z Read/Write RA[2] Default Drop()</p>
    <p>Register Array (RA)Match-Action Table</p>
  </div>
  <div class="page">
    <p>How to build a strongly-consistent, fault-tolerant, in-network key-value store</p>
    <p>How to store and serve key-value items?</p>
    <p>How to route queries according to chain structure?</p>
    <p>How to handle out-of-order delivery in network?</p>
    <p>How to handle switch failures?</p>
    <p>Data Plane</p>
    <p>Control Plane</p>
  </div>
  <div class="page">
    <p>NetChain routing: segment routing according to chain structure</p>
    <p>S0 S1 S2</p>
    <p>Head Replica Tail</p>
    <p>Write Request Write Reply H0</p>
    <p>Client  dstIP= S0</p>
    <p>SC= 2 S1 S2</p>
    <p>dstIP= S1  SC= 1 S2</p>
    <p>dstIP = S2</p>
    <p>SC= 0</p>
    <p>dstIP= H0  SC= 0</p>
  </div>
  <div class="page">
    <p>NetChain routing: segment routing according to chain structure</p>
    <p>S0 S1 S2</p>
    <p>Head Replica Tail</p>
    <p>Read Reply</p>
    <p>H0</p>
    <p>Client Read Request</p>
    <p>dstIP= S2  SC= 2 S1 S0</p>
    <p>dstIP= H0  SC= 2 S1 S0</p>
  </div>
  <div class="page">
    <p>How to build a strongly-consistent, fault-tolerant, in-network key-value store</p>
    <p>How to store and serve key-value items?</p>
    <p>How to route queries according to chain structure?</p>
    <p>How to handle out-of-order delivery in network?</p>
    <p>How to handle switch failures?</p>
    <p>Data Plane</p>
    <p>Control Plane</p>
  </div>
  <div class="page">
    <p>Problem of out-of-order delivery</p>
    <p>S0 S1 S2</p>
    <p>Head Replica Tail</p>
    <p>time</p>
    <p>foo=B foo=C foo=C</p>
    <p>foo=B foo=B foo=C</p>
    <p>foo=A foo=A foo=A</p>
    <p>W1: foo=B W2: foo=C</p>
    <p>Concurrent Writes</p>
    <p>Inconsistent values between three replicasSerialization with sequence number</p>
  </div>
  <div class="page">
    <p>How to build a strongly-consistent, fault-tolerant, in-network key-value store</p>
    <p>How to store and serve key-value items?</p>
    <p>How to route queries according to chain structure?</p>
    <p>How to handle out-of-order delivery in network?</p>
    <p>How to handle switch failures?</p>
    <p>Data Plane</p>
    <p>Control Plane</p>
  </div>
  <div class="page">
    <p>Handle a switch failure</p>
    <p>S0 S1 S2</p>
    <p>Fast Failover Failure Recovery</p>
    <p>S0 S3 S2S0 S2</p>
    <p>Failover to remaining f nodes  Tolerate f-1 failures  Efficiency: only need to update</p>
    <p>neighbor switches of failed switch</p>
    <p>Add another switch  Tolerate f+1 failures again  Consistency: two-phase atomic</p>
    <p>switching  Minimize disruption: virtual groups</p>
    <p>Before failure: tolerate f failures with f+1 nodes</p>
  </div>
  <div class="page">
    <p>Protocol correctness</p>
    <p>Invariant. For any key k that is assigned to a chain of nodes [S1, S2, , Sn], if 1   &lt;    (i.e., Si is a predecessor of Sj), then +,  .  +2  ..</p>
    <p>Guarantee strong consistency under packet loss, packet reordering, and switch failures</p>
    <p>See paper for TLA+ specification</p>
  </div>
  <div class="page">
    <p>Implementation  Testbed</p>
    <p>4 Barefoot Tofino switches and 4 commodity servers  Switch</p>
    <p>P4 program on 6.5 Tbps Barefoot Tofino  Routing: basic L2/L3 routing  Key-value store: up to 100K items, up to 128-byte values</p>
    <p>Server  16-core Intel Xeon E5-2630, 128 GB memory, 25/40 Gbps Intel NICs  Intel DPDK to generate query traffic: up to 20.5 MQPS per server</p>
  </div>
  <div class="page">
    <p>Evaluation  Can NetChain provide significant performance improvements?</p>
    <p>Can NetChain scale out to a large number of switches?</p>
    <p>Can NetChain efficiently handle failures?</p>
    <p>Can NetChain benefit applications?</p>
  </div>
  <div class="page">
    <p>Evaluation  Can NetChain provide significant performance improvements?</p>
    <p>Can NetChain scale out to a large number of switches?</p>
    <p>Can NetChain efficiently handle failures?</p>
    <p>Can NetChain benefit applications?</p>
  </div>
  <div class="page">
    <p>Orders of magnitude higher throughput</p>
    <p>T Kr</p>
    <p>ou gK</p>
    <p>Su t (</p>
    <p>)</p>
    <p>T Kr</p>
    <p>ou gK</p>
    <p>Su t (</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Orders of magnitude lower latency</p>
    <p>(a) Throughput vs. value size. (b) Throughput vs. store size. (c) Throughput vs. write ratio.</p>
    <p>(d) Throughput vs. loss rate.</p>
    <p>TKrougKSut (043S)</p>
    <p>/a te</p>
    <p>Qc y</p>
    <p>( s )</p>
    <p>ZooKeeSer (ZrLte) ZooKeeSer (read) 1etCKaLQ (read/ZrLte)</p>
    <p>(e) Latency vs. throughput. (f) Scalability (simulation).</p>
    <p>Figure 9: Performance results. (a-e) shows the experimental results of a three-switch NetChain prototype. Netchain(1), Netchain(2), Netchain(3) and Netchain(4) correspond to measuring the prototype performance with one, two, three and four servers respectively. NetChain(max) is the theoretical maximum throughput achievable by a three-switch chain; it is not a measured throughput. (f) shows the simulation results of spine-leaf networks of various sizes.</p>
    <p>four server machines. Each server machine is equipped with a 16-core CPU (Intel Xeon E5-2630) and 128 GB total memory (four Samsung 32GB DDR4-2133 memory). Three server machines are equipped with 40G NICs (Intel XL710) and the other one is equipped with a 25G NIC (Intel XXV710). The testbed is organized in a topology as shown in Figure 8.</p>
    <p>Comparison. We compare NetChain to Apache ZooKeeper-3.5.2 [33]. We implement a client to measure ZooKeepers performance with Apache Curator4.0.0 [34], which is a popular client library for ZooKeeper. The comparison is slightly unfair: NetChain does not provide all features of ZooKeeper (6), and ZooKeeper is a production-quality system that compromises its performance for many software-engineering objectives. But at a high level, the comparison uses ZooKeeper as a reference for server-based solutions to demonstrate the performance advantages of NetChain.</p>
    <p>packet is processed twice by a switch (e.g., a query from H0 follows path H0-S0-S1-S2-S1-S0-H0). Therefore, the maximum throughput of the chain is 2 BQPS in this setup. As the four servers cannot saturate the chain, we use NetChain(max) to denote the maximum throughput of the chain (shown as dotted lines in figures). For comparison, we run ZooKeeper on three servers, and a separate 100 client processes on the other server to generate queries. This experiment aims to thoroughly evaluate the throughput of one switch chain under various setups with real hardware switches. For large-scale deployments, a packet may traverse multiple hops to get from one chain switch to the next, and we evaluate the throughput with simulations in 8.3. Figure 9(a-d) shows the throughputs of the two systems. The default setting uses 64byte value size, 20K store size (i.e., the number of keyvalue items), 1% write ratio, and 0% link loss rate. We change one parameter in each experiment to show how the throughputs are affected by these parameters.</p>
    <p>Figure 9(a) shows the impact of value size. NetChain provides orders of magnitude higher throughput than ZooKeeper and both systems are not affected by the value size in the evaluated range. NetChain(4) keeps at 82 MQPS, meaning that NetChain can fully serve all the queries generated by the four servers. This is due to the nature of a switch ASIC: as long as the P4 program is compiled to fit the switch resource requirements, the switch is able to run NetChain at line rate. In fact,</p>
  </div>
  <div class="page">
    <p>Handle failures efficiently</p>
    <p>a three-switch chain is able to provide up to 2 BQPS, as denoted by NetChain(max). Our current prototype support value size up to 128 bytes. Larger values can be supported using more stages and using packet mirroring/recirculation as discussed in 6.</p>
    <p>Figure 9(b) shows the impact of store size. Similarly, both systems are not affected by the store size in the evaluated range, and NetChain provides orders of magnitude higher throughput. The store size is restricted by the allocated total size (8MB in our prototype) and the value size. The store size is large enough to be useful for coordination services as discussed in 6.</p>
    <p>Figure 9(c) shows the impact of write ratio. With readonly workloads, ZooKeeper achieves 230 KQPS. But even with a write ratio of 1%, its throughput drops to 140 KQPS. And when the write ratio is 100%, its throughput drops to 27 KQPS. As for comparison, NetChain(4) consistently achieves 82 MQPS. This is because NetChain uses chain replication and each switch is able to process both read and write queries at line rate. As the switches in the evaluated chain [S0,S1,S2] process the same number of packets for both read and write queries, the total throughput is not affected by the write ratio, which would be different in more complex topologies. As we will show in 8.3, NetChain has lower throughput for write queries for large deployments, as write queries require more hops than read queries.</p>
    <p>Figure 9(d) shows the impact of packet loss rate. We inject random packet loss rate to each switch, ranging from 0.001% to 10%. The throughput of ZooKeeper drops to 50 KQPS (3 KQPS) when the loss rate is 1% (10%). As for comparison, NetChain(4) keeps around 82 MQPS for packet loss rate between 0.001% and 1%, and only drops to 48 MPQS when the loss rate is 10%. The reason is because ZooKeeper uses TCP for reliable transmission which has a lot of overhead under high loss rate, whereas NetChain simply uses UDP and lets the clients retry a query upon packet loss. Although high packet loss rate is unlikely to happen frequently in datacenters, this experiment demonstrates that NetChain can provide high throughput even under extreme scenarios.</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>Su t</p>
    <p>(0 Q</p>
    <p>P S</p>
    <p>) failover failure recovery</p>
    <p>(a) 1 Virtual Group.</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>Su t</p>
    <p>(0 Q</p>
    <p>P S</p>
    <p>)</p>
    <p>failover failure recovery</p>
    <p>(b) 100 Virtual Groups.</p>
    <p>Figure 10: Failure handling results. It shows the throughput time series of one client server when one switch fails in a four-switch testbed. NetChain has fast failover. By using more virtual groups, NetChain provides smaller throughput drops for failure recovery.</p>
    <p>tency keeps at 9.7  s even when all four severs are generating queries to the system at 82 MQPS (the solid line of NetChain in the figure), and is expected to be not affected by throughput until the system is saturated at 2 BQPS (the dotted line of NetChain in the figure).</p>
    <p>As for comparison, ZooKeeper has a latency of 170  s for read queries and 2350  s for write queries at low throughput. The latencies slightly go up before the system is saturated (27 KQPS for writes and 230 KQPS for reads), because servers do not have deterministic perquery processing time as switch ASICs and the latency is affected by the system load. Overall, NetChain provides orders of magnitude lower latency than ZooKeeper at orders of magnitude higher throughput.</p>
    <p>reduce throughput drop with virtual groups</p>
  </div>
  <div class="page">
    <p>Conclusion  NetChain is an in-network coordination system that provides</p>
    <p>billions of operations per second with sub-RTT latencies</p>
    <p>Rethink distributed systems design  Conventional wisdom: avoid coordination  NetChain: lightning fast coordination with programmable switches</p>
    <p>Moores law is ending  Specialized processors for domain-specific workloads: GPU servers,</p>
    <p>FPGA servers, TPU servers  PISA servers: new generation of ultra-high performance systems for</p>
    <p>IO-heavy workloads enabled by PISA switches 35</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
</Presentation>
