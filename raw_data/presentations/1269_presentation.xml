<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Spool : Reliable Virtualized NVMe Storage Systems in Public Cloud Infrastructure</p>
    <p>Shuai Xue, Shang Zhao, Quan Chen, Gang Deng, Zheng Liu, Jie Zhang, Zhuo Song Tao Ma, Yong Yang, Yanbo Zhou, Keqiang Niu, Sijie Sun, Minyi Guo</p>
    <p>Dept. of Computer Science and Engineering, SJTU Alibaba Cloud</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Design of Spool</p>
    <p>Spool key ideas</p>
  </div>
  <div class="page">
    <p>Page . 3</p>
    <p>Introduction</p>
    <p>LOW PERFORMANCE</p>
    <p>HDD SATA NAND SSD NVMe NAND SSD NVMe V-NAND SSD</p>
    <p>AFFORDABLE PERFORMANCE</p>
    <p>HIGH PERFORMANCE</p>
    <p>EXPTRME PERFORMANCE</p>
    <p>With the development of storage hardware, software has become the performance bottleneck.</p>
  </div>
  <div class="page">
    <p>Page . 4</p>
    <p>The local NVMe SSD-based instance storage provided for: - Amazon EC2 I3 series - Azure Lsv2 series - Alibaba ECS I2 series</p>
    <p>The local NVMe SSD-based instance storage optimized for: - low latency - high throughput - high IOPS - low cost</p>
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>Page . 5</p>
    <p>Introduction</p>
    <p>VMM NVMe SSD1</p>
    <p>Host</p>
    <p>Hardware NVMe SSD1...</p>
    <p>Guest3Guest2Guest1 ...Guest</p>
    <p>High reliability is the most important and challenging problem: - restarting the virtualization system - removing the failed device - performing the upgrade</p>
  </div>
  <div class="page">
    <p>Page . 6</p>
    <p>Introduction</p>
    <p>Guset OS(VM) Virtio Frontend</p>
    <p>Virtio Frontend</p>
    <p>Hypervisor(VMM)</p>
    <p>Generic Block Layer</p>
    <p>NVMe Device</p>
    <p>Virtio</p>
    <p>Guset OS(VM) Guest Driver</p>
    <p>VFIO Driver</p>
    <p>NVMe Device</p>
    <p>PCI passthrough</p>
    <p>Guset OS(VM) Virtio Frontend</p>
    <p>Hypervisor(VMM)</p>
    <p>NVMe Device</p>
    <p>VFIO Driver</p>
    <p>Spool</p>
    <p>SPDK Driver</p>
    <p>Spool based on SPDK</p>
    <p>Spool is proposed based on the SPDK NVMe driver but focuses on the reliability of the virtualized storage system.</p>
  </div>
  <div class="page">
    <p>Page . 7</p>
    <p>Motivation</p>
    <p>Unnecessary Data Loss: reset device controller - For Azure, a device failure results in the entire machine being taken offline for repair. - For SPDK, the administrator directly replaces the failed device through hot-plug. - Only 6% of the hardware failures are due to real media errors.</p>
    <p>The current failure recovery method results in significant unnecessary data loss.</p>
  </div>
  <div class="page">
    <p>Page . 8</p>
    <p>Motivation</p>
    <p>Poor Availability - VM live migration is too costly. - The downtime for SPDK restart is up to 1,200 ms.</p>
    <p>The long downtime hurts the availability of the I/O virtualization system.</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Introduction and Motivation</p>
    <p>Design of Spool</p>
    <p>Spool key ideas</p>
  </div>
  <div class="page">
    <p>Page . 10</p>
    <p>Design of Amoeba Spool is comprised of - A cross-process journal: records each I/O request</p>
    <p>and its status to ensure data consistency. - A fast restart component:</p>
    <p>records the runtime data structures of the current Spool process reduce the downtime.</p>
    <p>- A failure recovery component: diagnoses the device failure type online to minimize unnecessary disk replacement.Bypass Kernel</p>
    <p>HARDWARENVMe</p>
    <p>IO worker</p>
    <p>NVMe</p>
    <p>KERNEL</p>
    <p>NVMe</p>
    <p>USERSPACE Control Data</p>
    <p>Restart Optimization</p>
    <p>SPDK User mode driver</p>
    <p>Guest/QEMU</p>
    <p>virtqueue</p>
    <p>blk dev</p>
    <p>Lvol Lvol Lvol St orage Pool</p>
    <p>Failure Recovery</p>
    <p>IO worker</p>
    <p>IO worker</p>
    <p>Spool</p>
    <p>blk dev</p>
    <p>block layer</p>
    <p>virtio-blk driver virtio-blk</p>
    <p>device</p>
    <p>Application</p>
    <p>UNIX domain Socket</p>
    <p>Journal</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Introduction and Motivation</p>
    <p>Design of Spool</p>
    <p>Spool key ideas</p>
  </div>
  <div class="page">
    <p>Page . 12</p>
    <p>Reliable Cross-Process Journal</p>
    <p>The I/O requests are processed in a producer-consumer model: 1. The guest driver places the head index of descriptor chain into the next ring entry of the</p>
    <p>available ring, and avail_idx of the available ring is increased. 2. The backend running in the host obtains several head indexes of the pending I/O</p>
    <p>requests in the available ring, increases last_idx of the available ring and submits the I/O requests to hardware driver.</p>
    <p>Available Ring</p>
    <p>last_idx avail_idx</p>
    <p>Available Ring</p>
    <p>last_idx avail_idx</p>
    <p>Used Ring</p>
    <p>used_idx</p>
    <p>IO1 IO2 IO3 IO4 IO2 IO3 IO4IO1</p>
    <p>Used Ring</p>
    <p>used_idx</p>
    <p>IO2</p>
    <p>N V</p>
    <p>M e D</p>
    <p>evice</p>
  </div>
  <div class="page">
    <p>Page . 13</p>
    <p>Reliable Cross-Process Journal</p>
    <p>Reliable problem: - The backend obtains two I/O requests, IO1 and IO2. - Then, the last_idx is incremented from IO1 to IO3 in the available ring. - If the storage virtualization system restarts at this moment, the last available index will be lost. Spool persists: - last_idx - the head index of each request - the states of each request:INFLIGHT, DONE,or NONE.</p>
    <p>Available Ring</p>
    <p>last_idx avail_idx</p>
    <p>Available Ring</p>
    <p>last_idx avail_idx</p>
    <p>Used Ring</p>
    <p>used_idx</p>
    <p>IO1 IO2 IO3 IO4 IO2 IO3 IO4IO1</p>
    <p>Used Ring</p>
    <p>used_idx</p>
    <p>IO2</p>
    <p>N V</p>
    <p>M e D</p>
    <p>evice</p>
  </div>
  <div class="page">
    <p>Page . 14</p>
    <p>Reliable Cross-Process Journal</p>
    <p>A multiple-instruction transaction model - In T0, we make a copy of the variable to be modified. - In T1, the transaction will be in the START state, and the variables are modified. - After all the variables modified completely, the transaction will be in the FINISHED state in T2.</p>
    <p>Valid Data</p>
    <p>State</p>
    <p>Write memory barrier</p>
    <p>Valid Data</p>
    <p>State</p>
    <p>last_avail_idx++ last_req_head=head</p>
    <p>req[head]=INFLIGHT</p>
    <p>T0: Init Phase T1: Instrs Execution T2: Valid Phase</p>
    <p>Invalid</p>
    <p>Valid Data</p>
    <p>State</p>
    <p>last_avail_idx++ last_req_head=head</p>
    <p>req[head]=INFLIGHT</p>
    <p>Memory Memory Memory</p>
    <p>Valid</p>
    <p>The challenge to ensure the consistency of the journal is to: - guarantee that instructions to increase last_idx and change the requests status are executed in an</p>
    <p>atomic manner.</p>
  </div>
  <div class="page">
    <p>Page . 15</p>
    <p>Reliable Cross-Process Journal</p>
    <p>An auxiliary data structure: - It is a valuable trick to efficiently maintain journal consistency to eliminate the overhead of</p>
    <p>making a copy in T0. - The state, last available index, and head index of the related request are padding to 64 bits</p>
    <p>and a union memory block with a 64-bit value. - The three records are updated within one instruction.</p>
    <p>union atomic_aux { struct {</p>
    <p>uint8_t pad0; uint8_t state; uint16_t last_avail_idx; uint16_t last_req_head; uint16_t pad1;</p>
    <p>}; uint64_t val; };</p>
    <p>val</p>
    <p>pad0 state</p>
    <p>last_avail_idx</p>
    <p>last_req_head</p>
    <p>pad1 8 bit</p>
  </div>
  <div class="page">
    <p>Page . 16</p>
    <p>Reliable Cross-Process Journal</p>
    <p>Every step Spool takes in algorithm 1 is likely to restart for upgrade.</p>
  </div>
  <div class="page">
    <p>Page . 17</p>
    <p>Reliable Cross-Process Journal</p>
    <p>The recovery algorithm: - The new Spool process before the restart only needs to check the state and decide</p>
    <p>whether to redo the transactions. - The states of IO request is repaired based on used index of vring and the last used</p>
    <p>index in the journal.</p>
  </div>
  <div class="page">
    <p>Page . 18</p>
    <p>Optimizing Spool Restart</p>
    <p>Start stage 1: Init EAL - Obtaining memory layout information: 70.9% of the total time. - The runtime configurations and memory layout information can be</p>
    <p>reused.</p>
    <p>Start stage 2: Probe device - Resetting the controller of NVMe devices: 90% of the total time. - The controller information can be reused.</p>
  </div>
  <div class="page">
    <p>Page . 19</p>
    <p>Optimizing Spool Restart</p>
    <p>Reusing Stable Configurations - Global runtime configurations. - Memory layout information.</p>
    <p>Skipping Controller - NVMe device controller-related information. - Gracefully terminate: SIGTERM and SIGINT signals.</p>
  </div>
  <div class="page">
    <p>Page . 20</p>
    <p>Hardware Fault Diagnosis and Processing</p>
    <p>Handling Hardware Failures - A device failures or hot-remove cause process to crash. - A SIGBUS handler is registered.</p>
    <p>Failure Model - Based on S.M.A.R.T. diagnosis - Hardware media error: hot-plug a new</p>
    <p>device. - Other hardware errors: reset the controller.</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Introduction and Motivation</p>
    <p>Design of Spool</p>
    <p>Spool key ideas</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Page . 22</p>
    <p>Experimental configuration</p>
  </div>
  <div class="page">
    <p>Page . 23</p>
    <p>Experimental configuration</p>
    <p>Performance: Bandwidth, IOPS,Average Latency</p>
  </div>
  <div class="page">
    <p>Page . 24</p>
    <p>Reliability of Handling Hardware Failure</p>
    <p>SSD2 is hot-removed and hot-pluged. The storage service for VM2 is back online automatically. The storage service for VM1 is not affected.</p>
  </div>
  <div class="page">
    <p>Page . 25</p>
    <p>Reliability of Handling Random Upgrades</p>
    <p>The file contents is verified with FIO on a Guest VM. Spool can guarantee data consistency during upgrades.</p>
  </div>
  <div class="page">
    <p>Page . 26</p>
    <p>Reducing Restart Time</p>
    <p>Spool reduces the total restart time from 1,218 ms to 115 ms.</p>
  </div>
  <div class="page">
    <p>Page . 27</p>
    <p>Case 1: Single VM Performance</p>
    <p>Spool achieves similar performance to SPDK.</p>
    <p>I/O Performance of Spool</p>
  </div>
  <div class="page">
    <p>Page . 28</p>
    <p>Case 2: Scaling to Multiple VMs</p>
    <p>Spool improves the IOPS of Randread by 13% compared with SPDK vhost-blk.</p>
    <p>Spool reduces the average data access latency of Randread by 54% compared with SPDK vhost-blk.</p>
    <p>I/O Performance of Spool</p>
  </div>
  <div class="page">
    <p>Page . 29</p>
    <p>Spool increases the average data access latency no more than 3%.</p>
    <p>And Spool reduces the IOPS by less than 0.76%</p>
    <p>Overhead of the Cross-Process Journal</p>
  </div>
  <div class="page">
    <p>Page . 30</p>
    <p>Deployment on an In-production Cloud</p>
    <p>The maximum IOPS of a single disk is 50% higher. The maximum IOPS of a largest specification instance</p>
    <p>is 51% higher.</p>
  </div>
  <div class="page">
    <p>Thanks for listening</p>
    <p>Question? xueshuai@sjtu.edu.cn or</p>
    <p>chen-quan@cs.sjtu.edu.cn</p>
  </div>
</Presentation>
