<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Active Random Fields</p>
    <p>Adrian Barbu</p>
  </div>
  <div class="page">
    <p>The MAP Estimation Problem  Estimation problem:</p>
    <p>Given input data y, solve</p>
    <p>Example: Image denoising  Given noisy image y, find denoised</p>
    <p>image x</p>
    <p>Issues  Modeling: How to approximate</p>
    <p>?  Computing: How to find x fast?</p>
    <p>Noisy image y</p>
    <p>Denoised image x</p>
  </div>
  <div class="page">
    <p>MAP Estimation Issues  Popular approach:</p>
    <p>Find a very accurate model  Find best optimum x of that model</p>
    <p>Problems with this approach  Hard to obtain good  Desired solution needs to be at global maximum  For many models , the global maximum</p>
    <p>cannot be obtained in any reasonable time.  Using suboptimal algorithms to find the</p>
    <p>maximum leads to suboptimal solutions</p>
    <p>E.g. Markov Random Fields</p>
  </div>
  <div class="page">
    <p>Markov Random Fields  Bayesian Models:</p>
    <p>Markov Random Field (MRF) prior</p>
    <p>E.g. Image Denoising model  Gaussian Likelihood</p>
    <p>Fields of Experts MRF prior</p>
    <p>Differential Lorentzian</p>
    <p>Image filters Ji</p>
    <p>Image Filters Ji</p>
    <p>Roth and Black, 2005</p>
  </div>
  <div class="page">
    <p>MAP Estimation (Inference) in MRF</p>
    <p>Exact inference is too hard  For the Potts model, one of the simplest MRFs</p>
    <p>it is already NP hard (Boykov et al, 2001)  Approximate inference is suboptimal</p>
    <p>Gradient descent  Iterated Conditional Modes (Besag 1986)  Belief Propagation (Yedidia et al, 2001)  Graph Cuts (Boykov et al, 2001)  Tree-Reweighted Message Passing (Wainwright</p>
    <p>et al, 2003)</p>
  </div>
  <div class="page">
    <p>Gradient Descent for Fields of Experts</p>
    <p>Energy function:</p>
    <p>Analytic gradient (Roth &amp; Black, 2005)</p>
    <p>Gradient descent iterations</p>
    <p>3000 iterations with small</p>
    <p>Takes more than 30 min per image on a modern computer</p>
    <p>FOE filters</p>
  </div>
  <div class="page">
    <p>Training the MRF  Gradient update in model parameters</p>
    <p>Minimize KL divergence between learned prior and true probability</p>
    <p>Gradient ascent in log-likelihood</p>
    <p>Need to know Normalization Constant Z  EX from training data</p>
    <p>Z and Ep obtained by MCMC</p>
    <p>Slow to train</p>
    <p>Training the FOE prior  Contrastive divergence (Hinton)</p>
    <p>An approximate ML technique  Initialize at data points and run a fixed number of iterations</p>
    <p>Takes about two days</p>
    <p>Model  m</p>
    <p>Trainining Phase Natural Images</p>
    <p>MCMC samples from current</p>
    <p>model</p>
    <p>Sufficient statistics</p>
    <p>Update  m</p>
  </div>
  <div class="page">
    <p>Going to Real-Time Performance</p>
    <p>Wainwright (2006)  In computation-limited settings, MAP estimation</p>
    <p>is not the best choice  Some biased models could compensate for the</p>
    <p>fast inference algorithm</p>
    <p>How much can we gain from biased models?  Proposed denoising approach:</p>
    <p>1-4 gradient descent iterations (not 3000)  Takes less than a second per image  1000-3000 times speedup vs MAP estimation  Better accuracy than FOE model</p>
  </div>
  <div class="page">
    <p>Active Random Field  Active Random Field = A pair (M,A) of</p>
    <p>a MRF model M, with parameters M  a fast and suboptimal inference algorithm A with</p>
    <p>parameters A  They cannot be separated since they are trained</p>
    <p>together  E.g. Active FOE for image denoising</p>
    <p>Fields of Experts model</p>
    <p>Algorithm: 1-4 iterations of gradient descent</p>
    <p>Parameters:</p>
  </div>
  <div class="page">
    <p>Training the Active Random Field</p>
    <p>Discriminative training  Training examples = pairs</p>
    <p>inputs yi+ desired outputs ti  Training=optimization</p>
    <p>Loss function L  Aka benchmark measure  Evaluates accuracy on training set  End-to-end training:</p>
    <p>covers entire process from input image to final result</p>
    <p>Model  m</p>
    <p>Algorithm  a</p>
    <p>Trainining PhaseInput images</p>
    <p>Current results</p>
    <p>Desired results</p>
    <p>Benchmark Measure</p>
    <p>Update  m and  a</p>
  </div>
  <div class="page">
    <p>Related Work  Energy Based Models (LeCun &amp; Huang,</p>
    <p>close to desired locations  Assumes exact inference (slow)</p>
    <p>Shape Regression Machine (Zhou &amp; Comaniciu, 2007)  Train a regressor to find an object  Uses a classifier to clean up result  Aimed for object detection, not MRFs</p>
  </div>
  <div class="page">
    <p>Related Work  Training model-algorithm combinations</p>
    <p>CRF based on pairwise potential trained for object classification Torralba et al, 2004</p>
    <p>AutoContext: Sequence of CRF-like boosted classifiers for object segmentation, Tu 2008</p>
    <p>Both minimize a loss function and report results on another loss function (suboptimal)</p>
    <p>Both train iterative classifiers that are more and more complex at each iteration  speed degrades quickly for improving accuracy</p>
  </div>
  <div class="page">
    <p>Related Work  Training model-algorithm combinations and</p>
    <p>reporting results on the same loss function for image denoising  Tappen, &amp; Orlando, 2007 - Use same type of training</p>
    <p>for obtaining a stronger MAP optimum in image denoising</p>
    <p>Gaussian Conditional Random Fields: Tappen et al, 2007  exact MAP but hundreds of times slower. Results comparable with 2-iteration ARF</p>
    <p>Common theme: trying to obtain a strong MAP optimum</p>
    <p>This work: fast and suboptimal estimator balanced by a complex model and appropriate training</p>
  </div>
  <div class="page">
    <p>Training Active Fields of Experts</p>
    <p>Training set  40 images from the Berkeley dataset</p>
    <p>(Martin 2001)  Same as Roth and Black 2005</p>
    <p>Separate training for each noise level</p>
    <p>Loss function L = PSNR  Same measure used for reporting results</p>
    <p>is the standard deviation of</p>
    <p>Trained Active</p>
    <p>FOE filters, niter=1</p>
  </div>
  <div class="page">
    <p>Training 1-Iteration ARF, =25 Follow Marginal Space Learning  Consider a sequence of subspaces</p>
    <p>Represent marginals by propagating particles between subspaces</p>
    <p>Propagate only one particle (mode) 1. Start with one filter, size 3x3</p>
    <p>Train until no improvement  We found the particle in this subspace</p>
    <p>PSNR training (blue), testing (red) while training the 1-iteration ARF, =25 filters</p>
    <p>x 10 4</p>
    <p>Steps x10000</p>
    <p>P S</p>
    <p>N R</p>
    <p>Training Data Testing Data (unseen)</p>
  </div>
  <div class="page">
    <p>Training other ARFs  Other levels initialized as</p>
    <p>Start with one iteration, =25  Each arrow takes about one day on a 8-core</p>
    <p>machine  3-iteration ARFs can also perform 4 iterations</p>
    <p>n iter</p>
    <p>=1  =15</p>
    <p>n iter =2  =15</p>
    <p>n iter</p>
    <p>=1  =10</p>
    <p>n iter =2  =10</p>
    <p>n iter</p>
    <p>=1  =20</p>
    <p>n iter =2  =20</p>
    <p>n iter</p>
    <p>=1  =25</p>
    <p>n iter =2  =25</p>
    <p>n iter</p>
    <p>=1  =50</p>
    <p>n iter =2  =50</p>
    <p>n iter =3  =10</p>
    <p>n iter =4  =10</p>
    <p>n iter =4  =15</p>
    <p>n iter =3  =15</p>
    <p>n iter =3  =20</p>
    <p>n iter =4  =20</p>
    <p>n iter =4  =25</p>
    <p>n iter =3  =25</p>
    <p>n iter =3  =50</p>
    <p>n iter =4  =50</p>
  </div>
  <div class="page">
    <p>Concerns about Active FOE  Avoid overfitting</p>
    <p>Use large patches to avoid boundary effect  Full size images instead of smaller patches</p>
    <p>Totally 6 million nodes  Lots of training data</p>
    <p>Use a validation set to detect overfitting</p>
    <p>Long training time  Easily parallelizable  1 -3 days on a 8 core PC  Good news: CPU power increases exponentially</p>
    <p>(Moores law)</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Corrupted with Gaussian noise, =25, PSNR=20.17</p>
    <p>Original Image</p>
  </div>
  <div class="page">
    <p>Standard Test Images</p>
    <p>Lena Barbara Boats</p>
    <p>House Peppers</p>
  </div>
  <div class="page">
    <p>Evaluation, Standard Test Images</p>
    <p>noise=25</p>
    <p>Lena Barbara Boat s</p>
    <p>Hou se</p>
    <p>Pepper s</p>
    <p>Averag e</p>
    <p>FOE (Roth &amp; Black, 2005) 30.8 2</p>
    <p>Active FOE, 1 iteration 30.1 5</p>
    <p>Active FOE, 2 iterations 30.6 6</p>
    <p>Active FOE, 3 iterations 30.7 6</p>
    <p>Active FOE, 4 iterations 30.8 6</p>
    <p>Wavelet Denoising (Portilla et al, 2003)</p>
    <p>Overcomplete DCT (Elad et al, 2006)</p>
    <p>Globally Trained Dictionary (Elad et al, 2006)</p>
    <p>KSVD (Elad et al, 2006) 31.3 2</p>
    <p>BM3D (Dabov et al, 2007) 32.0 8</p>
  </div>
  <div class="page">
    <p>Evaluation, Berkeley Dataset 68 images from the Berkeley dataset  Not used for training, not overfitted by other</p>
    <p>methods.  Roth &amp; Black 05 also evaluated on them.  A more realistic evaluation than on 5 images.</p>
  </div>
  <div class="page">
    <p>Evaluation, Berkeley Dataset</p>
    <p>Average PSNR on 68 images from the Berkeley dataset, not used for training.</p>
    <p>Algorithm, sigma = 50 (PSNR=14.15)</p>
    <p>P S</p>
    <p>N R</p>
    <p>Algorithm, sigma = 25 (PSNR=20.17)</p>
    <p>P S</p>
    <p>N R</p>
    <p>Algorithm, sigma=20 (PSNR=22.11)</p>
    <p>P S</p>
    <p>N R</p>
  </div>
  <div class="page">
    <p>Speed-Performance Comparison</p>
    <p>noise=25</p>
    <p>Frames per second</p>
    <p>P S</p>
    <p>N R</p>
    <p>ARF BM3D KSVD DCT Wavelet FOE Nonlocal NonLinDiff Wiener</p>
  </div>
  <div class="page">
    <p>Performance on Different Levels of Noise</p>
    <p>Trained for a specific noise level  No data term  Band-pass behavior</p>
    <p>Level of noise sigma</p>
    <p>P S</p>
    <p>N R</p>
  </div>
  <div class="page">
    <p>Adding a Data Term  Active FOE</p>
    <p>1-iteration version has no data term</p>
    <p>Modification with data term</p>
    <p>Equivalent</p>
  </div>
  <div class="page">
    <p>Performance with Data Term</p>
    <p>Data term removes band-pass behavior  1-iteration ARF as good as 3000-iteration FOE</p>
    <p>for a range of noises</p>
    <p>Level of noise sigma</p>
    <p>P S</p>
    <p>N R</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>An Active Random Field is a pair of  A Markov Random Field based model  A fast, approximate inference algorithm (estimator)</p>
    <p>Training = optimization of the MRF and algorithm parameters using  A benchmark measure on which the results will be</p>
    <p>reported  Training data as pairs of input and desired output</p>
    <p>Pros  Great speed and accuracy  Good control of overfitting using a validation set</p>
    <p>Cons  Slow to train</p>
  </div>
  <div class="page">
    <p>Future Work  Extending image denoising</p>
    <p>Learning filters over multiple channels  Learning the robust function  Learn filters for image sequences using</p>
    <p>temporal coherence</p>
    <p>Other applications  Computer Vision:</p>
    <p>Edge and Road detection, Image segmentation  Stereo matching, motion, tracking etc</p>
    <p>Medical Imaging  Learning a Discriminative Anatomical Network</p>
    <p>of Organ and Landmark Detectors</p>
  </div>
  <div class="page">
    <p>References  A. Barbu. Training an Active Random Field for Real-Time Image Denoising. IEEE Trans.</p>
    <p>Image Processing, 18, November 2009.  Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts.</p>
    <p>Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(11):12221239, 2001.  A. Buades, B. Coll, and J.M. Morel. A Non-Local Algorithm for Image Denoising. Computer</p>
    <p>Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, 2, 2005.</p>
    <p>K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering. Image Processing, IEEE Transactions on, 16(8):20802095, 2007.</p>
    <p>M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Trans. Image Process, 15(12):37363745, 2006.</p>
    <p>G.E. Hinton. Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14(8):17711800, 2002.</p>
    <p>Y. LeCun and F.J. Huang. Loss functions for discriminative training of energy-based models. Proc. of the 10-thInternational Workshop on Artificial Intelligence and Statistics (AIStats 05), 3, 2005.</p>
    <p>D. Martin, C. Fowlkes, D. Tal, and J. Malik. A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms. Proc. of ICCV01, 2:416 425.</p>
    <p>J. Portilla, V. Strela, MJ Wainwright, and EP Simoncelli. Image denoising using scale mixtures of Gaussians in the wavelet domain. Image Processing, IEEE Transactions on, 12(11):13381351, 2003.</p>
    <p>S. Roth and M.J. Black. Fields of Experts. International Journal of Computer Vision, 82(2):205229, 2009.</p>
  </div>
</Presentation>
