<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for</p>
    <p>Text Modeling</p>
    <p>Prince Wang, William Wang UC Santa Barbara</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>VAE and the KL vanishing problem  Motivation: why Riemannian Normalizing flow/WAE  Details  Experimental Results</p>
  </div>
  <div class="page">
    <p>VAE: KL vanishing</p>
    <p>KL term, gap between posterior and prior</p>
    <p>Can generate sentences given latent codes z</p>
    <p>i were to buy any groceries . horses are to buy any groceries . horses are to buy any animal . horses the favorite any animal .</p>
  </div>
  <div class="page">
    <p>Previous works</p>
    <p>Generating sentences from Continuous Space, (2015, Bowman)</p>
    <p>Improved Variational Autoencoder for text Modeling using Dilated Convolution, (2017, Yang)</p>
    <p>Spherical Latent Spaces for Stable Variational Autoencoder, (2018, Xu)</p>
    <p>Semi-Amortized VAE, (2018, Kim)  Cyclical Annealing Schedule: A Simple Approach to</p>
    <p>Mitigating KL Vanishing, (2019, Fu) 4</p>
  </div>
  <div class="page">
    <p>Riemannian Normalizing Flow/Wasserstein Distance</p>
  </div>
  <div class="page">
    <p>Normalizing Flow</p>
    <p>https://lilianweng.github.io/lil-log/2018/10/13/flowbased-deep-generative-models.html</p>
    <p>Making posterior harder to collapse to a standard Gaussian prior</p>
  </div>
  <div class="page">
    <p>Normalizing Flow</p>
    <p>Tighter likelihood approximation</p>
    <p>Reconstruction KLJacobian</p>
  </div>
  <div class="page">
    <p>Why Riemannian VAE?</p>
    <p>The Latent space is not flat Euclidean. It should be curved.</p>
  </div>
  <div class="page">
    <p>Riemannian Metric</p>
    <p>Jacobian</p>
    <p>Rie. Metric</p>
  </div>
  <div class="page">
    <p>Match latent manifold with input manifold</p>
    <p>Curve Length</p>
  </div>
  <div class="page">
    <p>Modeling curvature by NF</p>
    <p>Planar Flow</p>
    <p>Curvature:</p>
  </div>
  <div class="page">
    <p>Modeling curvature by NF</p>
    <p>To match geometry of latent space with input space, we need this determinant to be large when input manifold has high curvature</p>
    <p>Jacobian</p>
  </div>
  <div class="page">
    <p>Wasserstein Distance</p>
    <p>Replace KL with Maximum Mean Discrepancy (MMD)</p>
    <p>Wasserstein Autoencoder, (ICLR 2018, Ilya Tolstikhin)</p>
  </div>
  <div class="page">
    <p>Wasserstein RNF</p>
    <p>Reconstruction</p>
    <p>MMD loss</p>
    <p>KLD loss with NF</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Language Models: Negative Log-likelihood/KL/Perplexity</p>
  </div>
  <div class="page">
    <p>Results: KL divergence</p>
    <p>PTB Yelp</p>
  </div>
  <div class="page">
    <p>Results: Negative log-likelihood</p>
    <p>PTB Yelp</p>
    <p>WAE WAE-NF WAE-RNF WAE WAE-NF WAE-RNF</p>
  </div>
  <div class="page">
    <p>Mutual Information</p>
    <p>Mutual information 18</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Propose to use Normalizing Flow and Wasserstein Distance for variational language model</p>
    <p>Design Riemannian Normalizing Flow to learn a smooth latent space</p>
    <p>Empirical results indicate that Riemannian Normalizing Flow with Wasserstein Distance help avert KL vanishing</p>
    <p>Code: https://github.com/kingofspace0wzz/wae-rnf-lm 19</p>
  </div>
  <div class="page">
    <p>Thank you! Q &amp; A :)  Code: https://github.com/kingofspace0wzz/wae-rnf-lm</p>
    <p>Paper: https://arxiv.org/abs/1904.02399</p>
  </div>
</Presentation>
