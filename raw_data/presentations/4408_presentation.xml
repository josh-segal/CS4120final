<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>How much data is enough? Predicting accuracy on large datasets</p>
    <p>from smaller pilot data</p>
    <p>Mark Johnson, Peter Anderson, Mark Dras, Mark Steedman</p>
    <p>Macquarie University Sydney, Australia</p>
    <p>July 12, 2018</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction</p>
    <p>Empirical models of accuracy vs training data size</p>
    <p>Accuracy extrapolation task</p>
    <p>Conclusions and future work</p>
  </div>
  <div class="page">
    <p>ML as an engineering discipline  A mature engineering discipline should be able to predict the cost of a project before it starts  Collecting/producing training data is typically the most expensive part of an ML or NLP project  We usually have only the vaguest idea of how accuracy is related to training data size and quality I More data produces better accuracy I Higher quality data (closer domain, less noise) produces better accuracy</p>
    <p>I But we usually have no idea how much data or what quality of data is required to achieve a given performance goal</p>
    <p>Imagine if engineers designed bridges the way we build systems!</p>
    <p>See statistical power analysis for experimental design, e.g., Cohen (1992)</p>
  </div>
  <div class="page">
    <p>Goals of this research project</p>
    <p>Given desiderata (accuracy, speed, computational and data resource pricing, etc.) for an ML/NLP system, design for a system that meets these.  Example: design a semantic parser for a target application domain that achieves 95% accuracy across a given range of queries. I What hardware/software should I use? I How many labelled training examples do I need?</p>
    <p>Idea: Extrapolate performance from small pilot data to predict performance on much larger data</p>
  </div>
  <div class="page">
    <p>What this paper contributes</p>
    <p>Studies dierent methods for predicting accuracy on a full dataset from results on a small pilot dataset  We propose new accuracy extrapolation task, provide results for the 9 extrapolation methods on 8 text corpora I Uses the fastText document classier and corpora (Joulin et al., 2016)</p>
    <p>Investigates three extrapolation models and three item weighting functions for predicting accuracy as a function of training data size I Easily inverted to estimate training size required to achieve a target accuracy</p>
    <p>Highlights the importance of hyperparameter tuning and item weighting in extrapolation</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction</p>
    <p>Empirical models of accuracy vs training data size</p>
    <p>Accuracy extrapolation task</p>
    <p>Conclusions and future work</p>
  </div>
  <div class="page">
    <p>Overview  Extrapolation models of how error e (= 1accuracy) depends on training data size n I Power law: e(n) = bnc I Inverse square-root: e(n) = a + bn1/2 I Biased power law: e(n) = a + bnc</p>
    <p>Extrapolation model estimated from multiple runs using weighted least squares regression I Model trained on dierent-sized subsets of pilot data I Same test set is used to evaluate each run I The evaluation of each model training/test run is a training data point for extrapolation model</p>
    <p>Weighting functions for least squares regression I constant weight (1) I linear weight (n) I binomial weight (n/e(1 e))</p>
    <p>See e.g., Haussler et al. (1996); Mukherjee et al. (2003); Figueroa et al. (2012); Beleites et al. (2013); Hajian-Tilaki (2014); Cho et al.</p>
    <p>(2015); Sun et al. (2017); Barone et al. (2017); Hestness et al. (2017) 7/16</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction</p>
    <p>Empirical models of accuracy vs training data size</p>
    <p>Accuracy extrapolation task</p>
    <p>Conclusions and future work</p>
  </div>
  <div class="page">
    <p>Accuracy extrapolation task</p>
    <p>Corpus Labels Train (K) Test (K)</p>
    <p>Development ag_news 4 120 7.6 dbpedia 14 560 70 amazon_review_full 5 3,000 650 yelp_review_polarity 2 560 38</p>
    <p>Evaluation amazon_review_polarity 2 3,600 400 sogou_news 5 450 60 yahoo_answers 10 1,400 60 yelp_review_full 5 650 50</p>
    <p>FastText document classier &amp; data I 4 development corpora I 4 evaluation corpora I Joulin et al. (2016)s train/test division</p>
    <p>Pilot data is 0.5 or 0.1 of train data  Goal: use pilot data to predict test accuracy when trained on full train data</p>
  </div>
  <div class="page">
    <p>Extrapolation on ag_news corpus</p>
    <p>Pilot data size</p>
    <p>E rr</p>
    <p>o r</p>
    <p>ra te</p>
    <p>Pilot data</p>
    <p>==0.1</p>
    <p>&lt;=0.1</p>
    <p>==0.5</p>
    <p>&lt;=0.5</p>
    <p>Extrapolation with biased power-law model (e(n) = a + bnc) and binomial weights (n/e(1 e))  Extrapolation from 0.5 training data is generally good  Extrapolation from 0.1 training data is poor unless hyperparameters are optimised at each subset of pilot data</p>
  </div>
  <div class="page">
    <p>Relative residuals (e/e 1) on dev corpora ==0.1 &lt;=0.1 ==0.5 &lt;=0.5</p>
    <p>a g _ n e w</p>
    <p>s a m</p>
    <p>a zo</p>
    <p>n _ re</p>
    <p>vie w</p>
    <p>_ fu</p>
    <p>ll d b p e d ia</p>
    <p>ye lp</p>
    <p>_ re</p>
    <p>vie w</p>
    <p>_ p o la</p>
    <p>rity</p>
    <p>0.15</p>
    <p>0.10</p>
    <p>0.05</p>
    <p>0.075</p>
    <p>0.050</p>
    <p>0.025</p>
    <p>0.03</p>
    <p>0.02</p>
    <p>0.01</p>
    <p>0.02</p>
    <p>0.01</p>
    <p>Extrapolation</p>
    <p>b*n^c</p>
    <p>a+b*n^1/2</p>
    <p>a+b*n^c</p>
  </div>
  <div class="page">
    <p>RMS relative residuals on test corpora Pilot data</p>
    <p>amazon review polarity</p>
    <p>sogou news</p>
    <p>yahoo answers</p>
    <p>yelp review full</p>
    <p>Overall</p>
    <p>= 0.1 0.1016 0.2752 0.0519 0.0496 0.1510  0.1 0.0209 0.1900 0.0264 0.0406 0.0986</p>
    <p>= 0.5 0.0338 0.0438 0.0254 0.0160 0.0315  0.5 0.0049 0.0390 0.0053 0.0046 0.0200</p>
    <p>Based on dev corpora results, use: I biased power law model (e(n) = a + bnc) I binomial item weights (n/e(1 e))</p>
    <p>Evaluate extrapolations with RMS of relative residuals (e/e 1)  Larger pilot data  smaller extrapolation error  Optimise hyperparameters at each pilot subset  smaller extrapolation error</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction</p>
    <p>Empirical models of accuracy vs training data size</p>
    <p>Accuracy extrapolation task</p>
    <p>Conclusions and future work</p>
  </div>
  <div class="page">
    <p>Conclusions and future work</p>
    <p>The eld need methods for predicting how much training data a system needs to achieve a target performance  We introduced an extrapolation task for predicting a classiers accuracy on a large dataset from a small pilot dataset  Highlight the importance of hyperparameter tuning and item weighting  Future work: extrapolation methods that dont require expensive hyperparameter optimisation</p>
  </div>
  <div class="page">
    <p>We are recruiting PhD students and Postdocs!</p>
    <p>Centre for Research in AI and Language (CRAIL) Macquarie University</p>
    <p>Parsing, Dialog, Deep Unsupervised Learning, Language in Context Vision and Language, Language for Robot Control</p>
    <p>We are recruiting top PhD Students and Postdoc Researchers I With generous pay and top-up scholarships to $41K tax-free</p>
    <p>Send CV and sample papers to Mark.Johnson@MQ.edu.au 15/16</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Barone, A. V. M., Haddow, B., Germann, U., and Sennrich, R. (2017). Regularization techniques for ne-tuning in neural machine translation. CoRR, abs/1707.09920.</p>
    <p>Beleites, C., Neugebauer, U., Bocklitz, T., Krat, C., and Popp, J. (2013). Sample size planning for classication models. Analytica chimica acta, 760:2533.</p>
    <p>Cho, J., Lee, K., Shin, E., Choy, G., and Do, S. (2015). How much data is needed to train a medical image deep learning system to achieve necessary high accuracy? arXiv:1511.06348.</p>
    <p>Cohen, J. (1992). A power primer. Psychological bulletin, 112(1):155. Figueroa, R. L., Zeng-Treitler, Q., Kandula, S., and Ngo, L. H. (2012). Predicting sample size required for classication performance.</p>
    <p>BMC medical informatics and decision making, 12(1):8. Hajian-Tilaki, K. (2014). Sample size estimation in diagnostic test studies of biomedical informatics. Journal of biomedical</p>
    <p>informatics, 48:193204. Haussler, D., Kearns, M., Seung, H. S., and Tishby, N. (1996). Rigorous learning curve bounds from statistical mechanics. Machine</p>
    <p>Learning, 25(2). Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. (2017). Deep learning</p>
    <p>scaling is predictable, empirically. arXiv:1712.00409. Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2016). Bag of tricks for ecient text classication. arXiv:1607.01759. Mukherjee, S., Tamayo, P., Rogers, S., Rifkin, R., Engle, A., Campbell, C., Golub, T. R., and Mesirov, J. P. (2003). Estimating dataset size</p>
    <p>requirements for classifying DNA microarray data. Journal of computational biology, 10(2):119142. Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting unreasonable eectiveness of data in deep learning era.</p>
    <p>arXiv:1707.02968.</p>
  </div>
</Presentation>
