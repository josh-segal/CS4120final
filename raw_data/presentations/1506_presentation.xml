<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Analysis of the ECMWF Storage Landscape</p>
    <p>Matthias Grawinkel*, Lars Nagel*, Markus Msker*, Federico Padua*, Andr Brinkmann*, Lennart Sorth# * Johannes Gutenberg University Mainz, Germany # European Centre for Medium-range Weather Forecasts, Reading, UK</p>
  </div>
  <div class="page">
    <p>European Centre for Medium-range Weather Forecasts</p>
    <p>Global weather forecasts for up to 15 days and seasonal forecasts for up to 12 months</p>
    <p>Multiple supercomputers (Top 500 Nov. 2014: 28, 29, 82, 83)  ~100 PB total storage capacity in 2014/09  Two in-house developed data handling systems: ECFS, MARS  Compound annual growth rate (CAGR) &gt; 50%</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>How to build (active) archives?  Content &amp; behavior of existing systems  Current problems?  Future challenges?</p>
    <p>Only a few studies and traces available  Low coverage of the research topic  Required to design and evaluate systems</p>
    <p>First study of large-scale active archive  In depth-analysis of two systems  Characterization of content and usage  Analysis of caching behavior  Study of tape backend  Release of scripts &amp; trace files</p>
  </div>
  <div class="page">
    <p>ECMWF Storage Landscape</p>
    <p>Files are staged and cached on disk drives  Every file eventually has a primary copy on tape  Important files have secondary tape copy</p>
    <p>FDB Field cache</p>
    <p>ECFS ClientsECFS</p>
    <p>ClientsECFS Clients</p>
    <p>MARS ClientsMARS</p>
    <p>ClientsMARS Clients</p>
    <p>Off Site Backup</p>
    <p>Mars Server Disk Cache</p>
    <p>Disk Cache</p>
    <p>HPSS</p>
    <p>ECFS is a general purpose user accessible archive for intermediate and long-term file storage</p>
    <p>MARS is an object database for meteorological data</p>
  </div>
  <div class="page">
    <p>Investigated Trace Files</p>
    <p>A) ECFS / HPSS database snapshot of 2014/09  B) ECFS access trace: 2012/01 - 2014/05  C) MARS / HPSS database snapshot of 2014/09  D) MARS feedback logs: 2010/01 - 2014/02  E) HPSS WHPSS logs / robot mount logs: 2012/01 - 2013/12</p>
    <p>Extracted, sanitized, and obfuscated traces available now</p>
    <p>FDB Field cache</p>
    <p>ECFS ClientsECFS</p>
    <p>ClientsECFS Clients</p>
    <p>MARS ClientsMARS</p>
    <p>ClientsMARS Clients</p>
    <p>Off Site Backup</p>
    <p>Mars Server Disk Cache</p>
    <p>Disk Cache</p>
    <p>HPSS</p>
    <p>B)</p>
    <p>D)</p>
    <p>A) C)</p>
    <p>E)</p>
  </div>
  <div class="page">
    <p>Data Handling System: ECFS</p>
    <p>Client tools for PUT, GET, DEL, RENAME on full files  14.8 PB of primary data  137.5 mil. files in 5.5 mil. directories  0.34 PB disk cache (disk/tape ratio: 1:43)</p>
    <p>Cache categories defined by file size Group From To (incl.) Count Used Capacity</p>
    <p>Tiny 0 512 KB 36.0 mil. 4.4 TB Small 512 KB 1 MB 9.1 mil. 6.3 TB Medium 1 MB 8 MB 29.5 mil. 101 TB Large 8 MB 48 MB 30.0 mil. 585 TB Huge 48 MB 1 GB 29.7 mil. 6.2 PB Enormous 1 GB 32 GB 3.1 mil. 8 PB</p>
  </div>
  <div class="page">
    <p>ECFS Content Characterization</p>
    <p>Based on HPSS database snapshot  2014/09  Only 26.3 % of files on tape were ever read 1 times</p>
    <p>By file count By used capacity unknown (27.8%) unknown (39.3%)</p>
    <p>.gz (20.4%) .tar (21.3%) .tar (7.8%) .gz (12.5%) .nc (7.6%) .nc (7.9%)</p>
    <p>.grib2 (1.9%) .lfi (2.2%) .raw (1.7%) .pp (1.0%) .txt (1.5%) .sfx (0.9%) .Z (1.5%) .grb (0.8%)</p>
    <p>.bufr (1.4%) .grib (0.4%) .grb (1.4%) .bz2 (0.3%)</p>
  </div>
  <div class="page">
    <p>ECFS Workload Characterization</p>
    <p>Timespan 2012-01-01 to 2014-05-20  78.3 mil. PUT requests  11.8 PB  38.5 mil. GET requests  7.2 PB</p>
    <p>12.2 mil. unique files (9% of full file corpus)  Cache hit ratio by requests: 86.7%  Cache hit ratio by bytes: 45.9%</p>
  </div>
  <div class="page">
    <p>ECFS User Sessions</p>
    <p>Identified 1,190 users, 2.7 mil. sessions  Session lifetime from seconds up to 10 hours of constant traffic</p>
    <p>Key Count 5th P mean 99th P Total #actions per session 2.7 mil. 2 47 579 Sessions with GET requests 1.1 mil. 1 36 571 - Retrieved data 0.6 MB 7.2 GB 86 GB - #ReGET requests 0.13 mil. 1 32 442 Sessions with PUT Requests 2.3 mil. 1 34 373 - Uploaded data 0.02 MB 5.6 GB 65 GB</p>
  </div>
  <div class="page">
    <p>What is the impact of smaller or bigger caches?  Developed modular cache simulation environment</p>
    <p>MRU, FIFO, RANDOM, LRU, ARC, Bldy, ECMWF baseline  Cache per size-category (capacity + strategy)</p>
    <p>Replayed ECFS access trace  12 months warm up, measured following 17 months</p>
    <p>M edium</p>
    <p>: 1M B</p>
    <p>-8M B</p>
  </div>
  <div class="page">
    <p>Data Handling System: MARS</p>
    <p>Query: Get temperature &amp; humidity for Santa Clara# from $date till $date with a 5 minute resolution  MARS then assembles and writes out a results file</p>
    <p>170 bil. fields in 9.7 mil. files  200 mil. new fields each day (i.e. sensor data, model output)</p>
    <p>37.9 PB of primary data, 800 GB metadata  3-tiered caching hierarchy</p>
    <p>Field database (FDB) on HPC# storage: Variable size &lt;1 PB</p>
    <p>1 PB disk cache on MARS servers  250 TB reserved for manual optimizations</p>
    <p>HPSS/tape#</p>
  </div>
  <div class="page">
    <p>MARS Content Characterization</p>
    <p>Based on HPSS database snapshot 2014/09  Only 23 % of files on tape were ever read 1 times</p>
  </div>
  <div class="page">
    <p>MARS Workload Characterization</p>
    <p>MARS feedback logs from 2010-01-01 till 2014-02-27  Contain queries and description of results (#fields, bytes, source)</p>
  </div>
  <div class="page">
    <p>Totals over Observed Timeframe (1518 days)</p>
    <p>Total archive requests 115 mil. Total archived bytes (fields) 35.9 PB (114.7 bil.)</p>
    <p>Total retrieve requests 1.2 bil. - involving  1 tape loads 25.3 mil. (2.2%) - from HPSS/tape only 16 mil. (1.4%)</p>
    <p>Total retrieved bytes (fields) 91.6 PB (269 bil.) - from FDB bytes (fields) 54.2 PB (212 bil.) - from MARS/disk bytes (fields) 29.4 PB (43.3 bil.) - from HPSS/tape bytes (fields) 8 PB (13.3 bil.)</p>
  </div>
  <div class="page">
    <p>Tape Mount Logs</p>
    <p>Library Drive Mounted Volume Mounted</p>
    <p>load mount</p>
    <p>unmountunload</p>
  </div>
  <div class="page">
    <p>Tracked Tapes &amp; Drives: 2012+2013 32,712 tape identifiers 231 drive identifiers 9.6 mil. tape loads ~9 loads per minute</p>
  </div>
  <div class="page">
    <p>Tape is Actively Used</p>
    <p>Tape mount latencies in seconds #mounts median mean 99th P</p>
    <p>MARS 6.7 mil. 35 54.4 262 ECFS 2.8 mil. 32 48.2 257</p>
    <p>Mount requests per cartridge. # Absolute (top) + normalized (bottom)</p>
    <p>Tape mount frequencies #tapes median mean 99th P</p>
    <p>MARS 23,118 46 291 3,351 ECFS 9,594 85 297 2,470</p>
  </div>
  <div class="page">
    <p>Remounts and Reloads</p>
    <p>M AR</p>
    <p>S</p>
  </div>
  <div class="page">
    <p>Improve Tape (Un)loading?</p>
    <p>Goals: Minimize #drives, mean time to mount, tape mounts# Maximize tape re-use</p>
    <p>Identified hot tapes: 20% of tapes account for 80% of mounts  Analysis of drive utilization #</p>
    <p>showed exploitable idling times  Optimistic preloading?</p>
    <p>Correlation analysis showed potential  High tape reload rates suggest to#</p>
    <p>keep (certain) tapes in the drives  Further investigation required</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>ECMWF in operation since 1975  Lots of hands-on experience  Predictable production workloads</p>
    <p>Manual optimizations  Chaotic research workloads</p>
    <p>ECFS resembles archives investigated in related work  MARS opens a new category of archives  Tape + disk caches can be used to build efficient#</p>
    <p>non-interactive systems  Heavy use of tape has drawbacks</p>
    <p>High wear-out  Unpredictable, stacking latencies</p>
    <p>MARS-Error: Query requires too many tapes  Potential for smarter tape (un)loading strategies</p>
  </div>
  <div class="page">
    <p>github.com/zdvresearch/fast15-paper-extras/ Were hiring: research.zdv.uni-mainz.de</p>
  </div>
</Presentation>
