<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Transition-Based Directed Acyclic Graph Parser for Universal Conceptual Cognitive Annotation</p>
    <p>Daniel Hershcovich, Omri Abend and Ari Rappoport</p>
    <p>ACL 2017</p>
  </div>
  <div class="page">
    <p>TUPA  Transition-based UCCA Parser The first parser to support the combination of three properties:</p>
    <p>needed for many semantic schemes (e.g. AMR, UCCA).</p>
    <p>You want</p>
    <p>to</p>
    <p>take a long bath</p>
  </div>
  <div class="page">
    <p>TUPA  Transition-based UCCA Parser The first parser to support the combination of three properties:</p>
    <p>You want</p>
    <p>to</p>
    <p>take a long bath</p>
  </div>
  <div class="page">
    <p>TUPA  Transition-based UCCA Parser The first parser to support the combination of three properties:</p>
    <p>needed for many semantic schemes (e.g. AMR, UCCA).</p>
    <p>You want</p>
    <p>to</p>
    <p>take a long bath</p>
  </div>
  <div class="page">
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>Linguistic Structure Annotation Schemes</p>
    <p>Syntactic dependencies</p>
    <p>Semantic dependencies (Oepen et al., 2016)</p>
    <p>Syntactic (UD)</p>
    <p>You want to take a long bath</p>
    <p>root</p>
    <p>nsubj</p>
    <p>xcomp</p>
    <p>mark</p>
    <p>dobj</p>
    <p>det</p>
    <p>amod</p>
    <p>top ARG2</p>
    <p>ARG1</p>
    <p>ARG1 ARG2</p>
    <p>BV</p>
    <p>ARG1</p>
    <p>Semantic (DM)</p>
    <p>Bilexical dependencies.</p>
  </div>
  <div class="page">
    <p>Linguistic Structure Annotation Schemes</p>
    <p>Syntactic dependencies</p>
    <p>Semantic dependencies (Oepen et al., 2016)</p>
    <p>Semantic role labeling (PropBank, FrameNet)</p>
    <p>AMR (Banarescu et al., 2013)</p>
    <p>UCCA (Abend and Rappoport, 2013)</p>
    <p>Other semantic representation schemes1</p>
    <p>Semantic representation schemes attempt to abstract away from syntactic detail that does not affect meaning:</p>
    <p>. . . bathed = . . . took a bath</p>
  </div>
  <div class="page">
    <p>The UCCA Semantic Representation Scheme</p>
  </div>
  <div class="page">
    <p>Universal Conceptual Cognitive Annotation (UCCA) Cross-linguistically applicable (Abend and Rappoport, 2013). Stable in translation (Sulem et al., 2015).</p>
    <p>English</p>
    <p>Hebrew</p>
  </div>
  <div class="page">
    <p>Universal Conceptual Cognitive Annotation (UCCA) Rapid and intuitive annotation interface (Abend et al., 2017). Usable by non-experts. ucca-demo.cs.huji.ac.il</p>
    <p>Facilitates semantics-based human evaluation of machine translation (Birch et al., 2016). ucca.cs.huji.ac.il/mteval</p>
  </div>
  <div class="page">
    <p>Graph Structure UCCA generates a directed acyclic graph (DAG). Text tokens are terminals, complex units are non-terminal nodes. Remote edges enable reentrancy for argument sharing. Phrases may be discontinuous (e.g., multi-word expressions).</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
    <p>primary edge</p>
    <p>- - - remote edge</p>
    <p>You want to take a long bath</p>
    <p>P process A participant C center D adverbial F function</p>
  </div>
  <div class="page">
    <p>Transition-based UCCA Parsing</p>
  </div>
  <div class="page">
    <p>Transition-Based Parsing First used for dependency parsing (Nivre, 2004). Parse text w1 . . . wn to graph G incrementally by applying transitions to the parser state: stack, buffer and constructed graph.</p>
    <p>Initial state:</p>
    <p>stack buffer</p>
    <p>You want to take a long bath</p>
    <p>TUPA transitions: {Shift, Reduce, NodeX , Left-EdgeX , Right-EdgeX ,</p>
    <p>Left-RemoteX , Right-RemoteX , Swap, Finish}</p>
    <p>Support non-terminal nodes, reentrancy and discontinuity.</p>
  </div>
  <div class="page">
    <p>Transition-Based Parsing First used for dependency parsing (Nivre, 2004). Parse text w1 . . . wn to graph G incrementally by applying transitions to the parser state: stack, buffer and constructed graph.</p>
    <p>Initial state:</p>
    <p>stack buffer</p>
    <p>You want to take a long bath</p>
    <p>TUPA transitions: {Shift, Reduce, NodeX , Left-EdgeX , Right-EdgeX ,</p>
    <p>Left-RemoteX , Right-RemoteX , Swap, Finish}</p>
    <p>Support non-terminal nodes, reentrancy and discontinuity.</p>
  </div>
  <div class="page">
    <p>Transition-Based Parsing First used for dependency parsing (Nivre, 2004). Parse text w1 . . . wn to graph G incrementally by applying transitions to the parser state: stack, buffer and constructed graph.</p>
    <p>Initial state:</p>
    <p>stack buffer</p>
    <p>You want to take a long bath</p>
    <p>TUPA transitions: {Shift, Reduce, NodeX , Left-EdgeX , Right-EdgeX ,</p>
    <p>Left-RemoteX , Right-RemoteX , Swap, Finish}</p>
    <p>Support non-terminal nodes, reentrancy and discontinuity.</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>want to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeA</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>want to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You want</p>
    <p>buffer</p>
    <p>to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Swap</p>
    <p>stack</p>
    <p>want</p>
    <p>buffer</p>
    <p>You to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeP</p>
    <p>stack</p>
    <p>want</p>
    <p>buffer</p>
    <p>You to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack buffer</p>
    <p>to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>to take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You to</p>
    <p>buffer</p>
    <p>take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  NodeF</p>
    <p>stack</p>
    <p>You to</p>
    <p>buffer</p>
    <p>take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>take a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You take</p>
    <p>buffer</p>
    <p>a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  NodeC</p>
    <p>stack</p>
    <p>You take</p>
    <p>buffer</p>
    <p>a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeP</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>a long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You a</p>
    <p>buffer</p>
    <p>long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeF</p>
    <p>stack</p>
    <p>You a</p>
    <p>buffer</p>
    <p>long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>long bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You long</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Swap</p>
    <p>stack</p>
    <p>You long</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeD</p>
    <p>stack</p>
    <p>You long</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Swap</p>
    <p>stack buffer</p>
    <p>You bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeA</p>
    <p>stack buffer</p>
    <p>You bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack buffer</p>
    <p>You bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Reduce</p>
    <p>stack buffer</p>
    <p>You bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Left-RemoteA</p>
    <p>stack</p>
    <p>You</p>
    <p>buffer</p>
    <p>bath</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Shift</p>
    <p>stack</p>
    <p>You bath</p>
    <p>buffer</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Right-EdgeC</p>
    <p>stack</p>
    <p>You bath</p>
    <p>buffer</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Example  Finish</p>
    <p>stack</p>
    <p>You bath</p>
    <p>buffer</p>
    <p>graph</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Training An oracle provides the transition sequence given the correct graph:</p>
    <p>You</p>
    <p>A</p>
    <p>want</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>take</p>
    <p>C</p>
    <p>a</p>
    <p>F</p>
    <p>long bath</p>
    <p>C</p>
    <p>P</p>
    <p>A</p>
    <p>A</p>
    <p>D</p>
    <p>Shift, Right-EdgeA, Shift, Swap, Right-EdgeP , Reduce, Shift, Shift, NodeF , Reduce, Shift, Shift, NodeC , Reduce, Shift, Right-EdgeP , Shift, Right-EdgeF , Reduce, Shift, Swap, Right-EdgeD , Reduce, Swap, Right-EdgeA, Reduce, Reduce, Shift, Shift, Left-RemoteA, Shift, Right-EdgeC , Finish</p>
  </div>
  <div class="page">
    <p>TUPA Model Learn to greedily predict transition based on current state. Experimenting with three classifiers:</p>
    <p>Sparse Perceptron with sparse features (Zhang and Nivre, 2011). MLP Embeddings + feedforward NN (Chen and Manning, 2014). BiLSTM Embeddings + deep bidirectional LSTM + MLP</p>
    <p>(Kiperwasser and Goldberg, 2016).</p>
    <p>Effective lookahead encoded in the representation.</p>
    <p>Features: words, POS, syntactic dependencies, existing edge labels from the stack and buffer + parents, children, grandchildren; ordinal features (height, number of parents and children)</p>
    <p>stack buffer</p>
    <p>stack You take</p>
    <p>buffer a long bath</p>
    <p>graph</p>
    <p>You A</p>
    <p>want P</p>
    <p>to F</p>
    <p>take C</p>
    <p>a F</p>
    <p>long bath C</p>
    <p>You</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>want</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>to</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>take</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>a</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>long</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>bath</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>MLP</p>
    <p>NodeC</p>
  </div>
  <div class="page">
    <p>TUPA Model Learn to greedily predict transition based on current state. Experimenting with three classifiers:</p>
    <p>Sparse Perceptron with sparse features (Zhang and Nivre, 2011). MLP Embeddings + feedforward NN (Chen and Manning, 2014). BiLSTM Embeddings + deep bidirectional LSTM + MLP</p>
    <p>(Kiperwasser and Goldberg, 2016).</p>
    <p>Effective lookahead encoded in the representation.</p>
    <p>stack You take</p>
    <p>buffer a long bath</p>
    <p>graph</p>
    <p>You A</p>
    <p>want P</p>
    <p>to F</p>
    <p>take C</p>
    <p>a F</p>
    <p>long bath C</p>
    <p>You</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>want</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>to</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>take</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>a</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>long</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>bath</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>MLP</p>
    <p>NodeC</p>
  </div>
  <div class="page">
    <p>TUPA Model Learn to greedily predict transition based on current state. Experimenting with three classifiers:</p>
    <p>Sparse Perceptron with sparse features (Zhang and Nivre, 2011). MLP Embeddings + feedforward NN (Chen and Manning, 2014). BiLSTM Embeddings + deep bidirectional LSTM + MLP</p>
    <p>(Kiperwasser and Goldberg, 2016).</p>
    <p>Effective lookahead encoded in the representation.</p>
    <p>stack You take</p>
    <p>buffer a long bath</p>
    <p>graph</p>
    <p>You A</p>
    <p>want P</p>
    <p>to F</p>
    <p>take C</p>
    <p>a F</p>
    <p>long bath C</p>
    <p>You</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>want</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>to</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>take</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>a</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>long</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>bath</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>MLP</p>
    <p>NodeC</p>
  </div>
  <div class="page">
    <p>TUPA Model Learn to greedily predict transition based on current state. Experimenting with three classifiers:</p>
    <p>Sparse Perceptron with sparse features (Zhang and Nivre, 2011). MLP Embeddings + feedforward NN (Chen and Manning, 2014). BiLSTM Embeddings + deep bidirectional LSTM + MLP</p>
    <p>(Kiperwasser and Goldberg, 2016).</p>
    <p>Effective lookahead encoded in the representation.</p>
    <p>stack You take</p>
    <p>buffer a long bath</p>
    <p>graph</p>
    <p>You A</p>
    <p>want P</p>
    <p>to F</p>
    <p>take C</p>
    <p>a F</p>
    <p>long bath C</p>
    <p>You</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>want</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>to</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>take</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>a</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>long</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>bath</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>MLP</p>
    <p>NodeC</p>
  </div>
  <div class="page">
    <p>TUPA Model Learn to greedily predict transition based on current state. Experimenting with three classifiers:</p>
    <p>Sparse Perceptron with sparse features (Zhang and Nivre, 2011). MLP Embeddings + feedforward NN (Chen and Manning, 2014). BiLSTM Embeddings + deep bidirectional LSTM + MLP</p>
    <p>(Kiperwasser and Goldberg, 2016).</p>
    <p>Effective lookahead encoded in the representation.</p>
    <p>stack You take</p>
    <p>buffer a long bath</p>
    <p>graph</p>
    <p>You A</p>
    <p>want P</p>
    <p>to F</p>
    <p>take C</p>
    <p>a F</p>
    <p>long bath C</p>
    <p>You</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>want</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>to</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>take</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>a</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>long</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>bath</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>MLP</p>
    <p>NodeC</p>
  </div>
  <div class="page">
    <p>stack You take</p>
    <p>buffer a long bath</p>
    <p>graph</p>
    <p>You A</p>
    <p>want P</p>
    <p>to F</p>
    <p>take C</p>
    <p>a F</p>
    <p>long bath C</p>
    <p>You</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>want</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>to</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>take</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>a</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>long</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>bath</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>LSTM</p>
    <p>MLP</p>
    <p>NodeC</p>
  </div>
  <div class="page">
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>UCCA Wikipedia corpus ( train 4268 +</p>
    <p>dev 454 +</p>
    <p>test 503 sentences).</p>
    <p>Out-of-domain: English part of English-French parallel corpus, Twenty Thousand Leagues Under the Sea (506 sentences).</p>
  </div>
  <div class="page">
    <p>Baselines No existing UCCA parsers  conversion-based approximation. Bilexical DAG parsers (allow reentrancy):</p>
    <p>DAGParser (Ribeyre et al., 2014): transition-based.  TurboParser (Almeida and Martins, 2015): graph-based.</p>
    <p>Tree parsers (all transition-based):  MaltParser (Nivre et al., 2007): bilexical tree parser.  Stack LSTM Parser (Dyer et al., 2015): bilexical tree parser.  uparse (Maier, 2015): allows non-terminals, discontinuity.</p>
    <p>You want to take a long bath</p>
    <p>A</p>
    <p>A</p>
    <p>A</p>
    <p>F F</p>
    <p>D</p>
    <p>C</p>
    <p>UCCA bilexical DAG approximation (for tree, delete remote edges).</p>
  </div>
  <div class="page">
    <p>Bilexical Graph Approximation</p>
    <p>After</p>
    <p>L</p>
    <p>graduation</p>
    <p>P</p>
    <p>H</p>
    <p>, U</p>
    <p>Joe</p>
    <p>A</p>
    <p>moved</p>
    <p>P</p>
    <p>to</p>
    <p>R</p>
    <p>Paris</p>
    <p>C</p>
    <p>A</p>
    <p>H</p>
    <p>A</p>
    <p>After graduation , Joe moved to Paris</p>
    <p>L U</p>
    <p>A</p>
    <p>A</p>
    <p>H</p>
    <p>R</p>
    <p>A</p>
  </div>
  <div class="page">
    <p>Evaluation Comparing graphs over the same sequence of tokens,</p>
    <p>Match edges by their terminal yield and label.  Calculate labeled precision, recall and F1 scores.  Separate primary and remote edges.</p>
    <p>gold</p>
    <p>After</p>
    <p>L</p>
    <p>graduation</p>
    <p>P</p>
    <p>H ,</p>
    <p>U</p>
    <p>Joe</p>
    <p>A</p>
    <p>moved</p>
    <p>P</p>
    <p>to</p>
    <p>R</p>
    <p>Paris</p>
    <p>C</p>
    <p>A</p>
    <p>H</p>
    <p>A</p>
    <p>predicted</p>
    <p>After</p>
    <p>L</p>
    <p>graduation</p>
    <p>S</p>
    <p>H ,</p>
    <p>U</p>
    <p>Joe</p>
    <p>A</p>
    <p>moved</p>
    <p>P</p>
    <p>to</p>
    <p>F</p>
    <p>Paris</p>
    <p>A</p>
    <p>H</p>
    <p>A</p>
    <p>A</p>
    <p>Primary: LP LR LF</p>
    <p>Remote: LP LR LF</p>
  </div>
  <div class="page">
    <p>Results TUPABiLSTM obtains the highest F-scores in all metrics:</p>
    <p>Primary edges Remote edges LP LR LF LP LR LF</p>
    <p>TUPASparse 64.5 63.7 64.1 19.8 13.4 16 TUPAMLP 65.2 64.6 64.9 23.7 13.2 16.9 TUPABiLSTM 74.4 72.7 73.5 47.4 51.6 49.4 Bilexical DAG (91) (58.3) DAGParser 61.8 55.8 58.6 9.5 0.5 1 TurboParser 57.7 46 51.2 77.8 1.8 3.7 Bilexical tree (91)  MaltParser 62.8 57.7 60.2    Stack LSTM 73.2 66.9 69.9    Tree (100)  uparse 60.9 61.2 61.1</p>
    <p>Results on the Wiki test set.</p>
  </div>
  <div class="page">
    <p>Results Comparable on out-of-domain test set:</p>
    <p>Primary edges Remote edges LP LR LF LP LR LF</p>
    <p>TUPASparse 59.6 59.9 59.8 22.2 7.7 11.5 TUPAMLP 62.3 62.6 62.5 20.9 6.3 9.7 TUPABiLSTM 68.7 68.5 68.6 38.6 18.8 25.3 Bilexical DAG (91.3) (43.4) DAGParser 56.4 50.6 53.4  0 0 TurboParser 50.3 37.7 43.1 100 0.4 0.8 Bilexical tree (91.3)  MaltParser 57.8 53 55.3    Stack LSTM 66.1 61.1 63.5    Tree (100)  uparse 52.7 52.8 52.8</p>
    <p>Results on the 20K Leagues out-of-domain set.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>UCCAs semantic distinctions require a graph structure including non-terminals, reentrancy and discontinuity.</p>
    <p>TUPA is an accurate transition-based UCCA parser, and the first to support UCCA and any DAG over the text tokens.</p>
    <p>Outperforms strong conversion-based baselines.</p>
    <p>Future Work:  More languages (German corpus construction is underway).  Parsing other schemes, such as AMR.  Compare semantic representations through conversion.  Text simplification, MT evaluation and other applications.</p>
    <p>Code: github.com/danielhers/tupa Demo: bit.ly/tupademo Corpora: cs.huji.ac.il/oabend/ucca.html</p>
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>UCCAs semantic distinctions require a graph structure including non-terminals, reentrancy and discontinuity.</p>
    <p>TUPA is an accurate transition-based UCCA parser, and the first to support UCCA and any DAG over the text tokens.</p>
    <p>Outperforms strong conversion-based baselines. Future Work:</p>
    <p>More languages (German corpus construction is underway).  Parsing other schemes, such as AMR.  Compare semantic representations through conversion.  Text simplification, MT evaluation and other applications.</p>
    <p>Code: github.com/danielhers/tupa Demo: bit.ly/tupademo Corpora: cs.huji.ac.il/oabend/ucca.html</p>
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>UCCAs semantic distinctions require a graph structure including non-terminals, reentrancy and discontinuity.</p>
    <p>TUPA is an accurate transition-based UCCA parser, and the first to support UCCA and any DAG over the text tokens.</p>
    <p>Outperforms strong conversion-based baselines. Future Work:</p>
    <p>More languages (German corpus construction is underway).  Parsing other schemes, such as AMR.  Compare semantic representations through conversion.  Text simplification, MT evaluation and other applications.</p>
    <p>Code: github.com/danielhers/tupa Demo: bit.ly/tupademo Corpora: cs.huji.ac.il/oabend/ucca.html</p>
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>References I Abend, O. and Rappoport, A. (2013).</p>
    <p>Universal Conceptual Cognitive Annotation (UCCA). In Proc. of ACL, pages 228238.</p>
    <p>Abend, O. and Rappoport, A. (2017). The state of the art in semantic representation. In Proc. of ACL. to appear.</p>
    <p>Abend, O., Yerushalmi, S., and Rappoport, A. (2017). UCCAApp: Web-application for syntactic and semantic phrase-based annotation. In Proc. of ACL: System Demonstration Papers. to appear.</p>
    <p>Almeida, M. S. C. and Martins, A. F. T. (2015). Lisbon: Evaluating TurboSemanticParser on multiple languages and out-of-domain data. In Proc. of SemEval, pages 970973.</p>
    <p>Banarescu, L., Bonial, C., Cai, S., Georgescu, M., Griffitt, K., Hermjakob, U., Knight, K., Palmer, M., and Schneider, N. (2013). Abstract Meaning Representation for sembanking. In Proc. of the Linguistic Annotation Workshop.</p>
    <p>Birch, A., Abend, O., Bojar, O., and Haddow, B. (2016). HUME: Human UCCA-based evaluation of machine translation. In Proc. of EMNLP, pages 12641274.</p>
    <p>Chen, D. and Manning, C. (2014). A fast and accurate dependency parser using neural networks. In Proc. of EMNLP, pages 740750.</p>
  </div>
  <div class="page">
    <p>References II Dyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N. A. (2015).</p>
    <p>Transition-based dependeny parsing with stack long short-term memory. In Proc. of ACL, pages 334343.</p>
    <p>Kiperwasser, E. and Goldberg, Y. (2016). Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL, 4:313327.</p>
    <p>Maier, W. (2015). Discontinuous incremental shift-reduce parsing. In Proc. of ACL, pages 12021212.</p>
    <p>Nivre, J. (2004). Incrementality in deterministic dependency parsing. In Keller, F., Clark, S., Crocker, M., and Steedman, M., editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together, pages 5057, Barcelona, Spain. Association for Computational Linguistics.</p>
    <p>Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., Kubler, S., Marinov, S., and Marsi, E. (2007). MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95135.</p>
    <p>Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Cinkova, S., Flickinger, D., Hajic, J., Ivanova, A., and Uresova, Z. (2016). Towards comparability of linguistic graph banks for semantic parsing. In LREC.</p>
    <p>Ribeyre, C., Villemonte de la Clergerie, E., and Seddah, D. (2014). Alpage: Transition-based semantic graph parsing with syntactic features. In Proc. of SemEval, pages 97103.</p>
  </div>
  <div class="page">
    <p>References III Sulem, E., Abend, O., and Rappoport, A. (2015).</p>
    <p>Conceptual annotations preserve structure across translations: A French-English case study. In Proc. of S2MT, pages 1122.</p>
    <p>Zhang, Y. and Nivre, J. (2011). Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188193.</p>
  </div>
  <div class="page">
    <p>Backup</p>
  </div>
  <div class="page">
    <p>UCCA Corpora Wiki 20K</p>
    <p>Train Dev Test Leagues # passages 300 34 33 154 # sentences 4268 454 503 506 # nodes 298,993 33,704 35,718 29,315 % terminal 42.96 43.54 42.87 42.09 % non-term. 58.33 57.60 58.35 60.01 % discont. 0.54 0.53 0.44 0.81 % reentrant 2.38 1.88 2.15 2.03 # edges 287,914 32,460 34,336 27,749 % primary 98.25 98.75 98.74 97.73 % remote 1.75 1.25 1.26 2.27 Average per non-terminal node # children 1.67 1.68 1.66 1.61</p>
    <p>Corpus statistics.</p>
  </div>
  <div class="page">
    <p>Evaluation Mutual edges between predicted graph Gp = (Vp , Ep , `p ) and gold graph Gg = (Vg , Eg , `g ), both over terminals W = {w1, . . . , wn}:</p>
    <p>M(Gp , Gg ) = {</p>
    <p>(e1, e2)  Ep Eg  y (e1) = y (e2)`p (e1) = `g (e2)}</p>
    <p>The yield y (e)  W of an edge e = (u, v ) in either graph is the set of terminals in W that are descendants of v . ` is the edge label.</p>
    <p>Labeled precision, recall and F-score are then defined as:</p>
    <p>LP = |M(Gp , Gg )|</p>
    <p>|Ep | , LR =</p>
    <p>|M(Gp , Gg )| |Eg |</p>
    <p>,</p>
    <p>LF = 2  LP  LR LP + LR</p>
    <p>.</p>
    <p>Two variants: one for primary edges, and another for remote edges.</p>
  </div>
</Presentation>
