<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Improved File Synchronization Techniques for Maintaining Large Replicated Collections</p>
    <p>over Slow Networks</p>
    <p>Torsten Suel</p>
    <p>CIS Department Polytechnic University</p>
    <p>Joint work with Patrick Noel (1) and Dimitre Trendafilov (2)</p>
    <p>Current Affiliations: (1) National Bank of Haiti, (2) Google Inc. Work performed while at Polytechnic University</p>
  </div>
  <div class="page">
    <p>If server has copy of both files local problem at server (delta compression)</p>
    <p>Server Client</p>
    <p>update f_new f_old</p>
    <p>request</p>
    <p>File Synchronization Problem</p>
    <p>clients wants to update outdated file  server has new file but does not know old file  update without sending entire new file (using similarity)  rsync: file synchronization tool, part of Linux  note: files are unstructured (record/page oriented case is different)</p>
  </div>
  <div class="page">
    <p>Main Application: Mirroring/Replication</p>
    <p>to mirror web and ftp servers (e.g., downloads: tucows, linux)  to synchronize data between PCs (e.g., at work and at home)  our motivation: maintaining web collections for mining  our motivation: efficient web page subscription</p>
    <p>currently done using rsync protocol and tool  can we significantly improve performance?  recall: server has no knowledge of old version</p>
    <p>server mirror</p>
    <p>new version of collection</p>
    <p>or data</p>
    <p>old version of collection</p>
    <p>or data</p>
  </div>
  <div class="page">
    <p>network links are getting faster and faster</p>
    <p>but many clients still connected by fairly slow links</p>
    <p>56K and cell modems, USB, cable, DSL, 802.11</p>
    <p>bandwidth also a problem in P2P systems (large data)</p>
    <p>how can we deal with such slow links in ways that are transparent to the user? - at the application layer (e.g., http, mail) - application-independent at IP or TCP layer</p>
    <p>applications: web access, handheld synchronization, software updates, server mirroring, link compression</p>
    <p>General Background and Motivation:</p>
  </div>
  <div class="page">
    <p>Setup: Data Transmission over Slow Networks</p>
    <p>sender needs to send data set to receiver  data set may be similar to data the receiver already holds  sender may or may not know data at receiver - sender may know the data at receiver - sender may know something about data at receiver - sender may initially know nothing</p>
    <p>sender receiver</p>
    <p>data knowledge about data at receiver</p>
  </div>
  <div class="page">
    <p>caching: avoid sending same object again - widely used and important scenarios - done on the basis of objects - only works if objects completely unchanged - how about objects that are slightly changed?</p>
    <p>compression: remove redundancy in transmitted data - avoid repeated substrings in data (Lempel-Ziv approach) - often limited to within one object - but can be extended to history of past transmissions - often not feasible to extend to long histories and large data - overhead: sender may not be able to store total history - robustness: receiver data may get corrupted - does not apply when sender has never seen data at receiver</p>
    <p>Two Standard Techniques:</p>
  </div>
  <div class="page">
    <p>file synchronization  delta compression  database reconciliation (for structured data or blocks/pages)  substring caching techniques (Spring/Wetherall, LBFS, VB Cach.)</p>
    <p>Types of Techniques:</p>
    <p>Major Applications:  data mirroring (web servers, accounts, collections)  web access acceleration (NetZero, AOL 9.0, Sprint Vision)  handheld synchronization (Minsky/Trachtenberg/Zippel)  link compression (Spring/Wetherall, pivia)  content distribution networks  software updates (cellphone software upgrades: DoOnGo, others)</p>
  </div>
  <div class="page">
    <p>the rsync algorithm  performance of rsync  theoretical results - file similarity measures - theoretical bounds (Orlitsky 1984, Cormode et al, Orlitsky/Viswanathan)</p>
    <p>improved practical algorithms - Cormode et al (Soda 2000) - Orlitsky/Viswanathan (ISIT 2001) - Langford (2001)</p>
    <p>File Synchronization: State of the Art</p>
  </div>
  <div class="page">
    <p>Server Client</p>
    <p>encoded file f_new f_old</p>
    <p>hashes</p>
    <p>clients splits f_old into blocks of size b  compute a hash value for each block and send to server  server stores received hashes in dictionary</p>
    <p>server transmits f_new to client, but replaces any b-byte window that hashes to value in dictionary by reference</p>
    <p>The rsync Algorithm</p>
  </div>
  <div class="page">
    <p>The rsync Algorithm (cont.)</p>
    <p>simple, widely used, single roundtrip  optimizations: 4-byte rolling hash + 2-byte MD5, gzip for literals  choice of block size problematic (default: max{700, sqrt(n)} )  not good in theory: granularity of changes, sqrt(n lg n) lower bound</p>
  </div>
  <div class="page">
    <p>Some performance numbers for rsync</p>
    <p>rsync vs. gzip, vcdiff, xdelta, zdelta  files: gnu and emacs versions (also used by Hunt/Vo/Tichy)</p>
    <p>gcc size emacs size total 27288 27326 gzip 7479 8191 xdelta 461 2131 vcdiff 289 1821 zdelta 227 1431 rsync 964 4451</p>
    <p>Compressed size in KB (slightly outdated numbers)</p>
    <p>factor of 3-5 gap between rsync and delta !</p>
  </div>
  <div class="page">
    <p>Hamming distance - inserting one character gives large distance</p>
    <p>Levenshtein distance - insertion, deletion, change of single characters</p>
    <p>Edit distance with block moves - also allows blocks of data to be moved around</p>
    <p>Edit distance with block copies - allows copies, and deletions of repeated blocks</p>
    <p>Formal File Similarity Measures:</p>
    <p>Theoretical Results:</p>
  </div>
  <div class="page">
    <p>Suppose files of length n with distance k</p>
    <p>lower bound Omega(k lg n) bits (even for Hamming)</p>
    <p>general framework with asymptotically optimal upper bounds for many cases (Orlitsky 1994)</p>
    <p>one message not optimal, but three messages are  impractical: decoding takes exponential time!  recent practical algos with provable bounds: - Orlitsky/Viswanathan 2001, Cormode et al 2000, Langford 2001</p>
    <p>- lg n factor from optimal for measures without block copies</p>
    <p>- not fully implemented and optimized</p>
    <p>Some Known Bounds</p>
  </div>
  <div class="page">
    <p>How to choose block size in rsync ?  rsync has cost at least O(sqrt(n lg n))  Idea: recursive splitting of blocks Orlitsky/Viswanathan, Langford, Cormode et al.</p>
    <p>Upper bound O(k lg^2(n)) (does not work for block copies)</p>
    <p>This paper: - optimized practical multi-round protocols - several new techniques with significant benefits</p>
    <p>Ongoing work: - show optimal upper bounds - new approaches for single- and two-round protocols - applications</p>
    <p>Recursive approach to improve on rsync</p>
  </div>
  <div class="page">
    <p>server sends hash values to client (unlike in rsync)  client checks if it has a matching block if yes: client understands the hash  server recursively splits blocks that do not find match</p>
    <p>New Techniques in zsync  use of optimized delta compressor: zdelta  decomposable hash functions - after sending parent hash, need to only send one child hash</p>
    <p>continuation hashes to extend matches to left and right  optimized match verification</p>
    <p>several other minor optimizations</p>
    <p>Our Contribution: zsync</p>
  </div>
  <div class="page">
    <p>Our Framework: zsync</p>
    <p>multiple roundtrips (starting with 32K block size, down to 8 bytes)  servers sends hashes, client confirms and verifies  both parties maintain a map of the new file  at end, server sends unresolved parts (delta-compressed)</p>
  </div>
  <div class="page">
    <p>decomposable hash functions - after sending parent hash, need to only send one child hash</p>
    <p>Techniques and Ideas:</p>
    <p>continuation hashes - to extend hashes to left or right using fewer bits - standard hashes expensive: are compared to all shifts in f_old - continuation hashes: only compared to one position</p>
    <p>is this a continuation of an adjacent larger hash?</p>
    <p>- might give improved asymptotic bounds</p>
    <p>h(parent)</p>
    <p>h(left) h(right)</p>
    <p>decomposable: h(right) = f ( h(parent), h(left) )</p>
  </div>
  <div class="page">
    <p>Techniques and Ideas:</p>
    <p>optimized match verification - server sends a fairly weak hash to find likely matches - client sends one verification hash for several likely matches - closely related to group testing techniques and binary search with errors</p>
    <p>server: heres a hash. Look for something that matches</p>
    <p>client: how about this one? Here are 2 additional bits</p>
    <p>server: looks good, but I need a few more bits to be sure</p>
    <p>client: OK, but to save bits, I will send a joint hash for 8 matches to check if all of them are correct</p>
    <p>server: apparently not all 8 are correct. Lets back off</p>
    <p>client: OK, how about this joint hash for two matches?</p>
  </div>
  <div class="page">
    <p>An example:</p>
  </div>
  <div class="page">
    <p>Experimental evaluation with decomposable hash</p>
    <p>gcc data set  initial block size 32K, variable minimum block size</p>
  </div>
  <div class="page">
    <p>Basic results for emacs data set</p>
  </div>
  <div class="page">
    <p>Continuation hashes</p>
  </div>
  <div class="page">
    <p>Optimized Match Verification</p>
  </div>
  <div class="page">
    <p>Best Numbers</p>
    <p>&gt; 50 roundtrips  but within 1-2% with about 10 roundtrips  not per-file roundtrips!</p>
  </div>
  <div class="page">
    <p>Results for Web Repository Synchronization</p>
    <p>updating web pages to new version  for search/mining or subscription</p>
  </div>
  <div class="page">
    <p>Building zsync tool and library</p>
    <p>Improving asymptotic bounds - continuation hashes seem to do the trick</p>
    <p>New approaches to file synchronization</p>
    <p>Applications</p>
    <p>Substring caching, record-based data, and other techniques</p>
    <p>Current and Future Work</p>
  </div>
</Presentation>
