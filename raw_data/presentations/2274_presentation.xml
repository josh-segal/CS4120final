<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Neural Trees Using Neural Networks as an Alternative to Binary Comparison in Classical Search Trees</p>
    <p>Douglas Santry</p>
  </div>
  <div class="page">
    <p>Introduction  Finding Stuff  Binary Comparison  The &lt; operator has been directly supported by even the earliest digital computers.  Binary search was used from the beginning, but binary search as we know it is the result of Lehmers paper</p>
    <p>in 1960.  Nlog2(N) search is provably optimal for binary comparison.</p>
    <p>B Tree  Described by Bayer in 1970.  Provably optimal with respect to the number of comparisons.  The parameter, B, trades off memory for number media accesses.  Countless permutations.</p>
    <p>B spectrum  Today we think in terms of the B spectrum:  trades off between updates and searches.  The spectrum is the direct result of the binary comparison operator: it totally determines the physical</p>
    <p>structure of the tree.</p>
  </div>
  <div class="page">
    <p>Alternatives to Binary Comparison Are Not New  Learned Indices (Kraska et al, SIGMOD 2018)  Data are considered as a cumulative distribution: indexi+1 = NiCDFi(key)  A tree with a neural network root, linear regression between root and leaves.  Binary comparison is used in the leaves.</p>
    <p>Interpolation Search (Van Sandt et al, SIGMOD 2019)  Originally proposed in 1957 by Petersen.  Search based on linear regression.</p>
    <p>Operates on in-memory contiguous arrays of sorted data.  Binary comparison determines physical layout.  Insertion requires memcpy() of everything in front.</p>
    <p>Not appropriate for secondary storage as indexi requires indirection.</p>
    <p>Learn the data directly (distribution)</p>
    <p>Read-only</p>
  </div>
  <div class="page">
    <p>Key Technical Contributions  Supports secondary storage.  Discards requirement for contiguous sorted array in memory.  Indexing secondary storage is not a CDF, it is a mapping.</p>
    <p>Neural networks inherently include the indirection required for secondary storage.  Linear interpolation requires a mapping from logical index to physical address.</p>
    <p>Employs many tiny neural networks that are quick to train.  Training is in the write path.</p>
    <p>Straddles classical search trees by learning paths, not the data directly.  There are fewer paths than data.  Paths are relatively static compared to data.  Neural networks are more like network routers.</p>
    <p>Addresses inference error  Inferencing mistakes are expensive in a secondary storage index: superfluous reads.</p>
  </div>
  <div class="page">
    <p>Single Layer Perceptrons (SLP)</p>
    <p>v</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Neural Tree Architecture</p>
    <p>Model Blocks</p>
    <p>Leaf Blocks</p>
    <p>Logical Structure Physical Structure</p>
  </div>
  <div class="page">
    <p>Neural Tree Architecture</p>
    <p>Model Blocks</p>
    <p>Leaf Blocks</p>
    <p>Logical Structure Physical Structure</p>
  </div>
  <div class="page">
    <p>Neural Tree Architecture</p>
    <p>Model Blocks</p>
    <p>Leaf Blocks</p>
    <p>Logical Structure Physical Structure</p>
  </div>
  <div class="page">
    <p>Overflow</p>
  </div>
  <div class="page">
    <p>Learning on Write (LoW)</p>
    <p>LoW</p>
  </div>
  <div class="page">
    <p>Learning on Write (LoW)</p>
    <p>Cost of training</p>
    <p>amortized over all</p>
    <p>future inserts in sub-tree</p>
  </div>
  <div class="page">
    <p>Neural Tree Media Access Tuning</p>
    <p>Model Blocks</p>
    <p>Leaf Blocks</p>
    <p>Logical Structure Physical Structure</p>
  </div>
  <div class="page">
    <p>Neural Tree Media Access Tuning: Swap Models</p>
    <p>Model Blocks</p>
    <p>Leaf Blocks</p>
    <p>Logical Structure Physical Structure</p>
  </div>
  <div class="page">
    <p>Neural Tree Media Access Tuning: Short Circuit Models</p>
    <p>Model Blocks</p>
    <p>Leaf Blocks</p>
    <p>Logical Structure Physical Structure</p>
  </div>
  <div class="page">
    <p>Neural Tree Models  SLP neural networks so the number of weights is 3N + 1  C float  Cacheline efficient  Information density higher per byte, 4k pages yield fan-out: 723 vs. 500</p>
    <p>Neural networks have ranges and domains of [-1, 1].</p>
    <p>The keys and values of an index are arbitrary: they can be anything.  The first job is to turn the key in to a number between [-1, 1]: a(key)  The inference needs to be something useful (address of next model or leaf): b(y)</p>
    <p>Recursive: fi(a(key); wi)  y, fi+1 = b(y)   and  are the &quot;secret sauce&quot; of a NT implementation.</p>
    <p>Training set constructed as: { &lt;x1, &quot; #&quot;(address1)&gt;, , &lt;xN, $</p>
    <p>#&quot;(addressN)&gt; }</p>
    <p>() maps a key to a known good value, a value from the training set.  The guarantee: a (any key)  { x1,  xN}  Thus y will also be in the training set, by design.  This means that error is totally controlled in the training process.</p>
  </div>
  <div class="page">
    <p>Evaluation: Insertion as a Function of Height</p>
  </div>
  <div class="page">
    <p>Evaluation: Effects of Population on Insertion Times</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>The secret sauce of a Neural Tree implementation is the pair of functions, (a, b). Many are possible. Neural Trees let us control the data layout. Are there richer ways?  Temporal: organize data according to creation or access time?  Ontological, what would annotated data look?</p>
    <p>Neural Trees can also mimic other data structures. For example, geometric applications often use R-Trees and Hilbert Trees. Can a Neural Tree can be used to mimic both?  Moreover, once a Neural Tree has been implemented, can new behaviors simply be programmed with a new a()? This should be far easier (and cheaper) than implementing an entire new data structure.</p>
    <p>What would a file system based on Neural Trees look like?  Directory search could be so much richer than lexical matching.</p>
    <p>What do snapshots look like in a LoW world?  CoW on the physical structure would work, but can something be done with LoW in the logical</p>
    <p>representation? 18</p>
  </div>
  <div class="page">
    <p>Thank you.</p>
  </div>
</Presentation>
