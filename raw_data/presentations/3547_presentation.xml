<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Laplacian Spectrum Learning</p>
    <p>Pannaga Shivaswamy and Tony Jebara Columbia University</p>
    <p>September 23, 2010</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Learning with Graphs</p>
    <p>google</p>
    <p>mapquest</p>
    <p>ebay</p>
    <p>yahoo</p>
    <p>weather</p>
    <p>american idol</p>
    <p>walmart</p>
    <p>home depot</p>
    <p>target</p>
    <p>southwest airlines</p>
    <p>ask jeeves</p>
    <p>sears</p>
    <p>lowes</p>
    <p>travelocity</p>
    <p>best buy</p>
    <p>goggle bank of america</p>
    <p>american airlines</p>
    <p>orbitz verizon</p>
    <p>cingularticketmaster</p>
    <p>hotmail</p>
    <p>united airlines</p>
    <p>bed bath and beyond</p>
    <p>nascar</p>
    <p>yahoo mail</p>
    <p>kmart</p>
    <p>paypal</p>
    <p>billing</p>
    <p>kohls staples</p>
    <p>e-bay</p>
    <p>screen names</p>
    <p>sams club</p>
    <p>jcpenney</p>
    <p>southwest.com</p>
    <p>northwest airlines</p>
    <p>travelocity.com</p>
    <p>aa.com</p>
    <p>Many learning problems are on graphs</p>
    <p>E.g. given labels of some nodes, predict others</p>
    <p>WWW, social networks, communication, trade networks. . .</p>
  </div>
  <div class="page">
    <p>Graph Laplacian in Machine Learning</p>
    <p>x1 x2</p>
    <p>x3</p>
    <p>x4x5</p>
    <p>x6</p>
    <p>x1 x2</p>
    <p>x3</p>
    <p>x4x5</p>
    <p>x6</p>
    <p>Data Weighted Graph</p>
    <p>Many machine learning methods convert data into a graph</p>
    <p>Given inputs x1, . . . , xl , . . . , xn and labels y = (y1 . . . yl )</p>
    <p>Compute a graph as weighted adjacency matrix W  Rnn, e.g. Wij = k(xi , xj ) or W = k-nearest-neighbors</p>
    <p>The Laplacian is L = D  W where Dij = ij</p>
    <p>k Wik</p>
  </div>
  <div class="page">
    <p>Graph Laplacian in Machine Learning</p>
    <p>Clustering</p>
    <p>Spectral cut (Donath &amp; Hoffman 73) Normalized cut (Shi &amp; Malik 00) Spectral clustering (Ng, Jordan &amp; Weiss 01) Spectral graph partitioning (Joachims 03)</p>
    <p>Regularization and Diffusion</p>
    <p>Kernel PCA (Scholkopf et al. 98) Diffusion kernels (Kondor &amp; Lafferty 02) Laplacian regularization (Belkin &amp; Niyogi 02) Graph regularization (Smola &amp; Kondor 03)</p>
    <p>Semi-supervised learning</p>
    <p>Spectral graph transducer (Joachims 03) Gaussian fields and harmonic functions (Zhu et al. 03) Local and global consistency (Zhou et al. 04) Laplacian support vector machines (Belkin et al. 06) Graph transduction (Wang et al. 08)</p>
  </div>
  <div class="page">
    <p>Laplacian Regularization Properties</p>
    <p>The Laplacian L = D  W regularizes functions on the graph. Function values on adjacent nodes cant be too different</p>
    <p>f  Lf =</p>
    <p>i &lt;j</p>
    <p>Wij (f(i )  f(j)) 2</p>
    <p>Eigendecomposition is L = n</p>
    <p>i =1 i i   i</p>
    <p>where i  i +1 Laplacians bottom eigenvectors are smooth over the graph</p>
    <p>i &lt;j</p>
    <p>Wij ((i )  (j)) 2 = L =</p>
    <p>A graph kernel is built from the bottom q eigenvectors with transformed eigenvalues (Smola &amp; Kondor 03)</p>
    <p>K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>r (i )i   i</p>
  </div>
  <div class="page">
    <p>Graph Kernels as Transformed Spectra: Diffusion</p>
    <p>To get a kernel from Laplacian, try various r () functions</p>
    <p>K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>r (i )i   i</p>
    <p>Example: diffusion r () = exp(/2)</p>
    <p>W L K=4 K=2 K=1 K=1/2 K=1/4</p>
  </div>
  <div class="page">
    <p>Graph Kernels as Transformed Spectra: Diffusion</p>
    <p>To get a kernel from Laplacian, try various r () functions</p>
    <p>K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>r (i )i   i</p>
    <p>Example: diffusion r () = exp(/2)</p>
    <p>Example: kernel pca r () = 1  I( &lt; 2)</p>
    <p>Example: Gaussian field kernel r () = (2 + )1</p>
    <p>Any monotonically decreasing r () (Smola &amp; Kondor 03)</p>
    <p>Searching over functions is tedious, can we learn r ()?</p>
  </div>
  <div class="page">
    <p>Graph Kernels via Spectrum Learning</p>
    <p>Try max-margin multi-kernel learning (Lanckriet et al. 02)</p>
    <p>max margin s.t. K =</p>
    <p>{</p>
    <p>K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  0</p>
    <p>}</p>
    <p>Problem: does not enforce spectral monotonicity</p>
  </div>
  <div class="page">
    <p>Graph Kernels via Spectrum Learning</p>
    <p>Try max-margin multi-kernel learning (Lanckriet et al. 02)</p>
    <p>max margin s.t. K =</p>
    <p>{</p>
    <p>K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  0</p>
    <p>}</p>
    <p>Problem: does not enforce spectral monotonicity</p>
    <p>Try kernel target alignment with monotonicity (Zhu et al. 04)</p>
    <p>max</p>
    <p>A(K, yy) s.t. K  K, i  i +1</p>
    <p>Problem: not max-margin, classifier is learned separately</p>
  </div>
  <div class="page">
    <p>Graph Kernels via Spectrum Learning</p>
    <p>Try max-margin multi-kernel learning (Lanckriet et al. 02)</p>
    <p>max margin s.t. K =</p>
    <p>{</p>
    <p>K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  0</p>
    <p>}</p>
    <p>Problem: does not enforce spectral monotonicity</p>
    <p>Try kernel target alignment with monotonicity (Zhu et al. 04)</p>
    <p>max</p>
    <p>A(K, yy) s.t. K  K, i  i +1</p>
    <p>Problem: not max-margin, classifier is learned separately</p>
    <p>Try max margin with spectral monotonicity (Xu et al. 07)</p>
    <p>max margin s.t. K  K, i  i +1</p>
    <p>Problem: flawed convex program! Lets fix this...</p>
  </div>
  <div class="page">
    <p>Absolute Margin and Relative Margin</p>
    <p>Instead of just max margin, consider a strict generalization...</p>
    <p>min w,b</p>
    <p>i</p>
    <p>i s.t. yi (w xi + b)  1  i</p>
  </div>
  <div class="page">
    <p>Absolute Margin and Relative Margin</p>
    <p>Instead of just max margin, consider a strict generalization...</p>
    <p>min w,b</p>
    <p>i</p>
    <p>i s.t. yi (w xi + b)  1  i , |w</p>
    <p>xi + b|  B</p>
    <p>Relative margin machine (RMM) primal problem (S &amp; J 08)</p>
  </div>
  <div class="page">
    <p>Absolute Margin and Relative Margin</p>
    <p>Instead of just max margin, consider a strict generalization...</p>
    <p>min w,b</p>
    <p>i</p>
    <p>i s.t. yi (w xi + b)  1  i , |w</p>
    <p>xi + b|  B</p>
    <p>Relative margin machine (RMM) primal problem (S &amp; J 08)</p>
    <p>Figure: In red is the usual max-margin support vector machine (B = ) solution and in green is the max-relative margin (B  1) solution.</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Start with relative margin machine dual problem (S &amp; J 08) Include monotonic optimization over the spectrum Swap min and max (Boyd &amp; Vandeberghe 04)</p>
    <p>max (,,)</p>
    <p>min</p>
    <p>max (,,)</p>
    <p>1  B(1 + 1)</p>
    <p>s.t.K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  i +1  0</p>
    <p>where we have defined  = diag(y)   +  and  :</p>
    <p>{</p>
    <p>y  1 + 1 = 0, , ,   0,   C 1 }</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Start with relative margin machine dual problem (S &amp; J 08) Include monotonic optimization over the spectrum Swap min and max (Boyd &amp; Vandeberghe 04)</p>
    <p>max (,,)</p>
    <p>min</p>
    <p>max (,,)</p>
    <p>1  B(1 + 1)  1</p>
    <p>s.t. K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  i +1  0</p>
    <p>where we have defined  = diag(y)   +  and  : y  1 + 1 = 0, , ,   0,   C 1</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Start with relative margin machine dual problem (S &amp; J 08) Include monotonic optimization over the spectrum Swap min and max (Boyd &amp; Vandeberghe 04)</p>
    <p>max (,,)</p>
    <p>min</p>
    <p>max (,,)</p>
    <p>1  B(1 + 1)  1</p>
    <p>s.t. K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  i +1  0</p>
    <p>where we have defined  = diag(y)   +  and  : y  1 + 1 = 0, , ,   0,   C 1</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Consider inner minimization: (slight  change)</p>
    <p>min</p>
    <p>1</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i</p>
    <p>i</p>
    <p>s.t. i  i +1  0, q  0, 1  0,</p>
    <p>q</p>
    <p>i =1</p>
    <p>i = 1</p>
    <p>Dual is:</p>
    <p>max 0,</p>
    <p>+</p>
    <p>q</p>
    <p>i =1</p>
    <p>i</p>
    <p>s.t.</p>
    <p>i</p>
    <p>j=2</p>
    <p>i  = i + i1 +</p>
    <p>s.t.</p>
    <p>i</p>
    <p>j=2</p>
    <p>1  = 1 +</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Consider inner minimization: (slight  change)</p>
    <p>min</p>
    <p>1</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i</p>
    <p>i</p>
    <p>s.t. i  i +1  0, q  0, 1  0,</p>
    <p>q</p>
    <p>i =1</p>
    <p>i = 1</p>
    <p>Dual is:</p>
    <p>max 0,</p>
    <p>+</p>
    <p>q</p>
    <p>i =1</p>
    <p>i</p>
    <p>s.t.</p>
    <p>i</p>
    <p>j=2</p>
    <p>j  = i +  (i  1)</p>
    <p>s.t.</p>
    <p>i</p>
    <p>j=2</p>
    <p>1  = 1 +</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Theorem: Given the following max-min problem,</p>
    <p>max (,,)</p>
    <p>min</p>
    <p>1  B(1 + 1)</p>
    <p>s.t. K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  i +1  0</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Spectrum Estimation</p>
    <p>Theorem: Given the following max-min problem,</p>
    <p>max (,,)</p>
    <p>min</p>
    <p>1  B(1 + 1)</p>
    <p>s.t. K =</p>
    <p>q</p>
    <p>i =1</p>
    <p>i i   i , tr(K) = 1, i  i +1  0</p>
    <p>a nearly equivalent optimization (for small ) is</p>
    <p>max (,,)</p>
    <p>max 0,</p>
    <p>1  B(1 + 1)   +</p>
    <p>q</p>
    <p>i =1</p>
    <p>i</p>
    <p>s.t.</p>
    <p>i</p>
    <p>j=2</p>
    <p>j   i +  (i  1) i = 2, . . . , q</p>
    <p>s.t. 1</p>
    <p>1   1 +  .</p>
  </div>
  <div class="page">
    <p>Maximum Relative Margin with Monotonic Transformation</p>
    <p>End up with this final QCQP which requires O(ql 3)</p>
    <p>max (,,)</p>
    <p>max 0,</p>
    <p>1  B(1 + 1)   +</p>
    <p>q</p>
    <p>i =1</p>
    <p>i</p>
    <p>s.t.</p>
    <p>i</p>
    <p>j=2</p>
    <p>j   i +  (i  1) i = 2, . . . , q</p>
    <p>s.t. 1</p>
    <p>1   1 +</p>
    <p>where  = diag(y)   +  and  :</p>
    <p>{</p>
    <p>y  1 + 1 = 0, , ,   0,   C 1 }</p>
    <p>.</p>
    <p>Setting B =  gives the maximum absolute margin solution</p>
    <p>STORM: Spectral Transformations that Optimize the Relative Margin</p>
    <p>STOAM: Spectral Transformations that Optimize the Absolute Margin</p>
  </div>
  <div class="page">
    <p>Experiments - Methods</p>
    <p>Use a semi-supervised learning experimental setting</p>
    <p>The number of labeled examples is varied from 30 to 110</p>
    <p>The remaining examples used as unlabeled data</p>
    <p>The following algorithms were compared:</p>
    <p>Xu07 Flawed version of STOAM (Xu et al. 07)</p>
    <p>MKL-S (Lanckriet et al. 02) with max margin</p>
    <p>MKL-R (Lanckriet et al. 02) with max relative margin</p>
    <p>SGT Spectral graph transducer (Joachims 03)</p>
    <p>KTA-S (Zhu et al. 04) followed by an SVM</p>
    <p>KTA-R (Zhu et al. 04) followed by an RMM</p>
    <p>STOAM Spectral transformations that optimize absolute margin</p>
    <p>STORM Spectral transformations that optimize relative margin</p>
  </div>
  <div class="page">
    <p>Experiments - Mean error rates on text datasets</p>
    <p>l Xu07 MKL-S MKL-R SGT KTA-S KTA-R STOAM STORM</p>
    <p>r-a 30 44.89 37.14 37.14 19.46 22.98 22.99 25.81 25.81</p>
    <p>w-m 30 46.98 22.74 22.74 41.88 16.03 16.08 14.26 14.26</p>
    <p>p-m 30 46.48 41.21 40.99 39.58 28.00 28.05 30.58 30.58</p>
    <p>b-h 30 47.04 4.35 4.35 3.95 3.91 3.80 3.90 3.87</p>
    <p>m-m 30 48.11 12.35 12.35 41.30 7.35 7.36 7.60 6.88</p>
    <p>Table: In each row, minimum error rate is in red. Algorithms whose performance is not significantly worse (at 5% significance) are in blue.</p>
  </div>
  <div class="page">
    <p>Experiments - Mean error rates on digit datasets</p>
    <p>l Xu07 MKL-S MKL-R SGT KTA-S KTA-R STOAM STORM</p>
    <p>Table: In each row, minimum error rate is in red. Algorithms whose performance is not significantly worse (at 5% significance) are in blue.</p>
  </div>
  <div class="page">
    <p>Experiments - Summary</p>
    <p>Adjacency matrix W was a 5-nearest-neighbor graph</p>
    <p>Hyperparameters set to  = 106 and q = 200 throughout</p>
    <p>Parameters C and B found via cross-validation</p>
    <p>Summary of MNIST digits and Newsgroups text results:</p>
    <p>Xu07 MKL-S MKL-R SGT KTA-S KTA-R STOAM STORM</p>
    <p>#best 0 1 5 9 5 2 8 22 #O(best) 0 1 2 4 8 12 16 13 #total 0 2 7 13 13 14 24 35</p>
    <p>Table: For each method, we enumerate the number of times it performed best, the number of times it was not significantly worse than best and the total number of times it was either best or not significantly worse.</p>
  </div>
  <div class="page">
    <p>Experiments - Spectrum Visualization on Text Data</p>
    <p>eigenvalue</p>
    <p>m a</p>
    <p>g n</p>
    <p>itu d</p>
    <p>e</p>
    <p>KTA STORM MKLR</p>
    <p>eigenvalue m</p>
    <p>a g</p>
    <p>n itu</p>
    <p>d e</p>
    <p>KTA STORM MKLR</p>
    <p>Figure: Magnitudes of the top eigenvalues recovered by the different algorithms for problems m-m and p-m. The plots show average eigenspectra over all experiments. KTA and STORM have monotonically decreasing spectra.</p>
  </div>
  <div class="page">
    <p>Experiments - Spectrum Visualization on Digits Data</p>
    <p>eigenvalue</p>
    <p>m a</p>
    <p>g n</p>
    <p>itu d</p>
    <p>e</p>
    <p>KTA STORM MKLR</p>
    <p>eigenvalue m</p>
    <p>a g</p>
    <p>n itu</p>
    <p>d e</p>
    <p>KTA STORM MKLR</p>
    <p>Figure: Magnitudes of the top eigenvalues recovered by the different algorithms for problems 1-2 and 3-8. The plots show average eigenspectra over all experiments. KTA and STORM have monotonically decreasing spectra.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Implemented general graph kernel semi-supervised learning</p>
    <p>Explore any monotonic transformation of Laplacian spectrum</p>
    <p>Simultaneously learn kernel and classifier</p>
    <p>Both optimized under large margin criterion</p>
    <p>Better yet, both optimized under relative margin criterion</p>
    <p>All computations are efficient convex programs</p>
    <p>Significantly outperforms other kernel-learning methods</p>
  </div>
</Presentation>
