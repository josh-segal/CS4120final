<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Data-Centric Reconfiguration with Network-Attached Disks</p>
    <p>Alex Shraer (Technion)</p>
    <p>Joint work with:</p>
    <p>J.P. Martin, D. Malkhi, M. K. Aguilera (MSR) I. Keidar (Technion)</p>
  </div>
  <div class="page">
    <p>Preview</p>
    <p>The setting: data-centric replicated storage  Simple network-attached storage-nodes</p>
    <p>Our contributions:</p>
    <p>Allows to add/remove storage-nodes dynamically</p>
  </div>
  <div class="page">
    <p>Enterprise Storage Systems</p>
    <p>Highly reliable customized hardware</p>
    <p>Controllers, I/O ports may become a bottleneck</p>
    <p>Expensive</p>
    <p>Usually not extensible  Different solutions for different scale  Example(HP): High end - XP (1152 disks), Mid range  EVA (324 disks)</p>
  </div>
  <div class="page">
    <p>Alternative  Distributed Storage</p>
    <p>Made up of many storage nodes  Unreliable, cheap hardware  Failures are the norm, not an exception</p>
    <p>Challenges:  Achieving reliability and consistency  Supporting reconfigurations</p>
  </div>
  <div class="page">
    <p>Distributed Storage Architecture</p>
    <p>Unpredictable network delays (asynchrony)</p>
    <p>Cloud Storage Cloud</p>
    <p>Storage</p>
    <p>LAN/ WAN</p>
    <p>read write</p>
    <p>Storage Clients Dynamic,</p>
    <p>Fault-prone</p>
    <p>Fault-prone Storage Nodes</p>
  </div>
  <div class="page">
    <p>A Case for Data-Centric Replication</p>
    <p>Client-side code runs replication logic  Communicates with multiple storage nodes</p>
    <p>Simple storage nodes (servers)  Can be network-attached disks</p>
    <p>Not necessarily PCs with disks  Do not run application-specific code  Less fault-prone components</p>
    <p>Simply respond to client requests  High throughput</p>
    <p>Do not communicate with each other If storage-nodes communicate, their failure is likely to be correlated! Oblivious to where other replicas of each object are stored Scalable, same storage node can be used for many replication sets</p>
    <p>not-so-thin client</p>
    <p>thin storage</p>
    <p>node</p>
  </div>
  <div class="page">
    <p>Real Systems Are Dynamic</p>
    <p>The challenge: maintain consistency , reliability, availability</p>
    <p>LAN/ WAN</p>
    <p>reconfig {A, B}</p>
    <p>A</p>
    <p>B C D</p>
    <p>E</p>
    <p>reconfig {C, +F,, +I}</p>
    <p>F</p>
    <p>G</p>
    <p>I</p>
    <p>H</p>
  </div>
  <div class="page">
    <p>Pitfall of Nave Reconfiguration</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D {A, B, C, D}</p>
    <p>{A, B, C, D} {A, B, C, D}</p>
    <p>{A, B, C, D}</p>
    <p>{A, B, C, D}</p>
    <p>{A, B, C, D}</p>
    <p>{A, B, C, D, E}</p>
    <p>{A, B, C}</p>
    <p>{A, B, C, D, E}</p>
    <p>{A, B, C, D, E}</p>
    <p>{A, B, C}</p>
    <p>{A, B, C}</p>
    <p>E</p>
    <p>delayed</p>
    <p>delayed de</p>
    <p>la ye</p>
    <p>d</p>
    <p>de la</p>
    <p>ye d</p>
    <p>reconfig {+E}</p>
    <p>reconfig {-D}</p>
    <p>{A, B, C, D, E}</p>
  </div>
  <div class="page">
    <p>Returns Italy!</p>
    <p>Pitfall of Nave Reconfiguration</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D {A, B, C, D, E}</p>
    <p>{A, B, C}</p>
    <p>{A, B, C, D, E}</p>
    <p>{A, B, C, D, E}</p>
    <p>{A, B, C}</p>
    <p>{A, B, C}</p>
    <p>E</p>
    <p>write x Spain</p>
    <p>read x</p>
    <p>{A, B, C, D, E}</p>
    <p>X = Italy, 1</p>
    <p>X = Italy, 1</p>
    <p>X = Spain, 2</p>
    <p>X = Spain, 2</p>
    <p>X = Spain, 2</p>
    <p>X = Italy, 1</p>
    <p>X = Italy, 1</p>
    <p>X = Italy, 1</p>
    <p>Split Brain!</p>
  </div>
  <div class="page">
    <p>Reconfiguration Option 1: Centralized</p>
    <p>Can be automatic  E.g., Ursa Minor [Abd-El-Malek et al., FAST 05]</p>
    <p>Downtime  Most solutions stop R/W while reconfiguring</p>
    <p>Single point of failure  What if manager crashes while changing the system?</p>
    <p>Tomorrow Technion servers will be down for maintenance from 5:30am to 6:45am</p>
    <p>Virtually Yours, Moshe Barak</p>
  </div>
  <div class="page">
    <p>Reconfiguration Option 2: Distributed Agreement</p>
    <p>Servers agree on next configuration  Previous solutions not data-centric</p>
    <p>No downtime  In theory, might never terminate [FLP85]</p>
    <p>In practice, we have partial synchrony so it usually works</p>
  </div>
  <div class="page">
    <p>Reconfiguration Option 3: DynaStore [Aguilera, Keidar, Malkhi, S., PODC09]</p>
    <p>Distributed &amp; completely asynchronous</p>
    <p>No downtime</p>
    <p>Always terminates</p>
    <p>Not data-centric</p>
  </div>
  <div class="page">
    <p>In this work: DynaDisk dynamic data-centric R/W storage</p>
  </div>
  <div class="page">
    <p>Location Service  Used in practice, ignored in theory</p>
    <p>We formalize the weak external service as an oracle:</p>
    <p>Not enough to solve reconfiguration</p>
    <p>oracle.query( ) returns some legal configuration</p>
    <p>If reconfigurations stop and oracle. query() invoked infinitely many times, it eventually returns last system configuration</p>
  </div>
  <div class="page">
    <p>The Coordination Module in DynaDisk</p>
    <p>Storage devices in a configuration conf = {+A, +B, +C}</p>
    <p>z x</p>
    <p>y next config:</p>
    <p>z x</p>
    <p>y next config:</p>
    <p>z x</p>
    <p>y next config:</p>
    <p>A B C</p>
    <p>Distributed R/W objects Updated similarly to ABD Distributed weak snapshot object</p>
    <p>API: update(set of changes)OK scan()  set of updates</p>
  </div>
  <div class="page">
    <p>Coordination with Consensus</p>
    <p>z x</p>
    <p>y next config:</p>
    <p>z x</p>
    <p>y next config:</p>
    <p>z x</p>
    <p>y next config:</p>
    <p>A B C</p>
    <p>reconfig({C}) reconfig({+D})</p>
    <p>Consensus +DC</p>
    <p>+D+D +D</p>
    <p>+D+D +D</p>
    <p>update :</p>
    <p>scan: read &amp; write-back next config from majority  every scan returns +D or</p>
  </div>
  <div class="page">
    <p>Weak Snapshot  Weaker than consensus  No need to agree on the next configuration, as long as</p>
    <p>each process has a set of possible next configurations,</p>
    <p>and all such sets intersect  Intersection allows to converge and again use a single config</p>
    <p>Non-empty intersection property of weak snapshot:  Every two non-empty sets returned by scan( ) intersect  Example: Client 1s scan Client 2s scan</p>
    <p>{+D} {+D}</p>
    <p>{C} {+D, C}</p>
    <p>{+D} {C}</p>
    <p>Consensus</p>
  </div>
  <div class="page">
    <p>Coordination without consensus</p>
    <p>z x</p>
    <p>y next config: z y next config: z y next config:</p>
    <p>A B C</p>
    <p>reconfig({C}) reconfig({+D})</p>
    <p>update :</p>
    <p>scan: read &amp; write-back proposals from majority (twice)</p>
    <p>CAS({C}, , 0)</p>
    <p>+D</p>
    <p>CAS({C}, , 1)+D</p>
    <p>C</p>
    <p>WRITE ({C}, 0)OK OK</p>
  </div>
  <div class="page">
    <p>Tracking Evolving Configs</p>
    <p>With consensus: agree on next configuration</p>
    <p>Without consensus  usually a chain, sometimes a DAG:</p>
    <p>A, B, C A,B,C,D +D  C</p>
    <p>A,B</p>
    <p>A, B, D</p>
    <p>A, B, C</p>
    <p>+D</p>
    <p>+D  C</p>
    <p>C</p>
    <p>A,B,C,D</p>
    <p>A, B, D</p>
    <p>Inconsistent updates found</p>
    <p>and merged</p>
    <p>weak snapshot</p>
    <p>scan() returns {+D, -C}</p>
    <p>scan() returns {+D}</p>
    <p>All non-empty scans intersect</p>
  </div>
  <div class="page">
    <p>Consensus-based VS. Asynch. Coordination</p>
    <p>Two implementations of weak snapshots  Asynchronous  Partially synchronous (consensus-based)</p>
    <p>Active Disk Paxos [Chockler, Malkhi, 2005]  Exponential backoff for leader-election</p>
    <p>Unlike asynchronous coordination, consensus-based might not terminate [FLP85]</p>
    <p>Storage overhead  Asynchronous: vector of updates</p>
    <p>vector size  min(#reconfigs, #members in config)</p>
    <p>Consensus-based: 4 integers and the chosen update  Per storage device and configuration</p>
  </div>
  <div class="page">
    <p>Strong progress guarantees are not for free</p>
    <p>Consensus-based</p>
    <p>Asynchronous (no consensus)</p>
    <p>ms.</p>
    <p>Number of simultaneous reconfig operations</p>
    <p>Average write latency</p>
    <p>ms.</p>
    <p>Average reconfig latency</p>
    <p>Number of simultaneous reconfig operations</p>
    <p>Significant negative</p>
    <p>effect on R/W latency</p>
    <p>Slightly better,much more predictable</p>
    <p>reconfig latency when many reconfig execute</p>
    <p>simultaneously</p>
    <p>The same when no</p>
    <p>reconfigurations21</p>
  </div>
  <div class="page">
    <p>Future &amp; Ongoing Work</p>
    <p>Combine asynch. and partially-synch. coordination</p>
    <p>Consider other weak snapshot implementations  E.g., using randomized consensus</p>
    <p>Use weak snapshots to reconfigure other services  Not just for R/W</p>
  </div>
  <div class="page">
    <p>Summary  DynaDisk  dynamic data-centric R/W storage</p>
    <p>First decentralized solution  No downtime  Supports many objects, provides incremental reconfig  Uses one coordination object per config. (not per object)  Tunable reconfiguration method</p>
    <p>We implemented asynchronous and consensus-based  Many other implementations of weak-snapshots possible</p>
    <p>Asynchronous coordination in practice:  Works in more circumstances  more robust  But, at a cost  significantly affects ongoing R/W ops</p>
  </div>
</Presentation>
