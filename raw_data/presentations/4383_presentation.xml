<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy</p>
    <p>Learning Baolin Peng#, Xiujun Li*, Jianfeng Gao*, Jingjing Liu*, Kam-Fai Wong#, Shang-Yu Su$</p>
    <p>*Microsoft Research #The Chinese University of Hong Kong</p>
    <p>$National Taiwan University ACL 2018</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Task-completion dialogue as optimal decision making  Reinforcement learning using real or simulated experience  Deep Dyna-Q  Evaluation methodology  Simulated user evaluation  Human-in-the-loop evaluation  Conclusion</p>
  </div>
  <div class="page">
    <p>An Example Dialogue with Movie-Bot</p>
    <p>Source code available at https://github/com/MiuLab/TC-Bot</p>
    <p>Actual dialogues can be more complex:  Speech/Natural language understanding errors</p>
    <p>o Input may be spoken language form o Need to reason under uncertainty</p>
    <p>Constraint violation o Revise information collected earlier</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Task-oriented, slot-filling, Dialogues  Domain: movie, restaurant, flight,</p>
    <p>Slot: information to be filled in before completing a task o For Movie-Bot: movie-name, theater, number-of-tickets, price,</p>
    <p>Intent (dialogue act): o Inspired by speech act theory (communication as action)</p>
    <p>request, confirm, inform, thank-you,  o Some may take parameters:</p>
    <p>thank-you(), request(price), inform(price=$10)</p>
    <p>&quot;Is Kungfu Panda the movie you are looking for?&quot;</p>
    <p>confirm(moviename=kungfu panda) 4</p>
  </div>
  <div class="page">
    <p>A Multi-turn Task-oriented Dialogue Architecture</p>
    <p>(Spoken) Language Understanding (LU)</p>
    <p>State Tracking</p>
    <p>Dialog Policy Natural Language</p>
    <p>Generation / Synthesis</p>
    <p>Find me a Bill Murray movie</p>
    <p>Request(movie; actor=bill murray)</p>
    <p>Dialog Manager (DM)</p>
    <p>Request (release_year)</p>
    <p>When was it released</p>
    <p>Knowledge Base</p>
  </div>
  <div class="page">
    <p>A unified view: dialogue as optimal decision making</p>
    <p>Dialogue State (s) Action (a) Reward (r)</p>
    <p>Info Bots (Q&amp;A bot over KB, Web etc.)</p>
    <p>Understanding of user Intent (belief state)</p>
    <p>Clarification questions, Answers</p>
    <p>Relevance of answer # of turns</p>
    <p>Task Completion Bots (Movies, Restaurants, )</p>
    <p>Understanding of user goal (belief state)</p>
    <p>Dialog act + slot_value Task success rate # of turns</p>
    <p>Social Bot (XiaoIce)</p>
    <p>Conversation history Response Engagement</p>
    <p>Dialogue as a Markov Decision Process (MDP)  Given state , select action  according to (hierarchical) policy   Receive reward , observe new state   Continue the cycle until the episode terminates.</p>
    <p>Goal of dialogue learning: find optimal  to maximize expected rewards</p>
  </div>
  <div class="page">
    <p>Task-completion dialogue as RL</p>
    <p>semanticraw</p>
    <p>Pioneered by [Levin+ 00] Other early examples: [Singh+ 02; Pietquin+ 04; Williams&amp;Young 07; etc.]</p>
    <p>Observation and action o Raw representation</p>
    <p>(utterances in natural language form) o Semantic representation</p>
    <p>(intent-slot-value form)</p>
    <p>Reward o +10 upon successful termination o -10 upon unsuccessful termination o -1 per turn o</p>
  </div>
  <div class="page">
    <p>RL vs. SL (supervised learning) Differences from supervised learning  Learn by trial-and-error (experimenting)  Need efficient exploration</p>
    <p>Optimize long-term reward (1 + 2 + )  Need temporal credit assignment</p>
    <p>Similarities to supervised learning  Generalization and representation  Hierarchical problem solving true label</p>
    <p>input/feature</p>
    <p>SL</p>
    <p>teacher</p>
    <p>reward &amp; next-observation/state</p>
    <p>RL</p>
    <p>world</p>
    <p>action</p>
  </div>
  <div class="page">
    <p>Human-Human conversation data</p>
    <p>Dialog agent</p>
    <p>real experience</p>
    <p>Supervised/imitati on learning</p>
    <p>Acting RL</p>
    <p>- Expensive: need large amounts of real experience except for very simple tasks</p>
    <p>- Risky: bad experiences (during exploration) drive users away</p>
    <p>Learning w/ real users</p>
  </div>
  <div class="page">
    <p>Human-Human conversation data</p>
    <p>Dialog agent simulated experience</p>
    <p>Supervised/imitati on learning</p>
    <p>Acting</p>
    <p>RL</p>
    <p>- Inexpensive: generate large amounts of simulated experience for free</p>
    <p>- Overfitting: discrepancy btw real users and simulators</p>
    <p>Learning w/ user simulators</p>
  </div>
  <div class="page">
    <p>Dyna-Q: integrating planning and learning [Sutton+ 90]</p>
    <p>combining model-free and model-based RL</p>
    <p>tabular methods and linear function approximation</p>
    <p>direct reinforcement learning  (world) model learning  planning/search control</p>
  </div>
  <div class="page">
    <p>Human-Human conversation data</p>
    <p>World model (simulated user)</p>
    <p>Supervised/imitation learning</p>
    <p>Model learning</p>
    <p>Planning</p>
    <p>Acting Direct RL</p>
    <p>real experience (limited)</p>
    <p>DDQ  Based on Dyna-Q  Policy as DNN, trained using DQN  Apply to dialogue: simulated user as world model</p>
    <p>Dialogued agent trained using  Limited real user experience  Large amounts of simulated experience</p>
    <p>Limited real experience is used to improve  Dialog agent  World model (simulated user)</p>
    <p>Deep Dyna-Q (DDQ): Integrating Planning for Dialogue Policy Learning</p>
  </div>
  <div class="page">
    <p>Task-completion DDQ dialogue agent</p>
  </div>
  <div class="page">
    <p>The world model architecture</p>
    <p>Multi task MLP  Reward   User action</p>
    <p>Termination</p>
  </div>
  <div class="page">
    <p>Dialogue System Evaluation</p>
    <p>Metrics: what numbers matter? o Success rate: #Successful_Dialogues / #All_Dialogues o Average turns: average number of turns in a dialogue o User satisfaction o Consistency, diversity, engaging, ... o Latency, backend retrieval cost,</p>
    <p>Methodology: how to measure those numbers?</p>
  </div>
  <div class="page">
    <p>Evaluation methodology</p>
    <p>Lab user subjects</p>
    <p>Actual users</p>
    <p>Simulated users</p>
    <p>Truthfulness</p>
    <p>Scalability</p>
    <p>Flexibility</p>
    <p>Expense</p>
    <p>Risk</p>
    <p>A Hybrid Approach</p>
    <p>User Simulation</p>
    <p>Small-scale Human Evaluation (lab, Mechanical Turk, )</p>
    <p>Large-scale Deployment (optionally with continuing</p>
    <p>incremental refinement)</p>
  </div>
  <div class="page">
    <p>A Simulator for E2E Neural Dialogue System [Li+ 17]</p>
  </div>
  <div class="page">
    <p>Agenda-based Simulated User [Schatzmann &amp; Young 09]</p>
    <p>User state consists of (agenda, goal); goal is fixed throughout dialogue  Agenda is maintained (stochastically) by a first-in-last-out stack</p>
    <p>Implementation of a simplified user simulator: https://github.com/MiuLab/TC-Bot 18</p>
  </div>
  <div class="page">
    <p>Simulated user evaluation</p>
    <p>DQN vs DDQ () - : number of planning steps</p>
    <p>(generating K simulated dialogues per real dialogue)</p>
    <p>-  = 2</p>
  </div>
  <div class="page">
    <p>Simulated user evaluation</p>
    <p>DQN vs DDQ () - : number of planning steps</p>
    <p>(generating K simulated dialogues per real dialogue)</p>
    <p>-  = 2, 5, 10, 20</p>
  </div>
  <div class="page">
    <p>Impact of world model quality  DQN(10):</p>
    <p>perfect world model</p>
  </div>
  <div class="page">
    <p>Impact of world model quality  DQN(10)</p>
    <p>perfect world model  DDQ(10):</p>
    <p>pretrained on labeled data, and updated using real dialogue on the fly</p>
  </div>
  <div class="page">
    <p>Impact of world model quality  DQN(10)</p>
    <p>perfect world model  DDQ(10):</p>
    <p>pretrained on labeled data, and updated using real dialogue on the fly</p>
    <p>DDQ(10, rand-init):  pretrained on labeled data, and</p>
    <p>updated using real dialogue on the fly  DDQ(10, fixed):</p>
    <p>pretrained on labeled data, and updated using real dialogue on the fly</p>
  </div>
  <div class="page">
    <p>Human-in-the-loop experiments - learning dialogue via interacting with real users</p>
    <p>DDQ agents significantly outperforms the DQN agent</p>
    <p>A larger  leads to more aggressive planning and better results</p>
    <p>Pre-training world model with human conversational data improves the learning efficiency and the agents performance</p>
  </div>
  <div class="page">
    <p>Conclusion and Future Work</p>
    <p>Deep Dyna-Q: integrating planning for dialogue policy learning - Improves learning efficiency - Make the best use of limited real user experiences</p>
    <p>Future research - Learning when to switch between real and simulated users - Exploration in planning</p>
    <p>- Exploration: trying actions to improve the world model - Exploitation: trying to behave in the optimal way given the current world model</p>
  </div>
  <div class="page">
    <p>Microsoft Dialogue Challenge at SLT-2018</p>
    <p>07/16/2018: Registration is now open.  Task: build E2E task-completion dialogue systems  Data: labeled human conversations in 3 domains  Experiment platform with built-in user simulators for training and</p>
    <p>evaluation  Final evaluation in simulated setting and by human judges</p>
    <p>More information: https://github.com/xiul-msr/e2e_dialog_challenge</p>
  </div>
</Presentation>
