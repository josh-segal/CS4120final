<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Host-side Filesystem Journaling for Durable Shared Storage</p>
    <p>Andromachi Hatzieleftheriou, Stergios V. Anastasiadis Department of Computer Science &amp; Engineering</p>
    <p>University of Ioannina, Greece</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Design</p>
    <p>Implementation</p>
    <p>Evaluation</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Datacenter Storage</p>
    <p>Multi-tier distributed systems on clusters of commodity servers and disk drives</p>
    <p>client-side frontend  caching layer  backend storage</p>
    <p>A. Hatzieleftheriou 3</p>
    <p>Apps Client</p>
    <p>Backend Servers</p>
    <p>Network</p>
    <p>Apps Client</p>
    <p>Apps Client</p>
    <p>Caching Layer</p>
  </div>
  <div class="page">
    <p>Datacenter Storage</p>
    <p>Multi-tier distributed systems on clusters of commodity servers and disk drives</p>
    <p>client-side frontend  caching layer  backend storage</p>
    <p>A. Hatzieleftheriou 4</p>
    <p>Apps Client</p>
    <p>Backend Servers</p>
    <p>Network</p>
    <p>Apps Client</p>
    <p>Apps Client</p>
    <p>Frontend (virtualized or bare-metal)</p>
    <p>Caching Layer</p>
  </div>
  <div class="page">
    <p>Datacenter Storage</p>
    <p>Multi-tier distributed systems on clusters of commodity servers and disk drives</p>
    <p>client-side frontend  caching layer  backend storage</p>
    <p>Frontend tier: client-side</p>
    <p>stateless for reduced cross-layer communication  recent updates kept in volatile memory  lost data in case of client failure/reboot</p>
    <p>A. Hatzieleftheriou 5</p>
    <p>Apps Client</p>
    <p>Backend Servers</p>
    <p>Network</p>
    <p>Apps Client</p>
    <p>Apps Client</p>
    <p>Frontend (virtualized or bare-metal)</p>
    <p>Caching Layer</p>
  </div>
  <div class="page">
    <p>Representative System</p>
    <p>Ceph object-based scale-out file system  client-side memory-based caching</p>
    <p>Experiment  Filebench fileserver  writeback every 5 sec dirty data older than 30 sec (default)</p>
    <p>Outcome  on average, 24.3MB of dirty data only in volatile memory over time</p>
    <p>A. Hatzieleftheriou 6</p>
    <p>m ou</p>
    <p>nt (M</p>
    <p>B)</p>
    <p>Time (s)</p>
    <p>Unflushed Dirty Data</p>
    <p>Ceph</p>
  </div>
  <div class="page">
    <p>File-based  file sharing  improved performance  semantics awareness  compatibility limitations</p>
    <p>Object-based  scalability</p>
    <p>Block-based  virtualization flexibility  no sharing  translation overhead  semantic gap</p>
    <p>Storage Interfaces</p>
    <p>A. Hatzieleftheriou 7</p>
    <p>VM</p>
    <p>file/object</p>
    <p>VM</p>
    <p>Storage Servers</p>
    <p>Host</p>
    <p>VM file</p>
    <p>file/object</p>
    <p>VM</p>
    <p>Host</p>
    <p>Storage Servers</p>
    <p>Host</p>
    <p>VM block</p>
    <p>block/file</p>
    <p>VM VM</p>
    <p>block</p>
    <p>VM</p>
    <p>Host</p>
  </div>
  <div class="page">
    <p>The Problem of Block-based Caching</p>
    <p>Strengths Performance</p>
    <p>improved throughput  reduced latency</p>
    <p>Efficiency  reduced server &amp; network load</p>
    <p>Weaknesses Functionality</p>
    <p>no file sharing Overhead</p>
    <p>translation overhead  metadata persistence</p>
    <p>Consistency  semantic gap  ordering of requests  grouping of operations</p>
    <p>A. Hatzieleftheriou 8</p>
  </div>
  <div class="page">
    <p>Key Insight</p>
    <p>Improve</p>
    <p>the durability of the memory-based cache at the client of shared storage systems</p>
    <p>Gain</p>
    <p>performance  efficiency</p>
    <p>A. Hatzieleftheriou 9</p>
  </div>
  <div class="page">
    <p>Design Goals</p>
    <p>Interface  POSIX-like file-based interface</p>
    <p>Sharing  native file-sharing support within and across hosts</p>
    <p>Durability  recent updates survive client reboots</p>
    <p>Performance  sequential disk throughput for writes</p>
    <p>Scalability  scale-out backend servers</p>
    <p>A. Hatzieleftheriou 10</p>
  </div>
  <div class="page">
    <p>The Arion System Architecture</p>
    <p>Distributed filesystem  object-based  multiple data &amp; metadata servers  virtualized or bare-metal client  multiple backend replicas</p>
    <p>Local journal at the client-side  heterogeneous replication  reliable directly attached storage  log both data and corresponding</p>
    <p>metadata  one journal per client</p>
    <p>Guest Guest</p>
    <p>Hypervisor</p>
    <p>HOST Guest Guest</p>
    <p>Object Storage Servers journal device</p>
  </div>
  <div class="page">
    <p>Journaling and Writeback</p>
    <p>Local journal device attached to client at mount-time</p>
    <p>Commit  synchronously transfer data updates from memory to journal  periodically or explicit flush request</p>
    <p>Writeback  occasionally copy data blocks from memory to filesystem servers  periodically, under memory or journal space pressure</p>
    <p>Written-back and invalidated pages removed from journal A. Hatzieleftheriou 12</p>
    <p>Client RAM</p>
    <p>HOST Journal</p>
    <p>Object Storage Servers</p>
  </div>
  <div class="page">
    <p>Consistency</p>
    <p>Shared file access with tokens</p>
    <p>Normal operation - conflicting writes from different clients  checkpoint pending writes  invalidate related journal entries  revoke write token</p>
    <p>Failure - client reconnection or reboot after a crash  acquire required tokens  replay file updates only if journaled metadata is newer than metadata</p>
    <p>fetched from the server</p>
    <p>A. Hatzieleftheriou 13</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Prototype implementation  CephFS kernel-level client (Linux kernel v3.6.6)  Linux JBD2</p>
    <p>During commit  include metadata attributes in the journal tags of the descriptor block</p>
    <p>During recovery  compare journaled metadata attributes with those newly fetched from MDS  replay writes only for files not accessed after the transaction commit</p>
    <p>A. Hatzieleftheriou 14</p>
    <p>D D D C tag1 tag2</p>
    <p>header</p>
    <p>Descriptor block</p>
    <p>Data blocks Commit block</p>
    <p>inode info block count start offset end offset checksum flags</p>
    <p>CLIENT</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Experimentation Environment:  Backend Servers - Ceph v0.80.1 (3 OSDs, 1 MON, 1 MDS)</p>
    <p>3GB RAM, 2 x 300GB 15KRPM SAS disks, 2 x quad-core x86-64 2.66GHz  separate OSD journal device  replication factor 3</p>
    <p>Virtualized client  2GB RAM, 2 x VCPUs  journal size 2GB, commit interval 1s</p>
    <p>Host - XEN v4.2.0  2 x 300GB 15KRPM SAS disks (RAID0), 2 x quad-core x86-64 2.66GHz</p>
    <p>Workloads  Filebench fileserver &amp; mail server  Flexible I/O Tester</p>
  </div>
  <div class="page">
    <p>Filebench Fileserver</p>
    <p>Arion flushes dirty data to local journal every second</p>
    <p>Reduced amount of vulnerable data in memory  from 24.3MB to 5.4MB over time</p>
    <p>A. Hatzieleftheriou 16</p>
    <p>A m</p>
    <p>ou nt</p>
    <p>(M B)</p>
    <p>Time (s)</p>
    <p>Unflushed Dirty Data</p>
    <p>Ceph</p>
    <p>Arion</p>
  </div>
  <div class="page">
    <p>Filebench Mail Server</p>
    <p>A. Hatzieleftheriou 17</p>
    <p>Varying writeback and expiration intervals</p>
    <p>Arion achieves up to 58% higher operation throughput than Ceph</p>
    <p>OSD network traffic normalized by the number of completed operations  30% reduction of the received network load  27% reduction of the transmitted network load</p>
    <p>Ceph Ceph-1 Ceph-sync Arion-60 Arion-inf</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (O</p>
    <p>ps /s</p>
    <p>)</p>
    <p>Mail Server</p>
    <p>Ceph Ceph-1 Ceph-sync Arion-60 Arion-inf</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>N et</p>
    <p>w . L</p>
    <p>oa d</p>
    <p>(K B/</p>
    <p>IO )</p>
    <p>Mail Server (OSD)</p>
    <p>Received</p>
    <p>Transmitted</p>
  </div>
  <div class="page">
    <p>Flexible IO Tester</p>
    <p>Arion-60 achieves 22% reduced write latency</p>
    <p>Received OSD network traffic reduced by 42%</p>
    <p>A. Hatzieleftheriou 18</p>
    <p>Cu m</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>Re ce</p>
    <p>iv ed</p>
    <p>D at</p>
    <p>a (M</p>
    <p>B)</p>
    <p>Time (s)</p>
    <p>Network Load (OSD)</p>
    <p>Ceph</p>
    <p>Ceph-1</p>
    <p>Arion-60</p>
    <p>Arion-inf</p>
    <p>Ceph-sync</p>
    <p>Ceph Ceph-1 Ceph-sync Arion-60 Arion-inf</p>
    <p>A ve</p>
    <p>ra ge</p>
    <p>L at</p>
    <p>en cy</p>
    <p>(m s)</p>
    <p>Random Writes (Zipfian)</p>
  </div>
  <div class="page">
    <p>Flexible IO Tester</p>
    <p>Reduced disk utilization at the servers  filesystem device utilization reduced by 82%</p>
    <p>A. Hatzieleftheriou 19</p>
    <p>Jo ur</p>
    <p>na l D</p>
    <p>ev ic</p>
    <p>e (%</p>
    <p>)</p>
    <p>Time (s)</p>
    <p>Disk Utilization (OSD)</p>
    <p>Ceph-1</p>
    <p>Arion-60</p>
    <p>Fi le</p>
    <p>sy st</p>
    <p>em D</p>
    <p>ev ic</p>
    <p>e (%</p>
    <p>)</p>
    <p>Time (s)</p>
    <p>Disk Utilization (OSD)</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Future Work</p>
    <p>Durable shared storage through host-side journal integration at the client of a distributed filesystem</p>
    <p>Tunable control over  amount of dirty pages staged at the host  time period for dirty pages to reach the backend servers</p>
    <p>Benefits  improved durability of frontend memory caching  increased performance  resource efficiency (network &amp; disk)</p>
    <p>Future work  further experimentation  extend host-based journaling to support disk-based caching</p>
    <p>A. Hatzieleftheriou 20</p>
  </div>
</Presentation>
