<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Minimizing Communication in Sparse Matrix Solvers</p>
    <p>Marghoob Mohiyuddin, Mark Hoemmen, James Demmel, Kathy Yelick</p>
    <p>marghoob@eecs.berkeley.edu</p>
    <p>EECS Department, University of California at Berkeley</p>
    <p>SC09, Nov 17, 2009</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>What is communication?</p>
    <p>Algorithms incur 2 costs:</p>
    <p>Bandwidth (#words) and latency (#messages) components</p>
    <p>P ar</p>
    <p>al le</p>
    <p>l</p>
    <p>Between CPUs Between CPUs and coprocessors</p>
    <p>S eq</p>
    <p>u en</p>
    <p>ti al</p>
    <p>Between cache and DRAM Between DRAM and disk</p>
  </div>
  <div class="page">
    <p>What is communication?</p>
    <p>Algorithms incur 2 costs: 1 Arithmetic (flops) 2 Communication (data movement)</p>
    <p>Bandwidth (#words) and latency (#messages) components</p>
    <p>P ar</p>
    <p>al le</p>
    <p>l</p>
    <p>Between CPUs Between CPUs and coprocessors</p>
    <p>S eq</p>
    <p>u en</p>
    <p>ti al</p>
    <p>Between cache and DRAM Between DRAM and disk</p>
  </div>
  <div class="page">
    <p>What is communication?</p>
    <p>Algorithms incur 2 costs: 1 Arithmetic (flops) 2 Communication (data movement)</p>
    <p>Bandwidth (#words) and latency (#messages) components</p>
    <p>P ar</p>
    <p>al le</p>
    <p>l</p>
    <p>Between CPUs Between CPUs and coprocessors</p>
    <p>S eq</p>
    <p>u en</p>
    <p>ti al</p>
    <p>Between cache and DRAM Between DRAM and disk</p>
  </div>
  <div class="page">
    <p>What is communication?</p>
    <p>Algorithms incur 2 costs: 1 Arithmetic (flops) 2 Communication (data movement)</p>
    <p>Bandwidth (#words) and latency (#messages) components</p>
    <p>P ar</p>
    <p>al le</p>
    <p>l</p>
    <p>Between CPUs Between CPUs and coprocessors</p>
    <p>S eq</p>
    <p>u en</p>
    <p>ti al</p>
    <p>Between cache and DRAM Between DRAM and disk</p>
  </div>
  <div class="page">
    <p>What is communication?</p>
    <p>Algorithms incur 2 costs: 1 Arithmetic (flops) 2 Communication (data movement)</p>
    <p>Bandwidth (#words) and latency (#messages) components</p>
    <p>P ar</p>
    <p>al le</p>
    <p>l</p>
    <p>Between CPUs Between CPUs and coprocessors</p>
    <p>S eq</p>
    <p>u en</p>
    <p>ti al</p>
    <p>Between cache and DRAM Between DRAM and disk</p>
  </div>
  <div class="page">
    <p>Communication is expensive, computation is cheap</p>
    <p>Time per flop  1/bandwidth  latency Gap between processing power and communication cost increasing exponentially</p>
    <p>Annual improvements Flop rate 59%</p>
    <p>DRAM bandwidth 26% DRAM latency 5%</p>
    <p>Reduce communication  improve efficiency Trading off communication for computation is okay</p>
  </div>
  <div class="page">
    <p>The problem with sparse iterative solvers</p>
    <p>Conventional GMRES (solve for Ax = b) 1 for i = 1 to r 2 w = Avi1 /* SpMV */ 3 Orthogonalize w against {v0, . . . , vi1} /* MGS */ 4 Update vector vi , matrix H 5 Use H, {v0, . . . , vr} to construct the solution</p>
  </div>
  <div class="page">
    <p>The problem with sparse iterative solvers</p>
    <p>Conventional GMRES (solve for Ax = b) 1 for i = 1 to r 2 w = Avi1 /* SpMV */ 3 Orthogonalize w against {v0, . . . , vi1} /* MGS */ 4 Update vector vi , matrix H 5 Use H, {v0, . . . , vr} to construct the solution</p>
    <p>Repeated calls to sparse matrix vector multiply (SpMV) &amp; Modified Gram Schmidt orthogonalization (MGS)</p>
    <p>SpMV: performs 2 flops/matrix nonzero entry  communication bound MGS: vector dot-products (BLAS level 1)  communication bound</p>
  </div>
  <div class="page">
    <p>The problem with sparse iterative solvers</p>
    <p>Conventional GMRES (solve for Ax = b) 1 for i = 1 to r 2 w = Avi1 /* SpMV */ 3 Orthogonalize w against {v0, . . . , vi1} /* MGS */ 4 Update vector vi , matrix H 5 Use H, {v0, . . . , vr} to construct the solution</p>
    <p>Solution Replace SpMV and MGS by new kernels:</p>
    <p>SpMV by matrix powers MGS by block Gram-Schmidt + TSQR</p>
    <p>Reformulate to use the new kernels</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>The matrix powers kernel</p>
    <p>Usual kernel y = Ax communication-bound for large matrices</p>
    <p>Large  does not fit in cache Need to read stream through the matrix</p>
    <p>Given sparse matrix A, vector x , integer k &gt; 0, compute [p1(A)x , p2(A)x , . . . , pk (A)x ], pi (A) degree i polynomial iA Easier to consider the special case: [Ax , A2x , . . . , Ak x ]</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>x0 x1</p>
    <p>x2</p>
    <p>x3</p>
    <p>x4</p>
    <p>y0 y1</p>
    <p>y2</p>
    <p>y3</p>
    <p>y4</p>
    <p>y A x</p>
    <p>Tridiagonal only for illustration</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>x0 x1</p>
    <p>x2</p>
    <p>x3</p>
    <p>x4</p>
    <p>y0 y1</p>
    <p>y2</p>
    <p>y3</p>
    <p>y4</p>
    <p>y A</p>
    <p>y2</p>
    <p>x1 x2 x3 x</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>x0 x1</p>
    <p>x2</p>
    <p>x3</p>
    <p>x4</p>
    <p>y0 y1</p>
    <p>y2</p>
    <p>y3</p>
    <p>y4</p>
    <p>y A</p>
    <p>Ax y2</p>
    <p>x1 x2 x3 x</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>x0 x1</p>
    <p>x2</p>
    <p>x3</p>
    <p>x4</p>
    <p>y0 y1</p>
    <p>y2</p>
    <p>y3</p>
    <p>y4</p>
    <p>y A</p>
    <p>Ax</p>
    <p>x</p>
    <p>x</p>
    <p>A2x</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>Nave parallel algorithm</p>
    <p>Example: tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
    <p>k times min. latency cost</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
    <p>Green+black entries of x sufficient to compute all the local entries Blue entries represent redundant computation</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>A better parallel algorithm for matrix powers</p>
    <p>Example: Tridiagonal matrix, k = 3, 4 processors</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>processor 1 processor 2 processor 3 processor 4</p>
  </div>
  <div class="page">
    <p>General matrix/graph example</p>
    <p>Our algorithms work for general matrices Performance improvement best when the surface-to-volume ratio is small</p>
  </div>
  <div class="page">
    <p>General matrix/graph example</p>
    <p>Our algorithms work for general matrices Performance improvement best when the surface-to-volume ratio is small</p>
    <p>Red entries of x needed when k = 1</p>
  </div>
  <div class="page">
    <p>General matrix/graph example</p>
    <p>Our algorithms work for general matrices Performance improvement best when the surface-to-volume ratio is small</p>
    <p>Red+green entries of x needed when k = 2</p>
  </div>
  <div class="page">
    <p>General matrix/graph example</p>
    <p>Our algorithms work for general matrices Performance improvement best when the surface-to-volume ratio is small</p>
    <p>Red+green+blue entries of x needed when k = 3</p>
  </div>
  <div class="page">
    <p>Sequential algorithms: Explicitly blocked algorithm</p>
    <p>Example: 4040 tridiagonal matrix, k = 3</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>Simulate parallel algorithm on 1 processor Each block should be small enough to fit in cache Redundant flops performed Read the matrix once per k iterations (O(k ) improvement)  bandwidth savings</p>
  </div>
  <div class="page">
    <p>Sequential algorithms: Implicitly blocked algorithm</p>
    <p>Example: 4040 tridiagonal matrix, k = 3</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>Improve upon the explicit algorithm Eliminate redundant computation</p>
    <p>No redundant flops Implicit blocking by reordering computations Bookkeeping overhead for computation schedule Computation inside blocks depends on block order  need to solve Traveling Salesman problems</p>
  </div>
  <div class="page">
    <p>Hybrid algorithm for multicores</p>
    <p>Multicore  2 kinds of communication: Inter-core on-chip DRAM Off-chip</p>
    <p>Parallel algorithm minimizes inter-core on-chip communication Sequential algorithm minimizes off-chip communication Hierarchical blocking of the matrix and vectors</p>
    <p>Minimize inter-block communication: reordering may occur Cache blocks small enough to hold the matrix and vector entries in cache</p>
    <p>Redundant work due to parallelization (+explicit sequential algorithm)</p>
  </div>
  <div class="page">
    <p>Hybrid algorithm for multicores</p>
    <p>Multicore  2 kinds of communication: Inter-core on-chip DRAM Off-chip</p>
    <p>Parallel algorithm minimizes inter-core on-chip communication Sequential algorithm minimizes off-chip communication</p>
    <p>Hierarchical blocking of the matrix and vectors Minimize inter-block communication: reordering may occur Cache blocks small enough to hold the matrix and vector entries in cache</p>
    <p>Redundant work due to parallelization (+explicit sequential algorithm)</p>
  </div>
  <div class="page">
    <p>Hybrid algorithm for multicores</p>
    <p>Multicore  2 kinds of communication: Inter-core on-chip DRAM Off-chip</p>
    <p>Parallel algorithm minimizes inter-core on-chip communication Sequential algorithm minimizes off-chip communication Hierarchical blocking of the matrix and vectors</p>
    <p>Minimize inter-block communication: reordering may occur Cache blocks small enough to hold the matrix and vector entries in cache</p>
    <p>Redundant work due to parallelization (+explicit sequential algorithm)</p>
  </div>
  <div class="page">
    <p>Hybrid algorithm for multicores</p>
    <p>Multicore  2 kinds of communication: Inter-core on-chip DRAM Off-chip</p>
    <p>Parallel algorithm minimizes inter-core on-chip communication Sequential algorithm minimizes off-chip communication Hierarchical blocking of the matrix and vectors</p>
    <p>Minimize inter-block communication: reordering may occur Cache blocks small enough to hold the matrix and vector entries in cache</p>
    <p>Redundant work due to parallelization (+explicit sequential algorithm)</p>
  </div>
  <div class="page">
    <p>Tuning the matrix powers kernel</p>
    <p>Tuning parameters and choices: Sequential algorithm: explicit/implicit Explicit: using cyclic buffers or not Partitioning strategy: reorder or not, # partitions Solving the ordering problems SpMV tuning parameters: register tile size, SW prefetch distance</p>
    <p>Autotuning Choice of parameter values dependent on matrix structure</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Tall skinny QR factorization</p>
    <p>Compute the QR factorization of an n(k + 1) matrix Tall skinny matrix (n  k ) MPI_Reduce with QR as the reduction operator  only one reduction</p>
    <p>Reduction tree for 4 processors Reduction tree for 4 cache blocks</p>
    <p>Implementation uses a hybrid approach Sequential reduction inside a parallel reduction</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Block GRAM-Schmidt Orthogonalization</p>
    <p>Original MGS: orthogonalize a vector against a block of n orthogonal vectors</p>
    <p>BLAS level 1 operations: dot-products Orthogonalize a block of k vectors against a block of n orthogonal vectors</p>
    <p>BLAS level 3 operations: matrix-matrix multiplies  better cache reuse  better performance</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>CA-GMRES: Putting the pieces together</p>
    <p>Conventional GMRES (solve for Ax = b) 1 for i = 1 to r 2 w = Avi1 /* SpMV */ 3 Orthogonalize w against {v0, . . . , vi1} /* MGS */ 4 Update vector vi , matrix H 5 Use H, {v0, . . . , vr} to construct the solution</p>
    <p>CA-GMRES (Communication-Avoiding GMRES) 1 for i = 0, k , 2k , . . . , k (t 1) /* Outer iterations: t = r /k */ 2 W = {Avi , A2vi , . . . , Ak vi} /* Matrix powers */ 3 Make W orthogonal against {v0, . . . , vi} /* Block GS */ 4 Make W orthogonal /* TSQR */ 5 Update {vi+1, . . . , vi+k}, H 6 Use H, {v0, v1, . . . , vkt} to construct the solution</p>
  </div>
  <div class="page">
    <p>CA-GMRES: Putting the pieces together</p>
    <p>Conventional GMRES (solve for Ax = b) 1 for i = 1 to r 2 w = Avi1 /* SpMV */ 3 Orthogonalize w against {v0, . . . , vi1} /* MGS */ 4 Update vector vi , matrix H 5 Use H, {v0, . . . , vr} to construct the solution</p>
    <p>CA-GMRES (Communication-Avoiding GMRES) 1 for i = 0, k , 2k , . . . , k (t 1) /* Outer iterations: t = r /k */ 2 W = {Avi , A2vi , . . . , Ak vi} /* Matrix powers */ 3 Make W orthogonal against {v0, . . . , vi} /* Block GS */ 4 Make W orthogonal /* TSQR */ 5 Update {vi+1, . . . , vi+k}, H 6 Use H, {v0, v1, . . . , vkt} to construct the solution</p>
  </div>
  <div class="page">
    <p>Does CA-GMRES converge?</p>
    <p>Iteration count</p>
    <p>R el</p>
    <p>at iv</p>
    <p>e no</p>
    <p>rm of</p>
    <p>re si</p>
    <p>du al A x  b</p>
    <p>Original GMRES</p>
    <p>Monomial basis: matrix powers kernel computes [Ax , A2x , . . . , Ak x ]</p>
    <p>Newton basis: matrix powers kernel computes [(A1I)x , (A2I)(A1I)x , . . . , (Ak I)(A1I)x ]</p>
  </div>
  <div class="page">
    <p>Does CA-GMRES converge?</p>
    <p>Iteration count</p>
    <p>R el</p>
    <p>at iv</p>
    <p>e no</p>
    <p>rm of</p>
    <p>re si</p>
    <p>du al A x  b</p>
    <p>Original GMRES CA-GMRES (Monomial basis)</p>
    <p>Monomial basis: matrix powers kernel computes [Ax , A2x , . . . , Ak x ]</p>
    <p>Newton basis: matrix powers kernel computes [(A1I)x , (A2I)(A1I)x , . . . , (Ak I)(A1I)x ]</p>
  </div>
  <div class="page">
    <p>Does CA-GMRES converge?</p>
    <p>Iteration count</p>
    <p>R el</p>
    <p>at iv</p>
    <p>e no</p>
    <p>rm of</p>
    <p>re si</p>
    <p>du al A x  b</p>
    <p>Original GMRES CA-GMRES (Monomial basis) CA-GMRES (Newton basis)</p>
    <p>Monomial basis: matrix powers kernel computes [Ax , A2x , . . . , Ak x ]</p>
    <p>Newton basis: matrix powers kernel computes [(A1I)x , (A2I)(A1I)x , . . . , (Ak I)(A1I)x ]</p>
  </div>
  <div class="page">
    <p>Speedups over conventional GMRES: Sparse kernel</p>
    <p>Intel Clovertown (r = k t = 60)</p>
    <p>R un</p>
    <p>tim e</p>
    <p>/r un</p>
    <p>tim e(</p>
    <p>C A</p>
    <p>-G M</p>
    <p>R E</p>
    <p>S )</p>
    <p>Matrix powers SpMV</p>
    <p>Sparse: median speedup of 1.7</p>
  </div>
  <div class="page">
    <p>Speedups over conventional GMRES: Dense kernels</p>
    <p>Intel Clovertown (r = k t = 60)</p>
    <p>R un</p>
    <p>tim e</p>
    <p>/r un</p>
    <p>tim e(</p>
    <p>C A</p>
    <p>-G M</p>
    <p>R E</p>
    <p>S )</p>
    <p>TSQR MGS Block Gram-Schmidt Other dense ops</p>
    <p>Dense: median speedup of 2</p>
  </div>
  <div class="page">
    <p>Overall speedups over conventional GMRES</p>
    <p>Intel Clovertown (r = k t = 60)</p>
    <p>R un</p>
    <p>tim e</p>
    <p>/r un</p>
    <p>tim e(</p>
    <p>C A</p>
    <p>-G M</p>
    <p>R E</p>
    <p>S )</p>
    <p>Matrix powers TSQR Block Gram-Schmidt Other dense ops SpMV MGS</p>
    <p>Overall: medial speedup of 2.1</p>
  </div>
  <div class="page">
    <p>Overall speedups over conventional GMRES</p>
    <p>Intel Nehalem (k t = 60)</p>
    <p>R un</p>
    <p>tim e</p>
    <p>/r un</p>
    <p>tim e(</p>
    <p>C A</p>
    <p>-G M</p>
    <p>R E</p>
    <p>S )</p>
    <p>Matrix powers TSQR Block Gram-Schmidt Small Dense Ops SpMV MGS</p>
    <p>Median speedup of 1.6 More available bandwidth  speedups lower</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Conclusions/Future work</p>
    <p>Implemented a communication-avoiding solver using three new kernels</p>
    <p>Amortized reading matrix over multiple iterations Built on prior work, introduced new algorithms for modern multicores, auto-tuned implementation Achieve 2.1 median speedup on Intel Clovertown and 1.6 median speedup on Intel Nehalem</p>
    <p>Implication for HW design: communication-avoiding  lower bandwidth  lower cost</p>
    <p>Future work: Extending to distributed memory implementations Extensions to other iterative solvers Add preconditioning Incorporate TSP solver to solve the ordering problems Autotuning compositions of kernels</p>
  </div>
  <div class="page">
    <p>Conclusions/Future work</p>
    <p>Implemented a communication-avoiding solver using three new kernels</p>
    <p>Amortized reading matrix over multiple iterations Built on prior work, introduced new algorithms for modern multicores, auto-tuned implementation Achieve 2.1 median speedup on Intel Clovertown and 1.6 median speedup on Intel Nehalem</p>
    <p>Implication for HW design: communication-avoiding  lower bandwidth  lower cost Future work:</p>
    <p>Extending to distributed memory implementations Extensions to other iterative solvers Add preconditioning Incorporate TSP solver to solve the ordering problems Autotuning compositions of kernels</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>High performance implementations and co-tuning of all relevant kernels on multicore</p>
    <p>Simultaneous optimizations to reduce parallel and sequential communication</p>
    <p>New algorithm allows independent choice of restart length r and kernel size k</p>
    <p>Prior work required r = k , but want k  r in most cases Showed how to incorporate preconditioning</p>
    <p>Still need to implement</p>
    <p>See paper for lots of references on prior work Questions?</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Sparse Matrices</p>
    <p>(1M, 3M, 3) (141K, 7.3M, 51) (62K, 4M, 65) (123K, 3.1M, 25)</p>
    <p>pwtk shipsec xenon Pressurized wind tunnel FEM ship Complex zeolite,</p>
    <p>stiffness matrix section/detail sodalite crystals (218K, 12M, 55) (141K, 7.8M, 55) (157K, 3.9M, 25)</p>
  </div>
  <div class="page">
    <p>Example 1: CA-GMRES same as standard GMRES</p>
    <p>9</p>
    <p>8</p>
    <p>7</p>
    <p>6</p>
    <p>5</p>
    <p>4</p>
    <p>3</p>
    <p>2</p>
    <p>1</p>
    <p>Iteration count</p>
    <p>E xa</p>
    <p>ct r</p>
    <p>e si</p>
    <p>d u</p>
    <p>a l 2</p>
    <p>n</p>
    <p>o rm</p>
    <p>, lo</p>
    <p>g s</p>
    <p>ca le</p>
    <p>Joubert &amp; Carey 2D convdiff PDE: C*hx=0, 51 x 51 grid: Exact residual 2norm, log scale</p>
    <p>GMRES(60) MonomialGMRES(10,6); min,max basis cond #: 4.90e+09,2.04e+11; min,max basis scaling: 4.69e+00,7.27e+00 NewtonGMRES(10,6); min,max basis cond #: 1.27e+05,2.19e+05; min,max basis scaling: 2.49e+00,3.02e+00</p>
    <p>Discretized u = f in [0, 1]2 CA-GMRES w/ any basis converges as fast as standard (restarted) GMRES, but. . .</p>
  </div>
  <div class="page">
    <p>Example 2: CA-GMRES beats standard GMRES</p>
    <p>9</p>
    <p>8</p>
    <p>7</p>
    <p>6</p>
    <p>5</p>
    <p>4</p>
    <p>3</p>
    <p>2</p>
    <p>1</p>
    <p>Iteration count</p>
    <p>E xa</p>
    <p>ct r</p>
    <p>e si</p>
    <p>d u a l 2</p>
    <p>n o rm</p>
    <p>, lo</p>
    <p>g s</p>
    <p>ca le</p>
    <p>Joubert &amp; Carey 2D convdiff PDE: C*hx=128, 51 x 51 grid: Exact residual 2norm, log scale</p>
    <p>GMRES(60) MonomialGMRES(10,6); min,max basis cond #: 9.69e+19,5.96e+24; min,max basis scaling: 9.56e+01,1.20e+02 NewtonGMRES(10,6); min,max basis cond #: 3.58e+30,7.12e+33; min,max basis scaling: 2.50e+02,2.79e+02</p>
    <p>Added a Cux convection term to the PDE CA-GMRES beats standard restarted GMRES!</p>
  </div>
  <div class="page">
    <p>CA-GMRES may be better than GMRES</p>
    <p>9</p>
    <p>8</p>
    <p>7</p>
    <p>6</p>
    <p>5</p>
    <p>4</p>
    <p>3</p>
    <p>2</p>
    <p>1</p>
    <p>Iteration count</p>
    <p>E xa</p>
    <p>ct r</p>
    <p>e si</p>
    <p>d u a</p>
    <p>l 2</p>
    <p>n o rm</p>
    <p>, lo</p>
    <p>g s</p>
    <p>ca le</p>
    <p>Joubert &amp; Carey 2D convdiff PDE: C*hx=0, 51 x 51 grid: Exact residual 2norm, log scale</p>
    <p>GMRES(60) MonomialGMRES(10,6); min,max basis cond #: 4.90e+09,2.04e+11; min,max basis scaling: 4.69e+00,7.27e+00 NewtonGMRES(10,6); min,max basis cond #: 1.27e+05,2.19e+05; min,max basis scaling: 2.49e+00,3.02e+00</p>
    <p>9</p>
    <p>8</p>
    <p>7</p>
    <p>6</p>
    <p>5</p>
    <p>4</p>
    <p>3</p>
    <p>2</p>
    <p>1</p>
    <p>Iteration count</p>
    <p>E xa</p>
    <p>ct r</p>
    <p>e si</p>
    <p>d u a</p>
    <p>l 2</p>
    <p>n o rm</p>
    <p>, lo</p>
    <p>g s</p>
    <p>ca le</p>
    <p>Joubert &amp; Carey 2D convdiff PDE: C*hx=128, 51 x 51 grid: Exact residual 2norm, log scale</p>
    <p>GMRES(60) MonomialGMRES(10,6); min,max basis cond #: 9.69e+19,5.96e+24; min,max basis scaling: 9.56e+01,1.20e+02 NewtonGMRES(10,6); min,max basis cond #: 3.58e+30,7.12e+33; min,max basis scaling: 2.50e+02,2.79e+02</p>
    <p>Previous metric for success: CA-GMRES = GMRES For some problems, CA-GMRES converges faster Future work: investigate and control this phenomenon</p>
  </div>
</Presentation>
