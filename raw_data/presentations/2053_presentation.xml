<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>@spcl_eth</p>
    <p>TORSTEN HOEFLER, ROBERT ROSS, TIMOTHY ROSCOE</p>
    <p>Distributing the Data Plane for Remote Storage</p>
    <p>Access</p>
    <p>With slides input from M. Besta!</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Im an HPC (systems) guy</p>
    <p>Programming Models</p>
    <p>Performance Models</p>
    <p>Network (Models)</p>
    <p>My outing</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>CPU</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>L2</p>
    <p>RAM</p>
    <p>GPU</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>L2</p>
    <p>GPU</p>
    <p>MEM</p>
    <p>GPU</p>
    <p>MEM</p>
    <p>SCR SCR</p>
    <p>RAM</p>
    <p>SCR NVRAM</p>
    <p>Disk Disk</p>
    <p>NVRAM</p>
    <p>L3</p>
    <p>Disks Disks</p>
    <p>SCR</p>
    <p>SCM</p>
    <p>SCM</p>
    <p>Data movement is the new challenge!</p>
    <p>Memory systems become more complex!</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>CPU CPU</p>
    <p>NIC</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>RAM RAM</p>
    <p>SCM</p>
    <p>Disk Disk</p>
    <p>SCM</p>
    <p>CPU</p>
    <p>RAM RAM</p>
    <p>SCM</p>
    <p>Disk Disk</p>
    <p>SCM</p>
    <p>NIC NIC</p>
    <p>&lt;1 us latency</p>
    <p>&gt;12 GB/s bandwidth</p>
    <p>&lt;100 ns latency</p>
    <p>&gt;300 GB/s bandwidth ~100 ns latency</p>
    <p>&gt;400 GB/s bandwidth</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Our use-cases:</p>
    <p>Advanced parallel programming (e.g., MPI-IO [1])</p>
    <p>Data analytics chains (e.g., DataPath [2], Niad [3])</p>
    <p>File systems are served through the CPU</p>
    <p>Access to remote persistent storage in a</p>
    <p>closely-coupled computer cluster is one of</p>
    <p>the main obstacles to scaling performance</p>
    <p>Huge overheads: Energy, Time, Cost</p>
    <p>but software is lacking behind.</p>
    <p>NIC</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>[1]: Gropp, W., TH, Thakur, R., and Lusk, E. Using Advanced MPI: Modern Features of the Message-Passing Interface. MIT Press, Nov. 2014.</p>
    <p>[2]: Arumugam, S. et al.. The DataPath System: A Data-centric Analytic Processing Engine for Large Data Warehouses. In SIGMOD10</p>
    <p>[3]: Murray, D. G., McSherry, F., Isaacs, R., Isard, M., Barham, P., and Abadi, M. Naiad: A Timely Dataflow System. In SOSP13</p>
    <p>&gt;&gt;1 us latency</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Separating data and control plane</p>
    <p>Get the CPU/software out of the way!</p>
    <p>Software-defined IO (SDIO, cf. NASD [1], Aerie [2])</p>
    <p>We set up a network route right into the device!</p>
    <p>Key point is (direct) access to storage</p>
    <p>Allocation</p>
    <p>Read/Write</p>
    <p>Protection</p>
    <p>Caching</p>
    <p>Consistency, coherence, durability</p>
    <p>Our central research question:</p>
    <p>How can we design a fast software/hardware data plane for safe, secure,</p>
    <p>direct remote access to persistent storage devices?</p>
    <p>A (30-year-old) networking idea revived</p>
    <p>NIC</p>
    <p>MEM</p>
    <p>CPU</p>
    <p>[1]: Gibson, G., et al. A Cost-effective, High-bandwidth Storage Architecture. SIGPLAN Not. 33, 11 (Oct. 1998)</p>
    <p>[2]: Volos, H., et al.. Aerie: Flex-ible File-system Interfaces to Storage-class Memory, EuroSys14</p>
    <p>&lt;1 us latency</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Exokernel-like filesystem library</p>
    <p>Data path 100% in user-level</p>
    <p>Manages metadata, data,</p>
    <p>and coordination</p>
    <p>Data is stored in allocations</p>
    <p>An allocation</p>
    <p>Is an area of main memory or on a storage device</p>
    <p>Placed explicitly</p>
    <p>Can be created, opened, or closed</p>
    <p>Using a central or distributed control plane</p>
    <p>Has a contiguous address space</p>
    <p>Block translation implemented in the device</p>
    <p>Is the smallest unit of access control and sharing</p>
    <p>Named in a global namespace</p>
    <p>Access through capabilities (e.g., IB PDs)</p>
    <p>(1) Device allocations RAM RAM</p>
    <p>SCM</p>
    <p>Disk Disk</p>
    <p>SCM</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>VM A B C</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Allocations are accessed</p>
    <p>Locally via MMU-mappings (cf. Aerie)</p>
    <p>Remotely via IOMMUs or other address translations</p>
    <p>Not all devices are part of the physical address space</p>
    <p>Mainly legacy . two options:</p>
    <p>(2) Read/Write and protection</p>
    <p>VM A B C</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>MMU</p>
    <p>IOMMU</p>
    <p>[1]: M. Besta, TH: Active Access: A Mechanism for High-Performance Distributed Data-Centric Computations, ACM ICS15</p>
    <p>B</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Software caching</p>
    <p>Explicit caching, similar to RMA programming</p>
    <p>Allocate local cache and access it</p>
    <p>Can be application-specific or caching library (standard techniques)</p>
    <p>Hardware caching</p>
    <p>Set up local memory as cache for remote allocations</p>
    <p>Use standard (e.g., LRU) replacement policies</p>
    <p>Could be implemented by an extended IOMMU</p>
    <p>Would allocate incoming transactions in cache</p>
    <p>(3) Caching</p>
    <p>VM B C</p>
    <p>|</p>
    <p>B</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>MMU</p>
    <p>IOMMU</p>
    <p>RAM</p>
    <p>B</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Current RDMA does not support consistent atomic access</p>
    <p>At least not large enough</p>
    <p>We propose a weaker consistency model</p>
    <p>All read/write accesses are nonblocking!</p>
    <p>Arrange accesses into epochs separated by fence operations</p>
    <p>Modified data is only valid at the end of an epoch</p>
    <p>The type determines the isolation level</p>
    <p>Shared: only consistency after epoch ends</p>
    <p>Exclusive: consistency + atomicity</p>
    <p>Persistent: consistency + durability</p>
    <p>Optimistic: consistency + atomicity but can fail</p>
    <p>Types can be combined</p>
    <p>e.g., persistent + exclusive</p>
    <p>Implemented similar to other RMA programming models [1]</p>
    <p>May require remote flushes (in the worst case RPCs)</p>
    <p>(4) Consistency and coherence</p>
    <p>W(a)</p>
    <p>R(a)</p>
    <p>R(a)</p>
    <p>[1]: TH, J. Dinan, R. Thakur, B. Barrett, P. Balaji, W. Gropp, K. Underwood: Remote Memory Access Programming in MPI-3, ACM TOPC</p>
    <p>P0</p>
    <p>P1</p>
    <p>P2</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Crash recovery</p>
    <p>Use transactional (optimistic, exclusive, persistent) epochs for metadata</p>
    <p>Must ensure that locks time out if processes disappear</p>
    <p>Scalability</p>
    <p>Scoping limits context of coherency/epochs (e.g., a shared file)</p>
    <p>Integration with programming model (e.g., MPI-3 RMA)</p>
    <p>Compatibility</p>
    <p>Provide standard library of user-level file systems</p>
    <p>POSIX consistency with single-operation exclusive, persistent epochs</p>
    <p>Magic byte in allocation allows automated mounting like files</p>
    <p>Other filesystem requirements</p>
  </div>
  <div class="page">
    <p>@spcl_eth</p>
    <p>Discussion</p>
    <p>Merge the network transparently into the file system.</p>
  </div>
</Presentation>
