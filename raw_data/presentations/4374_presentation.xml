<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Token-level and sequence-level loss smoothing for RNN language models</p>
    <p>Maha Elbayad1,2, Laurent Besacier1,and Jakob Verbeek2</p>
    <p>ACL 2018 Melbourne, Australia</p>
  </div>
  <div class="page">
    <p>Language generation | Equivalence in the target space</p>
    <p>Ground truth sequences lie in a union of low-dimensional subspaces where sequences convey the same message. I France won the world cup for the second time. I France captured its second world cup title.</p>
    <p>Some words in the vocabulary share the same meaning. I Capture, conquer, win, gain, achieve, accomplish, . . .</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 1</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Take into consideration the nature of the target language space with:</p>
    <p>A token-level smoothing for a robust multi-class classification.  A sequence-level smoothing to explore relevant alternative sequences.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 2</p>
  </div>
  <div class="page">
    <p>Maximum likelihood estimation (MLE)</p>
    <p>For a pair (x, y ), we model the conditional distribution:</p>
    <p>p(y|x ) = |y| t</p>
    <p>p(yt|y&lt;t, x ) (1)</p>
    <p>Given the ground truth target sequence y?:</p>
    <p>`MLE(y ?, x ) =  ln p(y?|x )</p>
    <p>= DKL((y|y?)p(y|x )) (2)</p>
    <p>=</p>
    <p>|y?| t=1</p>
    <p>DKL((yt|y?t )p(yt|y ? &lt;t, x )) (3)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 3</p>
  </div>
  <div class="page">
    <p>Maximum likelihood estimation (ML)</p>
    <p>`MLE(y ?, x ) =  ln p(y?|x )</p>
    <p>= DKL((y|y?)p(y|x )) (2)</p>
    <p>=</p>
    <p>T t=1</p>
    <p>DKL((yt|y?t )p(yt|ht )) (3)</p>
    <p>Issues:</p>
    <p>Zero-one loss, all the outputs y 6= y? are treated equally.  Discrepancy at the sentence level between the training (1-gram) and</p>
    <p>evaluation metric (4-gram).</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 4</p>
  </div>
  <div class="page">
    <p>Loss smoothing</p>
    <p>(y?)</p>
    <p>DKL((y|y?)p(y|x ))</p>
    <p>smoothing</p>
    <p>r (y|y?)</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = DKL(r (y|y?)p(y|x )) (Norouzi et al, 2016)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 5</p>
  </div>
  <div class="page">
    <p>Loss smoothing</p>
    <p>(y?) (resp. (y?t ))</p>
    <p>DKL((y|y?)p(y|x )) T</p>
    <p>t=1</p>
    <p>DKL((yt|y?t )p(yt|ht ))</p>
    <p>smoothing</p>
    <p>r (y|y?) (resp. r (yt|y?t ))</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = DKL(r (y|y?)p(y|x )) (Norouzi et al, 2016)</p>
    <p>`tokRAML(y ?, x ) =</p>
    <p>T t=1</p>
    <p>DKL(r (yt|y?t )p(yt|ht ))</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 5</p>
  </div>
  <div class="page">
    <p>Token-level smoothing</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 6</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Token-level</p>
    <p>`tokRAML(y ?, x ) =</p>
    <p>T t=1</p>
    <p>DKL(r (yt|y?t )p(yt|ht )) (4)</p>
    <p>Uniform label smoothing over all words in the vocabulary:</p>
    <p>r (yt|y?t ) = (yt|y ? t ) + .u(V) (Szegedy et al. 2016)</p>
    <p>We can leverage word co-occurrence statistics to build a non-uniform and meaningful distribution.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 7</p>
  </div>
  <div class="page">
    <p>Loss smoothing</p>
    <p>`tokRAML(y ?, x ) =</p>
    <p>T t=1</p>
    <p>DKL(r (yt|y?t )p(yt|ht )) (4)</p>
    <p>Prerequisite: A word embedding w (e.g. Glove) in the target space and a distance d.</p>
    <p>r (yt|y?t ) = 1 Z</p>
    <p>exp</p>
    <p>( d(w (yt ), w (y?t ))</p>
    <p>) ,</p>
    <p>with a temperature  st. r  0</p>
    <p>.</p>
    <p>Z st.  ytV</p>
    <p>r (yt|y?t ) = 1</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 8</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Token-level</p>
    <p>= 0.12  = 0.70</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 9</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Token-level</p>
    <p>`tokRAML(y ?, x ) =</p>
    <p>T t=1</p>
    <p>DKL(r (yt|y?t )p(yt|ht )) (4)</p>
    <p>=</p>
    <p>T t=1</p>
    <p>ytV</p>
    <p>r (yt|y?t ) log (</p>
    <p>r (yt|y?t ) p(yt|ht )</p>
    <p>) (5)</p>
    <p>We can estimate the exact KL divergence for every target token. No approximation needed.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 10</p>
  </div>
  <div class="page">
    <p>Sequence-level smoothing</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 11</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = DKL(r (y|y?)p(y|x )) (6)</p>
    <p>Prerequisite: A distance d in the sequences space Vn, n N.</p>
    <p>r (y|y?) = 1 Z</p>
    <p>exp</p>
    <p>( d(y, y?)</p>
    <p>) ,</p>
    <p>Z st.</p>
    <p>yVn,nN r (y|y?) = 1</p>
    <p>Possible (pseudo-)distances:</p>
    <p>Hamming  Edit  1BLEU  1CIDEr</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 12</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level</p>
    <p>Can we evaluate the partition function Z for a given reward?</p>
    <p>r (yt|y?t ) = 1 Z</p>
    <p>exp</p>
    <p>( d(y, y?)</p>
    <p>) ,</p>
    <p>Z =</p>
    <p>yVn,nN exp</p>
    <p>( d(y, y?)</p>
    <p>) We can approximate Z for Hamming distance.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 13</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Hamming distance</p>
    <p>Assumption: consider only sequences of the same length as y? (d (y, y ) = 0 if |y| 6= |y |). We partition the set of sequences VT w.r.t. their distance to the ground truth y?:</p>
    <p>Sd = {y VTsub| d(y, y</p>
    <p>?) = d}, VT =</p>
    <p>d Sd,</p>
    <p>d, d : Sd  Sd = .</p>
    <p>The reward in each subset is a constant.  The cardinality of each subset is known.</p>
    <p>Z =  d</p>
    <p>|Sd|exp (</p>
    <p>d</p>
    <p>)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 14</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Hamming distance</p>
    <p>We can easily draw from r with Hamming distance:</p>
    <p>Monte Carlo estimation:</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = DKL(r (y|y?)p(y|x )) (6) = Er [log p(.|x )] + cst (7)</p>
    <p>(y l  r )  1 L</p>
    <p>L l=1</p>
    <p>log p(y l|x ) (8)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 15</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Hamming distance</p>
    <p>We can easily draw from r with Hamming distance:</p>
    <p>Monte Carlo estimation:</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = DKL(r (y|y?)p(y|x )) (6) = Er [log p(.|x )] + cst (7)</p>
    <p>(y l  r )  1 L</p>
    <p>L l=1</p>
    <p>log p(y l|x ) (8)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 15</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Other distances</p>
    <p>We cannot easily sample from more complicated rewards such as BLEU or CIDEr. Importance sampling:</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = Er [log p(.|x )] (9)</p>
    <p>= Eq [ r q</p>
    <p>log p] (10)</p>
    <p>(y l  q)  1 L</p>
    <p>L l=1</p>
    <p>l log p(y l|x ) (11)</p>
    <p>l  r (y l|y?)/q(y l|y?)L</p>
    <p>k=1 r (y k|y?)/q(y k|y?)</p>
    <p>,</p>
    <p>Choose q the reward distribution relative to Hamming distance.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 16</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Support reduction</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = DKL(r (y|y?)p(y|x )) (6)</p>
    <p>Can we reduce the support of r?</p>
    <p>r (y|y?) = 1 Z</p>
    <p>exp</p>
    <p>( d(y, y?)</p>
    <p>) , Z =</p>
    <p>yVT</p>
    <p>exp</p>
    <p>( d(y, y?)</p>
    <p>)</p>
    <p>Reduce the support from V|y ?| to V|y</p>
    <p>?| sub where Vsub V.</p>
    <p>Vsub = Vbatch : tokens occuring in the SGD mini-batch.  Vsub = Vrefs : tokens occuring in the available references.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 17</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Lazy training</p>
    <p>Default training</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = Er [log p(.|x )]</p>
    <p>1 L</p>
    <p>L l=1</p>
    <p>log p(y l|x )</p>
    <p>l, y l is: 1 forwarded in the RNN. 2 used as target.</p>
    <p>log p(yl|yl, x )</p>
    <p>Lazy training</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = Er [log p(.|x )]</p>
    <p>1 L</p>
    <p>L l=1</p>
    <p>log p(y l|x )</p>
    <p>l, y l is: 1 not forwarded in the RNN. 2 used as target.</p>
    <p>log p(yl|y?, x )</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 18</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Sequence-level | Lazy training</p>
    <p>Default training</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = Er [log p(.|x )]</p>
    <p>1 L</p>
    <p>L l=1</p>
    <p>log p(y l|x )</p>
    <p>l, y l is: 1 forwarded in the RNN. 2 used as target.</p>
    <p>log p(yl|yl, x )</p>
    <p>Complexity : O(2L.)</p>
    <p>Lazy training</p>
    <p>` seq RAML(y</p>
    <p>?, x ) = Er [log p(.|x )]</p>
    <p>1 L</p>
    <p>L l=1</p>
    <p>log p(y l|x )</p>
    <p>l, y l is: 1 not forwarded in the RNN. 2 used as target.</p>
    <p>log p(yl|y?, x )</p>
    <p>Complexity: O((L + 1))  = |y||cell|, where cell are the cell parameters.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 18</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 19</p>
  </div>
  <div class="page">
    <p>Image captioning on MS-COCO | Setup</p>
    <p>5 captions for every image.  |V| 10k words (freq  5)</p>
    <p>|images|</p>
    <p>Train 82k</p>
    <p>Dev 5k</p>
    <p>Test 5k</p>
    <p>(Lin et al. 2014, Karpathy et al. 2015)</p>
    <p>Architecture: Top-down attention (Anderson et al. 2017)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 20</p>
  </div>
  <div class="page">
    <p>Image captioning on MS-COCO | Results</p>
    <p>Loss Reward Vsub BLEU-1 BLEU-4 CIDEr</p>
    <p>MLE 73.40 33.11 101.63</p>
    <p>Tok Glove, cosine 74.01 33.25 102.81</p>
    <p>Seq Hamming V 73.12 32.71 101.25 Seq Hamming Vbatch 73.26 32.73 101.90 Seq, lazy Hamming Vbatch 73.43 32.95 102.03</p>
    <p>Seq CIDEr Vbatch 73.50 33.04 102.98 Seq CIDEr Vrefs 73.42 32.91 102.23 Seq, lazy CIDEr Vrefs 73.92 33.10 102.64</p>
    <p>Tok-Seq CIDEr Vrefs 74.28 33.34 103.81</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 21</p>
  </div>
  <div class="page">
    <p>Image captioning on MS-COCO | Results</p>
    <p>Loss Reward Vsub BLEU-1 BLEU-4 CIDEr</p>
    <p>MLE 73.40 33.11 101.63</p>
    <p>Tok Glove, cosine 74.01 33.25 102.81</p>
    <p>Seq Hamming V 73.12 32.71 101.25 Seq Hamming Vbatch 73.26 32.73 101.90 Seq, lazy Hamming Vbatch 73.43 32.95 102.03</p>
    <p>Seq CIDEr Vbatch 73.50 33.04 102.98 Seq CIDEr Vrefs 73.42 32.91 102.23 Seq, lazy CIDEr Vrefs 73.92 33.10 102.64</p>
    <p>Tok-Seq CIDEr Vrefs 74.28 33.34 103.81</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 21</p>
    <p>(Norouzi et al. 2016)</p>
  </div>
  <div class="page">
    <p>Image captioning on MS-COCO | Results</p>
    <p>Loss Reward Vsub BLEU-1 BLEU-4 CIDEr</p>
    <p>MLE 73.40 33.11 101.63</p>
    <p>Tok Glove, cosine 74.01 33.25 102.81</p>
    <p>Seq Hamming V 73.12 32.71 101.25 Seq Hamming Vbatch 73.26 32.73 101.90 Seq, lazy Hamming Vbatch 73.43 32.95 102.03</p>
    <p>Seq CIDEr Vbatch 73.50 33.04 102.98 Seq CIDEr Vrefs 73.42 32.91 102.23 Seq, lazy CIDEr Vrefs 73.92 33.10 102.64</p>
    <p>Tok-Seq CIDEr Vrefs 74.28 33.34 103.81</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 21</p>
  </div>
  <div class="page">
    <p>Image captioning on MS-COCO | Results</p>
    <p>Loss Reward Vsub BLEU-1 BLEU-4 CIDEr</p>
    <p>MLE 73.40 33.11 101.63</p>
    <p>Tok Glove, cosine 74.01 33.25 102.81</p>
    <p>Seq Hamming V 73.12 32.71 101.25 Seq Hamming Vbatch 73.26 32.73 101.90 Seq, lazy Hamming Vbatch 73.43 32.95 102.03</p>
    <p>Seq CIDEr Vbatch 73.50 33.04 102.98 Seq CIDEr Vrefs 73.42 32.91 102.23 Seq, lazy CIDEr Vrefs 73.92 33.10 102.64</p>
    <p>Tok-Seq CIDEr Vrefs 74.28 33.34 103.81</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 21</p>
  </div>
  <div class="page">
    <p>Machine translation | Setup</p>
    <p>Architecture: Bi-LSTM encoder-decoder with attention (Bahdanau et al. 2015)  Corpora:</p>
    <p>IWSLT14 DEEN</p>
    <p>|Pairs|</p>
    <p>Train 153k</p>
    <p>Dev 7k</p>
    <p>Test 7k</p>
    <p>|V| = 22k words.</p>
    <p>WMT14 ENFR</p>
    <p>|Pairs|</p>
    <p>Train 12M</p>
    <p>Dev 6k</p>
    <p>Test 3k</p>
    <p>|V| = 30k words.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 22</p>
  </div>
  <div class="page">
    <p>Machine translation | Results</p>
    <p>Loss Reward Vsub WMT14 EnFr IWSLT14 DeEn</p>
    <p>MLE 30.03 27.55</p>
    <p>tok Glove, cosine 30.19 27.83</p>
    <p>Seq Hamming V 30.85 27.98 Seq Hamming Vbatch 31.18 28.54 Seq BLEU-4 Vbatch 31.29 28.53</p>
    <p>Tok-Seq Hamming Vbatch 31.36 28.70 Tok-Seq BLEU-4 Vbatch 31.39 28.74</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 23</p>
  </div>
  <div class="page">
    <p>Machine translation | Results</p>
    <p>Loss Reward Vsub WMT14 EnFr IWSLT14 DeEn</p>
    <p>MLE 30.03 27.55</p>
    <p>tok Glove, cosine 30.19 27.83</p>
    <p>Seq Hamming V 30.85 27.98 Seq Hamming Vbatch 31.18 28.54 Seq BLEU-4 Vbatch 31.29 28.53</p>
    <p>Tok-Seq Hamming Vbatch 31.36 28.70 Tok-Seq BLEU-4 Vbatch 31.39 28.74</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 23</p>
    <p>(Norouzi et al. 2016)</p>
  </div>
  <div class="page">
    <p>Machine translation | Results</p>
    <p>Loss Reward Vsub WMT14 EnFr IWSLT14 DeEn</p>
    <p>MLE 30.03 27.55</p>
    <p>tok Glove, cosine 30.19 27.83</p>
    <p>Seq Hamming V 30.85 27.98 Seq Hamming Vbatch 31.18 28.54 Seq BLEU-4 Vbatch 31.29 28.53</p>
    <p>Tok-Seq Hamming Vbatch 31.36 28.70 Tok-Seq BLEU-4 Vbatch 31.39 28.74</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 23</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 24</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Improving over MLE with:  Sequence-level smoothing: an extension of RAML (Norouzi et al. 2016)</p>
    <p>I Reduced support of the reward distribution. I Importance sampling. I Lazy training.</p>
    <p>Token-level smoothing: smoothing across semantically similar tokens instead of the usual uniform noise.</p>
    <p>Both schemes can be combined for better results.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 25</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Improving over MLE with:  Sequence-level smoothing: an extension of RAML (Norouzi et al. 2016)</p>
    <p>I Reduced support of the reward distribution. I Importance sampling. I Lazy training.</p>
    <p>Token-level smoothing: smoothing across semantically similar tokens instead of the usual uniform noise.</p>
    <p>Both schemes can be combined for better results.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 25</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Validate on other seq2seq models besides LSTM encoder-decoders.  Validate on models with BPE instead of words.  Sequence-level smoothing:</p>
    <p>I Experiment with other distributions for sampling other than the Hamming distance.</p>
    <p>Token-level smoothing: I Sparsify the reward distribution for scalability.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 26</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 27</p>
  </div>
  <div class="page">
    <p>Appendices</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 28</p>
  </div>
  <div class="page">
    <p>Loss smoothing | Combination</p>
    <p>Hyper-parameters: ,1,2  (0,1) (,  = 1). Combininig ML and RAML:</p>
    <p>` seq RAML,(y</p>
    <p>?, x ) = `seqRAML(y ?, x ) + `MLE(y</p>
    <p>?, x ) (12)</p>
    <p>`tokRAML,(y ?, x ) = `tokRAML(y</p>
    <p>?, x ) + `MLE(y ?, x ) (13)</p>
    <p>Combininig the smoothing schemes:</p>
    <p>` seq, tok RAML,1,2</p>
    <p>(y?, x ) = 1Er [` tok RAML(y, x )] + 1`</p>
    <p>tok RAML(y</p>
    <p>?, x )</p>
    <p>= 1Er [2` tok RAML(y, x ) + 2`MLE(y, x )]</p>
    <p>+ 1(2` tok RAML(y</p>
    <p>?, x ) + 2`MLE(y ?, x )). (14)</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 29</p>
  </div>
  <div class="page">
    <p>Training time</p>
    <p>Average wall time to process a single batch (10 images 50 captions) when training the RNN language model with fixed CNN (without attention) on a Titan X GPU.</p>
    <p>Loss MLE Tok Seq Seq lazy Seq Seq lazy Seq Seq lazy Tok-Seq Tok-Seq Tok-Seq</p>
    <p>Reward Glove sim Hamming</p>
    <p>Vsub V V Vbatch Vbatch Vrefs Vrefs V Vbatch Vrefs ms/batch 347 359 390 349 395 337 401 336 445 446 453</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 30</p>
  </div>
  <div class="page">
    <p>Generated captions</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 31</p>
  </div>
  <div class="page">
    <p>Generated captions</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 32</p>
  </div>
  <div class="page">
    <p>Generated translations EnFr</p>
    <p>Source (en) I think its conceivable that these data are used for mutual benefit. Target (fr) Jestime quil est concevable que ces donnes soient utilises dans leur intrt mutuel. MLE Je pense quil est possible que ces donnes soient utilises  des fins rciproques. Tok-Seq Je pense quil est possible que ces donnes soient utilises pour le bnfice mutuel.</p>
    <p>Source (en) The public will be able to enjoy the technical prowess of young skaters , some of whom , like Hyeres young star , Lorenzo Palumbo , have already taken part in top-notch competitions.</p>
    <p>Target (fr) Le public pourra admirer les prouesses techniques de jeunes qui , pour certains , frquentent dj les comptitions au plus haut niveau ,  linstar du jeune prodige hyrois Lorenzo Palumbo.</p>
    <p>MLE Le public sera en mesure de profiter des connaissances techniques des jeunes garons , dont certains ,  linstar de la jeune star amricaine , Lorenzo , ont dj particip  des comptitions de comptition.</p>
    <p>Tok-Seq Le public sera en mesure de profiter de la finesse technique des jeunes musiciens , dont certains , comme la jeune star de lentreprise , Lorenzo , ont dj pris part  des comptitions de gymnastique.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 33</p>
  </div>
  <div class="page">
    <p>MS-COCO server results</p>
    <p>BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr SPICE</p>
    <p>c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40</p>
    <p>Google NIC+ (Vinyals et al., 2015) 71.3 89.5 54.2 80.2 40.7 69.4 30.9 58.7 25.4 34.6 53.0 68.2 94.3 94.6 18.2 63.6 Hard-Attention (Xu et al., 2015) 70.5 88.1 52.8 77.9 38.3 65.8 27.7 53.7 24.1 32.2 51.6 65.4 86.5 89.3 17.2 59.8 ATT-FCN+ (You et al., 2016) 73.1 90.0 56.5 81.5 42.4 70.9 31.6 59.9 25.0 33.5 53.5 68.2 94.3 95.8 18.2 63.1 Review Net+ (Yang et al., 2016) 72.0 90.0 55.0 81.2 41.4 70.5 31.3 59.7 25.6 34.7 53.3 68.6 96.5 96.9 18.5 64.9 Adaptive+ (Lu et al., 2017) 74.8 92.0 58.4 84.5 44.4 74.4 33.6 63.7 26.4 35.9 55.0 70.5 104.2 105.9 19.7 67.3</p>
    <p>SCST:Att2all+ (Rennie et al., 2017) 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7 - LSTM-A3+ (Yao et al., 2017) 78.7 93.7 62.7 86.7 47.6 76.5 35.6 65.2 27.0 35.4 56.4 70.5 116 118 - Up-Down+ (Anderson et al., 2017) 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5 -</p>
    <p>Ours: Tok-Seq CIDEr 72.6 89.7 55.7 80.9 41.2 69.8 30.2 58.3 25.5 34.0 53.5 68.0 96.4 99.4 - Ours: Tok-Seq CIDEr + 74.9 92.4 58.5 84.9 44.8 75.1 34.3 64.7 26.5 36.1 55.2 71.1 103.9 104.2 -</p>
    <p>Table: MS-COCO s server evaluation . (+) for ensemble submissions, () for submissions with CIDEr optimization and () for models using additional data.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 34</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang. 2017. Bottom-up and top-down attention for image captioning and visual question answering. arXiv preprint arXiv:1707.07998.</p>
    <p>J. Lu, C. Xiong, D. Parikh, and R. Socher. 2017. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In CVPR.</p>
    <p>S. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. 2017. Self-critical sequence training for image captioning. In CVPR.</p>
    <p>O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015. Show and tell: A neural image caption generator. In CVPR.</p>
    <p>K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In ICML.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 35</p>
  </div>
  <div class="page">
    <p>References II</p>
    <p>Z. Yang, Y. Yuan, Y. Wu, R. Salakhutdinov, and W. Cohen. 2016. Encode, review, and decode: Reviewer module for caption generation. In NIPS.</p>
    <p>T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei. 2017. Boosting image captioning with attributes. In ICLR.</p>
    <p>Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. 2016. Image captioning with semantic attention. In CVPR.</p>
    <p>ACL 2018, Melbourne M. Elbayad || Token-level and Sequence-level Loss Smoothing 36</p>
  </div>
</Presentation>
