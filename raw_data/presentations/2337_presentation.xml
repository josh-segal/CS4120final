<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>VOCL: An Optimized Environment for Transparent Virtualization of Graphics Processing Units</p>
    <p>Shucai Xiao1, Pavan Balaji2, Qian Zhu3, Rajeev Thakur2, Susan Coghlan2, Heshan Lin1, Gaojin Wen4, Jue Hong4, and Wu-chun Feng1</p>
  </div>
  <div class="page">
    <p>Motivation  GPUs are widely used as accelerators for scientific</p>
    <p>computation  Many applications are parallelized on GPUs  Speedup is reported compared to execution on CPUs</p>
  </div>
  <div class="page">
    <p>GPU-Based Supercomputers</p>
  </div>
  <div class="page">
    <p>Challenges of GPU Computing</p>
    <p>Provisioning Limitations  Not all computing nodes are configured with GPUs</p>
    <p>Budget and power consumption consideration  Multiple stages of investment</p>
    <p>Programmability  Current GPU programming models: CUDA and OpenCL  CUDA and OpenCL only support the utilization of local GPUs</p>
  </div>
  <div class="page">
    <p>Our Contributions  Virtual OpenCL (VOCL) framework for transparent</p>
    <p>virtualization of GPUs  Remote GPUs look like virtual local GPUs</p>
    <p>A program can use non-local GPUs  A program can use more GPUs than that can be installed locally</p>
    <p>Efficient resource management  Optimization of data transfer across different machines</p>
    <p>OpenCL OpenCL</p>
    <p>VOCL</p>
    <p>OpenCL</p>
    <p>MPI MPI</p>
  </div>
  <div class="page">
    <p>Outline  Motivation and Contributions</p>
    <p>Related Work</p>
    <p>VOCL Framework</p>
    <p>VOCL Optimization</p>
    <p>Experimental Results</p>
    <p>Conclusion &amp; Future work</p>
  </div>
  <div class="page">
    <p>Existing Frameworks for GPU Virtualization  rCUDA</p>
    <p>Good performance  Relative performance overhead is about 2% compared to the</p>
    <p>execution on a local GPU (GeForce 9800)  Lack of support for CUDA C extensions</p>
    <p>__kernel&lt;&lt;&lt;.&gt;&gt;&gt;()  Partial support for asynchronous data transfer</p>
    <p>MOSIX-VCL  Transparent virtualization  Large overhead even for local GPUs</p>
    <p>Average overhead: local GPU 25.95%; remote GPU 317.42%  No support for asynchronous data transfer</p>
  </div>
  <div class="page">
    <p>Outline  Motivation and Contributions</p>
    <p>Related Work</p>
    <p>VOCL Framework</p>
    <p>VOCL Optimization</p>
    <p>Experimental Results</p>
    <p>Conclusion &amp; Future work</p>
  </div>
  <div class="page">
    <p>Virtual OpenCL (VOCL) Framework Components</p>
    <p>VOCL library and proxy process</p>
    <p>Proxy</p>
    <p>Native OpenCL Library</p>
    <p>Application</p>
    <p>VOCL Library</p>
    <p>MPI</p>
    <p>Local node Remote node</p>
    <p>Proxy</p>
    <p>OpenCL API</p>
    <p>GPUGPU</p>
  </div>
  <div class="page">
    <p>VOCL Library  Located on each local node  Implements OpenCL functionality</p>
    <p>Application Programming Interface (API) compatibility  API functions in VOCL have the same interface as that in OpenCL  VOCL is transparent to application programs</p>
    <p>Application Binary Interface (ABI) compatibility  No recompilation is needed;  Needs relinking for static libraries  Uses an environment variable to preload the library for dynamic</p>
    <p>libraries</p>
    <p>Deals with both local and remote GPUs in a system  Local GPUs: Calls native OpenCL functions  Remote GPUs: Uses MPI API functions to send function calls to</p>
    <p>remote nodes 10</p>
  </div>
  <div class="page">
    <p>VOCL Abstraction: GPUs on Multiple Nodes  OpenCL object handle value</p>
    <p>Same node, each OpenCL object has a unique handle value  Different nodes, different OpenCL objects could share the same</p>
    <p>handle value</p>
    <p>VOCL abstraction  VOCL object</p>
    <p>GPU</p>
    <p>Native OpenCL Library</p>
    <p>GPU</p>
    <p>Native OpenCL Library</p>
    <p>OCLH1 OCLH3</p>
    <p>VOCL Library</p>
    <p>OCLH2</p>
    <p>OCLH1</p>
    <p>OCLH2 OCLH3</p>
    <p>OCLH1 != OCLH2 Application</p>
    <p>OCLH2 == OCLH3 struct voclObj { voclHandle vocl; oclHandle ocl; MPI_Comm com; int nodeIndex; }</p>
    <p>Each OpenCL object is translated to a VOCL object with a different handle value</p>
    <p>VOCLH1</p>
    <p>VOCLH2 VOCLH3</p>
    <p>VOCL object</p>
  </div>
  <div class="page">
    <p>VOCL Proxy  Daemon process: Initialized by the administrator  Located on each remote node</p>
    <p>Receives data communication requests (a separate thread)  Receives input data from and send output data to the application</p>
    <p>process  Calls native OpenCL functions for GPU computation</p>
    <p>Native OpenCL Library</p>
    <p>Remote node 2</p>
    <p>GPU</p>
    <p>VOCL Library</p>
    <p>Local node</p>
    <p>App</p>
    <p>GPU</p>
    <p>Native OpenCL Library</p>
    <p>Remote node 1</p>
    <p>Proxy</p>
    <p>GPU GPU</p>
    <p>MPIMPI Proxy</p>
  </div>
  <div class="page">
    <p>Outline  Motivation and Contributions</p>
    <p>Related Work</p>
    <p>VOCL Framework</p>
    <p>VOCL Optimization</p>
    <p>Experimental Results</p>
    <p>Conclusion &amp; Future work</p>
  </div>
  <div class="page">
    <p>Overhead in VOCL</p>
    <p>Local GPUs  Translation between OpenCL and VOCL handles  Overhead is negligible</p>
    <p>Remote GPUs  Translation between VOCL and OpenCL handles  Data communication between different machines</p>
    <p>GPU</p>
    <p>VOCL</p>
    <p>OpenCL</p>
    <p>VOCL</p>
    <p>OpenCL</p>
    <p>GPU</p>
    <p>OpencL</p>
    <p>Local node Remote node</p>
    <p>h o</p>
    <p>s t</p>
  </div>
  <div class="page">
    <p>Data Transfer: Between Host Memory and Device Memory  Pipelining approach</p>
    <p>Single block, each stage is transferred after another  Multiple blocks, transfer of first stage of one block can be</p>
    <p>overlapped by the second stage of another block  Pre-allocate buffer pool for data storage in the proxy</p>
    <p>GPU and memory</p>
    <p>CPU and memory</p>
    <p>CPU and memory</p>
    <p>Buffer pool</p>
    <p>Local node Remote node</p>
  </div>
  <div class="page">
    <p>Environment for Program Execution</p>
    <p>Node Configuration  Local Node</p>
    <p>2 Magny-Cours AMD CPUs  64 GB Memory</p>
    <p>Remote Node  Host: 2 Magny-Cours AMD CPUs (64GB memory)  2 Tesla M2070 GPUs (6GB global memory each)  CUDA 3.2 (OpenCL 1.1 specification)</p>
    <p>Network Connection  QDR InfiniBand</p>
    <p>CPU1CPU0</p>
    <p>GPU1GPU0 InfiniBand</p>
    <p>PCIe PCIe</p>
    <p>CPU3CPU2</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>Local node Remote node</p>
    <p>ProxyApp</p>
  </div>
  <div class="page">
    <p>Micro-benchmark Results  Continuously transfer a window of data blocks one after another  Call the clFinish() function to wait for completion</p>
    <p>for (i = 0; i &lt; N; i++) { clEnqueueWriteBuffer() } clFinish()</p>
    <p>GPU memory write bandwidth</p>
    <p>OpenCL, local VOCL, remote, pipelining VOCL, remote, no pipelining Percentage of the local GPU bandwidth, pipelining Percentage of the local GPU bandwidth, nopipelining</p>
    <p>Data block size (byte)</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>( G</p>
    <p>B /s</p>
    <p>)</p>
    <p>Pe rc</p>
    <p>en ta</p>
    <p>ge o</p>
    <p>f t</p>
    <p>h e</p>
    <p>lo ca</p>
    <p>l G P</p>
    <p>U b</p>
    <p>an d</p>
    <p>w id</p>
    <p>th</p>
    <p>Bandwidth increases from 50% to 80% of that of the local GPU</p>
    <p>CPU3CPU2</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>App CPU1CPU0</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>Proxy</p>
    <p>GPU1GPU0</p>
  </div>
  <div class="page">
    <p>Kernel Argument Setting</p>
    <p>Overhead of kernel execution for aligning one pair of sequences (6K letters) with Smith-Waterman</p>
    <p>Function Name RuntimeLocal GPU Runtime</p>
    <p>Remote GPU Overhead Number of</p>
    <p>Calls clSetKernelArg 4.33 420.45 416.02 86,028</p>
    <p>clEnqueueNDRangeKernel 1210.85 1316.92 106.07 12,288</p>
    <p>Total time 1215.18 1737.37 522.19 (Unit: ms)</p>
    <p>Local node Remote node</p>
    <p>clSetKernelArg()</p>
    <p>clSetKernelArg()</p>
    <p>clSetKernelArg()</p>
    <p>clEnqueueNDRangeKernel()</p>
    <p>int a; cl_mem b; b = clCreateBuffer(,); clSetKernelArg(hFoo, 0, sizeof(int), &amp;a); clSetKernelArg(hFoo, 1, sizeof(cl_mem), &amp;b) clEnqueueNDRangeKernel(,hFoo,);</p>
    <p>__kernel foo(int a, __global int *b) {}</p>
  </div>
  <div class="page">
    <p>Kernel Argument Setting Caching</p>
    <p>Overhead of functions related to kernel execution for aligning the same pair of sequence</p>
    <p>Function Name RuntimeLocal GPU Runtime</p>
    <p>Remote GPU Overhead Number of</p>
    <p>Calls clSetKernelArg 4.33 4.03 -0.30 86,028</p>
    <p>clEnqueueNDRangeKernel 1210.85 1344.01 133.71 12,288</p>
    <p>Total time 1215.18 1348.04 132.71 (Unit: ms)</p>
    <p>Local node Remote node</p>
    <p>clSetKernelArg()</p>
    <p>clSetKernelArg()</p>
    <p>clSetKernelArg()</p>
    <p>clEnqueueNDRangeKernel()</p>
    <p>or e</p>
    <p>ar gu</p>
    <p>m en</p>
    <p>ts lo</p>
    <p>ca lly</p>
  </div>
  <div class="page">
    <p>Outline  Motivation and Contributions</p>
    <p>Related Work</p>
    <p>VOCL Framework</p>
    <p>VOCL Optimization</p>
    <p>Experimental Results</p>
    <p>Conclusion &amp; Future work</p>
  </div>
  <div class="page">
    <p>Evaluation via Application Kernels</p>
    <p>Three application kernels  Matrix multiplication  Matrix transpose  Smith-Waterman</p>
    <p>Program execution time</p>
    <p>Relative overhead</p>
    <p>Relationship to time percentage of kernel execution</p>
    <p>CPU0</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>App CPU1</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>Proxy</p>
    <p>GPU</p>
  </div>
  <div class="page">
    <p>Matrix Multiplication</p>
    <p>Matrix size</p>
    <p>Pe rc</p>
    <p>en ta</p>
    <p>ge o</p>
    <p>f k e</p>
    <p>rn el</p>
    <p>e xe</p>
    <p>cu ti</p>
    <p>on ti</p>
    <p>m e</p>
    <p>OpenCL VOCL, local</p>
    <p>VOCL, remote Percentage of slowdown</p>
    <p>Matrix size</p>
    <p>P ro</p>
    <p>gr am</p>
    <p>e xe</p>
    <p>cu ti</p>
    <p>o n</p>
    <p>ti m</p>
    <p>e (m</p>
    <p>s)</p>
    <p>Pe rc</p>
    <p>en ta</p>
    <p>ge o</p>
    <p>f sl</p>
    <p>ow d</p>
    <p>ow n</p>
    <p>Time percentage of kernel execution</p>
    <p>Kernel execution time and performance overhead</p>
    <p>Multiple problem instances are issued consecutively for (i = 0; i &lt; N; i++) { clEnqueueWriteBuffer(); clEnqueueNDRangeKernel(); clEnqueueReadBuffer(); } clFinish();</p>
    <p>CPU0</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>App CPU1</p>
    <p>InfiniBand</p>
    <p>PCIe</p>
    <p>Proxy</p>
    <p>GPU</p>
  </div>
  <div class="page">
    <p>Matrix Transpose</p>
    <p>Time percentage of kernel execution</p>
    <p>Kernel execution time and performance overhead</p>
    <p>Ti m</p>
    <p>e pe</p>
    <p>rc en</p>
    <p>ta ge</p>
    <p>o f k</p>
    <p>er ne</p>
    <p>l e xe</p>
    <p>cu ti</p>
    <p>on</p>
    <p>Matrix size</p>
    <p>P ro</p>
    <p>gr am</p>
    <p>e xe</p>
    <p>cu ti</p>
    <p>o n</p>
    <p>ti m</p>
    <p>e (m</p>
    <p>s)</p>
    <p>Pe rc</p>
    <p>en ta</p>
    <p>ge o</p>
    <p>f sl</p>
    <p>ow d</p>
    <p>ow n</p>
    <p>Multiple problem instances are issued consecutively for (i = 0; i &lt; N; i++) { clEnqueueWriteBuffer(); clEnqueueNDRangeKernel(); clEnqueueReadBuffer(); } clFinish();</p>
  </div>
  <div class="page">
    <p>Smith-Waterman</p>
    <p>Time percentage of kernel execution</p>
    <p>Kernel execution time and performance overhead</p>
    <p>Ti m</p>
    <p>e pe</p>
    <p>rc en</p>
    <p>ta ge</p>
    <p>o f k</p>
    <p>er ne</p>
    <p>l e xe</p>
    <p>cu tio</p>
    <p>n</p>
    <p>OpenCL VOCL, local VOCL, remote Percentage of slowdown</p>
    <p>Sequence size</p>
    <p>P ro</p>
    <p>gr am</p>
    <p>e xe</p>
    <p>cu ti</p>
    <p>o n</p>
    <p>ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>Pe rc</p>
    <p>en ta</p>
    <p>ge o</p>
    <p>f sl</p>
    <p>ow d</p>
    <p>ow n</p>
    <p>Two Observations 1. SW needs a lot of kernel</p>
    <p>launches and large number of small messages are transferred</p>
    <p>for (i = 0; i &lt; N; i++) { clEnqueueWriteBuffer(); for (j = 0; j &lt; M; j++) { clEnqueueNDRangeKernel(); } clEnqueueReadBuffer(); } clFinish();</p>
  </div>
  <div class="page">
    <p>Outline  Motivation and Contributions</p>
    <p>Related Work</p>
    <p>VOCL Framework</p>
    <p>VOCL Optimization</p>
    <p>Experimental Results</p>
    <p>Conclusion &amp; Future work</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Virtual OpenCL Framework  Based on the OpenCL programming model  Internally use MPI for data communication</p>
    <p>VOCL Framework Optimization  Kernel arguments caching  GPU memory write and read pipelining</p>
    <p>Application Kernel Verification  SGEMM, n-body, Matrix transpose, and Smith-Waterman  Reasonable virtualization cost</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Extensions to the VOCL Framework  Live task migration (already done)  Super-GPU  Performance model for GPU utilization  Resource management strategies  Energy-efficient computing</p>
  </div>
  <div class="page">
    <p>For More Information</p>
    <p>Shucai Xiao  Email -- shucai@vt.edu</p>
    <p>Synergy  Website -- http://synergy.cs.vt.edu/</p>
    <p>Thanks Question?</p>
  </div>
</Presentation>
