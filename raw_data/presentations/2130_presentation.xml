<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Evalua&amp;on of Codes with Inherent Double Replica&amp;on for Hadoop</p>
    <p>M. Nikhil Krishnan, N. Prakash, V. Lalitha,</p>
    <p>Birenjith Sasidharan, P. Vijay Kumar</p>
    <p>Indian Institute of Science</p>
    <p>Srinivasan Narayanamurthy, Ranjit Kumar,</p>
    <p>Siddhartha Nandi</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Double Versus Triple Replica&amp;on  Triple replica&amp;on of data is common in Hadoop:</p>
    <p>Double replica&amp;on in comparison:</p>
    <p>o Data availability for MR opera&amp;ons (moderate workloads)</p>
    <p>o Resiliency to node failure o Via addi&amp;onal pari&amp;es (checksums) o Eg: RAID + mirroring (RAID + m)</p>
    <p>o Reduced OH</p>
    <p>o Data availability for MapReduce (MR) opera&amp;ons</p>
    <p>o Resiliency to node failure</p>
    <p>o Storage overhead (OH)</p>
  </div>
  <div class="page">
    <p>P P</p>
    <p>(10,9) RAID + m</p>
    <p>A simple example of double replica&amp;on with added parity</p>
    <p>P is an XOR of the 9 data blocks</p>
    <p>The two parity blocks ensure adequate resiliency</p>
    <p>RAID + Mirroring (RAID+m)</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>The Heptagon-Local Code (HLC) : An Alternate Code with Inherent Double Replica&amp;on</p>
    <p>G. M. Kamath, N. Prakash, V. Lalitha, and P. Vijay Kumar, ``Codes with Local Regenera&amp;on and Erasure Correc&amp;on', to appear in IEEE Trans. Inform. Theory, 2014.</p>
    <p>Looks complicated, but is not as we shall see..</p>
    <p>Heptagon 1 Heptagon 2</p>
    <p>Global Parity</p>
    <p>HLC</p>
  </div>
  <div class="page">
    <p>Performance Comparison: Overhead (OH) and Resiliency (MTTDL)</p>
    <p>The HLC has reduced overhead for the desired resiliency but there is an issue rela&amp;ng to Data Locality.</p>
    <p>Code OH MTTDL (in years)</p>
    <p>RAID + m 2.22 2.0E+09</p>
    <p>HLC 2.15 8.3E+09</p>
  </div>
  <div class="page">
    <p>Map Task Assignment: Local vs Remote Tasks</p>
    <p>DATA 1</p>
    <p>Free Processor DATA 2</p>
    <p>DATA n</p>
    <p>Busy Processor</p>
    <p>Node 1</p>
    <p>Local task</p>
    <p>Node 2 Node 3 Node 4</p>
    <p>Remote task</p>
    <p>Free map slot</p>
    <p>Busy map slot</p>
  </div>
  <div class="page">
    <p>Importance of Data Locality for Map Tasks</p>
    <p>Data Locality measured by percentage of tasks that are executed locally</p>
    <p>M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker, and I. Stoica, ``Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling,&quot; in ACM Eurosys, 2010.</p>
    <p>J. Dean and S. Ghemawat, ``Mapreduce: simplified data processing on large clusters,&quot; Communica4ons of the ACM, 2008.</p>
    <p>Increase in Data Locality</p>
    <p>Decrease in job &amp;me</p>
    <p>Decrease in network traffic</p>
  </div>
  <div class="page">
    <p>The Issue with Data Locality</p>
    <p>The HLC does not fare well in comparison with RAID+m in terms of locality for Map Task Assignment</p>
    <p>We will shortly see the reason for this</p>
    <p>One work around is to have a larger number of processors per node as we do here.</p>
  </div>
  <div class="page">
    <p>An Alternate Idea of Dealing with the Data Locality Issue</p>
    <p>Modify the HDFS to permit coding across files  this would eliminate this issue altogether (but not part of the present work)</p>
    <p>Other op&amp;ons ?</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Evolu&amp;on of the Heptagon-Local Code</p>
    <p>Regenera&amp;ng Codes and Locally Repairable Codes are two classes of codes designed with distributed storage in mind</p>
    <p>Heptagon Code</p>
    <p>Heptagon Local Code</p>
    <p>Pentagon Code</p>
    <p>Examples of a Regenera&amp;ng code</p>
    <p>Regenera&amp;ng Code</p>
    <p>Locally Repairable Code</p>
  </div>
  <div class="page">
    <p>N4</p>
    <p>The Pentagon Code</p>
    <p>An example of a code with inherent double replica&amp;on</p>
    <p>9 data blocks encoded into 20 blocks and stored in 5 nodes</p>
    <p>Each node stores 4 blocks</p>
    <p>N3 N2 N1 N5</p>
  </div>
  <div class="page">
    <p>Building the Pentagon Code</p>
    <p>N. B. Shah, K. V. Rashmi, P. Vijay Kumar and K. Ramchandran, ``Distributed Storage Codes with Repair- by-Transfer and Non-achievability of Interior Points on the Storage-Bandwidth Tradeoff, '' IEEE Trans. Inform. Theory, Mar. 2012.</p>
    <p>Input: 9 data blocks</p>
  </div>
  <div class="page">
    <p>Input: 9 data blocks  An addi&amp;onal XOR parity is</p>
    <p>added</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>Step 1: Adding a Single Checksum</p>
  </div>
  <div class="page">
    <p>Input: 9 data blocks</p>
    <p>An addi&amp;onal XOR parity is added</p>
    <p>The resultant 10 symbols are</p>
    <p>placed on the edges of a fully- connected pentagon</p>
    <p>P</p>
    <p>N1</p>
    <p>N2</p>
    <p>N3 N4</p>
    <p>N5</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>Step 2: Label the Edges of the Graph with these 10 Blocks</p>
  </div>
  <div class="page">
    <p>Each node stores the data appearing on its incoming edges and we are done!</p>
    <p>P</p>
    <p>N1</p>
    <p>N2</p>
    <p>N3 N4</p>
    <p>N5</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>Final Step: Using Edge Labels to Populate the Nodes</p>
    <p>Input: 9 data blocks</p>
    <p>An addi&amp;onal XOR parity is added</p>
    <p>The resultant 10 symbols are</p>
    <p>placed on the edges of a fully- connected pentagon</p>
  </div>
  <div class="page">
    <p>The Pentagon Code can be viewed as a compact rearrangement of the block of the (10,9) RAID+m code</p>
    <p>N1</p>
    <p>N2</p>
    <p>N3 N4</p>
    <p>N5</p>
    <p>An Alternate View: the Pentagon Code as a Rearrangement of RAID+m</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>RAID + m Code rearrange</p>
  </div>
  <div class="page">
    <p>Can tolerate 2 (out of 5) node failures</p>
    <p>P</p>
    <p>N1</p>
    <p>N2</p>
    <p>N3 N4</p>
    <p>N5</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>Resiliency of the Pentagon Code</p>
  </div>
  <div class="page">
    <p>Can tolerate 2 (out of 5) node failures</p>
    <p>All blocks except with the excep&amp;on of one block can be copied from the other 3 nodes</p>
    <p>P</p>
    <p>N1</p>
    <p>N2</p>
    <p>N3 N4</p>
    <p>N5</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>Resiliency of the Pentagon Code</p>
  </div>
  <div class="page">
    <p>Can tolerate 2 (out of 5) node failures</p>
    <p>All blocks except with the excep&amp;on of one block can be copied from the other 3 nodes</p>
    <p>Block shared between the two</p>
    <p>failed nodes, recovered via parity</p>
    <p>P</p>
    <p>N1</p>
    <p>N2</p>
    <p>N3 N4</p>
    <p>N5</p>
    <p>P 1 2 3 4 5 6 7 8 9</p>
    <p>Resiliency of the Pentagon Code</p>
    <p>P</p>
  </div>
  <div class="page">
    <p>Construc&amp;on extends to a polygon of any size</p>
    <p>P</p>
    <p>Pentagon : (10, 9) RAID + m</p>
    <p>Heptagon : (21, 20) RAID + m</p>
    <p>Extending the Pentagon to a Heptagon</p>
  </div>
  <div class="page">
    <p>Construc&amp;on extends to a polygon of any size</p>
    <p>P</p>
    <p>Pentagon : (10, 9) RAID + m</p>
    <p>Heptagon : (21, 20) RAID + m</p>
    <p>&lt; 20 9</p>
    <p>OH decreases</p>
    <p>But resiliency Is reduced</p>
    <p>Extending the Pentagon to a Heptagon</p>
  </div>
  <div class="page">
    <p>Finally: The Heptagon-Local Code (HLC)</p>
    <p>Global pari&amp;es offer a simple way to increase the resiliency of the Heptagon Code, at a slight increase in OH</p>
    <p>G. M. Kamath, N. Prakash, V. Lalitha, and P. Vijay Kumar, ``Codes with Local Regenera&amp;on and Erasure Correc&amp;on', accepted for publica&amp;on in IEEE Trans. Inform. Theory, 2014.</p>
    <p>Heptagon 1 Heptagon 2</p>
    <p>Global Pari&amp;es</p>
  </div>
  <div class="page">
    <p>Overall : 40 data blocks 86 encoded blocks  Stored in 15 nodes  Can tolerate any 3 (out of 15) node failures</p>
    <p>Global parity is computed on all 40</p>
    <p>data blocks Heptagon 1 Heptagon 2</p>
    <p>Heptagon-Local Code</p>
  </div>
  <div class="page">
    <p>Heptagon-Local Code: Recovering from Two Node Erasures</p>
  </div>
  <div class="page">
    <p>Heptagon-Local Code: Recovering from Three Node Erasures</p>
  </div>
  <div class="page">
    <p>Overhead vs Resiliency</p>
    <p>Q. Xin, E. L. Miller, T. Schwarz, D. D. Long, S. A. Brandt, and W. Litwin, ``Reliability mechanisms for very large storage systems,&quot; in IEEE Massive Storage Systems and Technology (MSST), 2003.</p>
    <p>MTTDL calculated for a 25 node system using standard models  The heptagon-local code is a berer choice than RAID + m</p>
    <p>Code Overhead MTTDL (in years)</p>
    <p>Pentagon 2.22 1.05E+08 Heptagon 2.1 2.68E+07</p>
    <p>(10, 9) RAID + m 2.22 2.03E+09 (12,11) RAID + m 2.18 6.50E+08 Heptagon-Local 2.15 8.34E+09</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Data Locality for the Heptagon Code</p>
    <p>Data Locality affected by the concentra&amp;on of data blocks in the Heptagon Code</p>
    <p>Say 2 map slots / node and MR job on 1 Heptagon-Coded file  No. of local tasks  (7 X 2) = 14  No. of remote tasks  (20  14) = 6</p>
    <p>Data Locality : RAID + m &gt; Heptagon-Local</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Experimental Set-up  Implemented the Pentagon and Heptagon-Local Codes on top</p>
    <p>of HDFS-RAID  HDFS-RAID : Facebook's open-source implementa&amp;on of</p>
    <p>Reed-Solomon codes over HDFS</p>
    <p>SET-UP A SET-UP B</p>
    <p>Processors (Map Slots) Per Node</p>
    <p>Nodes 25 10</p>
    <p>Plaxorms Dual-Core Laptops Server-Class Machines</p>
    <p>Codes Tested Pentagon, Heptagon-Local* Pentagon</p>
    <p>* The Heptagon code was used in the experiments, but both Heptagon and Heptagon-Local have the same Map-Reduce performance</p>
  </div>
  <div class="page">
    <p>MR Performance (Batch Jobs)</p>
    <p>As expected, substan&amp;al loss in performance with 2 map slots</p>
    <p>With 4 map slots, pentagon code performs well, even at load of 75 %</p>
    <p>Set-up 1 : 2 map slots / node</p>
    <p>Set-up 2 : 4 map slots / node</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>We discussed a coding scheme  the Heptagon-Local Code, having inherent double replica&amp;on for data blocks</p>
    <p>The Heptagon-Local Code enjoys good overhead and resiliency, when compared with other schemes such as RAID +m</p>
    <p>For good data locality, a larger number of processor cores / node is needed</p>
  </div>
  <div class="page">
    <p>Future Work : Restoring Data Locality  Modify the HDFS to permit erasure coding across files</p>
    <p>which would eliminate the issue of data concentra&amp;on</p>
    <p>Heptagon 1 Heptagon 2</p>
  </div>
  <div class="page">
    <p>These codes possess a single copy of data block and in the Hadoop context, are perhaps most suited for cold-data storage.</p>
    <p>Related Work : Erasure Codes in Distributed Storage</p>
    <p>Implementa&amp;on of locally repairable codes, focus on overhead vs repair-cost</p>
    <p>(Huang et al, Sathiamoorthy et al)</p>
    <p>Regenera&amp;ng codes for cloud based systems, focus</p>
    <p>on decreasing repair bandwidth</p>
    <p>(Hu et al, Runhui Li et al)</p>
    <p>A new erasure coding scheme having berer single-node repair cost than a Reed-</p>
    <p>Solomon code (Rashmi et al)</p>
    <p>A new class of codes (rotated Reed-Solomon codes) for improved</p>
    <p>degraded read performance (Khan et al)</p>
  </div>
  <div class="page">
    <p>Thanks !</p>
  </div>
</Presentation>
