<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>NetTLP: A Development Platform for PCIe Devices in Software Interacting with Hardware</p>
    <p>Yohei Kuga (The University of Tokyo) Ryo Nakamura (The University of Tokyo)</p>
    <p>Takeshi Matsuya (Keio University) Yuji Sekiya (The University of Tokyo)</p>
  </div>
  <div class="page">
    <p>PCI Express-Based Heterogeneous Computing  PCI Express (PCIe) is the most popular</p>
    <p>interconnect standard for communicating between accelerator, storage, and network devices</p>
    <p>PCIe is a packet-based protocol  PCIe topology is flexible  PCIe switch and root complex</p>
    <p>forward PCIe packets to other PCIe devices</p>
    <p>PCIe devices can communicate directly by using the PCIe switch</p>
    <p>CPU Memory</p>
    <p>Root ComplexPCIe Switch</p>
    <p>GPU RDMA HCA Accelerator Accelerator</p>
    <p>CPUMemory</p>
    <p>Root Complex</p>
    <p>RDMA HCA NVMe</p>
    <p>CPU-to-Device Device-to-Device Remote DMA</p>
  </div>
  <div class="page">
    <p>Problem: Lack of Productivity and Observability on PCIe</p>
    <p>Why cant we develop PCIe the same way as IP networking  Although both PCIe and IP are packet-based data communication standards</p>
    <p>Prototyping a PCIe device by FPGA still requires significant effort  Such as in the NetFPGA project</p>
    <p>Observing PCIe transactions is also difficult  Because they are confined in hardware and require special analyzers</p>
    <p>IP networks PCI Express</p>
    <p>Type of data communication Packet-based Packet-based</p>
    <p>Components Software and hardware Hardware</p>
    <p>Analyzing by tcpdump, Wireshark, etc FPGA, special hardware</p>
    <p>Gap between Software and Hardware</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>PCIe device Software Hardware</p>
    <p>Root complex Software QEMU Hardware NetTLP FPGA/ASIC</p>
    <p>Bridge the gap between hardware and software for PCIe  QEMU performs everything in software but without actual PCIe protocols  FPGA and ASIC handle actual PCIe transactions in hardware,</p>
    <p>but developing them is still hard compared with software-based platforms</p>
    <p>NetTLP provides high productivity and observability for PCIe developments by connecting software PCIe devices to hardware root complexes</p>
  </div>
  <div class="page">
    <p>NetTLP approach</p>
    <p>Separating the PCIe transaction layer into software  Software PCIe devices communicate with hardware root complexes on</p>
    <p>the PCIe transaction layer</p>
    <p>Bridging the software transaction layer with hardware data link layer by delivering TLPs over Ethernet</p>
    <p>It is possible because both use packet-based data communication</p>
    <p>Transaction Layer</p>
    <p>Data Link Layer</p>
    <p>Physical Layer TX RX</p>
    <p>Data Link Layer</p>
    <p>Physical Layer TX RX</p>
    <p>Software-Hardware bridge</p>
    <p>PCIe link</p>
    <p>Root Complex PCIe device TLP Software-based</p>
    <p>Transaction LayerTLP manipulation platform  [ExpEther HOTI06]  [Thunderclap NDSS19] NetTLP target  Software PCIe device</p>
  </div>
  <div class="page">
    <p>NetTLP Overview</p>
    <p>PCIe devices work as Linux commands NetTLP is composed of two hosts:  Adapter host has the NetTLP</p>
    <p>adapter which bridges a PCIe link and an Ethernet link</p>
    <p>Device host has LibTLP-based application that performs the role of the NetTLP adapter</p>
    <p>Device HostNetTLP Adapter</p>
    <p>IP N</p>
    <p>et w</p>
    <p>or k</p>
    <p>S ta</p>
    <p>ck</p>
    <p>Userspcae Applications ./dma_read</p>
    <p>./msix ./memory</p>
    <p>etc</p>
    <p>Root Complex</p>
    <p>PCIe Device CPU Memory</p>
    <p>LibTLP</p>
    <p>Linux kernel</p>
    <p>Adapter Host Et he</p>
    <p>rn et</p>
    <p>N IC</p>
    <p>E th</p>
    <p>er ne</p>
    <p>t P H</p>
    <p>Y</p>
    <p>PCIe config space</p>
    <p>BAR Addresses MSI-X registers</p>
    <p>BAR0: Adapter Configs</p>
    <p>Requester ID Encap Addresses</p>
    <p>PCIe Interface</p>
    <p>BAR4 BAR2: MSI-X table</p>
    <p>UDPencaped TLPs</p>
    <p>A PCIe device that you can develop in software</p>
    <p>NetTLP Adapter: Encap/Decap TLPs in IP headers</p>
    <p>LibTLP: A software library performing PCIe Transaction Layer</p>
    <p>Ethernet IP</p>
    <p>UDP NetTLP</p>
    <p>TLP TLP data</p>
  </div>
  <div class="page">
    <p>Example 1: DMA Read by Software from the Device Host</p>
    <p>Device HostNetTLP Adapter</p>
    <p>IP N</p>
    <p>et w</p>
    <p>or k</p>
    <p>S ta</p>
    <p>ck</p>
    <p>Userspcae Applications</p>
    <p>./dma_read</p>
    <p>Root Complex</p>
    <p>PCIe Device CPU Memory</p>
    <p>LibTLP</p>
    <p>Linux kernel</p>
    <p>A PCIe device</p>
    <p>Adapter Host Et he</p>
    <p>rn et</p>
    <p>N IC</p>
    <p>E th</p>
    <p>er ne</p>
    <p>t P H</p>
    <p>Y</p>
    <p>tcpdump can see the TLPs here!</p>
    <p>sends the inner DMA read TLP to the root complex</p>
    <p>PCIe config space</p>
    <p>BAR Addresses MSI-X registers</p>
    <p>BAR0: Adapter Configs</p>
    <p>Requester ID Encap Addresses</p>
    <p>PCIe Interface</p>
    <p>BAR4 BAR2: MSI-X table</p>
    <p>UDPencaped TLPs</p>
  </div>
  <div class="page">
    <p>Example 2: Generating MSI-X Interrupts in NetTLP Platform</p>
    <p>the NetTLP adapter and MSI-X message address and data from the MSI-X table in BAR2</p>
    <p>Device HostNetTLP Adapter</p>
    <p>IP N</p>
    <p>et w</p>
    <p>or k</p>
    <p>S ta</p>
    <p>ck</p>
    <p>Userspcae Applications</p>
    <p>./msix</p>
    <p>Root Complex</p>
    <p>PCIe Device CPU Memory</p>
    <p>LibTLP</p>
    <p>Linux kernel</p>
    <p>A PCIe device</p>
    <p>Adapter Host Et he</p>
    <p>rn et</p>
    <p>N IC</p>
    <p>E th</p>
    <p>er ne</p>
    <p>t P H</p>
    <p>Y</p>
    <p>PCIe config space</p>
    <p>BAR Addresses MSI-X registers</p>
    <p>BAR0: Adapter Configs</p>
    <p>Requester ID Encap Addresses</p>
    <p>PCIe Interface</p>
    <p>BAR4 BAR2: MSI-X table</p>
    <p>UDPencaped TLPs</p>
  </div>
  <div class="page">
    <p>Example 3: Capturing TLPs from Other PCIe Devices</p>
    <p>Device HostNetTLP Adapter</p>
    <p>IP N</p>
    <p>et w</p>
    <p>or k</p>
    <p>S ta</p>
    <p>ck</p>
    <p>Userspcae Applications</p>
    <p>./memory</p>
    <p>Root Complex</p>
    <p>PCIe Device CPU Memory</p>
    <p>LibTLP</p>
    <p>Linux kernel</p>
    <p>A PCIe device</p>
    <p>Adapter Host Et he</p>
    <p>rn et</p>
    <p>N IC</p>
    <p>E th</p>
    <p>er ne</p>
    <p>t P H</p>
    <p>Y</p>
    <p>tcpdump can see the TLPs here!</p>
    <p>PCIe config space</p>
    <p>BAR Addresses MSI-X registers</p>
    <p>BAR0: Adapter Configs</p>
    <p>Requester ID Encap Addresses</p>
    <p>PCIe Interface</p>
    <p>BAR4 BAR2: MSI-X table</p>
    <p>UDPencaped TLPs</p>
    <p>Original DMA path</p>
  </div>
  <div class="page">
    <p>LibTLP Design: DMA APIs</p>
    <p>DMA APIs are inspired by read(2) and write(2) system calls  dma_read() attempts to read up to `count` bytes into `buf`  dma_write() writes up to `count` bytes from `buf`  `addr` indicates a target address of DMA transaction  The return values of the functions  Success: the number of bytes read or written  Error: returns -1 and sets errno</p>
    <p>ssize_t dma_read(struct nettlp *nt, uintptr_t addr, void *buf, size_t count); ssize_t dma_write(struct nettlp *nt, uintptr_t addr, void *buf, size_t count);</p>
  </div>
  <div class="page">
    <p>LibTLP Design: PIO APIs</p>
    <p>Register the functions receiving the request TLPs using callback API  Call nettlp_run_cb() / nettlp_stop_cb() to start/stop the software device</p>
    <p>struct nettlp_cb { int (*mrd)(struct nettlp *nt, struct tlp_mr_hdr *mh, ); int (*mwr)(struct nettlp *nt, struct tlp_mr_hdr *mh, ); int (*cpl)(struct nettlp *nt, struct tlp_cpl_hdr *ch, ); int (*cpld)(struct nettlp *nt, struct tlp_cpl_hdr *ch, ); int (*other)(struct nettlp *nt, struct tlp_hdr *tlp, );</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Example) dma_read.c</p>
    <p>Programing PCIe devices in the same manner as IP packet processing with Linux</p>
    <p>#include &lt;stdio.h&gt; #include &lt;arpa/inet.h&gt; #include &lt;libtlp.h&gt;</p>
    <p>int main(int argc, char **argv) { uintptr_t addr = 0x0; struct nettlp nt; char buf[128]; int ret;</p>
    <p>inet_pton(AF_INET, &quot;192.168.10.1&quot;, &amp;nt.remote_addr); inet_pton(AF_INET, &quot;192.168.10.3&quot;, &amp;nt.local_addr); nt.requester = (0x1a &lt;&lt; 8 | 0x00); nt.tag = 0;</p>
    <p>nettlp_init(&amp;nt);</p>
    <p>ret = dma_read(&amp;nt, addr, buf, sizeof(buf)); if (ret &lt; 0) {</p>
    <p>perror(&quot;dma_read&quot;); return ret;</p>
    <p>}</p>
    <p>printf(&quot;DMA read: %d bytes from 0x%lxn&quot;, ret, addr); return 0;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Observing Actual TLPs with Tcpdump and Wireshark!</p>
    <p>Captured the DMA read TLPs from the physical NIC</p>
    <p>./memory replied with the Completion TLPs to the NIC</p>
    <p>Detail of TLP header Weve implemented an FPGA-based NetTLP adapter with 10Gbps Ethernet and PCIe Gen2 interface</p>
  </div>
  <div class="page">
    <p>Challenge 1: Receiving Burst TLPs  PCIe could momentarily send TLPs at Ethernet wire-speed</p>
    <p>PCIe endpoints use different TLP tag values to send consecutive DMA read requests (split-transaction)</p>
    <p>The encapsulated DMA read TLP is 64 bytes = Ethernet short packet size  LibTLP needs to receive such burst TLPs</p>
    <p>DMA Read Requests for writing 8 blocks issued from Samsung PM1725a NVMe (captured by NetTLP)</p>
    <p>This NVMe sends 64 DMA read requests at a time in this experiment</p>
  </div>
  <div class="page">
    <p>Challenge 1: Receiving Burst TLPs  Exploiting multi-cores and multi-queues for PCIe transactions from software  NetTLP adapter maps TLP tag values to UDP port numbers for encapsulation</p>
    <p>TLPs are delivered through different UDP flows based on the tag field  LibTLP receives the flows by different NIC queues and CPU cores</p>
    <p>Our implementation with 16 core: DMA read 3.6 Gbps</p>
    <p>LibTLPNIC</p>
    <p>Fl ow</p>
    <p>D ire</p>
    <p>ct or</p>
    <p>HW RX Queue #0</p>
    <p>HW RX Queue #1</p>
    <p>HW RX Queue #2</p>
    <p>N et</p>
    <p>w or</p>
    <p>k st</p>
    <p>ac k</p>
    <p>Thread CPU #0</p>
    <p>Thread CPU #1</p>
    <p>Thread CPU #2</p>
    <p>UDP port 0x3000</p>
    <p>NetTLP adapter</p>
    <p>TLP tag 0</p>
    <p>TLP tag 1</p>
    <p>TLP tag 2 DMA read throughput from</p>
    <p>NetTLP adapter to LibTLP</p>
    <p>DMA read throughput from LibTLP to the NetTLP adapter</p>
    <p>th ro</p>
    <p>ug hp</p>
    <p>ut (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>D0A 256B D0A 512B D0A 1024B</p>
    <p>th ro</p>
    <p>ug hp</p>
    <p>ut (</p>
    <p>G bp</p>
    <p>s)</p>
  </div>
  <div class="page">
    <p>Challenge 2: Completion Timeout  PCIe specification defines the completion timeout</p>
    <p>Minimal range is 50 us to 10 ms  PCIe specification recommends that PCIe devices</p>
    <p>do not expire in less than 10 ms  Intel X520 NIC sets the range from 50 us to 50 ms</p>
    <p>Our software implementation result:  99% DMA read latency is less than 27 us</p>
    <p>DMA read latency from LibTLP to NetTLP adapter</p>
    <p>Completion timeout of Intel X520 NIC</p>
    <p>C D</p>
    <p>)</p>
    <p>D0A 1B D0A 256B D0A 1024B</p>
    <p>$ sudo lspci -vv 01:00.0 Ethernet controller: Intel Corporation 82599ES DevCtl: MaxPayload 128 bytes, MaxReadReq 512 bytes DevCtl2: Completion Timeout: 50us to 50ms,</p>
  </div>
  <div class="page">
    <p>Adapter Host</p>
    <p>./memory</p>
    <p>Use Case 1: Observing Root Complex and PCIe Switch Behavior</p>
    <p>./dma_read sends a 512B DMA read request  Root complex splits the 512B DMA read into</p>
    <p>eight 64B request TLPs and rebuilds two 256B completion TLPs (MaxPayloadSize = 256B)</p>
    <p>Root complex (Intel Core i9-9820X)</p>
    <p>Root Complex / PCIe Switch</p>
    <p>Ethernet Switch</p>
    <p>NetTLP Adapter 1</p>
    <p>NetTLP Adapter 2</p>
    <p>./dma_read</p>
    <p>PC Ie</p>
    <p>Et he</p>
    <p>rn et</p>
    <p>Port mirror and tcpdump</p>
    <p>A software PCIe device</p>
    <p>A software PCIe device</p>
    <p>PCIe switch (PLX8747)</p>
  </div>
  <div class="page">
    <p>Use Case 2: A Nonexistent NIC  To confirm the productivity of NetTLP, we implemented an Ethernet NIC</p>
    <p>Target NIC: simple-nic introduced by [pcie-bench SIGCOMM18]  A theoretical model of a simple Ethernet NIC</p>
    <p>./simple-nic uses a tap interface as its Ethernet port</p>
    <p>Device Host</p>
    <p>Network Stack</p>
    <p>PCIe Interface</p>
    <p>./simple-nic</p>
    <p>Root Complex</p>
    <p>CPU Memory</p>
    <p>Linux kernel</p>
    <p>Adapter Host</p>
    <p>Ethernet NIC10 G</p>
    <p>E</p>
    <p>th er</p>
    <p>ne t</p>
    <p>P H</p>
    <p>YNetTLP Adapter</p>
    <p>LibTLP</p>
    <p>An Ethernet NIC</p>
    <p>eth0 tap0</p>
  </div>
  <div class="page">
    <p>The simple-nic model certainly works with a root complex</p>
    <p>MWr, 3DW, WD, tc 0, flags [none], attrs [none], len 1, requester 00:00, tag 0x01, last 0x0, first 0xf, Addr 0xb0000010 MRd, 3DW, tc 0, flags [none], attrs [none], len 4, requester 1b:00, tag 0x01, last 0xf, first 0xf, Addr 0x2f004000 CplD, 3DW, WD, tc 0, flags [none], attrs [none], len 4, completer 00:00, success, byte count 16, requester 1b:00, tag 0x01, lowaddr 0x00 MRd, 3DW, tc 0, flags [none], attrs [none], len 25, requester 1b:00, tag 0x01, last 0x3, first 0xf, Addr 0x3bdc1000 CplD, 3DW, WD, tc 0, flags [none], attrs [none], len 25, completer 00:00, success, byte count 98, requester 1b:00, tag 0x01, lowaddr 0x00 MWr, 3DW, WD, tc 0, flags [none], attrs [none], len 1, requester 1b:00, tag 0x01, last 0x0, first 0xf, Addr 0xfee1a000</p>
    <p>./simple-nic on the NetTLP platform can TX/RX packets  All the PCIe interactions with the root complex can be observed by tcpdump  The device code is 400 LoC in C</p>
    <p>tcpdump outputs (packet info only) for sending an ICMP echo packet from the host</p>
  </div>
  <div class="page">
    <p>Summary  NetTLP enables developing PCIe devices in software with IP networking style</p>
    <p>NetTLP adapter is the bridge between PCIe and Ethernet links  LibTLP enables software PCIe devices on top of IP network stacks</p>
    <p>In the results  Observing actual TLPs with tcpdump and Wireshark  Implemented the simple Ethernet NIC model in 400 lines of C code</p>
    <p>Benchmarks, other use cases (capturing TLPs from 4 product devices and memory introspection), and their details are available in our paper</p>
    <p>Source code and raw pcap data are available at https://haeena.dev/nettlp</p>
  </div>
</Presentation>
