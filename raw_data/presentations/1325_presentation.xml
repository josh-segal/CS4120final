<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Cluster-Based Delta Compression of a Collection of Files</p>
    <p>Zan Ouyang Nasir Memon</p>
    <p>Torsten Suel Dimitre Trendafilov</p>
    <p>CIS Department Polytechnic University</p>
    <p>Brooklyn, NY 11201</p>
  </div>
  <div class="page">
    <p>Overview:</p>
    <p>Problem: how to achieve better compression of large collections of files by exploiting similarity among the files</p>
  </div>
  <div class="page">
    <p>Delta Compression: (differential compression) 1. Introduction</p>
    <p>Goal: to improve communication and storage efficiency by exploiting file similarity</p>
    <p>Example: Distribution of software updates - new and old version are similar - only need to send a patch to update - delta compression: how to compute concise patch</p>
    <p>a delta compressor encodes a target file w.r.t. a reference file by computing a highly compressed representation of their differences</p>
  </div>
  <div class="page">
    <p>Application Scenario:</p>
    <p>server has copy of both files local problem at server</p>
    <p>Server Client</p>
    <p>d(F_new, F_old) F_new</p>
    <p>F_old F_old</p>
    <p>request</p>
    <p>Remote File Synchronization: (e.g., rsync)</p>
    <p>server does not know need protocol between two parties</p>
  </div>
  <div class="page">
    <p>Applications:</p>
    <p>patches for software upgrades  revision control systems (RCS, CVS)  versioning file systems  improving HTTP performance - optimistic deltas for latency (AT&amp;T) - deltas for wireless links</p>
    <p>storing document collections (web sites, AvantGo)  incremental backup  compression of file collection (improving tar+gzip)</p>
  </div>
  <div class="page">
    <p>Problem in this Paper:</p>
    <p>we are given a set of N files</p>
    <p>some of the files share some similarities, but we do not know which are most similar</p>
    <p>what is the best way to efficiently encode the collection using delta compression?</p>
    <p>limitation: we allow only a single reference file for each target file</p>
    <p>need to find a good sequence of delta compression operations between pairs of files</p>
  </div>
  <div class="page">
    <p>Delta Compression Tools and Approaches</p>
    <p>compute an edit sequence between the files - used by diff and bdiff utilities</p>
    <p>- what is a good distance measure? (block move, copy)</p>
    <p>practical approach based on Lempel/Ziv: - encode repeated substrings by pointing to previous occurrences and occurrences in the reference file</p>
    <p>available tools - vdelta/vcdiff K.-P. Vo (AT&amp;T) - xdelta J. MacDonald (UCB) - zdelta Trendafilov, Memon, Suel (Poly)</p>
  </div>
  <div class="page">
    <p>Some Technical Details:</p>
    <p>in reality, some more difficulties  how to express distances in reference file</p>
    <p>- sources of copies in F_ref aligned - better coding</p>
    <p>how to handle long files - window shifting in both files</p>
    <p>F_target</p>
    <p>F_ref</p>
    <p>F_target</p>
    <p>F_ref</p>
  </div>
  <div class="page">
    <p>Performance of delta compressors:</p>
    <p>gzip, vcdiff, zdelta, zdelta  files: - gnu/emacs (Hunt/Vo/Tichy)  caveat for running times: I/O issues gzip &amp; zdelta: first number direct access, second standard I/O vcdiff numbers: standard I/O</p>
    <p>gcc size (KB) time (s) orig. 27288 gzip 7479 24/30 xdelta 461 20 vcdiff 289 33 zdelta 250 26/32</p>
    <p>emacs size (KB) time (s) orig. 27326 gzip 8191 26/35 xdelta 2131 29 vcdiff 1821 36 zdelta 1465 35/42</p>
  </div>
  <div class="page">
    <p>reduction to the Maximum Branching Problem in a weighted directed graph</p>
    <p>a branching is an acyclic subset of edges with at most one incoming edge for each node</p>
    <p>a maximum branching is a branching of max. weight (sum of the weights of the edges)</p>
    <p>can be solved in near-linear time</p>
  </div>
  <div class="page">
    <p>empty D_1</p>
    <p>D_2</p>
    <p>D_3</p>
    <p>D_4</p>
    <p>w(i,j) = size of delta between D_i and D_j</p>
    <p>Reduction to Optimum Branching:</p>
    <p>one node for each document  directed edges between every pair of nodes  weight equal to benefit due to delta compression</p>
    <p>Algorithm:</p>
    <p>generate graph  compute optimum branching  delta compression according to branching</p>
  </div>
  <div class="page">
    <p>Summary of known results:</p>
    <p>optimal delta in polynomial time if each file uses only one reference file (optimal branching)</p>
    <p>Problem with the optimal branching approach: - N^2 edges between N documents - solution might have long chains</p>
    <p>NP-complete if we limit length of chains [Tate 91]</p>
    <p>NP-complete if we use more than one reference per file (hypergraph branching, Adler/Mitzenmacher 2001)</p>
  </div>
  <div class="page">
    <p>Experimental Results for Optimum Branching:</p>
    <p>fast implementation based on Boost library  tested on collections of pages from web sites</p>
    <p>zdelta</p>
    <p>gzip</p>
    <p>inefficient for more than a few hundred files  most time spent on computing the weights  branching much faster (but also N^2)</p>
  </div>
  <div class="page">
    <p>Challenge: to compute a good branching without computing all N^2 edges precisely</p>
    <p>General Framework:</p>
    <p>(1) Collection Analysis Perform a clustering computation that identifies pairs of files that are good candidates for delta compression. Build a sparse subgraph G with edges between pairs.</p>
    <p>(2) Weight Assignment Compute or approximate edge weights for G.</p>
    <p>(3) Maximum Branching Perform a maximum branching computation on G</p>
  </div>
  <div class="page">
    <p>Techniques for efficient clustering</p>
    <p>Recent clustering techniques based on sampling by Broder and by Indyk</p>
    <p>- first generate all q-grams (substrings of length q) - files are similar if they have many q-grams in common</p>
    <p>- use minwise independent hashing to select a small number w of representative q-grams for each file</p>
    <p>Broder: O(w* N^2) to estimate distances between all N^2 pairs of files (also by Manber/Wu)</p>
    <p>Indyk: O(w *N * log(N)) to identify pairs of files with distance below a chosen threshold</p>
  </div>
  <div class="page">
    <p>Techniques for efficient clustering (cont.)</p>
    <p>Broder: minwise hashing (MH) - use random hash function to map q-grams to integers - retain the smallest integer obtained as sample - repeat w times to get w samples (can be optimized) - use samples to estimate distance between each pair</p>
    <p>Indyk: Locally sensitive hashing (LSH) - sampling same as before</p>
    <p>- instead of computing distance between each pair of samples, construct signatures from sample</p>
    <p>- signature = bitwise concatenation of several samples</p>
    <p>- samples have large intersection if identical signature - repeat a few times</p>
  </div>
  <div class="page">
    <p>Discussion of Distance Measure</p>
    <p>Two possible distance measures on q-grams:</p>
    <p>- Let S(F) be the set of q-grams of file F - Intersection: d(F, F) = (S(F) intersect S(F)) / (S(F) union S(F))</p>
    <p>- Containment: d(F, F) = (S(F) intersect S(F)) / S(F)</p>
    <p>Intersection symmetric, Containment asymmetric  There are formal bounds relating both measures to edit distance with and without block moves</p>
    <p>Inaccuracy due to distance measure and</p>
    <p>sampling  In practice OK (as we will see)</p>
  </div>
  <div class="page">
    <p>Design decisions:</p>
    <p>Intersection or inclusion measure?  Which edges to keep? - all edges above a certain similarity threshold - the k best incoming edges for each file (NN)</p>
    <p>Approximate or exact weights for G ?  Different sampling approaches  Sampling ratios  Value of q for q-grams (we used q=4)  Indyk approach supports only intersection and similarity threshold (extension difficult)</p>
    <p>Indyk does not provide weight estimate</p>
  </div>
  <div class="page">
    <p>Implementation in C++ using zdelta compressor</p>
    <p>Experiments on P4 and UltraSparc architectures</p>
    <p>Data sets: - union of 6 commercial web sites used before (49 MB) - 21000 pages from poly.edu domain (257 MB) - gcc and emacs distributions (&gt;1000 files, 25 MB) - changed pages from web sites</p>
    <p>Comparison with gzip, tar+gzip, and cat+gzip</p>
    <p>See paper and report for details</p>
  </div>
  <div class="page">
    <p>Threshold-Based Approach:</p>
    <p>Keep edges with distance below threshold  Based on Broder (MH)  Fixed sample size and fixed sampling ratio</p>
    <p>algorithm smp size threshold remaining edges br. Size benefit over zlib</p>
    <p>optimal 2782224 1667 6980935 MH 100 20% 357961 1616 6953569 intersect 40% 154533 1434 6601218</p>
    <p>MH 1/128 20% 391682 1641 6961645 intersect 40% 165563 1481 6665907</p>
    <p>MH 1/128 20% 1258272 1658 6977748 contain 40% 463213 1638 6943999</p>
  </div>
  <div class="page">
    <p>Discussion of threshold-based approach:</p>
    <p>Sampling method not critical</p>
    <p>Containment slightly better than Intersection (but can result in undesirable pairings)</p>
    <p>There is no really good threshold - low threshold keeps too many edges (inefficient) - high threshold cuts too many edges (bad compression)</p>
    <p>Possible solutions - use k nearest neighbors - use estimates instead of precise edge weights - use other heuristics to prune edges from dense areas</p>
  </div>
  <div class="page">
    <p>Nearest Neighbor-Based Approach:</p>
    <p>Keep edges from k nearest neighbors (k = 1, 2, 4, 8)  Based on Broder (MH) with fixed sampling ratio</p>
    <p>sample sizek cluster timeweighting timebr. time benefit over zlib</p>
    <p>Good compression even for small k  Time for computing weights increases linearly with k  Clustering time linear in ratio and quadratic in # files  Open theoretical question on performance of k-NN</p>
  </div>
  <div class="page">
    <p>Nearest Neighbor with Estimated Weights:</p>
    <p>Keep edges from k nearest neighbors (k = 1, 2, 3, 4)  Use intersection estimate as weight of edge</p>
    <p>Compression only slightly worse than for exact weights  Time for computing weights reduced to zero  Clustering time still quadratic in # files but OK for up to thousands of files  zdelta time affected by disk (compared to tar+gzip)</p>
    <p>k cluster timebranching timezdelta time benefit over zlib (MB)</p>
  </div>
  <div class="page">
    <p>Indyk (LSH) with Pruning Heuristic:</p>
    <p>Threshold and fixed sampling ratio  Heuristic: keep only sparse subgraph everywhere</p>
    <p>Compression degrades somewhat</p>
    <p>Could try to use more sound approach (biased sampling)</p>
    <p>Clustering time better than with Broder for a few thousand files and more</p>
    <p>Could do Indyk (LSH) followed by Broder (MH)</p>
    <p>threshold edges branching sizebenefit over zlib</p>
  </div>
  <div class="page">
    <p>Experiments on Large Data Set:</p>
    <p>21000 pages, 257 MB  Results still very unoptimized in terms of speed</p>
    <p>However: cat+bzip2 does even better in some cases! - depends on page ordering and file sizes - our techniques are (now) faster</p>
    <p>algorithm running timesize (MB)</p>
    <p>zlib 74 42.3 cat+zlib 80 30.5 best MH 996 23.7 best LSH 800 21.7</p>
  </div>
  <div class="page">
    <p>We studied compression of collections of files  Framework for cluster-based delta compression  Implemented and optimized several approaches  Rich set of possible problems and techniques  Results still preliminary  Part of ongoing research effort on delta compression and file synchronization</p>
  </div>
  <div class="page">
    <p>Current and future work</p>
    <p>Extending zdelta to several base files  Compressing a collection using several base files - optimal solution NP-Complete - approximation results for optimum benefit</p>
    <p>Approach based on Hamiltonian Path - greedy heuristic for Hamiltonian Path - almost as good as Maximum Branching - useful for case of several base files - useful for tar+gzip and tar+bzip2</p>
    <p>Goal: towards a better generic utility (than tar+gzip)  Study based on data sets from ftp sites and web</p>
  </div>
  <div class="page">
    <p>Application: Large Web Archival System</p>
    <p>Consider a massive collection of web pages  Internet Archive: 10 billion pages, 100TB  Many versions of each page: Wayback Machine (at www.archive.org)  Archive does not currently employ delta compression [Kahle 2002]</p>
    <p>How to build a TB storage system that - employs delta compression - has good insertion and random read performance - has good streaming performance.</p>
    <p>Limited to very simple heuristics</p>
  </div>
</Presentation>
