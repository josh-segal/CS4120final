<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic</p>
    <p>properties</p>
    <p>Alexis Conneau, German Kruszewski, Guillaume Lample, Loc Barrault, Marco Baroni</p>
    <p>Facebook AI Research Universit Le Mans (LIUM)</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>The quest for universal sentence embeddings</p>
  </div>
  <div class="page">
    <p>Now-famous Ray Mooneys quote</p>
    <p>You cant cram the meaning of a single $&amp;!#* sentence into a</p>
    <p>single $!#&amp;* vector!</p>
    <p>Professor Raymond J. Mooney</p>
    <p>While not capturing meaning, we might still be able to build useful transferable sentence features  But what can we actually cram into these vectors?</p>
  </div>
  <div class="page">
    <p>The evaluation of universal sentence embeddings</p>
    <p>Transfer learning on many other tasks</p>
    <p>Learn a classifier on top of pretrained sentence embeddings for transfer tasks</p>
    <p>SentEval downstream tasks:  Sentiment/topic classification  Natural Language Inference  Semantic Textual Similarity</p>
  </div>
  <div class="page">
    <p>The evaluation of universal sentence embeddings</p>
    <p>Downstream tasks are complex</p>
    <p>Hard to infer what information the embeddings really capture</p>
    <p>Probing tasks to the rescue!  designed for inference  evaluate simple isolated properties</p>
  </div>
  <div class="page">
    <p>Probing tasks and downstream tasks</p>
    <p>Natural Language Inference downstream task</p>
    <p>Subject Number probing task</p>
    <p>Premise: A lot of people walking outside a row of shops with an older man with his hands in his pocket is closer to the camera . Hypothesis: A lot of dogs barking outside a row of shops with a cat teasing them . Label: contradiction</p>
    <p>Sentence: The hobbits waited patiently .</p>
    <p>Label: Plural (NNS)</p>
    <p>Probing tasks are simpler and focused on a single property!</p>
  </div>
  <div class="page">
    <p>Our contributions</p>
    <p>An extensive analysis of sentence embeddings using probing tasks</p>
    <p>We vary the architecture of the encoder (3) and the training task (7)</p>
    <p>We open-source 10 horse-free classification probing tasks.</p>
    <p>Each task being designed to probe a single linguistic property</p>
    <p>Shi et al. (EMNLP 2016)  Does string-based neural MT learn source syntax? Adi et al. (ICLR 2017)  Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</p>
  </div>
  <div class="page">
    <p>Probing tasks: understanding sentence embeddings content</p>
    <p>Probing task</p>
    <p>Sentence Encoder</p>
  </div>
  <div class="page">
    <p>Probing tasks</p>
    <p>Probing task</p>
    <p>Sentence Encoder</p>
    <p>What they have in common:</p>
    <p>Artificially-created datasets all framed as classification</p>
    <p>... but based on natural sentences extracted from the TBC (5-to-28 words)</p>
    <p>100k training set, 10k valid, 10k test, with balanced classes</p>
    <p>Carefully removed obvious biases (words highly predictive of a class, etc)</p>
  </div>
  <div class="page">
    <p>Probing tasks</p>
    <p>Probing task</p>
    <p>Sentence Encoder</p>
    <p>Grouped in three categories:</p>
    <p>Surface information</p>
    <p>Syntactic information</p>
    <p>Semantic information</p>
  </div>
  <div class="page">
    <p>Probing tasks (1/10)  Sentence Length</p>
    <p>Goal: Predict the length range of the input sentence (6 bins)</p>
    <p>Question: Do embeddings preserve information about sentence length?</p>
    <p>She had not come all this way to let one stupid wagon turn all of that hard work into a waste !</p>
    <p>input output</p>
    <p>Surface information</p>
  </div>
  <div class="page">
    <p>Probing tasks (2/10)  Word Content</p>
    <p>Goal: 1000 output words. Which one (only one) belongs to the sentence?</p>
    <p>Question: Do embeddings preserve information about words?</p>
    <p>Helen took a pen from her purse and wrote something on her cocktail napkin.</p>
    <p>wrote MLP classifier</p>
    <p>input output</p>
    <p>Adi et al. (ICLR 2017)  Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</p>
    <p>Surface information</p>
  </div>
  <div class="page">
    <p>Probing tasks (3/10)  Top Constituents</p>
    <p>Goal: Predict top-constituents of parse-tree (20 classes)</p>
    <p>Note: 19 most common top-constituent sequences + 1 category for others</p>
    <p>Question: Can we extract grammatical information from the embeddings?</p>
    <p>Slowly he lowered his head toward mine.</p>
    <p>ADVP_NP_VP_.</p>
    <p>The anger in his voice surprised even himself .</p>
    <p>NP_VP_.</p>
    <p>MLP classifier</p>
    <p>outputinput</p>
    <p>Shi et al. (EMNLP 2016)  Does string-based neural MT learn source syntax?</p>
    <p>Syntactic information</p>
  </div>
  <div class="page">
    <p>Probing tasks (4/10)  Bigram Shift</p>
    <p>Goal: Predict whether a bigram has been shifted or not.</p>
    <p>Question: Are embeddings sensible to word order?</p>
    <p>This new was information . 1</p>
    <p>We 're married getting . 1</p>
    <p>MLP classifier</p>
    <p>outputinput</p>
    <p>Syntactic information</p>
  </div>
  <div class="page">
    <p>Probing tasks  5 more</p>
    <p>5/10: Tree Depth (depth of the parse tree)</p>
    <p>6/10: Tense prediction (main clause tense, past or present)</p>
    <p>7-8/10: Object/Subject Number (singular or plural)</p>
    <p>9/10: Semantic Odd Man Out (noun/verb replaced by one with same POS)</p>
  </div>
  <div class="page">
    <p>Probing tasks (10/10)  Coordination Inversion</p>
    <p>Goal: Sentences made of two coordinate clauses: inverted (I) or not (O)?</p>
    <p>Note: human evaluation: 85%</p>
    <p>Question: Can extract sentence-model information?</p>
    <p>They might be only memories, but I can still feel each one O</p>
    <p>I can still feel each one, but they might be only memories.</p>
    <p>I</p>
    <p>MLP classifier</p>
    <p>outputinput</p>
    <p>Semantic information</p>
  </div>
  <div class="page">
    <p>Experiments and results</p>
  </div>
  <div class="page">
    <p>Experiments We analyse almost 30 encoders trained in different ways:</p>
    <p>Our baselines:  Human evaluation, Length (1-dim vector)  NB-uni and NB-uni/bi with TF-IDF  CBOW (average of word embeddings)</p>
    <p>Our 3 architectures:  Three encoders: BiLSTM-last/max, and Gated ConvNet</p>
    <p>Our 7 training tasks:  Auto-encoding, Seq2Tree, SkipThought, NLI  Seq2seq NMT without attention En-Fr, En-De, En-Fi</p>
  </div>
  <div class="page">
    <p>Experiments  training tasks</p>
    <p>Source and target examples for seq2seq training tasks</p>
    <p>Sutskever et al. (NIPS 2014)  Sequence to sequence learning with neural networks Kiros et al. (NIPS 2015)  SkipThought vectors Vinyals et al. (NIPS 2015)  Grammar as a Foreign Language</p>
  </div>
  <div class="page">
    <p>Baselines and sanity checks</p>
    <p>Probing tasks evaluation baselines</p>
    <p>A C C U</p>
    <p>R A C Y</p>
    <p>SentLen WC TopConst BShift ObjNum</p>
    <p>Hum. Eval. NB-uni-tfidf NB-bi-tfidf CBOW Majority vote</p>
  </div>
  <div class="page">
    <p>Impact of training tasks</p>
    <p>Probing tasks results for BiLSTM-last trained in different ways</p>
    <p>A cc</p>
    <p>u ra</p>
    <p>cy</p>
    <p>SentLen WC TopConst BShift ObjNum</p>
    <p>CBOW AutoEncoder NMT En-Fr NMT En-Fi Seq2Tree SkipThought NLI</p>
  </div>
  <div class="page">
    <p>Impact of model architecture</p>
    <p>Average accuracies for different models</p>
    <p>SentLen WC TopConst BShift ObjNum CoordInv</p>
    <p>BiLSTM-max BiLSTM-last GatedConvNet</p>
  </div>
  <div class="page">
    <p>Evolution during training</p>
    <p>Evaluation on probing tasks at each epoch of training</p>
    <p>What do embeddings encode along training?</p>
    <p>NMT: Most increase and converge rapidly (only SentLen decreases). WC correlated with BLEU.</p>
  </div>
  <div class="page">
    <p>Correlation with downstream tasks</p>
    <p>Strong correlation between WC and downstream tasks</p>
    <p>Word-level information important for downstream tasks (classification, NLI, STS)</p>
    <p>If WC good predictor -&gt; maybe current downstream tasks are not the right ones?</p>
    <p>Correlation between probing and downstream tasks</p>
    <p>Blue=higher - Red=lower - Grey=not significant</p>
  </div>
  <div class="page">
    <p>Take-home messages and future work</p>
    <p>Sentence embeddings need not be good on probing tasks</p>
    <p>Probing tasks are simply meant to understand what linguistic features are encoded and to designed to compare encoders.</p>
    <p>Future work  Understanding the impact of multi-task learning  Studying the impact of language model pretraining (ELMO)  Study other encoders (Transformer, RNNG)</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Publicly available in SentEval</p>
    <p>Automatically generated datasets (generalize to other languages)</p>
    <p>Natural sentences from Toronto Book Corpus</p>
    <p>Used Stanford parser for grammatical tasks</p>
    <p>https://github.com/facebookresearch/SentEval/tree/master/data/ probing</p>
  </div>
  <div class="page">
    <p>Probing tasks  Semantic Odd Man Out</p>
    <p>Goal: Predict whether a sentence has been modified or not: one verb/noun randomly by another verb/noun with same POS</p>
    <p>Note: preserved bigrams frequency, human eval.: 81.2%</p>
    <p>Question: Can we identify well-formed sentences (sentence model)?</p>
    <p>No one could see this Hayes and I wanted to know if it was real or a spoonful (orig: ploy)</p>
    <p>M MLP classifier</p>
  </div>
</Presentation>
