<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Abigail See*, Stephen Roller+, Douwe Kiela+, Jason Weston+</p>
    <p>*Stanford NLP +Facebook AI Research</p>
    <p>What makes a good conversation? How controllable attributes affect human judgments</p>
  </div>
  <div class="page">
    <p>Natural Language Generation task spectrum</p>
    <p>Machine Translation</p>
    <p>Chitchat Dialogue</p>
    <p>Sentence Compression</p>
    <p>Abstractive Summarization</p>
    <p>Story Generation</p>
    <p>Less open-ended More open-ended</p>
    <p>Mostly word-level decisions Requires high-level decisions</p>
    <p>Neural LMs more successful Neural LMs less successful</p>
    <p>Makes errors like repetition and generic response (under certain decoding algorithms).</p>
    <p>Difficulty learning to make high-level decisions.</p>
  </div>
  <div class="page">
    <p>Natural Language Generation task spectrum</p>
    <p>Machine Translation</p>
    <p>Sentence Compression</p>
    <p>Abstractive Summarization</p>
    <p>Control is less important Control is more important</p>
    <p>Less open-ended More open-ended</p>
    <p>Mostly word-level decisions Requires high-level decisions</p>
    <p>Neural LMs more successful Neural LMs less successful</p>
    <p>Control = ability to specify desired attributes of the text at test time.</p>
    <p>We can use control to fix errors, and allow us to handle some high-level decisions.</p>
    <p>Chitchat Dialogue</p>
    <p>Story Generation</p>
  </div>
  <div class="page">
    <p>Natural Language Generation task spectrum</p>
    <p>Machine Translation</p>
    <p>Sentence Compression</p>
    <p>Abstractive Summarization</p>
    <p>Control is less important Control is more important</p>
    <p>Less open-ended More open-ended</p>
    <p>Mostly word-level decisions Requires high-level decisions</p>
    <p>Neural LMs more successful Neural LMs less successful</p>
    <p>Eval is difficult Eval is fiendish</p>
    <p>No automatic metric for overall quality.</p>
    <p>Dialogue is even more complex: Single-turn or multi-turn eval? Interactive or static conversation?</p>
    <p>Chitchat Dialogue</p>
    <p>Story Generation</p>
  </div>
  <div class="page">
    <p>By controlling multiple attributes of generated text and human-evaluating multiple aspects of conversational quality, we aim to answer the following:</p>
    <p>Our research questions</p>
  </div>
  <div class="page">
    <p>Persona:  I have two dogs.  I like to work on vintage cars.  My favorite music is country.  I own two vintage Mustangs.</p>
    <p>Persona:  I love to drink fancy tea.  I have a big library at home.  I'm a museum tour guide.  I'm partly deaf.</p>
    <p>PersonaChat task (Zhang et al 2018)</p>
    <p>Hello, how are you doing?</p>
    <p>Great thanks, just listening to my favorite Johnny Cash album!</p>
    <p>Nice! I'm not much of a music fan myself, but I do love to read.</p>
    <p>Me too! I just read a book about the history of the auto industry. 6</p>
  </div>
  <div class="page">
    <p>The PersonaChat task was the focus of the NeurIPS 2018 ConvAI2 Competition.</p>
    <p>Most successful teams built neural sequence generation systems. (Dinan et al 2019)</p>
    <p>The winning team, Lost in Conversation, used a finetuned version of GPT.</p>
    <p>Our baseline model is a standard LSTM-based seq2seq architecture with attention.</p>
    <p>It is pretrained on 2.5 million Twitter message/response pairs, then finetuned on PersonaChat.</p>
    <p>PersonaChat task (Zhang et al 2018)</p>
  </div>
  <div class="page">
    <p>What attributes do we control?</p>
    <p>Goal: Reduce repetition (within and across utterances)</p>
    <p>Goal: Reduce genericness of responses (e.g. oh that's cool)</p>
    <p>Goal: Respond more on-topic; don't ignore user</p>
    <p>Goal: Find the optimal rate of question-asking</p>
  </div>
  <div class="page">
    <p>What quality aspects do we measure?</p>
    <p>Does the bot repeat itself?</p>
    <p>Did you find the bot interesting to talk to?</p>
    <p>Does the bot say things that don't make sense?</p>
    <p>Does the bot use English naturally?</p>
    <p>Does the bot pay attention to what you say?</p>
    <p>Does the bot ask a good amount of questions?</p>
  </div>
  <div class="page">
    <p>What quality aspects do we measure?</p>
    <p>Is it a person or a bot?</p>
    <p>Is it enjoyable to talk to?</p>
    <p>Note: ConvAI2 competition asked only this question. Our eval is a superset of ConvAI2's.</p>
  </div>
  <div class="page">
    <p>Control methods</p>
    <p>We evaluate and compare two existing general-purpose control methods, using them to control all four controllable attributes.</p>
    <p>Conditional Training (CT): Train the model to generate response y, conditioned on the input x, and the desired output attribute z. (Kikuchi et al 2016, Peng et al 2018, Fan et al 2018)</p>
    <p>Weighted Decoding (WD): During decoding, increase/decrease the probability of generating words w in proportion to features f(w). (Ghazvininejad et al 2017, Baheti et al 2018)</p>
  </div>
  <div class="page">
    <p>Conditional Training (CT):</p>
    <p>Requires sufficient training examples for the attribute ( repetition)</p>
    <p>Ineffective at learning complex relationships between input and output ( response-relatedness)</p>
    <p>Effective for:  specificity,  question-asking</p>
    <p>Q1: How effectively can we control attributes? Attributes: repetition, specificity, question-asking, response-relatedness</p>
    <p>Weighted Decoding (WD):</p>
    <p>Requires attribute to be defined at the word-level ( question-asking)</p>
    <p>Effective for:  repetition,  response-relatedness,  specificity</p>
  </div>
  <div class="page">
    <p>Controlling specificity (WD and CT)</p>
    <p>More generic</p>
    <p>More specific</p>
  </div>
  <div class="page">
    <p>Controlling specificity (WD and CT)</p>
    <p>More generic</p>
    <p>More specific</p>
    <p>More generic</p>
    <p>More specific</p>
    <p>WD: Large range, but degenerate output at the extremes</p>
    <p>CT: Smaller range, but generally wellformed output</p>
  </div>
  <div class="page">
    <p>Controlling response-relatedness (WD)</p>
    <p>Output is degenerate when weight is too high</p>
    <p>Less related</p>
    <p>More related</p>
  </div>
  <div class="page">
    <p>Q2: How does control affect human eval?</p>
    <p>Reduce n-gram repetition to human level</p>
  </div>
  <div class="page">
    <p>Q2: How does control affect human eval?</p>
    <p>Increase specificity (reduce genericness)</p>
    <p>to human level</p>
  </div>
  <div class="page">
    <p>Q2: How does control affect human eval?</p>
    <p>Increase responserelatedness (similarity</p>
    <p>to last utterance)</p>
  </div>
  <div class="page">
    <p>Q2: How does control affect human eval?</p>
    <p>Increase questionasking rate to 65.7% (more than baseline 50%, human 28.8%)</p>
  </div>
  <div class="page">
    <p>Q3: Can we make a better chatbot overall?</p>
    <p>Yes! By controlling repetition, specificity and question-asking, we achieve near-human engagingness (i.e. enjoyability) ratings.</p>
    <p>Reduce repetition Increase specificity Increase question-asking</p>
    <p>Our raw engagingness score matches the ConvAI2 competition winner's GPT-based model, even though ours is:</p>
    <p>much smaller (2 layers vs 12)  trained on 12x less data</p>
  </div>
  <div class="page">
    <p>Q3: Can we make a better chatbot overall?</p>
    <p>However: On the humanness (i.e. Turing test) metric, our models are nowhere near human-level!</p>
    <p>Reduce repetition Increase specificity Increase question-asking</p>
  </div>
  <div class="page">
    <p>Finding: Our bots are (almost) as engaging as humans, but they're clearly non-human.</p>
    <p>Two conclusions:</p>
    <p>Engagingness vs Humanness</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Control is a good idea for your neural sequence generation dialogue system.</p>
    <p>Using simple control, we matched performance of GPT-based contest winner.</p>
    <p>Don't repeat yourself. Don't be boring. Ask more questions.</p>
    <p>Multi-turn phenomena (repetition, question-asking frequency) are important  so need multi-turn eval to detect them.</p>
    <p>Engagingness  Humanness, so think carefully about which to use.</p>
    <p>Paid Turkers are not engaging conversationalists, or good judges of engaging conversation. Humans chatting for fun may be better.</p>
    <p>Problem: Manually finding the best combination of control settings is painful. 23</p>
  </div>
  <div class="page">
    <p>Control is a good idea for your neural sequence generation dialogue system.</p>
    <p>Using simple control, we matched performance of GPT-based contest winner.</p>
    <p>Don't repeat yourself. Don't be boring. Ask more questions.</p>
    <p>Multi-turn phenomena (repetition, question-asking frequency) are important  so need multi-turn eval to detect them.</p>
    <p>Engagingness  Humanness, so think carefully about which to use.</p>
    <p>Paid Turkers are not engaging conversationalists, or good judges of engaging conversation. Humans chatting for fun may be better.</p>
    <p>Problem: Manually finding the best combination of control settings is painful.</p>
    <p>Conclusions</p>
    <p>Question-askingRepetition Response-relatednessSpecificity</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Control is a good idea for your neural sequence generation dialogue system.</p>
    <p>Using simple control, we matched performance of GPT-based contest winner.</p>
    <p>Don't repeat yourself. Don't be boring. Ask more questions.</p>
    <p>Multi-turn phenomena (repetition, question-asking frequency) are important  so need multi-turn eval to detect them.</p>
    <p>Engagingness  Humanness, so think carefully about which to use.</p>
    <p>Paid Turkers are not engaging conversationalists, or good judges of engaging conversation. Humans chatting for fun may be better.</p>
    <p>Problem: Manually finding the best combination of control settings is painful.</p>
    <p>Code, models, demo, eval logs available at https://parl.ai/projects/controllable_dialogue</p>
  </div>
</Presentation>
