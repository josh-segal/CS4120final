<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Neural Models for Documents with Metadata</p>
    <p>Dallas Card, Chenhao Tan, Noah A. Smith</p>
    <p>July 18, 2018</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Main points of this talk:</p>
    <p>Background (LDA, SAGE, SLDA, etc.) Model and related work Experiments and Results</p>
  </div>
  <div class="page">
    <p>Latent Dirichlet Allocation</p>
    <p>Blei, Ng, and Jordan. Latent Dirichlet Allocation. JMLR. 2003. David Blei. Probabilistic topic models. Comm. ACM. 2012 2</p>
  </div>
  <div class="page">
    <p>Types of metadata</p>
    <p>Date or time</p>
    <p>Author(s)</p>
    <p>Rating</p>
    <p>Sentiment</p>
    <p>Ideology</p>
    <p>etc.</p>
  </div>
  <div class="page">
    <p>Variations and extensions</p>
    <p>Author topic model (Rosen-Zvi et al 2004)</p>
    <p>Supervised LDA (SLDA; McAuliffe and Blei, 2008)</p>
    <p>Dirichlet multinomial regression (Mimno and McCallum, 2008)</p>
    <p>Sparse additive generative models (SAGE; Eisenstein et al, 2011)</p>
    <p>Structural topic model (Roberts et al, 2014)</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Desired features of model</p>
    <p>Fast, scalable inference.</p>
    <p>Easy modification by end-users.</p>
    <p>Incorporation of metadata: Covariates: features which influences text (as in SAGE). Labels: features to be predicted along with text (as in SLDA).</p>
    <p>Possibility of sparse topics.</p>
    <p>Incorporate additional prior knowledge.</p>
    <p>Use variational autoencoder (VAE) style of inference (Kingma and Welling, 2014)</p>
  </div>
  <div class="page">
    <p>Desired features of model</p>
    <p>Fast, scalable inference.</p>
    <p>Easy modification by end-users.</p>
    <p>Incorporation of metadata: Covariates: features which influences text (as in SAGE). Labels: features to be predicted along with text (as in SLDA).</p>
    <p>Possibility of sparse topics.</p>
    <p>Incorporate additional prior knowledge.</p>
    <p>Use variational autoencoder (VAE) style of inference (Kingma and Welling, 2014)</p>
  </div>
  <div class="page">
    <p>Desired features of model</p>
    <p>Fast, scalable inference.</p>
    <p>Easy modification by end-users.</p>
    <p>Incorporation of metadata: Covariates: features which influences text (as in SAGE). Labels: features to be predicted along with text (as in SLDA).</p>
    <p>Possibility of sparse topics.</p>
    <p>Incorporate additional prior knowledge.</p>
    <p>Use variational autoencoder (VAE) style of inference (Kingma and Welling, 2014)</p>
  </div>
  <div class="page">
    <p>Desired features of model</p>
    <p>Fast, scalable inference.</p>
    <p>Easy modification by end-users.</p>
    <p>Incorporation of metadata: Covariates: features which influences text (as in SAGE). Labels: features to be predicted along with text (as in SLDA).</p>
    <p>Possibility of sparse topics.</p>
    <p>Incorporate additional prior knowledge.</p>
    <p>Use variational autoencoder (VAE) style of inference (Kingma and Welling, 2014)</p>
  </div>
  <div class="page">
    <p>Desired features of model</p>
    <p>Fast, scalable inference.</p>
    <p>Easy modification by end-users.</p>
    <p>Incorporation of metadata: Covariates: features which influences text (as in SAGE). Labels: features to be predicted along with text (as in SLDA).</p>
    <p>Possibility of sparse topics.</p>
    <p>Incorporate additional prior knowledge.</p>
    <p>Use variational autoencoder (VAE) style of inference (Kingma and Welling, 2014)</p>
  </div>
  <div class="page">
    <p>Desired outcome</p>
    <p>Coherent groupings of words (something like topics), with offsets for observed metadata</p>
    <p>Encoder to map from documents to latent representations</p>
    <p>Classifier to predict labels from from latent representation</p>
  </div>
  <div class="page">
    <p>Desired outcome</p>
    <p>Coherent groupings of words (something like topics), with offsets for observed metadata</p>
    <p>Encoder to map from documents to latent representations</p>
    <p>Classifier to predict labels from from latent representation</p>
  </div>
  <div class="page">
    <p>Desired outcome</p>
    <p>Coherent groupings of words (something like topics), with offsets for observed metadata</p>
    <p>Encoder to map from documents to latent representations</p>
    <p>Classifier to predict labels from from latent representation</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )p( i w)</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )p( i w)q( i w)</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )p( i w)q( i w)</p>
    <p>ELBO = Eq[log p(words | i)]DKL[q(i | words)p(i)]</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ELBO = Eq[log p(words | i)]DKL[q(i | words)p(i)]</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>ELBO = Eq[log p(words | ri)]DKL[q(ri | words)p(ri)]</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>ELBO  1 S</p>
    <p>S s=1[log p(words | r</p>
    <p>(s) i )]DKL[q(ri | words)p(ri)]</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>(0, I)</p>
    <p>ELBO  1 S</p>
    <p>S s=1[log p(words | r</p>
    <p>(s) i )]DKL[q(ri | words)p(ri)]</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>(0, I)</p>
    <p>= q + (s) q</p>
    <p>ELBO  1 S</p>
    <p>S s=1[log p(words | r</p>
    <p>(s) i )]DKL[q(ri | words)p(ri)]</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>(0, I)</p>
    <p>= q + (s) q</p>
    <p>Srivastava and Sutton, 2017, Miao et al, 2016</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>(0, I)</p>
    <p>= q + (s) q</p>
    <p>yi</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>(0, I)</p>
    <p>= q + (s) q</p>
    <p>yi</p>
    <p>ci</p>
  </div>
  <div class="page">
    <p>Model</p>
    <p>i k</p>
    <p>words</p>
    <p>generator network: p(w i) = fg( )</p>
    <p>words</p>
    <p>encoder network: q( i w) = fe( )</p>
    <p>ri k</p>
    <p>i = softmax(ri)</p>
    <p>(0, I)</p>
    <p>= q + (s) q</p>
    <p>yi</p>
    <p>ci</p>
    <p>, ci, yi</p>
  </div>
  <div class="page">
    <p>Scholar</p>
    <p>Generator network:</p>
    <p>p(word | i,ci) = softmax(d + Ti B (topic) + cTi B</p>
    <p>(cov))</p>
    <p>Optionally include interactions between topics and covariates</p>
    <p>p(yi | i,ci) = fy(i,ci)</p>
    <p>Encoder:</p>
    <p>i = f(words,ci,yi)</p>
    <p>log i = f(words,ci,yi)</p>
    <p>Optional incorporation of word vectors to embed input</p>
  </div>
  <div class="page">
    <p>Scholar</p>
    <p>Generator network:</p>
    <p>p(word | i,ci) = softmax(d + Ti B (topic) + cTi B</p>
    <p>(cov))</p>
    <p>Optionally include interactions between topics and covariates</p>
    <p>p(yi | i,ci) = fy(i,ci)</p>
    <p>Encoder:</p>
    <p>i = f(words,ci,yi)</p>
    <p>log i = f(words,ci,yi)</p>
    <p>Optional incorporation of word vectors to embed input</p>
  </div>
  <div class="page">
    <p>Scholar</p>
    <p>Generator network:</p>
    <p>p(word | i,ci) = softmax(d + Ti B (topic) + cTi B</p>
    <p>(cov))</p>
    <p>Optionally include interactions between topics and covariates</p>
    <p>p(yi | i,ci) = fy(i,ci)</p>
    <p>Encoder:</p>
    <p>i = f(words,ci,yi)</p>
    <p>log i = f(words,ci,yi)</p>
    <p>Optional incorporation of word vectors to embed input</p>
  </div>
  <div class="page">
    <p>Scholar</p>
    <p>Generator network:</p>
    <p>p(word | i,ci) = softmax(d + Ti B (topic) + cTi B</p>
    <p>(cov))</p>
    <p>Optionally include interactions between topics and covariates</p>
    <p>p(yi | i,ci) = fy(i,ci)</p>
    <p>Encoder:</p>
    <p>i = f(words,ci,yi)</p>
    <p>log i = f(words,ci,yi)</p>
    <p>Optional incorporation of word vectors to embed input</p>
  </div>
  <div class="page">
    <p>Optimization</p>
    <p>Stochastic optimization using mini-batches of documents</p>
    <p>Tricks from Srivastava and Sutton, 2017: Adam optimizer with high-learning rate to bypass mode collapse Batch-norm layers to avoid divergence</p>
    <p>Annealing away from batch-norm output to keep results interpretable</p>
  </div>
  <div class="page">
    <p>Output of Scholar</p>
    <p>B(topic),B(cov): Coherent groupings of positive and negative deviations from background ( topics)</p>
    <p>f, f: Encoder network: mapping from words to topics: i = softmax(fe(words,ci,yi,))</p>
    <p>fy: Classifier mapping from i to labels: y = fy(i,ci)</p>
  </div>
  <div class="page">
    <p>Output of Scholar</p>
    <p>B(topic),B(cov): Coherent groupings of positive and negative deviations from background ( topics) f, f: Encoder network: mapping from words to topics: i = softmax(fe(words,ci,yi,))</p>
    <p>fy: Classifier mapping from i to labels: y = fy(i,ci)</p>
  </div>
  <div class="page">
    <p>Output of Scholar</p>
    <p>B(topic),B(cov): Coherent groupings of positive and negative deviations from background ( topics) f, f: Encoder network: mapping from words to topics: i = softmax(fe(words,ci,yi,))</p>
    <p>fy: Classifier mapping from i to labels: y = fy(i,ci)</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Quantitative results: basic model</p>
    <p>Perplexity</p>
    <p>Coherence</p>
    <p>LDA 0.0</p>
    <p>IMDB dataset (Maas, 2011)</p>
  </div>
  <div class="page">
    <p>Quantitative results: basic model</p>
    <p>Perplexity</p>
    <p>Coherence</p>
    <p>LDA SAGE 0.0</p>
    <p>IMDB dataset (Maas, 2011)</p>
  </div>
  <div class="page">
    <p>Quantitative results: basic model</p>
    <p>Perplexity</p>
    <p>Coherence</p>
    <p>LDA SAGE NVDM 0.0</p>
    <p>IMDB dataset (Maas, 2011)</p>
  </div>
  <div class="page">
    <p>Quantitative results: basic model</p>
    <p>Perplexity</p>
    <p>Coherence</p>
    <p>LDA SAGE NVDM Scholar 0.0</p>
    <p>IMDB dataset (Maas, 2011)</p>
  </div>
  <div class="page">
    <p>Quantitative results: basic model</p>
    <p>Perplexity</p>
    <p>Coherence</p>
    <p>LDA SAGE NVDM Scholar Scholar +wv</p>
    <p>IMDB dataset (Maas, 2011) 28</p>
  </div>
  <div class="page">
    <p>Quantitative results: basic model</p>
    <p>Perplexity</p>
    <p>Coherence</p>
    <p>LDA SAGE NVDM Scholar Scholar +wv</p>
    <p>Scholar +sparsity</p>
    <p>IMDB dataset (Maas, 2011) 29</p>
  </div>
  <div class="page">
    <p>Classification results</p>
    <p>LR SLDA Scholar (labels)</p>
    <p>Scholar (covariates)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>IMDB dataset (Maas, 2011)</p>
  </div>
  <div class="page">
    <p>Exploratory Data Analysis</p>
    <p>Data: Media Frames Corpus (Card et al, 2015)</p>
    <p>Collection of thousands of news articles annotated in terms of tone and framing</p>
    <p>Relevant metadata: year of publication, newspaper, etc.</p>
  </div>
  <div class="page">
    <p>Tone as a label</p>
    <p>p(pro-immigration | topic)</p>
    <p>arrested charged charges agents operation state gov benefits arizona law bill bills bush border president bill republicans labor jobs workers percent study wages asylum judge appeals deportation court visas visa applications students citizenship boat desert died men miles coast haitian english language city spanish community</p>
  </div>
  <div class="page">
    <p>Tone as a covariate, with interactions</p>
    <p>Base topics Anti-immigration Pro-immigration ice customs agency criminal customs detainees detention population born percent jobs million illegals english newcomers judge case court guilty guilty charges man asylum court judge patrol border miles patrol border died authorities desert licenses drivers card foreign sept visas green citizenship card island story chinese smuggling federal island school ellis guest worker workers bill border house workers tech skilled benefits bill welfare republican california law welfare students</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Variational autoencoders (VAEs) provide a powerful framework for latent variable modeling</p>
    <p>We use the VAE framework to create a customizable model for documents with metadata</p>
    <p>We obtain comparable performance with enhanced flexibility and scalability</p>
    <p>Code is available: www.github.com/dallascard/scholar</p>
  </div>
</Presentation>
