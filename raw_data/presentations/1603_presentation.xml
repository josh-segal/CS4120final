<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Hallucination Bound for the BSC</p>
    <p>Anant Sahai and Stark Draper</p>
    <p>Wireless Foundations Department of Electrical Engineering and Computer Sciences</p>
    <p>University of California at Berkeley</p>
    <p>ECE Department University of Wisconsin at Madison</p>
    <p>Major Support from NSF CISE</p>
    <p>July 8th 2008: ISIT Tu-AM-3.3</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 1 / 17</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 2 / 17</p>
  </div>
  <div class="page">
    <p>Review: Fixed blocks</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>Hard decision regions cover space</p>
    <p>Feedback is pointless at high rates. (Dobrushin and Haroutunian)</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 3 / 17</p>
  </div>
  <div class="page">
    <p>Review: Fixed blocks: Forney-68</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>Refuse to decide when ambiguous</p>
    <p>Decision regions catch the typical sets only</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 3 / 17</p>
  </div>
  <div class="page">
    <p>Review: Fixed blocks: Forney-68</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>Refuse to decide when ambiguous</p>
    <p>Decision regions catch the typical sets only</p>
    <p>Can interpret as expected block-length</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 3 / 17</p>
  </div>
  <div class="page">
    <p>Review: Fixed blocks: Forney-68</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>x x</p>
    <p>x</p>
    <p>Refuse to decide when ambiguous</p>
    <p>Decision regions catch the typical sets only</p>
    <p>Can interpret as expected block-length</p>
    <p>No converse except at zero rate</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 3 / 17</p>
  </div>
  <div class="page">
    <p>Review: Fixed blocks, Soft deadlines: Burnashev-76</p>
    <p>e rr</p>
    <p>o r</p>
    <p>e xp</p>
    <p>o n e n t</p>
    <p>average communication rate (bits/channel use)</p>
    <p>Burnashev exponent Forney exponent Sphere packing exponentSpherepacking bound</p>
    <p>Random signature performance</p>
    <p>Forney performance</p>
    <p>Burnashev bound</p>
    <p>Showed C1(1  R C ) was a bound where C1 = maxi,j D(pi||pj)</p>
    <p>Considered expected stopping time and used Martingale arguments.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 4 / 17</p>
  </div>
  <div class="page">
    <p>Review: Fixed blocks, Soft deadlines: Burnashev-76</p>
    <p>e rr</p>
    <p>o r</p>
    <p>e xp</p>
    <p>o n e n t</p>
    <p>average communication rate (bits/channel use)</p>
    <p>Burnashev exponent Forney exponent Sphere packing exponentSpherepacking bound</p>
    <p>Random signature performance</p>
    <p>Forney performance</p>
    <p>Burnashev bound</p>
    <p>Ack/Nak</p>
    <p>( 1   )</p>
    <p>Enc Dec</p>
    <p>n</p>
    <p>Decision feedback = m</p>
    <p>..... retransmissions</p>
    <p>possible .....</p>
    <p>Block length n</p>
    <p>Data Transmission</p>
    <p>n</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 4 / 17</p>
  </div>
  <div class="page">
    <p>Streaming: an opportunity presents itself</p>
    <p>Data Blocks Nak</p>
    <p>Acks</p>
    <p>decoding error</p>
    <p>What if we only sent NAKs when needed?</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 5 / 17</p>
  </div>
  <div class="page">
    <p>Sliding blocks with collective punishment only (Kudryashov-79)</p>
    <p>decision delaydecision delay Final decision on blue msg</p>
    <p>(unless detect Nak) Final decision on purple msg</p>
    <p>(unless detect Nak)</p>
    <p>Emergency msg, fixed length = decision delay Data Blocks</p>
    <p>Make packet length n much smaller than soft deadline .</p>
    <p>A NAK collectively denies the past n  1 packets</p>
    <p>Error only if n  1 NAKs are all missed</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 6 / 17</p>
  </div>
  <div class="page">
    <p>Reason for hope: Csiszars result 80</p>
    <p>Can pack-in control messages at lower rates and give each subset their own random-coding bound!</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 7 / 17</p>
  </div>
  <div class="page">
    <p>Even simpler: need only one special message</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 8 / 17</p>
  </div>
  <div class="page">
    <p>Even simpler: need only one special message</p>
    <p>Typical noise sphere</p>
    <p>Hypothesis testing threshold:</p>
    <p>or is it data? Is it a Nak</p>
    <p>Unequal Error Protection</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 8 / 17</p>
  </div>
  <div class="page">
    <p>Specialize to BSC case</p>
    <p>y</p>
    <p>composition</p>
    <p>q</p>
    <p>output composition</p>
    <p>codeword BSC (p)</p>
    <p>p</p>
    <p>q</p>
    <p>Gap</p>
    <p>q (1p) + (1q) p 0.5</p>
    <p>Use all zero for NAK</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 9 / 17</p>
  </div>
  <div class="page">
    <p>Specialize to BSC case</p>
    <p>y</p>
    <p>composition</p>
    <p>q</p>
    <p>output composition</p>
    <p>codeword BSC (p)</p>
    <p>p</p>
    <p>q</p>
    <p>Gap</p>
    <p>q (1p) + (1q) p 0.5</p>
    <p>Use all zero for NAK</p>
    <p>Use composition q code for data: R &lt; H(qy)  H(p)</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 9 / 17</p>
  </div>
  <div class="page">
    <p>Specialize to BSC case</p>
    <p>y</p>
    <p>composition</p>
    <p>q</p>
    <p>output composition</p>
    <p>codeword BSC (p)</p>
    <p>p</p>
    <p>q</p>
    <p>Gap</p>
    <p>q (1p) + (1q) p 0.5</p>
    <p>Use all zero for NAK</p>
    <p>Use composition q code for data: R &lt; H(qy)  H(p)</p>
    <p>Probability of missed NAK is 2nD(qy||p)</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 9 / 17</p>
  </div>
  <div class="page">
    <p>Resulting exponents</p>
    <p>rr o</p>
    <p>r e</p>
    <p>xp o</p>
    <p>n e</p>
    <p>n t</p>
    <p>average communication rate (bits/channel use)</p>
    <p>Delay exponent Burnashev exponent Forney exponent Sphere packing exponent</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 10 / 17</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 11 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>B(0.5)1 B(2p)</p>
    <p>X</p>
    <p>X+</p>
    <p>+</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>B(0.5)1 B(2p)</p>
    <p>X</p>
    <p>X+</p>
    <p>+</p>
    <p>Trivial bound: let channel disconnect (2p)n</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>B(0.5)1 B(2p)</p>
    <p>X</p>
    <p>X+</p>
    <p>+</p>
    <p>Trivial bound: let channel disconnect (2p)n</p>
    <p>What is the chance we land on something 12n</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>B(0.5)1 B(2p)</p>
    <p>X</p>
    <p>X+</p>
    <p>+</p>
    <p>Trivial bound: let channel disconnect (2p)n</p>
    <p>What is the chance we land on something 12n How many decode to normal codewords? &gt; 2nR</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>B(0.5)1 B(2p)</p>
    <p>X</p>
    <p>X+</p>
    <p>+</p>
    <p>Trivial bound: let channel disconnect (2p)n</p>
    <p>What is the chance we land on something 12n How many decode to normal codewords? &gt; 2nR</p>
    <p>Exponent at most: log2 1 p  R</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>y</p>
    <p>composition</p>
    <p>q</p>
    <p>output composition</p>
    <p>codeword BSC (p)</p>
    <p>p</p>
    <p>q</p>
    <p>Gap</p>
    <p>q (1p) + (1q) p 0.5</p>
    <p>Each normal message needs 2nH(p) to decode to it.</p>
    <p>Must claim 2n(R+H(p)) volume.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>y</p>
    <p>composition</p>
    <p>q</p>
    <p>output composition</p>
    <p>codeword BSC (p)</p>
    <p>p</p>
    <p>q</p>
    <p>Gap</p>
    <p>q (1p) + (1q) p 0.5</p>
    <p>Each normal message needs 2nH(p) to decode to it.</p>
    <p>Must claim 2n(R+H(p)) volume.</p>
    <p>Place special-message as far away as possible.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Towards the Hallucination Bound</p>
    <p>B(p)</p>
    <p>+</p>
    <p>No converse for Forney, unlike Burnashev and Sphere-packing.</p>
    <p>What about for the minimum probability of error with feedback?</p>
    <p>y</p>
    <p>composition</p>
    <p>q</p>
    <p>output composition</p>
    <p>codeword BSC (p)</p>
    <p>p</p>
    <p>q</p>
    <p>Gap</p>
    <p>q (1p) + (1q) p 0.5</p>
    <p>Each normal message needs 2nH(p) to decode to it.</p>
    <p>Must claim 2n(R+H(p)) volume.</p>
    <p>Place special-message as far away as possible.</p>
    <p>Matches achievability!</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 12 / 17</p>
  </div>
  <div class="page">
    <p>Generalizes to general symmetric channels</p>
    <p>Consider fixed block-length and moderate probability of correct decoding for many regular codewords. Bergers source-coding game reveals that typical channel outputs can only reach Py output types.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 13 / 17</p>
  </div>
  <div class="page">
    <p>Generalizes to general symmetric channels</p>
    <p>Consider fixed block-length and moderate probability of correct decoding for many regular codewords. Bergers source-coding game reveals that typical channel outputs can only reach Py output types. 2nR regular messages could have their normal (non-erased) decoding regions packed into any of those types.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 13 / 17</p>
  </div>
  <div class="page">
    <p>Generalizes to general symmetric channels</p>
    <p>Consider fixed block-length and moderate probability of correct decoding for many regular codewords. Bergers source-coding game reveals that typical channel outputs can only reach Py output types. 2nR regular messages could have their normal (non-erased) decoding regions packed into any of those types. Typical footprint at least minpx 2</p>
    <p>nH(Y|X) inside.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 13 / 17</p>
  </div>
  <div class="page">
    <p>Generalizes to general symmetric channels</p>
    <p>Consider fixed block-length and moderate probability of correct decoding for many regular codewords. Bergers source-coding game reveals that typical channel outputs can only reach Py output types. 2nR regular messages could have their normal (non-erased) decoding regions packed into any of those types. Typical footprint at least minpx 2</p>
    <p>nH(Y|X) inside. Lower bound: pick most distant py  Py.</p>
    <p>2n(D(py||epy)+H(py))2nR+nH(Y|X)</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 13 / 17</p>
  </div>
  <div class="page">
    <p>Generalizes to general symmetric channels</p>
    <p>Consider fixed block-length and moderate probability of correct decoding for many regular codewords. Bergers source-coding game reveals that typical channel outputs can only reach Py output types. 2nR regular messages could have their normal (non-erased) decoding regions packed into any of those types. Typical footprint at least minpx 2</p>
    <p>nH(Y|X) inside. Lower bound: pick most distant py  Py.</p>
    <p>2n(D(py||epy)+H(py))2nR+nH(Y|X)</p>
    <p>Allow convex hull of</p>
    <p>Ehal(R) = max I(px,P)R</p>
    <p>max epyPy</p>
    <p>[D(P(px)||py) + (I(px, P)  R)]</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 13 / 17</p>
  </div>
  <div class="page">
    <p>Generalizes to general symmetric channels</p>
    <p>Consider fixed block-length and moderate probability of correct decoding for many regular codewords. Bergers source-coding game reveals that typical channel outputs can only reach Py output types. 2nR regular messages could have their normal (non-erased) decoding regions packed into any of those types. Typical footprint at least minpx 2</p>
    <p>nH(Y|X) inside. Lower bound: pick most distant py  Py.</p>
    <p>2n(D(py||epy)+H(py))2nR+nH(Y|X)</p>
    <p>Allow convex hull of</p>
    <p>Ehal(R) = max I(px,P)R</p>
    <p>max epyPy</p>
    <p>[D(P(px)||py) + (I(px, P)  R)]</p>
    <p>On Friday Borade, et al will show a more general proof that holds with expected block-length.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 13 / 17</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 14 / 17</p>
  </div>
  <div class="page">
    <p>The basic intuition</p>
    <p>Bit Enters</p>
    <p>Bit is decoded</p>
    <p>If we hallucinate for , we will miss our opportunity to decode.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 15 / 17</p>
  </div>
  <div class="page">
    <p>The basic intuition</p>
    <p>If we hallucinate for , we will miss our opportunity to decode.</p>
    <p>Challenge: overlaps with other bit-footprints.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 15 / 17</p>
  </div>
  <div class="page">
    <p>The idea of the proof</p>
    <p>View a block of many n.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 16 / 17</p>
  </div>
  <div class="page">
    <p>The idea of the proof</p>
    <p>View a block of many n. Count the volume of normal decoding regions in two different ways.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 16 / 17</p>
  </div>
  <div class="page">
    <p>The idea of the proof</p>
    <p>View a block of many n. Count the volume of normal decoding regions in two different ways.</p>
    <p>As a big block code based on correct decoding.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 16 / 17</p>
  </div>
  <div class="page">
    <p>The idea of the proof</p>
    <p>View a block of many n. Count the volume of normal decoding regions in two different ways.</p>
    <p>As a big block code based on correct decoding.  With a tree-structure based on the desired exponent.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 16 / 17</p>
  </div>
  <div class="page">
    <p>The idea of the proof</p>
    <p>We can guess early</p>
    <p>Bit Enters</p>
    <p>Bit is decoded</p>
    <p>View a block of many n. Count the volume of normal decoding regions in two different ways.</p>
    <p>As a big block code based on correct decoding.  With a tree-structure based on the desired exponent.</p>
    <p>Technical condition: impose sequentiality on the code: we can usually guess the answer even before the deadline runs out.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 16 / 17</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The Hallucination Bound is the probability that that the decoder imagines that everything is normal despite your best efforts to tell it otherwise.</p>
    <p>This corresponds to the best probability of error for a special message in the fixed block-code setting.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 17 / 17</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The Hallucination Bound is the probability that that the decoder imagines that everything is normal despite your best efforts to tell it otherwise.</p>
    <p>This corresponds to the best probability of error for a special message in the fixed block-code setting. Future work</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 17 / 17</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The Hallucination Bound is the probability that that the decoder imagines that everything is normal despite your best efforts to tell it otherwise.</p>
    <p>This corresponds to the best probability of error for a special message in the fixed block-code setting. Future work</p>
    <p>Eliminate the technical condition for the streaming case.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 17 / 17</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The Hallucination Bound is the probability that that the decoder imagines that everything is normal despite your best efforts to tell it otherwise.</p>
    <p>This corresponds to the best probability of error for a special message in the fixed block-code setting. Future work</p>
    <p>Eliminate the technical condition for the streaming case.  Get a two-way Hallucination bound for the case of noisy feedback.</p>
    <p>Sahai/Draper (UC Berkeley) Hallucination Bound Jul 8, 2008 17 / 17</p>
  </div>
</Presentation>
