<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Two Methods for Domain Adaptation of</p>
    <p>Bilingual Tasks:</p>
    <p>Delightfully Simple and Broadly Applicable</p>
    <p>Viktor Hangya1, Fabienne Braune1,2, Alexander Fraser1, Hinrich Schutze1</p>
    <p>fabienne.braune@volkswagen.de</p>
    <p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No 640550).</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>I Bilingual transfer learning is important for overcoming data sparsity in the target language</p>
    <p>I Bilingual word embeddings eliminate the gap between source and target language vocabulary</p>
    <p>I Resources required for bilingual methods are often out-of-domain:</p>
    <p>I Texts for embeddings I Source language training samples</p>
    <p>I We focused on domain-adaptation of word embeddings and better use of unlabeled data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>I Cross-lingual sentiment analysis of tweets good buenogreatgrande</p>
    <p>super super</p>
    <p>bad malo</p>
    <p>awful horrible</p>
    <p>sad triste</p>
    <p>red rojo</p>
    <p>today hoy</p>
    <p>mug jarra</p>
    <p>cool</p>
    <p>OMG ?</p>
    <p>I Combination of two methods: I Domain adaptation of bilingual word embeddings I Semi-supervised system for exploiting unlabeled data</p>
    <p>I No additional annotated resource is needed: I Cross-lingual sentiment classification of tweets I Medical bilingual lexicon induction</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>I Cross-lingual sentiment analysis of tweets good buenogreatgrande</p>
    <p>super super</p>
    <p>bad malo</p>
    <p>awful horrible</p>
    <p>sad triste</p>
    <p>red rojo</p>
    <p>today hoy</p>
    <p>mug jarra</p>
    <p>cool</p>
    <p>OMG ?</p>
    <p>I Combination of two methods: I Domain adaptation of bilingual word embeddings I Semi-supervised system for exploiting unlabeled data</p>
    <p>I No additional annotated resource is needed: I Cross-lingual sentiment classification of tweets I Medical bilingual lexicon induction</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>I Cross-lingual sentiment analysis of tweets good buenogreatgrande</p>
    <p>super super</p>
    <p>bad malo</p>
    <p>awful horrible</p>
    <p>sad triste</p>
    <p>red rojo</p>
    <p>today hoy</p>
    <p>mug jarra</p>
    <p>cool</p>
    <p>OMG ?</p>
    <p>I Combination of two methods: I Domain adaptation of bilingual word embeddings I Semi-supervised system for exploiting unlabeled data</p>
    <p>I No additional annotated resource is needed: I Cross-lingual sentiment classification of tweets I Medical bilingual lexicon induction</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>I Cross-lingual sentiment analysis of tweets good buenogreatgrande</p>
    <p>super super</p>
    <p>bad malo</p>
    <p>awful horrible</p>
    <p>sad triste</p>
    <p>red rojo</p>
    <p>today hoy</p>
    <p>mug jarra</p>
    <p>cool</p>
    <p>OMG ?</p>
    <p>I Combination of two methods: I Domain adaptation of bilingual word embeddings I Semi-supervised system for exploiting unlabeled data</p>
    <p>I No additional annotated resource is needed: I Cross-lingual sentiment classification of tweets I Medical bilingual lexicon induction</p>
  </div>
  <div class="page">
    <p>Word Embedding Adaptation</p>
    <p>Target</p>
    <p>Source Out-of-domain</p>
    <p>In-domain</p>
    <p>In-domain</p>
    <p>BWE</p>
    <p>W2V</p>
    <p>W2V</p>
    <p>MWE</p>
    <p>MWE</p>
    <p>Mapping</p>
    <p>Out-of-domain</p>
    <p>I Goal: domain-specific bilingual word embeddings with general domain semantic knowledge</p>
    <p>I Easily accessible general (out-of-domain) data I Domain-specific data</p>
    <p>I Small seed lexicon containing word pairs is needed</p>
    <p>I Simple and intuitive but crucial for the next step!</p>
  </div>
  <div class="page">
    <p>Word Embedding Adaptation</p>
    <p>Target</p>
    <p>Source Out-of-domain</p>
    <p>In-domain</p>
    <p>In-domain</p>
    <p>BWE</p>
    <p>W2V</p>
    <p>W2V</p>
    <p>MWE</p>
    <p>MWE</p>
    <p>Mapping</p>
    <p>Out-of-domain</p>
    <p>I Goal: domain-specific bilingual word embeddings with general domain semantic knowledge</p>
    <p>I Easily accessible general (out-of-domain) data I Domain-specific data</p>
    <p>I Small seed lexicon containing word pairs is needed</p>
    <p>I Simple and intuitive but crucial for the next step!</p>
  </div>
  <div class="page">
    <p>Word Embedding Adaptation</p>
    <p>Target</p>
    <p>Source Out-of-domain</p>
    <p>In-domain</p>
    <p>In-domain</p>
    <p>BWE</p>
    <p>W2V</p>
    <p>W2V</p>
    <p>MWE</p>
    <p>MWE</p>
    <p>Mapping</p>
    <p>Out-of-domain</p>
    <p>I Goal: domain-specific bilingual word embeddings with general domain semantic knowledge</p>
    <p>I Easily accessible general (out-of-domain) data I Domain-specific data</p>
    <p>I Small seed lexicon containing word pairs is needed</p>
    <p>I Simple and intuitive but crucial for the next step!</p>
  </div>
  <div class="page">
    <p>Word Embedding Adaptation</p>
    <p>Target</p>
    <p>Source Out-of-domain</p>
    <p>In-domain</p>
    <p>In-domain</p>
    <p>BWE</p>
    <p>W2V</p>
    <p>W2V</p>
    <p>MWE</p>
    <p>MWE</p>
    <p>Mapping</p>
    <p>Out-of-domain</p>
    <p>I Goal: domain-specific bilingual word embeddings with general domain semantic knowledge</p>
    <p>I Easily accessible general (out-of-domain) data I Domain-specific data</p>
    <p>I Small seed lexicon containing word pairs is needed</p>
    <p>I Simple and intuitive but crucial for the next step!</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Approach</p>
    <p>I Goal: Unlabeled samples for training I Tailored system from computer vision to NLP (Hausser et al., 2017)</p>
    <p>I Labeled/unlabeled samples in the same class are similar I Sample representation is given by the n  1th layer I Walking cycles: labeled  unlabeled  labeled I Maximize the number of correct cycles</p>
    <p>I L = 1 Lclassification + 2 Lwalker + 3 Lvisit</p>
    <p>SL1 S L 2 S</p>
    <p>L 3 S</p>
    <p>L 4 S</p>
    <p>L 5</p>
    <p>SU1 S U 2 S</p>
    <p>U 3 S</p>
    <p>U 4 S</p>
    <p>U 5 S</p>
    <p>U 6</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Approach</p>
    <p>I Goal: Unlabeled samples for training I Tailored system from computer vision to NLP (Hausser et al., 2017)</p>
    <p>I Labeled/unlabeled samples in the same class are similar I Sample representation is given by the n  1th layer I Walking cycles: labeled  unlabeled  labeled I Maximize the number of correct cycles</p>
    <p>I L = 1 Lclassification + 2 Lwalker + 3 Lvisit</p>
    <p>SL1 S L 2 S</p>
    <p>L 3 S</p>
    <p>L 4 S</p>
    <p>L 5</p>
    <p>SU1 S U 2 S</p>
    <p>U 3 S</p>
    <p>U 4 S</p>
    <p>U 5 S</p>
    <p>U 6</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Approach</p>
    <p>I Goal: Unlabeled samples for training I Tailored system from computer vision to NLP (Hausser et al., 2017)</p>
    <p>I Labeled/unlabeled samples in the same class are similar I Sample representation is given by the n  1th layer I Walking cycles: labeled  unlabeled  labeled I Maximize the number of correct cycles</p>
    <p>I L = 1 Lclassification + 2 Lwalker + 3 Lvisit</p>
    <p>SL1 S L 2 S</p>
    <p>L 3 S</p>
    <p>L 4 S</p>
    <p>L 5</p>
    <p>SU1 S U 2 S</p>
    <p>U 3 S</p>
    <p>U 4 S</p>
    <p>U 5 S</p>
    <p>U 6</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Approach</p>
    <p>I Goal: Unlabeled samples for training I Tailored system from computer vision to NLP (Hausser et al., 2017)</p>
    <p>I Labeled/unlabeled samples in the same class are similar I Sample representation is given by the n  1th layer I Walking cycles: labeled  unlabeled  labeled I Maximize the number of correct cycles</p>
    <p>I L = 1 Lclassification + 2 Lwalker + 3 Lvisit</p>
    <p>SL1 S L 2 S</p>
    <p>L 3 S</p>
    <p>L 4 S</p>
    <p>L 5</p>
    <p>SU1 S U 2 S</p>
    <p>U 3 S</p>
    <p>U 4 S</p>
    <p>U 5 S</p>
    <p>U 6</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Approach</p>
    <p>I Goal: Unlabeled samples for training I Tailored system from computer vision to NLP (Hausser et al., 2017)</p>
    <p>I Labeled/unlabeled samples in the same class are similar I Sample representation is given by the n  1th layer I Walking cycles: labeled  unlabeled  labeled I Maximize the number of correct cycles</p>
    <p>I L = 1 Lclassification + 2 Lwalker + 3 Lvisit</p>
    <p>SL1 S L 2 S</p>
    <p>L 3 S</p>
    <p>L 4 S</p>
    <p>L 5</p>
    <p>SU1 S U 2 S</p>
    <p>U 3 S</p>
    <p>U 4 S</p>
    <p>U 5 S</p>
    <p>U 6</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Approach</p>
    <p>I Goal: Unlabeled samples for training I Tailored system from computer vision to NLP (Hausser et al., 2017)</p>
    <p>I Labeled/unlabeled samples in the same class are similar I Sample representation is given by the n  1th layer I Walking cycles: labeled  unlabeled  labeled I Maximize the number of correct cycles</p>
    <p>I L = 1 Lclassification + 2 Lwalker + 3 Lvisit</p>
    <p>I Adapted bilingual word embeddings make the models able to find correct cycles at the beginning of the training and improve them later on.</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Sentiment Analysis of Tweets</p>
    <p>I RepLab 2013 sentiment classification (+/0/-) of En/Es tweets (Amigo et al., 2013)</p>
    <p>I @churcaballero jajaja con lo bien que iba el volvo...</p>
    <p>I General domain data: 49.2M OpenSubtitles sentences (Lison and Tiedemann, 2016)</p>
    <p>I Twitter specific data: I 22M downloaded tweets I RepLab Background</p>
    <p>I Seed lexicon: frequent English words from BNC (Kilgarriff, 1997)</p>
    <p>I Labeled data: RepLab En training set</p>
    <p>I Unlabeled data: RepLab Es training set</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Sentiment Analysis of Tweets</p>
    <p>I Our method is easily applicable to word embedding-based off-the-shelf classifiers</p>
    <p>muy</p>
    <p>chido</p>
    <p>fiesta</p>
    <p>...</p>
    <p>very</p>
    <p>coool</p>
    <p>party</p>
    <p>...</p>
    <p>CNN classifier (Kim, 2014)</p>
  </div>
  <div class="page">
    <p>Medical Bilingual Lexicon Induction</p>
    <p>I Mine Dutch translations of English medical words (Heyman et al., 2017)</p>
    <p>I sciatica  ischias</p>
    <p>I General domain data: 2M Europarl (v7) sentences</p>
    <p>I Medical data: 73.7K medical Wikipedia sentences</p>
    <p>I Medical seed lexicon (Heyman et al., 2017)</p>
    <p>I Unlabeled</p>
  </div>
  <div class="page">
    <p>Medical Bilingual Lexicon Induction I Classifier based approach (Heyman et al., 2017)</p>
    <p>I Word pairs as training set (negative sampling)</p>
    <p>I Character level LSTM to learn orthographic similarity</p>
    <p>...</p>
    <p>...</p>
    <p>a n a l o g u o s a n a l o o g &lt;p&gt; &lt;p&gt;</p>
  </div>
  <div class="page">
    <p>Medical Bilingual Lexicon Induction I Classifier based approach (Heyman et al., 2017)</p>
    <p>I Word pairs as training set (negative sampling)</p>
    <p>I Word embeddings to learn semantic similarity</p>
    <p>...</p>
    <p>...</p>
    <p>a n a l o g u o s a n a l o o g &lt;p&gt; &lt;p&gt;</p>
  </div>
  <div class="page">
    <p>Medical Bilingual Lexicon Induction I Classifier based approach (Heyman et al., 2017)</p>
    <p>I Word pairs as training set (negative sampling)</p>
    <p>I Dense-layer scores word pairs</p>
    <p>...</p>
    <p>...</p>
    <p>a n a l o g u o s a n a l o o g &lt;p&gt; &lt;p&gt;</p>
  </div>
  <div class="page">
    <p>Results: Sentiment Analysis</p>
    <p>labeled data En</p>
    <p>En En+Es</p>
    <p>unlabeled data</p>
    <p>Es</p>
    <p>Baseline 59.05%</p>
    <p>BACKGROUND 58.50%</p>
    <p>Subtitle+BACKGROUND 59.34%</p>
    <p>Subtitle+22M tweets 61.06%</p>
    <p>Table 1: Accuracy on cross-lingual sentiment analysis of tweets</p>
  </div>
  <div class="page">
    <p>Results: Sentiment Analysis</p>
    <p>labeled data En En</p>
    <p>En+Es</p>
    <p>unlabeled data - Es</p>
    <p>Baseline 59.05% 58.67% (-0.38%)</p>
    <p>BACKGROUND 58.50% 57.41% (-1.09%)</p>
    <p>Subtitle+BACKGROUND 59.34% 60.31% (0.97%)</p>
    <p>Subtitle+22M tweets 61.06% 63.23% (2.17%)</p>
    <p>Table 1: Accuracy on cross-lingual sentiment analysis of tweets</p>
  </div>
  <div class="page">
    <p>Results: Sentiment Analysis</p>
    <p>labeled data En En En+Es unlabeled data - Es</p>
    <p>Baseline 59.05% 58.67% (-0.38%) BACKGROUND 58.50% 57.41% (-1.09%)</p>
    <p>Subtitle+BACKGROUND 59.34% 60.31% (0.97%) 62.92% (2.61%) Subtitle+22M tweets 61.06% 63.23% (2.17%) 63.82% (0.59%)</p>
    <p>Table 1: Accuracy on cross-lingual sentiment analysis of tweets</p>
  </div>
  <div class="page">
    <p>Results: Bilingual Lexicon Induction</p>
    <p>labeled lexicon medical BNC</p>
    <p>medical medical</p>
    <p>unlabeled lexicon -</p>
    <p>medical BNC</p>
    <p>Baseline 35.70 20.73</p>
    <p>Europarl+Medical 40.71 22.10</p>
    <p>Table 2: F1 scores of medical bilingual lexicon induction</p>
  </div>
  <div class="page">
    <p>Results: Bilingual Lexicon Induction</p>
    <p>labeled lexicon medical BNC medical medical unlabeled lexicon - - medical BNC</p>
    <p>Baseline 35.70 20.73 36.20 (0.50) 35.04 (-0.66) Europarl+Medical 40.71 22.10 41.44 (0.73) 41.01 (0.30)</p>
    <p>Table 2: F1 scores of medical bilingual lexicon induction</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>I Bilingual transfer learning yield poor results when using out-of-domain resource</p>
    <p>I We showed that performance can be increased by using only additional unlabeled monolingual data</p>
    <p>I Delightfully simple approach to adapt embeddings I Broadly applicable method to exploit unlabeled data</p>
    <p>I Language and task independent approaches</p>
  </div>
  <div class="page">
    <p>Thank your for your attention!</p>
    <p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No 640550).</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[1] Enrique Amigo, Jorge Carrillo de Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Tamara Martn, Edgar Meij, Maarten de Rijke, Damiano Spina, Enrique Amigo, Jorge Carrillo de Albornoz, Tamara Martin, and Maarten de Rijke. 2013. Overview of replab 2013: Evaluating online reputation monitoring systems. In Proc. CLEF.</p>
    <p>[2] Philip Hausser, Alexander Mordvintsev, and Daniel Cremers. 2017. Learning by Association - A versatile semi-supervised training method for neural networks. In Proc. CVPR.</p>
    <p>[3] Geert Heyman, Ivan Vulic, and Marie-Francine Moens. 2017. Bilingual lexicon induction by learning to combine word-level and character-level representations. In Proc. EACL.</p>
    <p>[4] Adam Kilgarriff. 1997. Putting frequencies in the dictionary. International Journal of Lexicography.</p>
    <p>[5] Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proc. EMNLP.</p>
    <p>[6] Pierre Lison and Jorg Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. In Proc. LREC.</p>
    <p>[7] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proc. ICLR.</p>
    <p>[8] Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168.</p>
  </div>
</Presentation>
