<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Da Zheng</p>
    <p>Randal Burns</p>
    <p>Alex Szalay Johns Hopkins University</p>
    <p>A Parallel Page Cache: IOPS and Caching for Multicore Systems</p>
  </div>
  <div class="page">
    <p>Parallel page cache</p>
    <p>Scalable for 1M IOPS workload</p>
    <p>Partition the global cache into many small page sets  Eliminate lock contentions</p>
    <p>For SMP and NUMA</p>
    <p>Designed for cloud data service</p>
    <p>page cache page set</p>
    <p>page set</p>
    <p>page set</p>
    <p>page set</p>
    <p>page set</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Linux page cache</p>
    <p>Goal</p>
    <p>Design  Set-Associative cache</p>
    <p>NUMA-SA cache</p>
    <p>Experiments</p>
    <p>Future work</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Hardware trend: high IOPS + many cores  Cloud I/O: randomness + lower cache hits  Key/value stores: index lookup</p>
    <p>OS page cache  Designed for magnetic disks (thousands of IOPS)  Designed for high cache hits</p>
    <p>The current OS page cache doesnt work with cloud workload</p>
  </div>
  <div class="page">
    <p>Can we use direct I/O?</p>
    <p>The performance of SSD and memory</p>
    <p>Memory cache provides higher throughput and hides</p>
    <p>latency</p>
    <p>Random IOPS Latency Granularity</p>
    <p>ioDrive Octal 1,300,000 45,000ns 512B</p>
    <p>OCZ Vertex 4 120,000 20,000ns 512B</p>
    <p>DDR3-1333 7,300,000 15ns 128B</p>
    <p>Memory cache is necessary</p>
  </div>
  <div class="page">
    <p>Linux page cache</p>
    <p>Pages are managed in a global page pool.  Linux use a radix tree as index for pages in a file.  Searching the index: protected by Read-Copy-Update (RCU).  Good for reading cache</p>
    <p>Page replacement requires to grab spin locks.</p>
    <p>Lock</p>
    <p>Lock</p>
    <p>Locks destroy the performance of Linux page cache</p>
  </div>
  <div class="page">
    <p>Our goal</p>
    <p>A memory cache with extremely low overhead for</p>
    <p>massive parallel access</p>
    <p>Focus on cloud workload  Most accesses are reads  Power law distribution: few pages are accessed many times; most</p>
    <p>pages are accessed few times</p>
  </div>
  <div class="page">
    <p>Set-associative cache</p>
    <p>Problem: reduce lock contention  Solution: replace a global lock with fine-grain locks</p>
  </div>
  <div class="page">
    <p>Set-associative cache  Each set  Small number of pages: 8  Non-resident pages  Lock: protects the page metadata in the set</p>
    <p>All metadata of a page set in 4 cache lines  Few cache misses for</p>
    <p>searching and updating</p>
  </div>
  <div class="page">
    <p>Set-associative cache</p>
    <p>Page eviction policy works inside a set  Pages: LFU  Non-resident pages: LRU</p>
    <p>Designed for SMP</p>
  </div>
  <div class="page">
    <p>NUMA-SA cache</p>
    <p>Problem: remote memory access has long latency  Avoid remote memory access</p>
    <p>Solution: partition the cache by NUMA nodes  All cores in a NUMA node share a cache partition  A NUMA node is treated as a node in the distributed system</p>
  </div>
  <div class="page">
    <p>NUMA-SA cache</p>
    <p>Requests are hashed and redistributed</p>
    <p>A work thread is attached to a core to serve requests</p>
    <p>Inter-core communication via message passing</p>
    <p>hash function</p>
    <p>NUMA node 1</p>
    <p>core</p>
    <p>core</p>
    <p>thread</p>
    <p>NUMA node 2</p>
    <p>core</p>
    <p>core</p>
    <p>request 1 request 2 request 3</p>
    <p>SA</p>
    <p>cache SA</p>
    <p>cache</p>
    <p>q u e u e q u e u e</p>
    <p>q u e u e q u e u e</p>
  </div>
  <div class="page">
    <p>NUMA-SA cache</p>
    <p>Requests are bundled for efficiency  Tradeoff: throughput vs. latency</p>
    <p>Asynchronous IO-like programming interface  ssize_t access(io_request *requests, int num)  void wait_replies(int max_replies,</p>
    <p>reply_callback_t func)</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Experiment 1: scalability of our cache with a high page turnover rate  Under random workload without cache hits</p>
    <p>Experiment 2: cache hit rate of our cache  Under zipfian workload</p>
    <p>Experiment 3: overall run-time performance of our cache  Under zipfian workload</p>
  </div>
  <div class="page">
    <p>Experiment setup</p>
    <p>Random read-only workload: YCSB</p>
    <p>On Ramdisk</p>
    <p>48-core NUMA machine</p>
  </div>
  <div class="page">
    <p>SA-cache vs. direct IO</p>
    <p>SA-cache scale as well as direct IO in SMP</p>
  </div>
  <div class="page">
    <p>Why does Linux cache scale poorly?</p>
    <p>Lock overhead leads to poor performance in the Linux</p>
    <p>page cache</p>
  </div>
  <div class="page">
    <p>NUMA-SA cache vs. direct IO</p>
    <p>NUMA-SA cache scales well</p>
  </div>
  <div class="page">
    <p>Cache hits  zipfian workload</p>
    <p>Frenquency works better than recency</p>
    <p>SA-LFU(NR) realizes a cache hit rate similar to</p>
    <p>ClockPro.</p>
  </div>
  <div class="page">
    <p>Runtime zipfian workload</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Performance evaluation on SSD array  Performance is currently measured on ramdisk  Overhead in the block layer and below may impact our design.</p>
    <p>In-kernel implementation  Our cache is currently implemented in the user space</p>
    <p>Dynamic cache sizing  Our cache size is static.</p>
    <p>Email: dzheng5@jhu.edu</p>
  </div>
</Presentation>
