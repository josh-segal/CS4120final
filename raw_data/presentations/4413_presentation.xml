<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>TCP Throughput Collapse in Cluster-based Storage Systems</p>
    <p>Amar Phanishayee</p>
    <p>Elie Krevat, Vijay Vasudevan,</p>
    <p>David Andersen, Greg Ganger,</p>
    <p>Garth Gibson, Srini Seshan</p>
    <p>Carnegie Mellon University</p>
  </div>
  <div class="page">
    <p>Cluster-based Storage Systems</p>
    <p>Client Switch</p>
    <p>Storage Servers</p>
    <p>R R</p>
    <p>R R</p>
    <p>Data Block</p>
    <p>Server Request Unit (SRU)</p>
    <p>Synchronized Read</p>
    <p>Client now sends next batch of requests</p>
  </div>
  <div class="page">
    <p>TCP Throughput Collapse: Setup</p>
    <p>Test on an Ethernet-based storage cluster</p>
    <p>Client performs synchronized reads</p>
    <p>Increase # of servers involved in transfer  SRU size is fixed</p>
    <p>TCP used as the data transfer protocol</p>
  </div>
  <div class="page">
    <p>TCP Throughput Collapse: Incast</p>
    <p>[Nagle04] called this Incast  Cause of throughput collapse: TCP timeouts</p>
    <p>Collapse!</p>
  </div>
  <div class="page">
    <p>Hurdle for Ethernet Networks</p>
    <p>FibreChannel, InfiniBand  Specialized high throughput networks</p>
    <p>Expensive</p>
    <p>Commodity Ethernet networks  10 Gbps rolling out, 100Gbps being drafted  Low cost  Shared routing infrastructure (LAN, SAN, HPC)</p>
    <p>TCP throughput collapse (with synchronized reads)</p>
  </div>
  <div class="page">
    <p>Our Contributions</p>
    <p>Study network conditions that cause TCP throughput collapse</p>
    <p>Analyse the effectiveness of various networklevel solutions to mitigate this collapse.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation : TCP throughput collapse</p>
    <p>High-level overview of TCP</p>
    <p>Characterizing Incast</p>
    <p>Conclusion and ongoing work</p>
  </div>
  <div class="page">
    <p>TCP overview</p>
    <p>Reliable, in-order byte stream  Sequence numbers and cumulative</p>
    <p>acknowledgements (ACKs)</p>
    <p>Retransmission of lost packets</p>
    <p>Adaptive  Discover and utilize available link bandwidth</p>
    <p>Assumes loss is an indication of congestion</p>
    <p>Slow down sending rate</p>
  </div>
  <div class="page">
    <p>TCP: data-driven loss recovery</p>
    <p>Sender Receiver</p>
    <p>Ack 1</p>
    <p>Ack 1</p>
    <p>Ack 1</p>
    <p>Ack 1</p>
    <p>Seq #</p>
    <p>Retransmit packet 2 immediately</p>
    <p>In SANs recovery in usecs after loss.</p>
    <p>Ack 5</p>
  </div>
  <div class="page">
    <p>TCP: timeout-driven loss recovery</p>
    <p>Sender Receiver</p>
    <p>Retransmission Timeout (RTO)</p>
    <p>Ack 1</p>
    <p>Seq #</p>
    <p>Timeouts are expensive (msecs to recover after loss)</p>
  </div>
  <div class="page">
    <p>TCP: Loss recovery comparison</p>
    <p>Sender Receiver</p>
    <p>Ack 1</p>
    <p>Ack 1 Ack 1 Ack 1</p>
    <p>Retransmit 2</p>
    <p>Seq #</p>
    <p>Ack 5</p>
    <p>Sender Receiver</p>
    <p>Retransmission Timeout (RTO)</p>
    <p>Ack 1</p>
    <p>Seq #</p>
    <p>Timeout driven recovery is slow (ms)</p>
    <p>Data-driven recovery is super fast (us) in SANs</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation : TCP throughput collapse</p>
    <p>High-level overview of TCP</p>
    <p>Characterizing Incast  Comparing real-world and simulation results</p>
    <p>Analysis of possible solutions</p>
    <p>Conclusion and ongoing work</p>
  </div>
  <div class="page">
    <p>Link idle time due to timeouts</p>
    <p>Client Switch</p>
    <p>R R</p>
    <p>R R</p>
    <p>Synchronized Read</p>
    <p>Link is idle until server experiences a timeout</p>
  </div>
  <div class="page">
    <p>Client Link Utilization</p>
  </div>
  <div class="page">
    <p>Characterizing Incast</p>
    <p>Incast on storage clusters</p>
    <p>Simulation in a network simulator (ns-2)  Can easily vary</p>
    <p>Number of servers</p>
    <p>Switch buffer size</p>
    <p>SRU size</p>
    <p>TCP parameters</p>
    <p>TCP implementations</p>
  </div>
  <div class="page">
    <p>Incast on a storage testbed</p>
    <p>~32KB output buffer per port</p>
    <p>Storage nodes run Linux 2.6.18 SMP kernel</p>
  </div>
  <div class="page">
    <p>Simulating Incast: comparison</p>
    <p>Simulation closely matches real-world result</p>
  </div>
  <div class="page">
    <p>Outline  Motivation : TCP throughput collapse</p>
    <p>High-level overview of TCP</p>
    <p>Characterizing Incast  Comparing real-world and simulation results</p>
    <p>Analysis of possible solutions  Varying system parameters</p>
    <p>Increasing switch buffer size</p>
    <p>Increasing SRU size</p>
    <p>TCP-level solutions  Ethernet flow control</p>
    <p>Conclusion and ongoing work</p>
  </div>
  <div class="page">
    <p>Increasing switch buffer size</p>
    <p>Timeouts occur due to losses  Loss due to limited switch buffer space</p>
    <p>Hypothesis: Increasing switch buffer size delays throughput collapse</p>
    <p>How effective is increasing the buffer size in mitigating throughput collapse?</p>
  </div>
  <div class="page">
    <p>Increasing switch buffer size: results</p>
    <p>per-port output buffer</p>
  </div>
  <div class="page">
    <p>Increasing switch buffer size: results</p>
    <p>per-port output buffer</p>
  </div>
  <div class="page">
    <p>Increasing switch buffer size: results</p>
    <p>More servers supported before collapse</p>
    <p>Fast (SRAM) buffers are expensive</p>
    <p>per-port output buffer</p>
  </div>
  <div class="page">
    <p>Increasing SRU size</p>
    <p>No throughput collapse using netperf  Used to measure network throughput and latency</p>
    <p>netperf does not perform synchronized reads</p>
    <p>Hypothesis: Larger SRU size  less idle time  Servers have more data to send per data block</p>
    <p>One server waits (timeout), others continue to send</p>
  </div>
  <div class="page">
    <p>Increasing SRU size: results</p>
    <p>SRU = 10KB</p>
  </div>
  <div class="page">
    <p>Increasing SRU size: results</p>
    <p>SRU = 10KB</p>
    <p>SRU = 1MB</p>
  </div>
  <div class="page">
    <p>Increasing SRU size: results</p>
    <p>SRU = 10KB</p>
    <p>SRU = 1MB</p>
    <p>SRU = 8MB</p>
    <p>Significant reduction in throughput collapse</p>
    <p>More pre-fetching, kernel memory</p>
  </div>
  <div class="page">
    <p>Fixed Block Size</p>
  </div>
  <div class="page">
    <p>Outline  Motivation : TCP throughput collapse</p>
    <p>High-level overview of TCP</p>
    <p>Characterizing Incast  Comparing real-world and simulation results</p>
    <p>Analysis of possible solutions  Varying system parameters</p>
    <p>TCP-level solutions  Avoiding timeouts</p>
    <p>Alternative TCP implementations</p>
    <p>Aggressive data-driven recovery</p>
    <p>Reducing the penalty of a timeout</p>
    <p>Ethernet flow control</p>
  </div>
  <div class="page">
    <p>Avoiding Timeouts: Alternative TCP impl.</p>
    <p>NewReno better than Reno, SACK (8 servers)</p>
    <p>Throughput collapse inevitable</p>
  </div>
  <div class="page">
    <p>Timeouts are inevitable</p>
    <p>Sender Receiver</p>
    <p>Ack 1</p>
    <p>Ack 1</p>
    <p>Aggressive data-driven recovery does not help.</p>
    <p>Sender Receiver</p>
    <p>Retransmission Timeout (RTO)</p>
    <p>Retransmitted packets are lost</p>
    <p>Sender Receiver</p>
    <p>Retransmission Timeout (RTO)</p>
    <p>Complete window of data is lost (most cases)</p>
  </div>
  <div class="page">
    <p>Reducing the penalty of timeouts</p>
    <p>Reduced RTOmin helps But still shows 30% decrease for 64 servers</p>
    <p>Reduce penalty by reducing Retransmission TimeOut period (RTO)</p>
    <p>NewReno with RTOmin = 200ms</p>
    <p>RTOmin = 200us</p>
  </div>
  <div class="page">
    <p>Issues with Reduced RTOmin</p>
    <p>Implementation Hurdle - Requires fine grained OS timers (us)</p>
    <p>- Very high interrupt rate</p>
    <p>- Current OS timers  ms granularity</p>
    <p>- Soft timers not available for all platforms</p>
    <p>Unsafe - Servers talk to other clients over wide area</p>
    <p>- Overhead: Unnecessary timeouts, retransmissions</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation : TCP throughput collapse</p>
    <p>High-level overview of TCP</p>
    <p>Characterizing Incast  Comparing real-world and simulation results</p>
    <p>Analysis of possible solutions</p>
    <p>Varying system parameters</p>
    <p>TCP-level solutions  Ethernet flow control</p>
    <p>Conclusion and ongoing work</p>
  </div>
  <div class="page">
    <p>Ethernet Flow Control</p>
    <p>Flow control at the link level  Overloaded port sends pause frames to all</p>
    <p>senders (interfaces)</p>
    <p>EFC disabled</p>
    <p>EFC enabled</p>
  </div>
  <div class="page">
    <p>Issues with Ethernet Flow Control</p>
    <p>Can result in head-of-line blocking</p>
    <p>Pause frames not forwarded across switch hierarchy</p>
    <p>Switch implementations are inconsistent</p>
    <p>Flow agnostic  e.g. all flows asked to halt</p>
    <p>irrespective of send-rate</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Synchronized Reads and TCP timeouts cause TCP Throughput Collapse</p>
    <p>No single convincing network-level solution</p>
    <p>Current Options  Increase buffer size (costly)</p>
    <p>Reduce RTOmin (unsafe)</p>
    <p>Use Ethernet Flow Control (limited applicability)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>No throughput collapse in InfiniBand</p>
    <p>Number of servers</p>
    <p>Results obtained from Wittawat Tantisiriroj</p>
  </div>
  <div class="page">
    <p>Varying RTOmin</p>
    <p>RTOmin (seconds)</p>
  </div>
</Presentation>
