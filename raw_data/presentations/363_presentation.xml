<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deriving Private Information from Randomized Data</p>
    <p>Zhengli Huang Wenliang (Kevin) Du</p>
    <p>Biao Chen</p>
    <p>Syracuse University</p>
  </div>
  <div class="page">
    <p>Privacy-Preserving Data Mining Data Mining</p>
    <p>Data Collection</p>
    <p>Data Disguising</p>
    <p>Central Database</p>
    <p>Classification Association Rules Clustering</p>
  </div>
  <div class="page">
    <p>Random Perturbation</p>
    <p>+</p>
    <p>Original Data X Random Noise R Disguised Data Y</p>
  </div>
  <div class="page">
    <p>How Secure is Randomization Perturbation?</p>
  </div>
  <div class="page">
    <p>A Simple Observation  We cant perturb the same number for</p>
    <p>several times.  If we do that, we can estimate the</p>
    <p>original data:  Let t be the original data,  Disguised data: t + R1, t + R2, , t + Rm  Let Z = [(t+R1)+  + (t+Rm)] / m  Mean: E(Z) = t</p>
  </div>
  <div class="page">
    <p>This looks familiar   This is the data set (x, x, x, x, x, x, x, x)  Random Perturbation:</p>
    <p>(x+r1, x+r2,, x+rm)</p>
    <p>We know this is NOT safe.</p>
    <p>Observation: the data set is highly correlated.</p>
  </div>
  <div class="page">
    <p>Lets Generalize!  Data set: (x1, x2, x3, , xm)  If the correlation among data attributes</p>
    <p>are high, can we use that to improve our estimation (from the disguised data)?</p>
  </div>
  <div class="page">
    <p>Data Reconstruction (DR)</p>
    <p>Original Data X</p>
    <p>Disguised Data Y</p>
    <p>Distribution of random noise Reconstructed Data X</p>
    <p>Whats their difference?</p>
    <p>Data Reconstruction</p>
  </div>
  <div class="page">
    <p>Reconstruction Algorithms</p>
    <p>Principal Component Analysis (PCA)</p>
    <p>Bayes Estimate Method</p>
  </div>
  <div class="page">
    <p>PCA-Based Data Reconstruction</p>
  </div>
  <div class="page">
    <p>PCA-Based Reconstruction</p>
    <p>Disguised Information</p>
    <p>Reconstructed Information</p>
    <p>Squeeze</p>
    <p>Information Loss</p>
  </div>
  <div class="page">
    <p>How?  Observation:</p>
    <p>Original data are correlated.  Noise are not correlated.</p>
    <p>Principal Component Analysis  Useful for lossy compression</p>
  </div>
  <div class="page">
    <p>PCA Introduction</p>
    <p>The main use of PCA: reduce the dimensionality while retaining as much information as possible.</p>
    <p>1st PC: containing the greatest amount of variation.</p>
    <p>2nd PC: containing the next largest amount of variation.</p>
  </div>
  <div class="page">
    <p>For the Original Data  They are correlated.  If we remove 50% of the dimensions,</p>
    <p>the actual information loss might be less than 10%.</p>
  </div>
  <div class="page">
    <p>For the Random Noises  They are not correlated.  Their variance is evenly distributed to</p>
    <p>any direction.  If we remove 50% of the dimensions,</p>
    <p>the actual noise loss should be 50%.</p>
  </div>
  <div class="page">
    <p>PCA-Based Reconstruction</p>
    <p>Disguised Data</p>
    <p>Reconstructed Data</p>
    <p>PCA Compression</p>
    <p>De-Compression</p>
  </div>
  <div class="page">
    <p>Bayes-Estimation-Based Data Reconstruction</p>
  </div>
  <div class="page">
    <p>A Different Perspective</p>
    <p>What is the Most likely X?</p>
    <p>Disguised Data Y</p>
    <p>Possible XPossible XPossible X</p>
    <p>Random Noise</p>
  </div>
  <div class="page">
    <p>The Problem Formulation  For each possible X, there is a</p>
    <p>probability: P(X | Y).  Find an X, s.t., P(X | Y) is maximized.  How to compute P(X | Y)?</p>
  </div>
  <div class="page">
    <p>The Power of the Bayes Rule</p>
    <p>P(X|Y) is difficult!</p>
    <p>P(X| Y)?</p>
    <p>P(Y|X)</p>
    <p>P(Y)</p>
    <p>P(X)*</p>
  </div>
  <div class="page">
    <p>Computing P(X | Y)?  P(X|Y) = P(Y|X)* P(X) / P(Y)  P(Y|X): remember Y = X + R  P(Y): A constant (we dont care)  How to get P(X)?</p>
    <p>This is where the correlation can be used.  Assume Multivariate Gaussian Distribution</p>
    <p>The parameters are unknown.</p>
  </div>
  <div class="page">
    <p>Multivariate Gaussian Distribution</p>
    <p>A Multivariate Gaussian distribution  Each variable is a Gaussian distribution</p>
    <p>with mean i  Mean vector  = (1 ,, m)  Covariance matrix</p>
    <p>Both  and  can be estimated from Y  So we can get P(X)</p>
  </div>
  <div class="page">
    <p>Bayes-Estimate-based Data Reconstruction</p>
    <p>Original X Disguised Data Y</p>
    <p>Randomization</p>
    <p>Estimated X Which X maximizes</p>
    <p>P(X|Y)</p>
    <p>P(X)P(Y|X)</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Increasing the Number of Attributes</p>
  </div>
  <div class="page">
    <p>Increasing Eigenvalues of the Non-Principal Components</p>
  </div>
  <div class="page">
    <p>How to improve Random Perturbation?</p>
  </div>
  <div class="page">
    <p>Observation from PCA</p>
    <p>How to make it difficult to squeeze out noise?  Make the correlation of the noise similar to</p>
    <p>the original data.  Noise now concentrates on the principal</p>
    <p>components, like the original data X.  How to get the correlation of X?</p>
  </div>
  <div class="page">
    <p>Improved Randomization</p>
  </div>
  <div class="page">
    <p>Conclusion And Future Work  When does randomization fail:</p>
    <p>Answer: when the data correlation is high.  Can it be cured? Using correlated noise</p>
    <p>similar to the original data  Still Unknown:</p>
    <p>Is the correlated-noise approach really better?</p>
    <p>Can other information affect privacy?</p>
  </div>
</Presentation>
