<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Understanding latent sector errors and how to protect against them</p>
    <p>Bianca Schroeder, Sotirios Damouras, Phillipa Gill</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>What is a latent sector error (LSE)?  Individual sectors on a drive become inaccessable (media error)</p>
    <p>Prevalence?  3.5% of drives experience LSE(s) [Bairavasundaram2007]</p>
    <p>7-9% for some disk models!</p>
    <p>Consequence of an LSE?  In a system without redundancy: data loss  In RAID-5, if discovered during reconstruction: data loss</p>
    <p>One of the main motivations for RAID-6  Growing concern with growing disk capacities</p>
  </div>
  <div class="page">
    <p>How to protect against them?</p>
    <p>Intra-disk redundancy  Replicate selected metadata [e.g. FFS]  Add parity block per file [e.g. Iron file systems]  Add parity block per group of sectors [Dholak.08]</p>
    <p>XOR</p>
    <p>Periodic scrubbing  Proactively detect LSEs and correct them.</p>
  </div>
  <div class="page">
    <p>Our goal</p>
    <p>Understand potential of different protection schemes  Understand characteristics of LSEs</p>
    <p>From point of view of protection</p>
    <p>How?  Using real data from production machines  Subset of data in Bairavasundaram et al. (Sigmetrics07)  Thanks for sharing!</p>
  </div>
  <div class="page">
    <p>The data</p>
    <p>1.5 million drives  SATA &amp; SCSI  LSEs detected by - application access - scrubber (bi-weekly)</p>
    <p>NetApp storage</p>
    <p>systems in the field The systems</p>
    <p>Covers 32 months  Focus on - 4 SATA models - 4 SCSI models  For each LSE: - Time of detection - LBN</p>
    <p>The data</p>
  </div>
  <div class="page">
    <p>How effective are protection schemes?</p>
    <p>Scrubbing  Intra-disk redundancy</p>
  </div>
  <div class="page">
    <p>Scrubbing</p>
    <p>Why?</p>
    <p>Detect and correct errors early  Reduces probability to encounter LSE during RAID</p>
    <p>reconstruction</p>
  </div>
  <div class="page">
    <p>Scrubbing  Standard sequential scrubbing</p>
  </div>
  <div class="page">
    <p>Scrubbing  Standard sequential scrubbing</p>
    <p>Localized scrubbing</p>
  </div>
  <div class="page">
    <p>Scrubbing  Standard sequential scrubbing</p>
    <p>Localized scrubbing</p>
    <p>Accelerated scrubbing</p>
  </div>
  <div class="page">
    <p>Scrubbing  Standard sequential scrubbing</p>
    <p>Localized scrubbing</p>
    <p>Accelerated scrubbing</p>
    <p>Staggered scrubbing [Oprea et al. 10]</p>
  </div>
  <div class="page">
    <p>Scrubbing  Standard sequential scrubbing</p>
    <p>Localized scrubbing</p>
    <p>Accelerated scrubbing</p>
    <p>Staggered scrubbing [Oprea et al. 10]</p>
    <p>Accelerated staggered scrubbing</p>
    <p>How do those approaches perform in practice, i.e. on</p>
    <p>real-world data?</p>
  </div>
  <div class="page">
    <p>Scrubbing: Evaluation on NetApp data</p>
    <p>No significant improvement from local &amp; accelerated scrubs  They dont reduce the time to detect whether there are any errors  Errors are close in space, so even standard scrub finds them soon</p>
    <p>Local scrub</p>
    <p>Accelerated scrub</p>
    <p>Staggered scrub</p>
    <p>Staggered accel. scrub</p>
  </div>
  <div class="page">
    <p>Scrubbing: Evaluation on NetApp data</p>
    <p>10-35% improvement with staggered scrubs!  Even better than the original paper claims!  Without introducing any overheads or additional reads  Relatively insensitive to choice of parameters</p>
    <p>Local scrub</p>
    <p>Accelerated scrub</p>
    <p>Staggered scrub</p>
    <p>Staggered accel. scrub</p>
  </div>
  <div class="page">
    <p>Intra-disk redundancy</p>
    <p>Why?  Recover LSEs in systems without redundancy  Recover LSEs during reconstruction in RAID-5</p>
    <p>Goal:  Evaluate potential protection</p>
    <p>What fraction of errors could be recovered  Qualitative discussion of overheads</p>
  </div>
  <div class="page">
    <p>Intra-disk redundancy</p>
    <p>Simplest scheme: Single Parity Check (SPC)  Can recover up to one LSE per parity group</p>
    <p>Data Parity Data Data Data</p>
    <p>k data sectors 1 parity sector</p>
    <p>Results from evaluation on Netapp data:  25-50% of drives have errors that SPC cannot recover</p>
    <p>Consider stronger schemes?</p>
  </div>
  <div class="page">
    <p>Stronger schemes?</p>
    <p>Additional parity =&gt; additional overhead in updating parity  When would that be interesting?</p>
    <p>In environments   like archival systems, that dont have updates and dont like</p>
    <p>scrubs since they require powering up the system   with read-mostly workloads, i.e. parity updates are rare   for selected critical data on a drive, such as meta-data</p>
  </div>
  <div class="page">
    <p>Inter-leaved Parity Check (IPC) [Dholakia08]</p>
    <p>Data Parity Data Data Data Data Data Parity</p>
    <p>k data sectors m redundant sectors</p>
    <p>Requires only 1 parity update per data update  Can tolerate up to m consecutive errors</p>
    <p>Parity</p>
  </div>
  <div class="page">
    <p>Inter-leaved Parity Check (IPC) [Dholakia08]</p>
    <p>Data Parity Data Data Data Data Data Parity</p>
    <p>k data sectors m redundant sectors</p>
    <p>Claim: Achieves protection as good as MDS codes [Dholakia08]  MDS=Maximum distance separable, e.g. Reed-Solomon  Expensive, but can tolerate loss of any m sectors</p>
    <p>Parity</p>
    <p>Results: (from evaluation on NetApp data)  Far weaker than MDS!  Not significantly better than SPC</p>
    <p>Implications  Need different ideas for improving on SPC  Maybe reuse ideas from RAID-6? (see paper for details &amp; results)</p>
    <p>Results differ from [Dholakia08]  Importance of real-world data.  Paper provides models &amp; parameters</p>
  </div>
  <div class="page">
    <p>Questions unanswered</p>
    <p>What level of protection to use when?  E.g. what is the right scrub frequency?  Depends on error probability at a given time</p>
  </div>
  <div class="page">
    <p>Do previous errors predict future?</p>
    <p>Probability of future errors</p>
    <p>Number of future errors</p>
    <p>Many previous errors =&gt; higher chance of future errors =&gt; higher number of future errors  Big differences between models</p>
    <p>Adapt protection based on previous errors</p>
    <p>Know your patient ..</p>
  </div>
  <div class="page">
    <p>Does first error interval predict future?</p>
    <p>#errors in first scrub with errors #errors in first scrub with errors</p>
    <p>Number of errors in first error interval: - Do increase expected number of future errors - Dont significantly increase probability of future occurrence</p>
  </div>
  <div class="page">
    <p>For how long are probabilities increased?</p>
    <p>Exponential drop-off, but still significant after tens of weeks  Independent of number of errors in first interval</p>
    <p>Taper off added protection over time,</p>
    <p>e.g. reduce scrub rate</p>
  </div>
  <div class="page">
    <p>Questions unanswered</p>
    <p>What level of protection to use when?  What is the error probability at a given time?</p>
    <p>What level of protection to use where?  Are all areas of the drive equally likely to develop errors?</p>
  </div>
  <div class="page">
    <p>Where on the drive are errors located?</p>
    <p>Up to 50% of errors concentrated in top/bottom 10% of drive  Also increased probability in some other parts of the drive</p>
    <p>Stronger protection for those areas</p>
    <p>Dont use for important data</p>
  </div>
  <div class="page">
    <p>Questions unanswered</p>
    <p>What level of protection to use when?  What is the error probability at a given time?</p>
    <p>Same protection scheme across entire drive?  Are all parts equally likely to develop errors?</p>
    <p>Scrubbing potentially harmful?  Do additional read operations increase error rate?</p>
  </div>
  <div class="page">
    <p>Does utilization affect LSEs?</p>
    <p>Collected data in Google data center (&gt;10,000 drives) on  Number of LSEs  Number of reads &amp; number of writes</p>
    <p>Results:  No correlation between #reads and #LSEs  No correlation between #writes and #LSEs</p>
    <p>Needs further investigation (future work).</p>
    <p>Maybe need not worry about scrubs introducing new errors?</p>
  </div>
  <div class="page">
    <p>Questions unanswered</p>
    <p>What level of protection to use when?  What is the error probability at a given time?</p>
    <p>Same protection scheme across entire drive?  Are all parts equally likely to develop errors?</p>
    <p>Scrubbing potentially harmful?  Do additional read operations increase error rate?</p>
    <p>What is the common distance between errors   Important for example for replica placement</p>
  </div>
  <div class="page">
    <p>How far are errors spaced apart?</p>
    <p>20-60% of errors have a neighbor within &lt; 10 sectors  Probability concentration (bumps) at certain distances</p>
    <p>Avoid placing replicas at certain distances</p>
    <p>Explains why single parity scheme not always helpful</p>
  </div>
  <div class="page">
    <p>Questions unanswered</p>
    <p>What level of protection to use when?  What is the error probability at a given time?</p>
    <p>Different protection for different parts of the drive?  Are all parts equally likely to develop errors?</p>
    <p>Scrubbing potentially harmful?  Do additional read operations increase error rate?</p>
    <p>What is the common distance between errors   Important for replica placement</p>
    <p>Are errors that are close in space also close in time?  Yes!</p>
  </div>
  <div class="page">
    <p>Questions unanswered</p>
    <p>What level of protection to use when?  What is the error probability at a given time?</p>
    <p>Different protection for different parts of the drive?  Are all parts equally likely to develop errors?</p>
    <p>Scrubbing potentially harmful?  Do additional read operations increase error rate?</p>
    <p>What is the common distance between errors   Important for replica placement</p>
    <p>Are errors that are close in space also close in time?  Yes!</p>
    <p>And many other questions  see paper!</p>
  </div>
  <div class="page">
    <p>Conclusion  Evaluated potential of different protection schemes</p>
    <p>Scrubbing  Simple new scheme (staggered scrubbing) performs very well!</p>
    <p>Intra-disk redundancy  Single parity can recover LSEs in 50-75% of the drives  Need to look at more complex schemes for coverage beyond that</p>
    <p>Looked at characteristics of LSEs  And how to exploit them for reliability</p>
    <p>Many characteristics not captured well by simple models  Provided parameters for models</p>
  </div>
  <div class="page">
    <p>Thanks!  To NetApp for sharing the data  To you for listening</p>
    <p>Questions?</p>
  </div>
</Presentation>
