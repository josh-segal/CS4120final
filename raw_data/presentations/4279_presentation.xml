<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>P R O B A B I L I S T I C F A S T T E X T F O R M U LT I - S E N S E W O R D E M B E D D I N G S</p>
    <p>B E N A T H I W A R A T K U N , A N D R E W G O R D O N W I L S O N , A N I M A A N A N D K U M A R</p>
  </div>
  <div class="page">
    <p>!2</p>
    <p>Gaussian Mixture Embeddings</p>
    <p>music</p>
    <p>jazz</p>
    <p>rock</p>
    <p>basalt pop</p>
    <p>stone</p>
    <p>rock ~rock,0</p>
    <p>~rock,1</p>
    <p>Words as probability densities</p>
    <p>Each word = Gaussian Mixture density</p>
    <p>Disentangled meanings</p>
    <p>Probabilistic FastText = FastText + Gaussian Mixture Embeddings</p>
  </div>
  <div class="page">
    <p>!3</p>
    <p>Gaussian Mixture Embeddings</p>
    <p>music</p>
    <p>jazz</p>
    <p>rock</p>
    <p>basalt pop</p>
    <p>stone</p>
    <p>rock ~rock,0</p>
    <p>~rock,1</p>
    <p>Words as probability densities</p>
    <p>Each word = Gaussian Mixture density</p>
    <p>Disentangled meanings</p>
    <p>FastText</p>
    <p>~abnormal</p>
    <p>~zabnorm</p>
    <p>~znorm</p>
    <p>~zab</p>
    <p>~zabnor</p>
    <p>~z...</p>
    <p>~z...</p>
    <p>Word embeddings: word vectors are derived from subword vectors</p>
    <p>SoA on many benchmarks especially RareWord</p>
    <p>Character based models allow for estimating vectors of unseen words and enhancing</p>
    <p>Probabilistic FastText = FastText + Gaussian Mixture Embeddings</p>
  </div>
  <div class="page">
    <p>!4</p>
    <p>Probabilistic FastText (PFT)</p>
    <p>rock</p>
    <p>basalt</p>
    <p>stone</p>
    <p>music</p>
    <p>jazzpop</p>
    <p>rock</p>
    <p>~rock,0</p>
    <p>~rock,1</p>
    <p>+</p>
    <p>Gaussian Mixture Embeddings</p>
    <p>music</p>
    <p>jazz</p>
    <p>rock</p>
    <p>basalt pop</p>
    <p>stone</p>
    <p>rock ~rock,0</p>
    <p>~rock,1</p>
    <p>FastText</p>
    <p>~abnormal</p>
    <p>~zabnorm</p>
    <p>~znorm</p>
    <p>~zab</p>
    <p>~zabnor</p>
    <p>~z...</p>
    <p>~z...</p>
  </div>
  <div class="page">
    <p>P R O B A B I L I S T I C F A S T T E X T</p>
    <p>!5</p>
    <p>C O O LL[cool] =</p>
    <p>L[coolz] = ?</p>
    <p>L[coolzz] = ?</p>
    <p>dictionary-based embeddings</p>
    <p>C O O L Z Z</p>
    <p>C O O L Z</p>
    <p>C O O Lf(cool) =</p>
    <p>f(coolz) =</p>
    <p>f(coolzz) =</p>
    <p>character-based probabilistic embeddings</p>
    <p>rock</p>
    <p>basalt</p>
    <p>stone</p>
    <p>music</p>
    <p>jazzpop rock</p>
    <p>~rock,0</p>
    <p>~rock,1</p>
    <p>Able to estimate distributions of unseen words</p>
  </div>
  <div class="page">
    <p>P R O B A B I L I S T I C F A S T T E X T</p>
    <p>!6</p>
    <p>C O O LL[cool] =</p>
    <p>L[coolz] = ?</p>
    <p>L[coolzz] = ?</p>
    <p>dictionary-based embeddings</p>
    <p>C O O L Z Z</p>
    <p>C O O L Z</p>
    <p>C O O Lf(cool) =</p>
    <p>f(coolz) =</p>
    <p>f(coolzz) =</p>
    <p>character-based probabilistic embeddings</p>
    <p>rock</p>
    <p>basalt</p>
    <p>stone</p>
    <p>music</p>
    <p>jazzpop rock</p>
    <p>~rock,0</p>
    <p>~rock,1</p>
    <p>Able to estimate distributions of unseen words</p>
    <p>High semantic quality for rare words via root sharing</p>
    <p>w2gm FastText PFT</p>
  </div>
  <div class="page">
    <p>P R O B A B I L I S T I C F A S T T E X T</p>
    <p>!7</p>
    <p>C O O LL[cool] =</p>
    <p>L[coolz] = ?</p>
    <p>L[coolzz] = ?</p>
    <p>dictionary-based embeddings</p>
    <p>C O O L Z Z</p>
    <p>C O O L Z</p>
    <p>C O O Lf(cool) =</p>
    <p>f(coolz) =</p>
    <p>f(coolzz) =</p>
    <p>character-based probabilistic embeddings</p>
    <p>rock</p>
    <p>basalt</p>
    <p>stone</p>
    <p>music</p>
    <p>jazzpop rock</p>
    <p>~rock,0</p>
    <p>~rock,1</p>
    <p>Able to estimate distributions of unseen words</p>
    <p>High semantic quality for rare words via root sharing</p>
    <p>disentangled meanings</p>
    <p>Word Component Nearest neighbors (cosine similarity)</p>
    <p>rock 0 rocks:0, rocky:0, mudrock:0, rockscape:0</p>
    <p>rock 1 punk:0, punk-rock:0, indie:0, pop-rock:0</p>
    <p>w2gm FastText PFT</p>
  </div>
  <div class="page">
    <p>P R O B A B I L I S T I C F A S T T E X T</p>
    <p>!8</p>
    <p>C O O LL[cool] =</p>
    <p>L[coolz] = ?</p>
    <p>L[coolzz] = ?</p>
    <p>dictionary-based embeddings</p>
    <p>C O O L Z Z</p>
    <p>C O O L Z</p>
    <p>C O O Lf(cool) =</p>
    <p>f(coolz) =</p>
    <p>f(coolzz) =</p>
    <p>character-based probabilistic embeddings</p>
    <p>rock</p>
    <p>basalt</p>
    <p>stone</p>
    <p>music</p>
    <p>jazzpop rock</p>
    <p>~rock,0</p>
    <p>~rock,1</p>
    <p>Able to estimate distributions of unseen words</p>
    <p>High semantic quality for rare words via root sharing</p>
    <p>Applicable to foreign languages without any changes in model hyperparameters!</p>
    <p>disentangled meanings</p>
    <p>Word Component Nearest neighbors (cosine similarity)</p>
    <p>rock 0 rocks:0, rocky:0, mudrock:0, rockscape:0</p>
    <p>rock 1 punk:0, punk-rock:0, indie:0, pop-rock:0</p>
    <p>Word Component /</p>
    <p>Meaning Nearest neighbors (English Translation)</p>
    <p>secondo 0 / 2nd Secondo (2nd), terzo (3rd) , quinto (5th), primo (first)</p>
    <p>secondo 1 / according to conformit (compliance), attenendosi (following), cui (which)</p>
    <p>w2gm FastText PFT</p>
  </div>
  <div class="page">
    <p>V E C T O R E M B E D D I N G S &amp; F A S T T E X T</p>
    <p>!9</p>
  </div>
  <div class="page">
    <p>W O R D E M B E D D I N G S</p>
    <p>word2vec (Mikolov et al., 2013)</p>
    <p>GloVe (Pennington et al., 2014)</p>
    <p>!10</p>
    <p>- 0 . 1</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>dimension ~ 50 - 1000</p>
    <p>.</p>
    <p>.</p>
    <p>. 0</p>
    <p>size of vocabulary ~ Millions</p>
    <p>one-hot vector dense representation</p>
    <p>} vectors</p>
    <p>abnormal</p>
    <p>modulation</p>
    <p>normal abnormality</p>
    <p>harmonics</p>
    <p>amplitude</p>
  </div>
  <div class="page">
    <p>D E N S E R E P R E S E N T A T I O N O F W O R D S</p>
    <p>!11 Mikolov 2013</p>
    <p>i.e. China - Beijing ~ Japan - Tokyo</p>
    <p>Meaningful nearest neighbors Relationship deduction from vector arithmetic</p>
    <p>vindicate</p>
    <p>modulation</p>
    <p>vindicates</p>
    <p>exonerate</p>
    <p>exculpate absolve</p>
    <p>harmonics</p>
    <p>modulations</p>
    <p>amplitude</p>
  </div>
  <div class="page">
    <p>C H A R - M O D E L : S U B W O R D R E P R E S E N T A T I O N</p>
    <p>!12</p>
    <p>~w = 1</p>
    <p>|NGw| + 1</p>
    <p>@~vw + X</p>
    <p>g2NGw</p>
    <p>~zg</p>
    <p>A</p>
    <p>FastText (P Bojanowski, 2017)  representation = average of n-gram</p>
    <p>vectors  automatic semantic extraction of</p>
    <p>stems/prefixes/suffices w = &lt;abnormal&gt;</p>
    <p>N-grams(w) 3 {hab, abn, . . . , habn, abnor, . . . , }</p>
    <p>~abnormal</p>
    <p>~zabnorm</p>
    <p>~znorm</p>
    <p>~zab</p>
    <p>~zabnor</p>
    <p>~z...</p>
    <p>~z...</p>
  </div>
  <div class="page">
    <p>C H A R - M O D E L : S U B W O R D R E P R E S E N T A T I O N</p>
    <p>!13</p>
    <p>~w = 1</p>
    <p>|NGw| + 1</p>
    <p>@~vw + X</p>
    <p>g2NGw</p>
    <p>~zg</p>
    <p>A</p>
    <p>FastText (P Bojanowski, 2017)  representation = average of n-gram</p>
    <p>vectors  automatic semantic extraction of</p>
    <p>stems/prefixes/suffices</p>
    <p>w = &lt;abnormal&gt;</p>
    <p>N-grams(w) 3 {hab, abn, . . . , habn, abnor, . . . , }</p>
    <p>~abnormal</p>
    <p>~zabnorm</p>
    <p>~znorm</p>
    <p>~zab</p>
    <p>~zabnor</p>
    <p>~z...</p>
    <p>~z... abnorm</p>
    <p>abnor'</p>
    <p>cosine similarity between vector and n-gram vectors</p>
    <p>~w  ~z</p>
  </div>
  <div class="page">
    <p>S U B W O R D C O N T R I B U T I O N T O O V E R A L L S E M A N T I C S</p>
    <p>!14</p>
    <p>abnormal</p>
    <p>abnormality</p>
    <p>cosine similarity between n-gram vectors and mean vectors</p>
    <p>Similar n-grams with high contribution</p>
    <p>Similar words have similar semantics</p>
  </div>
  <div class="page">
    <p>F A S T T E X T W I T H W O R D 2 G M</p>
    <p>Augment Gaussian mixture representation with character-structure (FastText)</p>
    <p>Promote independence: using dictionary-level vectors for other components</p>
    <p>!15</p>
    <p>rock</p>
    <p>rock</p>
    <p>~rock,0 = ~ (0) rock</p>
    <p>pop</p>
    <p>pop</p>
    <p>~pop,0 = ~ (0) pop</p>
    <p>(j) w,i =</p>
    <p>|NGw| + 1</p>
    <p>@~v(j)w + X</p>
    <p>g2NGw</p>
    <p>~z(j)g</p>
    <p>A</p>
    <p>~rock,1 = ~v (1) rock</p>
    <p>~pop,1 = ~v (1) pop</p>
  </div>
  <div class="page">
    <p>S I M I L A R I T Y S C O R E ( E N E R G Y ) B E T W E E N D I S T R I B U T I O N S</p>
    <p>!16</p>
    <p>vector space function space</p>
  </div>
  <div class="page">
    <p>E N E R G Y O F T W O G A U S S I A N M I X T U R E S</p>
    <p>!17</p>
    <p>rock:0 pop:0</p>
    <p>pop:1rock:1</p>
    <p>0,0</p>
    <p>1,1</p>
    <p>1,0</p>
    <p>bang, crack, snap</p>
    <p>basalt, boulder, sand jazz, punk, indie</p>
    <p>funk, pop-rock, band</p>
    <p>0,1</p>
    <p>closed form! total energy = weighted sum of pairwise partial energies</p>
    <p>i,j =</p>
  </div>
  <div class="page">
    <p>W O R D S A M P L I N G</p>
    <p>!18</p>
    <p>I like that rock band</p>
    <p>wi wi+1wi1wi2 wi+2</p>
    <p>Dataset: ukWac + WackyPedia (3.5 billion tokens)</p>
  </div>
  <div class="page">
    <p>L O S S F U N C T I O N</p>
    <p>!19</p>
    <p>Energy-based Max Margin</p>
    <p>rock band word: w</p>
    <p>context word: c</p>
    <p>rock dog</p>
    <p>negative context: c</p>
    <p>word: w</p>
    <p>high E(w,c)</p>
    <p>low E(w,c)</p>
    <p>Minimize the objective</p>
  </div>
  <div class="page">
    <p>Model parameters:</p>
    <p>dictionary vectors</p>
    <p>char n-gram vectors</p>
    <p>M U LT I M O D A L R E P R E S E N T A T I O N M I X T U R E O F G A U S S I A N S</p>
    <p>!20</p>
    <p>R O C K</p>
    <p>R O C K</p>
    <p>S T O N E</p>
    <p>S T O N E</p>
    <p>J A Z Z</p>
    <p>~w = 1</p>
    <p>|NGw| + 1</p>
    <p>@~vw + X</p>
    <p>g2NGw</p>
    <p>~zg</p>
    <p>A</p>
    <p>{{vwi } i=K i=1 }w</p>
    <p>{zg}</p>
    <p>Model hyperparameters:</p>
    <p>, m</p>
    <p>(covariance scale, margin)</p>
  </div>
  <div class="page">
    <p>T R A I N I N G - I L L U S T R A T I O N</p>
    <p>!21</p>
    <p>R O C K</p>
    <p>R O C K</p>
    <p>S T O N E</p>
    <p>J A Z Z</p>
    <p>S T O N E</p>
    <p>J A Z Z</p>
    <p>Mixture of Gaussians</p>
    <p>Train with max margin objective using minibatch SGD (AdaGrad)</p>
    <p>Model parameters:</p>
    <p>dictionary vectors</p>
    <p>char n-gram vectors</p>
    <p>{{vwi } i=K i=1 }w</p>
    <p>{zg}</p>
  </div>
  <div class="page">
    <p>T R A I N I N G - I L L U S T R A T I O N</p>
    <p>!22</p>
    <p>R O C K</p>
    <p>R O C K</p>
    <p>S T O N E</p>
    <p>J A Z Z</p>
    <p>S T O N E</p>
    <p>J A Z Z</p>
    <p>Mixture of Gaussians</p>
    <p>Train with max margin objective using minibatch SGD (AdaGrad)</p>
    <p>Model parameters:</p>
    <p>dictionary vectors</p>
    <p>char n-gram vectors</p>
    <p>{{vwi } i=K i=1 }w</p>
    <p>{zg}</p>
  </div>
  <div class="page">
    <p>T R A I N I N G - I L L U S T R A T I O N</p>
    <p>!23</p>
    <p>R O C K</p>
    <p>R O C K S T O N E</p>
    <p>J A Z Z</p>
    <p>S T O N E</p>
    <p>J A Z Z</p>
    <p>Mixture of Gaussians</p>
    <p>Train with max margin objective using minibatch SGD (AdaGrad)</p>
    <p>Model parameters:</p>
    <p>dictionary vectors</p>
    <p>char n-gram vectors</p>
    <p>{{vwi } i=K i=1 }w</p>
    <p>{zg}</p>
  </div>
  <div class="page">
    <p>E V A L U A T I O N</p>
    <p>!24</p>
  </div>
  <div class="page">
    <p>Q U A L I T A T I V E E V A L U A T I O N - N E A R E S T N E I G H B O R S</p>
    <p>!25</p>
    <p>rock</p>
    <p>basalt</p>
    <p>stone</p>
    <p>rock</p>
    <p>popjazz</p>
  </div>
  <div class="page">
    <p>N E A R E S T N E I G H B O R S</p>
    <p>!26</p>
    <p>Word Gaussian Mixture</p>
    <p>Component Nearest neighbors (cosine similarity)</p>
    <p>rock 0 rocks:0, rocky:0, mudrock:0, rockscape:0, boulders:0 , coutcrops:0</p>
    <p>rock 1 punk:0, punk-rock:0, indie:0, pop-rock:0, pop-punk:0, indie-rock:0, band:1</p>
    <p>bank 0 banks:0, banker:0, bankers:0, bankcard:0, Citibank:0, debits:0</p>
    <p>bank 1 banks:1, river:0, riverbank:0, embanking:0, banks:0, confluence:1</p>
    <p>star 0 stars:0, stellar:0, nebula:0, starspot:0, stars.:0, stellas:0, constellation:1</p>
    <p>star 1 stars:1, star-star:0, 5-stars:0, movie-star:0, mega-star:0, super-star:0</p>
    <p>PFT-GM</p>
    <p>Word Nearest neighbors (cosine similarity)</p>
    <p>rock rock-y, rockn, rock-, rock-funk, rock/, lava-rock, nu-rock, rock-pop, rock/ice, coral-rock</p>
    <p>bank bank-, bank/, bank-account, bank., banky, bank-to-bank, banking, Bank, bank/cash, banks.**</p>
    <p>star movie-stars, star-planet, G-star, star-dust, big-star, starsailor, 31-star, star-lit, Star, starsign</p>
    <p>FastText</p>
  </div>
  <div class="page">
    <p>Q U A N T I T A T I V E E V A L U A T I O N</p>
    <p>!27</p>
    <p>W O R D P A I R H U M A N S C O R E</p>
    <p>E M B E D D I N G S I M I L A R I T Y</p>
    <p>C U P C O F F E E 6 . 5 8 S ( C U P, C O F F E E ) = 0 . 7</p>
    <p>C U P S U B S T A N C E 1 . 9 2 S ( C U P, S U B S T A N C E ) = 0 . 2</p>
    <p>S T O C K M A R K E T 8 . 0 8 S ( S T O C K , M A R K E T ) = 0 . 9</p>
    <p>S T O C K P H O N E 1 . 6 2 S ( S T O C K , P H O N E ) = 0 . 0 5</p>
    <p>K I N G Q U E E N 8 . 5 8 S ( K I N G , Q U E E N ) = 0 . 8</p>
    <p>K I N G C A B B A G E 0 . 2 3 S ( K I N G , C A B B A G E ) = 0 . 2</p>
    <p>C U P</p>
    <p>C O F F E E</p>
    <p>C U PC O F F E E</p>
    <p>s(cup, coffee) = similarity between cup and coffee</p>
    <p>} Spearman correlation coefficient</p>
  </div>
  <div class="page">
    <p>S I M I L A R I T Y M E T R I C</p>
    <p>!28</p>
    <p>Expected Likelihood</p>
    <p>Pairwise Maximum Cosine Similarity</p>
    <p>s(rock, stone)</p>
    <p>rock</p>
    <p>rock</p>
    <p>stone</p>
    <p>stone</p>
    <p>R O C K</p>
    <p>S T O N E</p>
    <p>R O C KS T O N E</p>
    <p>max i,j</p>
    <p>h~rock,i, ~stone,ji Z</p>
    <p>frock(x)gstone(x)dx</p>
  </div>
  <div class="page">
    <p>S P E A R M A N C O R R E L A T I O N S</p>
    <p>!29</p>
    <p>W O R D S I M D A T A S E T S F A S T T E X T W 2 G M P F T- G M</p>
    <p>S L - 9 9 9 3 8 . 0 3 3 9 . 6 2 3 9 . 6 0</p>
    <p>W S - 3 5 3 7 8 . 8 8 7 9 . 3 8 7 6 . 1 1</p>
    <p>M E N - 3 K 7 6 . 3 7 7 8 . 7 6 7 9 . 6 5</p>
    <p>M C - 3 0 8 1 . 2 0 8 4 . 5 8 8 0 . 9 3</p>
    <p>R G - 6 5 7 9 . 9 8 8 0 . 9 5 7 9 . 8 1</p>
    <p>Y P - 1 3 0 5 3 . 3 3 4 7 . 1 2 5 4 . 9 3</p>
    <p>M T- 2 8 7 6 7 . 9 3 6 9 . 6 5 6 9 . 4 4</p>
    <p>M T- 7 7 1 6 6 . 8 9 7 0 . 3 6 6 9 . 6 8</p>
    <p>R W - 2 K ( R A R E W O R D ) 4 8 . 0 9 4 2 . 7 3 4 9 . 3 6</p>
    <p>A V G . 4 9 . 2 8 4 9 . 5 4 5 1 . 1 0</p>
    <p>- PFT performs much better on RareWord dataset compared to w2gm, even slightly better than FastText</p>
    <p>- Based on the average spearman correlation, PFT-GM performs the best.</p>
    <p>- First multi-sense models that achieve high scores on RareWord</p>
  </div>
  <div class="page">
    <p>C O M P A R I S O N W I T H O T H E R M U LT I P R O T O T Y P E E M B E D D I N G S</p>
    <p>!30</p>
    <p>- PFT performs better than other multiprototype embeddings on SCWS, a benchmark for word similarity with multiple meanings.</p>
  </div>
  <div class="page">
    <p>F O R E I G N L A N G U A G E E M B E D D I N G S</p>
    <p>!31</p>
  </div>
  <div class="page">
    <p>F U T U R E W O R K : M U LT I - L I N G U A L E M B E D D I N G S</p>
    <p>!32</p>
    <p>Literature: align embeddings of many languages after training (Conneau, 2018)</p>
    <p>Use disentangled embeddings to disambiguate alignment</p>
  </div>
  <div class="page">
    <p>C O N C L U S I O N</p>
    <p>Elegant representation of semantics using multimodal distributions</p>
    <p>Suitable modeling words with multiple meanings</p>
    <p>Model words as character levels</p>
    <p>Better semantics for rare words</p>
    <p>Able to estimate semantics of unseen words</p>
    <p>!33</p>
  </div>
</Presentation>
