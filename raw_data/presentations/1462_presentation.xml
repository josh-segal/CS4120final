<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BRAVO: Ballot-polling Risk-limiting Audits to Verify Outcomes</p>
    <p>Mark Lindeman, Philip B. Stark, and Vincent S. Yates</p>
    <p>EVT/WOTE 2012 Bellevue, WA</p>
  </div>
  <div class="page">
    <p>Evidence-Based Elections: Compliance Audits + Materiality Audits</p>
    <p>Effective compliance audit</p>
    <p>Determine whether the audit trail is trustworthy enough to determine who won.</p>
    <p>Was the system, as deployedincluding curation of the audit trail strongly software independent?</p>
    <p>If not, do not declare an outcome (nb: danger of DOS attacks).</p>
    <p>Effective materiality audit</p>
    <p>If the outcome is wrong, correct it (with high probability).</p>
    <p>Requires intact audit trailneed to pass compliance audit first. Might require counting the entire audit trail by hand.</p>
  </div>
  <div class="page">
    <p>Risk-Limiting Audits for Materiality</p>
    <p>Historically, much debate over how large a sample to start with. Detection paradigm.</p>
    <p>If we want audits to correct wrong outcomes, crucial question is when to stop auditing.</p>
    <p>Answer: If theres compelling evidence that outcome is right, stop; else, keep auditing.</p>
  </div>
  <div class="page">
    <p>What is Compelling Quantitative Evidence?</p>
    <p>What is the biggest chance thatif the outcome is wrongthe audit would observe what it did observe (votes, errors, . . . )?</p>
    <p>If chance is small, implausible that the outcome is wrong; else, keep auditing.</p>
    <p>Eventually, either have strong evidence that the outcome is right, or the whole contest has been counted by hand and correct outcome is known.</p>
    <p>Can guarantee a large probability of correcting the outcome if the outcome is wrong, no matter why its wrong (if the audit trail is good enough)</p>
    <p>C.f., Rivest &amp; Shen: Bayesian audits</p>
  </div>
  <div class="page">
    <p>What is Risk in a Risk-Limiting Audit? The risk is the largest possible chance that a wrong outcome will not be corrected by the audit.</p>
    <p>The risk-calculation assumes the outcome is wrong in the way thats hardest to detect, as if a smart, malicious opponent were trying to commit fraud and not get caught .</p>
    <p>Chance of not correcting a wrong outcome is typically much smaller than the risk, e.g., if machine malfunction, configuration error, or voter error is at fault.</p>
    <p>The risk is not the chance that the outcome, after auditing, is wrong.</p>
    <p>For instance, if 99% of outcomes are right in the first place and we audit with a risk limit of 10%, after auditing, on average more than 99.9% of outcomes will be correct: On average, audit corrects at least 90% of the 1% that are wrong.</p>
  </div>
  <div class="page">
    <p>Role of Statistics</p>
    <p>Limiting the risk is easy</p>
    <p>No statistics needed: just count all the ballots by hand.</p>
    <p>Statistics lets you do less counting when the outcome is right, but still ensure a big chance of a full hand count when outcome is wrong.</p>
  </div>
  <div class="page">
    <p>Risk-Limiting Audits</p>
    <p>16 pilot audits in CA, CO, and OH; another 14 planned (OC next, 8/20)</p>
    <p>EAC funding for pilots in CA and CO and Cuyahoga County, OH  CO has law; CA has pilot law  simple measures  measures requiring super-majority  multi-candidate contests  vote-for-n contests,  multiple contests audited simultaneously with one sample  contest sizes: 200 ballots to 121,000 ballots  counting burden: 16 ballots to 7,000 ballots  cost per audited ballot: nil to about $0.55.</p>
  </div>
  <div class="page">
    <p>Ballot-polling audits and Comparison Audits</p>
    <p>Comparison audit: 1. LEO commits to vote data at some level of aggregation. 2. Audit checks that the committed data produces the same results</p>
    <p>as claimed. Should be perfect. 3. Audit samples and checks the committed data until there is</p>
    <p>strong evidence that the data are accurate enough to produce the right election outcome (or until the true outcome is known).</p>
    <p>Ballot polling audit: Sample/examine ballots until there is strong evidence that looking at the rest would confirm the outcome (or until the true outcome is known).</p>
  </div>
  <div class="page">
    <p>Tradeoffs  Comparison audit</p>
    <p>Heavy demands on voting system for reporting and export  Requires LEO to commit to auditable subtotals  Requires ability to retrieve ballots that correspond to CVRs or subtotals  May compromise voter privacy (small-batch or ballot-level reporting) &amp;</p>
    <p>enable coercion through pattern voting  Most efficient (ballot-level) may require re-scanning all ballots  Checks tabulation (but not for transitive audits [Calandrino, Halderman,</p>
    <p>&amp; Felten] unless subtotals are cross-checked)  Ballot-level comparison audits require least hand counting</p>
    <p>Ballot polling audit  Requires more counting than ballot-level comparison audit  Does not check tabulation: Outcome could be right b/c errors cancel  Virtually no set-up costs  Requires nothing of voting system  Generally, need a ballot manifest to draw sample  Preserves voter anonymity except possibly for sampled ballots  Counting burden comparable to precinct-based comparison audit,</p>
    <p>unless margin is very small</p>
  </div>
  <div class="page">
    <p>Counting errors versus counting votes</p>
    <p>Johnson (2004): statistical recount versus statistical error count. Like two-sample t -test versus paired t -test.</p>
    <p>If constrained to examine batches of a given size, much more efficient statistically (in counting effort) to count errors in those batches than to count votes in those batches.</p>
    <p>But if:</p>
    <p>you can only examine precinct-level batches for error  exporting precinct-level data is hard/complex/time-consuming  you can examine individual ballots to count votes</p>
    <p>then counting votes can be much more efficient overall.</p>
  </div>
  <div class="page">
    <p>Getting CVRs for Individual Ballots is Hard!</p>
    <p>Federally certified voting systems do not provide CVRs.  Even getting precinct-level data from todays voting systems into</p>
    <p>a usable form can take hours of hand editing . . . and then the batch size is too large for efficient audits.</p>
    <p>Todays talk by Kai Wangthe Wang/Wagner et al. software is great, but need LEOs to re-scan ballots, need to program ballot definitions, etc. Serious obstacles to ballot-level comparison audits.</p>
    <p>Need ballot manifests for any kind of risk-limiting auditcomparison or ballot-polling.</p>
  </div>
  <div class="page">
    <p>Ballot-Polling Audit: Intuition</p>
    <p>Like opinion poll or exit poll, but sample until observed winners percentage (i.e., sample percentage), discounted by margin of error, is above 50% (for 2-candidate contest).</p>
    <p>If winners true percentage of valid votes is more than 50%, she won.  If the true margin is in fact small, confirming outcome might require</p>
    <p>looking at a lot of ballots; if its big, dont expect to need to see many randomly selected ballots to have strong evidence that the winner got more than 50%.</p>
    <p>E.g., chance the first 4 ballots selected all would show votes for the reported winner if the reported winner didnt get more than 50% of the vote is 6.25% (less than 10%).</p>
    <p>If the true margin is in fact negative (i.e., if the reported winner really lost), very unlikely that sample percentage, discounted by margin of error, will be over 50%.</p>
  </div>
  <div class="page">
    <p>Wald ballot-polling audit, 2 candidates, risk limit  1. Pick D, maximum draws before full hand count. Set T = 1, d = 0.</p>
    <p>s is winners share of the valid votes according to the vote tabulation system.</p>
    <p>step 2.</p>
    <p>Theorem: limits risk to .</p>
  </div>
  <div class="page">
    <p>Ballot-polling audit Monterey Peninsula Water District 1</p>
    <p>Conducted in Monterey County in May, 2011, before certification  10% risk limit  Expected number of ballots to examine: 58  Actual: 92 draws (89 distinct ballots)  Monterey County staff Bates stamped every ballot  Thanks to RoV Linda Tulett &amp; staff!</p>
  </div>
  <div class="page">
    <p>Monterey County 2011</p>
  </div>
  <div class="page">
    <p>Monterey County 2011</p>
  </div>
  <div class="page">
    <p>Monterey County 2011</p>
  </div>
  <div class="page">
    <p>Monterey County 2011</p>
  </div>
  <div class="page">
    <p>Monterey County 2011</p>
  </div>
  <div class="page">
    <p>Expected sample size to confirm Obama won Vote share 61.1%:</p>
    <p>100 ballots from whole state   25 from LA County   75 total from largest 12 counties (including LA)   1 total from the smallest 14 counties.</p>
    <p>If Obamas share had been 52%:</p>
    <p>2,900 from whole state ( 0.02% of ballots)   725 from LA county   2175 total from largest 12 counties (including LA)   29 total from smallest 14 counties</p>
  </div>
  <div class="page">
    <p>Expected Workload: Two Candidates</p>
    <p>Winners Quantiles True Share 25th 50th 75th 90th 99th Mean</p>
    <p>Means and percentiles of #ballots with valid votes to inspect for 10% risk limit. Estimated using 107 replications.</p>
  </div>
  <div class="page">
    <p>Ballot-Polling Auditk -winner contest: Intuition</p>
    <p>Again, like opinion poll or exit poll, but sample until for every (winner, loser) pair , observed winners fraction of votes on ballots that report a vote for either or both, discounted by margin of error, is above 50%.</p>
    <p>That is, sample until theres strong statistical evidence that every reported winner actually got more votes than every reported loser.</p>
    <p>If any (winner, loser) margin is in fact small, might require looking at a lot of ballots.</p>
  </div>
  <div class="page">
    <p>General BRAVO: C-candidate, k -winner contest</p>
    <p>Test that every winner w W beat every loser ` L. k(C  k) null hypotheses: loser ` beat winner w .</p>
    <p>Test w/ same sample, but one test statistic per pair: {Tw`}.</p>
    <p>Define sw`  sw/(sw + s`), fraction of votes w was reported to have received among ballots reported to show a vote for w or ` or both.</p>
    <p>Can be calculated from standard reported election results.</p>
    <p>Define w` to be actual fraction of votes w received among ballots that show a vote for exactly one of {w,`}.</p>
    <p>Assertion and Sufficient Condition</p>
    <p>w W,` L:  If w reportedly beat `, sw` &gt; 50%.  If w actually beat `, w` &gt; 50%.</p>
  </div>
  <div class="page">
    <p>BRAVO for C-candidate k -winner contest</p>
    <p>d  d + 1. 3. If the ballot shows a valid vote for a reported winner w , then for each ` in L</p>
    <p>that did not receive a valid vote on that ballot, multiply Tw` by 2sw`. Repeat for all such w .</p>
    <p>Theorem: Limits risk to at most .</p>
  </div>
  <div class="page">
    <p>Steampunk audit</p>
    <p>Equipment needed: dice, pencil and paper (or a sliderule).</p>
    <p>Calculations very transparent (even if underlying theorems are hard).</p>
    <p>Process very observable: What votes does this ballot show?</p>
  </div>
  <div class="page">
    <p>Multiplicity in pairwise testing for k -winner contest</p>
    <p>Stopping short of a full hand count is an error only if at least one of the null hypotheses is in fact true.</p>
    <p>BRAVO stops short of full hand count only if all k(C  k) null hypotheses are rejected.</p>
    <p>Consider the set of null hypotheses that are true. Chance of erroneously rejecting all of those is at most the smallest chance of erroneously rejecting any individually.</p>
    <p>Hence, by testing every (winner, loser) pair individually at level , the chance of stopping short of a full hand count if any of the C  k apparent losers actually won is at most .</p>
    <p>Moreover, works simultaneously for any number of contests, using the same sample.</p>
  </div>
  <div class="page">
    <p>Grouping losers</p>
    <p>Could combine subsets of winners or of losers to reduce the number of tests.</p>
    <p>E.g., winner has 60%, losers have 25% and 15%. Combine losers into a single fictitious losing candidate with 40%.</p>
    <p>Theorem: grouping does not reduce expected sample size.</p>
  </div>
  <div class="page">
    <p>Workload at 10% Risk Limit</p>
    <p>(On the assumption that the outcomes were right.)</p>
  </div>
  <div class="page">
    <p>Selecting ballots at random For transparency, want initial mechanical source of randomness (Cordero, Wagner, &amp; Dill).</p>
    <p>Dice courtesy of Ron Rivest.</p>
  </div>
  <div class="page">
    <p>Use as Seed in Good PRNG SHA-256 of seed catenated with sample number (Rivest)</p>
  </div>
  <div class="page">
    <p>Ballot Manifest</p>
  </div>
  <div class="page">
    <p>Look-up</p>
  </div>
  <div class="page">
    <p>Gotchya!</p>
    <p>Better ballot accounting</p>
    <p>Ballot manifests are not a solved problem.</p>
    <p>Its easy to deal with errors in ballot manifest if theres an upper bound on the number of ballots in each container (Banuelos &amp; Stark).</p>
    <p>But sometimes there isnt a good upper boundesp. with multipage ballots.</p>
  </div>
  <div class="page">
    <p>GOTA: Get out the Audit! Ballot-polling audits are possible for the November 2012 presidential election in any jurisdiction that has VVPRsand has knows how many and where they are.</p>
    <p>Workload not large in most states; preparations minimal. Equipment needed: dice, pencil, and paper. (Alternatively, dice and simple web-based tools.)</p>
    <p>Compliance audit needs attentionensure audit trail adequately accurate. Coordination across jurisdictions needs attentionlogistics and transparency.</p>
    <p>Verified Voting is working to get ballot-polling audits in several states for November 2012 presidential election.</p>
    <p>Lets Get out the Audit!</p>
  </div>
</Presentation>
