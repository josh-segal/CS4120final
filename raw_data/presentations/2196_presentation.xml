<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Tao Lu*, Eric Suchyta, Jong Choi, Norbert Podhorszki, and Scott Klasky, Qing Liu*, Dave Pugmire and Matt Wolf, and Mark Ainsworth+</p>
    <p>* New Jersey Institute of Technology Oak Ridge National Laboratory + Brown University</p>
    <p>Canopus: Enabling Extreme-Scale Data Analytics on Big HPC Storage via Progressive Refactoring</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Background and related work</p>
    <p>Progressive data refactoring</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>HPC systems  Mission: illuminate phenomena that are often impossible to study in a laboratory [Oak Ridge National Lab, 2013]  Climate impacts of energy use  Fusion in a reactor not yet built  Galaxy formation</p>
    <p>Methodology: Modeling and simulation, along with data exploration  Data generation  Storage  Analysis  Visualization 3</p>
  </div>
  <div class="page">
    <p>HPC systems (contd)</p>
    <p>The big data management challenge [Shalf et. al., 2014]  Worsening I/O bottleneck for exascale systems  Exponentially increasing multi-scale, multi-physics data</p>
  </div>
  <div class="page">
    <p>Data compression in exascale (nextgeneration HPC) systems</p>
    <p>Goal: 10x to 100x data reduction ratio [Ian Foster et. al., 2017]  Reduce data by at least 90%</p>
    <p>Data features  Temporal and spatial  High-dimensional  Floating-point</p>
  </div>
  <div class="page">
    <p>Data compressors</p>
    <p>Lossless compression  Deduplication  GZIP  FPC [Burtscher et. al., 2009]</p>
    <p>Lossy compression  ZFP [Lindstrom et. al., 2014]  ISABELA [Lakshminarasimhan et.al., 2011]  SZ [Shen et. al., 2016]</p>
  </div>
  <div class="page">
    <p>Lossy compression</p>
    <p>Original Data</p>
    <p>Compressed data2. Data</p>
    <p>transformation 3. Curve fitting</p>
    <p>Floating-point Block:</p>
    <p>Mantissa and exponent :</p>
    <p>integer</p>
    <p>Integers: 848960939002912256</p>
    <p>-3157387122695243776 -2086581739634989568 -558511819984848000</p>
    <p>Compressed low bits</p>
    <p>Compressed high bits</p>
    <p>Workflow of curve fitting based compression (e.g. ISABELA and SZ)</p>
    <p>Workflow of quantization and transformation based compression (e.g. ZFP)</p>
  </div>
  <div class="page">
    <p>Can floating-point compressors achieve a near 100x compression ratio?</p>
  </div>
  <div class="page">
    <p>Performance of compressors</p>
    <p>Co m pr es si on r at io</p>
    <p>Data set</p>
    <p>fpc gzip isb zfp sz</p>
    <p>Relative error bound 0.000001</p>
    <p>Co m pr es si on r at io</p>
    <p>Data set</p>
    <p>fpc gzip isb zfp sz</p>
    <p>Relative error bound 0.001</p>
  </div>
  <div class="page">
    <p>Can floating-point compressors achieve a near 100x compression ratio?</p>
    <p>Yes. If Dataset contains a lot of identical values;</p>
    <p>Or, data values highly skew with moderate compression error bounds.</p>
    <p>No. For most datasets.</p>
    <p>Can floating-point compressors achieve a 10x compression ratio?</p>
    <p>Yes. For a lot of datasets with moderate compression error bounds.</p>
  </div>
  <div class="page">
    <p>What if the compression ratio is rushed to 100x by loosening error bounds?</p>
  </div>
  <div class="page">
    <p>Visualization and blob detection on compressed Dpot data.</p>
  </div>
  <div class="page">
    <p>Limitations of data compression by reducing floating-point accuracy</p>
    <p>Near 100x compression ratio is hardly achievable</p>
    <p>Lost data accuracy cannot be restored</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Background and related work</p>
    <p>Progressive data refactoring</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>We propose Canopus</p>
    <p>Compressing HPC data in another dimension (resolution)</p>
    <p>Enabling progressive data refactoring</p>
    <p>User transparent implementation</p>
  </div>
  <div class="page">
    <p>Canopus I/O configuration</p>
  </div>
  <div class="page">
    <p>Canopus: basic idea</p>
    <p>Refactor the simulation results (via decimation) into a base dataset along with a series of deltas  Base dataset is saved in fast devices, deltas in slow devices  Base dataset can be used separately (at a lower resolution) for analysis  Selected subset of deltas to be retrieved to restore data to a target accuracy</p>
    <p>Simulation</p>
    <p>ADIOS Write API</p>
    <p>Canopus (I/O, refactoring, compression, placement, retrieval, restoration )</p>
    <p>I/O Transport</p>
    <p>MPI MPI_LUSTREPOSIX Dataspaces</p>
    <p>Data Analytics</p>
    <p>ADIOS Query API</p>
    <p>MPI_AGGREGATE FLEXPATH</p>
    <p>Node-local Storage (NVRAM, SSDs) Burst Buffer</p>
    <p>ADIOS Kernel (buffering, metadata, scheduling, etc.)</p>
    <p>Remote Parallel File System Campaign Storage</p>
    <p>Storage Tiers</p>
    <p>Canopus in HPC Systems</p>
  </div>
  <div class="page">
    <p>Canopus workflow</p>
    <p>HPC Simulations (high accuracy)</p>
    <p>Base ST1</p>
    <p>Delta2x ST2</p>
    <p>Deltafull ST3 Analytics Pipeline n (high accuracy)</p>
    <p>Analytics Pipeline 2 (medium accuracy)</p>
    <p>Analytics Pipeline 1 (low accuracy)</p>
    <p>Storage Hierarchy</p>
    <p>base = L4x</p>
    <p>base + delta2x</p>
    <p>base + delta2x + deltafull</p>
    <p>Refactoring (decimation, compression)</p>
    <p>Retrieving &amp; Reconstruction</p>
  </div>
  <div class="page">
    <p>Data refactoring</p>
    <p>Mesh (Full) Mesh (4x reduction)</p>
    <p>Full L4x deltafull</p>
    <p>delta2x</p>
    <p>Delta calculationDecimationOriginal</p>
  </div>
  <div class="page">
    <p>Mesh decimation</p>
    <p>Vl+1i =  (Vli + Vlj)</p>
    <p>Vli</p>
    <p>Vlj</p>
    <p>Vlk Vlh</p>
    <p>Vl+1k Vl+1h</p>
    <p>Vl+1i</p>
    <p>Delta Calculation  For mesh data, its common that each vertex corresponds to a value (floating-point)  After triangular mesh decimation:</p>
    <p>deltaln = F(Vln) - 1/3*F(Vl+1i) 1/3*F(Vl+1j) - 1/3*F(Vl+1k)</p>
  </div>
  <div class="page">
    <p>Compression</p>
    <p>The floating-point values corresponding to vertexes are compressed using ZFP compressor</p>
    <p>A potential optimization to our framework is supporting adaptive compressors based on dataset features</p>
  </div>
  <div class="page">
    <p>Progressive data exploration (reverse the data refactoring procedures)  I/O (read the base dataset and deltas) Decompression Restoration</p>
  </div>
  <div class="page">
    <p>Performance gain of Canopus for data analytics</p>
  </div>
  <div class="page">
    <p>Impact on Data Analytics</p>
    <p>Original 2x reduction 4x reduction</p>
  </div>
  <div class="page">
    <p>A quantitative evaluation of blob detection</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Storage stacks of HPC systems</p>
    <p>Progressive data refactoring</p>
    <p>Conclusion and future work</p>
  </div>
  <div class="page">
    <p>Conclusion Lossy compression may devastate the usefulness of data to achieve high compression ratio (such as 100x)</p>
    <p>It is critical to compress data in multiple orthogonal dimensions such as accuracy and resolution</p>
    <p>Canopus combines mesh compression and floatingpoint compression, possibly delivering a high compression ratio without devastate the usefulness of data</p>
  </div>
  <div class="page">
    <p>Future work  Investigate the impact of lossy compression on analytical applications other than visualization  Original data A == B, compressed data A == B ?</p>
    <p>F(D) == F(D)? F is a function</p>
  </div>
  <div class="page">
    <p>References  Oak Ridge National Lab, Solving Big Problems: Science and Technology at Oak Ridge National Laboratory, 2013</p>
    <p>Foster, I., Ainsworth, M., Allen, B., Bessac, J., Cappello, F., Choi, J. Y.,  Yoo, S. (2017). Computing Just What You Need : Online Data Analysis and Reduction at Extreme Scales, 116.</p>
    <p>Shalf, J., Dosanjh, S., &amp; Morrison, J. (2014). Top ten exascaleresearch challenges, 125. Retrieved from http://link.springer.com/chapter/10.1007/978-3-642-19328-6_1</p>
    <p>Burtscher, M., &amp; Ratanaworabhan, P. (2009). FPC: A high-speed compressor for doubleprecision floating-point data. IEEE Transactions on Computers, 58(1), 1831.</p>
    <p>Lindstrom, P. (2014). Fixed-rate compressed floating-point arrays. IEEE Transactions on Visualization and Computer Graphics, 20(12), 26742683.</p>
    <p>Lakshminarasimhan, S., Shah, N., Ethier, S., Klasky, S., Latham, R., Ross, R., &amp; Samatova, N. F. (n.d.). Compressing the Incompressible with ISABELA : In-situ Reduction of SpatioTemporal Data, 114.</p>
  </div>
  <div class="page">
    <p>Thanks &amp; Questions</p>
  </div>
</Presentation>
