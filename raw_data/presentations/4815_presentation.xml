<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Effectiveness of Lloydtype Methods for the k</p>
    <p>means Problem Chaitanya Swamy</p>
    <p>University of Waterloo</p>
    <p>Joint work with Rafi Ostrovsky, Yuval Rabani, Leonard</p>
    <p>Schulman UCLA Technion Caltech</p>
  </div>
  <div class="page">
    <p>The k-means Problem</p>
    <p>Given: n points in d-dimensional space</p>
    <p>partition X into k clusters X1,, Xk assign each point in Xi to a common</p>
    <p>center ciRd</p>
    <p>Goal: Minimize i xXi d(x,ci)2 d: L2 distance</p>
    <p>X  Rd : point set with |X| = n</p>
    <p>c1</p>
    <p>c3 c2</p>
    <p>X1 X2 X3</p>
  </div>
  <div class="page">
    <p>k-means (contd.)</p>
    <p>Given the cis, best clustering is to assign each point to nearest center: Xi = {xX: ci is ctr. nearest to x}</p>
    <p>Given the Xis, best choice of centers is to set ci = center of mass of Xi = ctr(Xi) = xXi x / |Xi|</p>
    <p>Optimal solution satisfies both properties</p>
    <p>Problem is NP-hard even for k=2 (n, d not fixed)</p>
  </div>
  <div class="page">
    <p>Related Work k-means problem dates back to Steinhaus (1956). a) Approximation algorithms  algorithms with provable guarantees</p>
    <p>PTASs with varying runtime dependence on n, d, k: poly/linear in n, could be exponential in d and/or k</p>
    <p>Matousek (poly(n), exp(d,k))  Kumar, Sabharwal &amp; Sen (KSS04) (lin(n,d),</p>
    <p>exp(k))</p>
    <p>O(1)-approximation algorithms for kmedian: any point set with any metric, runtime poly(n,d,k); guarantees also translate to k-means</p>
    <p>Charikar, Guha, Tardos &amp; Shmoys  Arya et al. + Kanungo et al.: (9+)</p>
    <p>approximation</p>
  </div>
  <div class="page">
    <p>b) Heuristics: Lloyds method invented in 1957 and remains an extremely popular heuristic even today 1) Start with k initial / seed centers c1,, ck.</p>
    <p>to obtain clustering X1,, Xk.</p>
    <p>b)Update ci  ctr(Xi) = xXi x/|Xi| .</p>
  </div>
  <div class="page">
    <p>to obtain clustering X1,, Xk.</p>
    <p>b)Update ci  ctr(Xi) = xXi x/|Xi| .</p>
    <p>b) Heuristics: Lloyds method invented in 1957 and remains an extremely popular heuristic even today</p>
  </div>
  <div class="page">
    <p>to obtain clustering X1,, Xk.</p>
    <p>b)Update ci  ctr(Xi) = xXi x/|Xi| .</p>
    <p>b) Heuristics: Lloyds method invented in 1957 and remains an extremely popular heuristic even today</p>
  </div>
  <div class="page">
    <p>Some bounds on number of iterations of Lloydtype methods: Inaba-Katoh-Imai; Har-Peled-Sadri; Arthur-Vassilvitskii (06)</p>
    <p>Performance very sensitive to choice of seed centers; lot of literature on finding good seeding methods for Lloyd</p>
    <p>But, almost no analysis that proves performance guarantees about quality of final solution for arbitrary k and dimension</p>
    <p>Lloyds method: Whats known?</p>
    <p>Our Goal: to analyze Lloyd and try to prove rigorous performance guarantees for Lloydtype methods</p>
  </div>
  <div class="page">
    <p>Our Results</p>
    <p>Introduce a clusterability or separation condition.  Give a novel, efficient sampling process for seeding</p>
    <p>Lloyds method with initial centers.</p>
    <p>Show that if data satisfies our clusterabililty condition:  seeding + 1 Lloyd step yields a constant-approximation</p>
    <p>in time linear in n and d, poly(k): is potentially faster than Lloyd variants which require multiple reseedings</p>
    <p>seeding + KSS04-sampling gives a PTAS: algorithm is faster and simpler than PTAS in KSS04.</p>
    <p>Main Theorem: If data has a meaningful kclustering, then there is a simple, efficient seeding method s.t. Lloyd-type methods return a near-optimal solution.</p>
  </div>
  <div class="page">
    <p>Meaningful k-Clustering Settings where one would NOT consider data to possess a meaningful k-clustering:</p>
  </div>
  <div class="page">
    <p>We formalize 2).</p>
    <p>Let k 2(X) = cost of best k-clustering of X.</p>
    <p>X is -seperated for k-means iff k 2(X) / k</p>
    <p>Simple condition. Drop in k-clustering cost is already used by practitioners to choose the right k</p>
    <p>Can show that (roughly), X is separated for k-means</p>
    <p>two low-cost kclusterings disagree on only a small fraction of data</p>
  </div>
  <div class="page">
    <p>Some basic facts Fact: pRd, xX d(x, p)</p>
    <p>n.d(p,c)2</p>
    <p>{x,y}X d(x,y) 2 = n.1</p>
    <p>[Write d(x, p)2 = (x-c + c-p)T(x-c + c-p) and expand.]</p>
    <p>Lemma: Let X = X1X2 be a partition of X with ci = ctr(Xi). Then 1</p>
    <p>n).d(c1,c2) 2.</p>
    <p>Proof: 1 2(X)= xX1 d(x,c)</p>
    <p>= (12(X1) + |X1|.d(c1,c)2) + (12(X2) + |</p>
    <p>X2|.d(c2,c) 2)</p>
    <p>= 1 2(X1) + 1</p>
    <p>n = |X| c = ctr(X)</p>
    <p>X2</p>
    <p>c2c</p>
    <p>X1</p>
    <p>c1 c = (|X1|.c1+|</p>
    <p>X2|.c2) / n</p>
  </div>
  <div class="page">
    <p>r*2 r*1</p>
    <p>The 2-means problem (k=2)</p>
    <p>X*1, X * 2 : optimal clusters</p>
    <p>c*i= ctr(X * i), D</p>
    <p>* = d(c*1,c * 2)</p>
    <p>ni = |X * i|, (r</p>
    <p>* i)</p>
    <p>= avg. squared distance in cluster X*i</p>
    <p>Lemma: For i=1, 2, (r*i) 2  2/(1-2).D*2 .</p>
    <p>Proof: 2 2(X) /</p>
    <p>n).D*2 .</p>
    <p>X is -separated for 2means.</p>
    <p>c*1 c*2D*</p>
    <p>X*1 X * 2</p>
  </div>
  <div class="page">
    <p>The 2-means algorithm</p>
    <p>picking the pair x, yX with probability d(x,y)2.</p>
    <p>d(c1,c2)/3}.</p>
    <p>Update ci  ctr(Bi); return these as final centers.</p>
    <p>Sampling can be implemented in O(nd) time, so entire algorithm runs in O(nd) time.</p>
    <p>c*1 c*2D*</p>
    <p>X*1 X * 2</p>
  </div>
  <div class="page">
    <p>c1</p>
    <p>c*1 c*2D*</p>
    <p>X*1 X * 2</p>
    <p>core(X*1 )</p>
    <p>core(X*2 )</p>
    <p>c2</p>
    <p>Let core(X*i) = {xX * i : d(x,c)</p>
    <p>* i)</p>
    <p>(2) &lt; 1.</p>
    <p>Seeding lemma: With prob. 1O(), c1,c2 lie in cores of X*1, X</p>
    <p>* 2.</p>
    <p>Proof: |core(X*i)|  (1-)ni for i=1,2.</p>
    <p>Let A = xcore(X*1), ycore(X*2) d(x,y) 2</p>
    <p>(1-) 2n1n2D</p>
    <p>*2.</p>
    <p>B= {x,y}X d(x,y) 2</p>
    <p>= n.1 2(X)  n1n2D</p>
    <p>*2.</p>
    <p>Probability = A / B  (1-) 2</p>
    <p>= 1 O().</p>
  </div>
  <div class="page">
    <p>c1 c*1 c*2D*</p>
    <p>X*1 X * 2</p>
    <p>core(X*1 )</p>
    <p>core(X*2 )</p>
    <p>c2</p>
    <p>Recall that Bi = {xX: d(x,ci)  d(c1,c2)/3}</p>
    <p>Ball-k-means lemma: For i=1,2, core(X*i) Bi X</p>
    <p>* i .</p>
    <p>Therefore d(ctr(Bi), c * i)</p>
    <p>Intuitively, since Bi  X * i and Bi contains</p>
    <p>almost all of the mass of X*i, ctr(Bi) must be close to ctr(X*i) = c</p>
    <p>* i.</p>
    <p>B1 B2</p>
  </div>
  <div class="page">
    <p>c1 c*1 c*2D*</p>
    <p>X*1 X * 2</p>
    <p>core(X*1 )</p>
    <p>core(X*2 )</p>
    <p>c2</p>
    <p>Recall that Bi = {xX: d(x,ci)  d(c1,c2)/3}</p>
    <p>Ball-k-means lemma: For i=1,2, core(X*i) Bi X*i . Therefore d(ctr(Bi), c</p>
    <p>* i)</p>
    <p>Proof: 1 2(X*i)  (|Bi| |X</p>
    <p>* i \ Bi| / ni).d(ctr(Bi),</p>
    <p>ctr(X*i \ Bi)) 2</p>
    <p>B1 B2</p>
  </div>
  <div class="page">
    <p>Some basic facts Fact: pRd, xX d(x, p)</p>
    <p>n.d(p,c)2</p>
    <p>{x,y}X d(x,y) 2 = n.1</p>
    <p>[Write d(x, p)2 = (x-c + c-p)T(x-c + c-p) and expand.]</p>
    <p>Lemma: Let X = X1X2 be a partition of X, ci = ctr(Xi).</p>
    <p>Then 1 2(X) = 1</p>
    <p>n).d(c1,c2) 2.</p>
    <p>n = |X| c = ctr(X)</p>
    <p>X2</p>
    <p>c2c</p>
    <p>X1</p>
    <p>c1 c = (|X1|.c1+|</p>
    <p>X2|.c2) / n</p>
  </div>
  <div class="page">
    <p>c1 c*1 c*2D*</p>
    <p>X*1 X * 2</p>
    <p>core(X*1 )</p>
    <p>core(X*2 )</p>
    <p>c2</p>
    <p>Recall that Bi = {xX: d(x,ci)  d(c1,c2)/3}</p>
    <p>Ball-k-means lemma: For i=1,2, core(X*i) Bi X*i . Therefore d(ctr(Bi), c</p>
    <p>* i)</p>
    <p>Proof:  2(X*i)  (|Bi| |X</p>
    <p>* i \ Bi| / ni).d(ctr(Bi), ctr(X</p>
    <p>* i</p>
    <p>\ Bi)) 2</p>
    <p>Also d(ctr(Bi), c * i) = (|X</p>
    <p>* i \ Bi| / ni).d(ctr(Bi),</p>
    <p>ctr(X*i \ Bi))</p>
    <p>ni(r * i)</p>
    <p>* i \ Bi|).d(ctr(Bi), c</p>
    <p>* i)</p>
    <p>B1 B2</p>
  </div>
  <div class="page">
    <p>Theorem: With probability 1O(), cost of final clustering is at most 2</p>
    <p>get a (1/(1))-approximation algorithm.</p>
    <p>Since  = O(2), we have</p>
    <p>approximation ratio  1 as 0. probability of success  1 as</p>
    <p>0.</p>
  </div>
  <div class="page">
    <p>Arbitrary k Algorithm and analysis follow the same outline as in 2-means. If X is -separated for k-means, can again show that all clusters are well separated, that is, cluster radius &lt;&lt; inter-cluster distance, r*i =</p>
    <p>O().d(c*i, c * j) i,j</p>
    <p>distinct seed center very near it</p>
    <p>Theorem: If X is -separated for k-means, then one can obtain an ()-approximation algorithm where ()  1 as   0.</p>
  </div>
  <div class="page">
    <p>Schematic of entire algorithm</p>
    <p>Simple sampling: Pick k centers as follows.  first pick 2 centers c1, c2 as in 2-means  to pick center ci+1, pick xX with probability</p>
    <p>minji d(x,cj) 2</p>
    <p>Simple sampling: success probability = exp(-k)</p>
    <p>Oversampling + deletion: sample O(k) centers, then greedily delete till k remain O(1) success probability, O(nkd+k3d)</p>
    <p>Greedy deletion: O(n3d)</p>
    <p>Greedy deletion: Start with n centers and keep deleting the center that causes least cost increase till k centers remain</p>
    <p>k wellplace d seeds</p>
    <p>Ball k-means or Lloyd step: gives O(1)-approx.</p>
    <p>KSS04sampling: gives PTAS</p>
  </div>
  <div class="page">
    <p>Simple sampling: analysis sketch</p>
    <p>X*1,, X * k : optimal clusters</p>
    <p>c*i = ctr(X * i), ni = |X</p>
    <p>* i|, (r</p>
    <p>* i)</p>
    <p>ni = 1 2(X*i) / ni</p>
    <p>core(X*i) = {xX * i : d(x,c</p>
    <p>* i)</p>
    <p>/ } where = (2/3) Lemma: With probability (1O())k, all sampled centers lie in the cores of distinct optimal clusters.</p>
    <p>Proof: Will show inductively that if c1,, ci lie in distinct cores, then with probability 1O(), so does center ci+1.</p>
    <p>Base case: X is -separated for k-means  X*i  X</p>
    <p>* j is -separated for 2-means for every i j</p>
    <p>(because merging two clusters causes a huge increase in cost). So by 2-means analysis, first two centers c1,c2 lie in distinct cores.</p>
    <p>X is -separated for kmeans.</p>
  </div>
  <div class="page">
    <p>Simple sampling: analysis (contd.)Inductive step: Assume c1,, ci lie in cores of X*1,</p>
    <p>, X*i</p>
    <p>Let C = {c1,, ci}.A = j  i+1 xcore(X*j) d(x,C)</p>
    <p>B = j  k, xX*j d(x,C) 2</p>
    <p>j  i 1 2(X*j) + j  i+1(12(X*j) + nj d(c*j,C)2)</p>
    <p>X*2X * 1</p>
    <p>core(X*1 )</p>
    <p>c*1 c1</p>
    <p>X*i</p>
    <p>core(X*i)</p>
    <p>c*i</p>
    <p>ci</p>
    <p>c*i+1 X*i+ 1</p>
    <p>c*k</p>
    <p>X*k</p>
  </div>
  <div class="page">
    <p>Simple sampling: analysis (contd.)Inductive step: Assume c1,, ci lie in cores of X*1,</p>
    <p>, X*i</p>
    <p>Let C = {c1,, ci}.A = j  i+1 xcore(X*j) d(x,C)</p>
    <p>B = j  k, xX*j d(x,C) 2</p>
    <p>j  i 1 2(X*j) + j  i+1(12(X*j) + nj d(c*j,C)2)  j  i+1 nj</p>
    <p>d(c*j,C) 2</p>
    <p>Probability = A/B = 1O()</p>
    <p>X*2</p>
    <p>c*i+1 c * k</p>
    <p>X*1</p>
    <p>core(X*1 )</p>
    <p>c*1 c1</p>
    <p>X*i</p>
    <p>core(X*i)</p>
    <p>c*i</p>
    <p>ci</p>
    <p>X*i+ 1</p>
    <p>X*k</p>
  </div>
  <div class="page">
    <p>Open Questions</p>
    <p>Deeper analysis of Lloyd: are there weaker conditions under which one can prove performance guarantes for Lloyd-type methods?</p>
    <p>PTAS for k-means with polytime dependence on n, k and d? Is it APX hard in geometric setting?</p>
    <p>PTAS for k-means under our separation condition?</p>
    <p>Other applications of separation condition?</p>
  </div>
  <div class="page">
    <p>Thank You.</p>
  </div>
</Presentation>
