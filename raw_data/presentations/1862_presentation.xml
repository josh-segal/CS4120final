<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration</p>
    <p>Yu-Ting Chen, Jason Cong, Zhenman Fang, Jie Lei and Peng Wei</p>
    <p>University of California, Los Angeles</p>
  </div>
  <div class="page">
    <p>We are forced to explore heterogeneity</p>
    <p>When Dennard scaling comes to the end and Moores law slows down as components reach atomic scale</p>
    <p>Shift from single-core to multi-core</p>
    <p>Take the pain to learn parallel programming  Memory ordering, locking, load balancing,</p>
    <p>Even homogeneous multi-core architectures are not able to drive continued perf. and energy improvement that we have come to expect in the past</p>
  </div>
  <div class="page">
    <p>Heterogeneous Architecture</p>
    <p>Why heterogeneity?  Customized accelerators promise orders-of</p>
    <p>magnitude perf./watt gains</p>
    <p>Accelerator-centric architectures  Application-specific functional units</p>
    <p>Domain-specific building blocks</p>
    <p>Reconfigurable/reprogrammable logic  GPUs, CGRAs, FPGAs,</p>
    <p>Source: Bob Broderson, Berkeley Wireless group</p>
    <p>Tensor Processing Unit Source: Google</p>
    <p>GPGPU Source: NVIDIA</p>
    <p>FPGA Source: Alpha Data</p>
  </div>
  <div class="page">
    <p>What is an FPGA?</p>
    <p>Field Programmable Gate Array (FPGA)  Reconfigurable hardware</p>
    <p>Can be used to solve any problem which is computable</p>
    <p>Not always fast, though  Is well-known to be used for ASIC</p>
    <p>prototyping and hardware emulation</p>
    <p>Prototype accelerators  Can also be directly used as a compute</p>
    <p>accelerator</p>
    <p>Xilinx Virtex-7 FPGA Source: Xilinx</p>
    <p>Altera Stratix-10 FPGA Source: Altera</p>
    <p>http://w w w .slideshare.net/omutukuda/presentation-1993175</p>
  </div>
  <div class="page">
    <p>FPGAs are reconfigurable  You do not want to be as extravagant as Google to tape-out an ASIC chip only for one app  Not many applications are so important as to be equipped with a customized chip</p>
    <p>FPGAs are energy-efficient  A high-end FPGA board typically consumes ~20 watt</p>
    <p>Concrete examples have demonstrated harnessing FPGAs in datacenter  Microsoft Catapult for the Bing search engine: 2x speedup with 10% more power consumption</p>
    <p>An FPGA fabric is (probably) going to be a free lunch in the near future  Altera, now part of Intel =&gt; An FPGA fabric will probably be a default component in a server</p>
    <p>CPU-FPGA systems are not fully elucidated  So many research opportunities, so much potential</p>
    <p>Why FPGAs in datacenter?</p>
  </div>
  <div class="page">
    <p>Accelerating an application that is a matter of life or death  DNA sequencing is expected to be widely used in precision medicine, like cancer treatment  A cancer cells genome mutates probably in a few days  The state-of-the-art sequencing pipeline typically takes a week or so to sequence a patients</p>
    <p>genome, which, for clinic use, is too long</p>
    <p>Proposing an approach to efficient integration of FPGAs into Apache Spark  Presenting what issues need to be addressed to operate FPGAs at scale  Attracting more attention from system architects to make better CPU-FPGA</p>
    <p>systems</p>
    <p>The Objectives of This Case Study</p>
  </div>
  <div class="page">
    <p>Next-Generation DNA Sequencing</p>
    <p>Fragmented into reads</p>
    <p>Sequenced by chemical sequencer</p>
    <p>Mapped by software aligner INDEPENDENTLY</p>
    <p>An individuals genome samples</p>
  </div>
  <div class="page">
    <p>Processing billions of reads (strings) independently  Fit the MapReduce programming model perfectly  As for the long reference genome? Sparks broadcast variables</p>
    <p>Inside each reads alignment process  Step #1: Seeding</p>
    <p>Exact string matching  Linear time complexity</p>
    <p>Step #2: Extending  Approximate string matching  The Smith-Waterman dynamic programming algorithm (Quadratic time complexity)  Accelerated by FPGAs</p>
    <p>The Solution: Heterogeneous Cluster Computing</p>
  </div>
  <div class="page">
    <p>Straightforward Integration: 1+1 &lt; 0.001</p>
    <p>The Spark Program  CS-BWAMEM [HitSeq `15]  Aligning billions of short reads onto the</p>
    <p>reference human genome in parallel</p>
    <p>The Accelerator [FCCM  15]  A throughput-oriented FPGA accelerator</p>
    <p>for the Smith-Waterman DP kernel</p>
    <p>The Straightforward JNI Integration  CPU: 2.1 x 103 reads per second  FPGA: 1.6 reads per second</p>
    <p>AXI Interconnect Bus</p>
    <p>FIFO FIFO FIFO ..</p>
    <p>P E</p>
    <p>PE Array M</p>
    <p>Distributor &amp; Collector</p>
    <p>B R</p>
    <p>A M</p>
    <p>P E</p>
    <p>P E</p>
    <p>N</p>
    <p>P E</p>
    <p>PE Array 1</p>
    <p>Distributor &amp; Collector</p>
    <p>B R</p>
    <p>A M</p>
    <p>P E</p>
    <p>P E</p>
    <p>N</p>
    <p>P E</p>
    <p>PE Array 2</p>
    <p>Distributor &amp; Collector</p>
    <p>B R</p>
    <p>A M</p>
    <p>P E</p>
    <p>P E</p>
    <p>N</p>
    <p>Controller distributor &amp; collector Batched Task BRAM</p>
    <p>256 bits</p>
    <p>256 bits 256 bits 256 bits</p>
    <p>32 bits</p>
    <p>4 bits</p>
    <p>On CPU One read</p>
    <p>24 DPs  20 s per DP  2.1 x 103 reads/s</p>
    <p>On CPU One read</p>
    <p>24 DPs  20 s per DP  2.1 x 103 reads/s</p>
    <p>On FPGA One DP</p>
    <p>25 ms data transfer  1.6 reads/s</p>
    <p>On FPGA One DP</p>
    <p>25 ms data transfer  1.6 reads/s</p>
    <p>While JNI serves as a standard approach to connect JVMs with FPGAs, a straightforward integration through JNI degrades the performance by 1000x.</p>
    <p>While JNI serves as a standard approach to connect JVMs with FPGAs, a straightforward integration through JNI degrades the performance by 1000x.</p>
  </div>
  <div class="page">
    <p>Java Heap  Native Memory</p>
    <p>Host Memory  Device Memory</p>
    <p>What happened in a CPU-FPGA communication instance?</p>
    <p>Host Device</p>
    <p>DataData</p>
  </div>
  <div class="page">
    <p>Each map function is likely to process only a small volume of data with a small amount of execution time</p>
    <p>One read is only 101 ASCII characters  One line of a text file  One record of a NoSQL table</p>
    <p>Communication overhead can be amortized by batch processing</p>
    <p>Why communication matters? def map_func(input:U):V = { // U =&gt; P =&gt; Q =&gt; V t1:P = cnv1(input) t2:Q = cnv2(t1) t3:V = cnv3(t2) t3</p>
    <p>} rdd_out = rdd_in.map(ele=&gt;map_func(ele))</p>
    <p>def map_func(input:Array[U]):Array[V] = { // Array[U] =&gt;  =&gt; Array[V] t1:Array[P] = cnv1_batch(input) t2:Array[Q] = cnv2_batch(t1) t3:Array[V] = cnv3_batch(t2) t3</p>
    <p>} rdd_out = rdd_in.map(ele=&gt;map_func(ele))</p>
  </div>
  <div class="page">
    <p>Lets first do batch processing manually</p>
    <p>CPU: 2.1</p>
    <p>FPGA: 7.8</p>
    <p>Read #3</p>
    <p>Read #4</p>
    <p>Read #5</p>
    <p>Read #6</p>
    <p>Read #2</p>
    <p>Read #1</p>
    <p>Batch #1</p>
    <p>Batch #8</p>
    <p>Batch #4</p>
    <p>Batch #3</p>
    <p>Batch #6</p>
    <p>Batch #7</p>
    <p>Batch #2</p>
    <p>Batch #5</p>
    <p>S-W #1</p>
    <p>S-W #2</p>
    <p>S-W #3</p>
    <p>S-W #4</p>
    <p>S-W #1</p>
    <p>S-W #1</p>
    <p>S-W #2</p>
    <p>S-W #3</p>
    <p>S-W #4</p>
    <p>S-W #5</p>
    <p>S-W #6</p>
    <p>S-W #1</p>
    <p>S-W #2</p>
    <p>S-W #3</p>
    <p>S-W #4</p>
    <p>S-W #5</p>
    <p>S-W #6</p>
    <p>S-W #1</p>
    <p>S-W #2</p>
    <p>S-W #3</p>
    <p>S-W #1</p>
    <p>S-W #2</p>
    <p>S-W #3</p>
    <p>S-W #1</p>
    <p>S-W #2</p>
    <p>S-W #3</p>
    <p>S-W #4</p>
    <p>S-W #5</p>
    <p>S-W #6</p>
    <p>S-W #7</p>
    <p>S-W #8</p>
    <p>S-W Batch</p>
    <p>R ea</p>
    <p>d B</p>
    <p>a tc</p>
    <p>h</p>
    <p>Dependency Chain of a reads S-W Tasks</p>
    <p>Dependency/Irregularity-Aware Batch Processing</p>
  </div>
  <div class="page">
    <p>Accelerator-as-a-Service</p>
    <p>Client RM AM NM</p>
    <p>NM Container Container</p>
    <p>Accelerator status</p>
    <p>GAM NAM</p>
    <p>NAM</p>
    <p>FPGA</p>
    <p>GPU</p>
    <p>Global Accelerator Manager: accelerator-centric scheduling</p>
    <p>Node Accelerator Manager: local accelerator service management, JVM-to-ACC communication optimization</p>
    <p>GAM</p>
    <p>NAM</p>
    <p>Source: https://spark-summit.org/2016/events/deploying-accelerators-at-dat acent er-scale-using-spark/</p>
    <p>CPU: 137.0</p>
    <p>FPGA: 362.5</p>
  </div>
  <div class="page">
    <p>From homogeneous to heterogeneous =&gt; requirement of corresponding systems  lightweight, unified and efficient processor-accelerator communication</p>
    <p>While various FPGA-based accelerators have been proposed, CPU-FPGA systems are being built on top of naive communication stack</p>
    <p>Pageable memory =&gt; pinned memory =&gt; device memory</p>
    <p>MapReduce does not take accelerators into consideration, and does not seem to be accelerator-friendly</p>
    <p>A map function often has little execution time, but considerable aggregate time  It must be greatly helpful if automatic code transformation for batch processing works</p>
    <p>A generic accelerator-aware big-data framework is needed, even if not now</p>
    <p>Lessons Learned and Open Discussion</p>
  </div>
  <div class="page">
    <p>Acknowledgement</p>
  </div>
  <div class="page">
    <p>THANKS FOR YOUR ATTENTION.</p>
  </div>
</Presentation>
