<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Jing Ma (CUHK) 2018/7/15 1</p>
    <p>Rumor Detection on Twitter with Tree-structured Recursive Neural Networks</p>
    <p>Jing Ma1, Wei Gao2, Kam-Fai Wong1,3 1The Chinese University of Hong Kong</p>
    <p>July 15-20, 2018  ACL 2018 @ Melboume, Australia</p>
  </div>
  <div class="page">
    <p>Introduction  Related Work  Problem Statement  RvNN-based Rumor Detection  Evaluation  Conclusion and Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>What are rumors?</p>
    <p>A story or statement whose truth value is unverified or deliberately false</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>people tend to stop spreading a rumor if it is known as false. (Zubiaga et al., 2016b)</p>
    <p>Previous studies focused on text mining from sequential microblog streams, we want to bridge the content semantics and propagation clues.</p>
    <p>How the fake news propagated?</p>
    <p>supportive</p>
    <p>denial</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>We generally are not good at distinguishing rumors</p>
    <p>It is crucial to track and debunk rumors early to minimize their harmful effects.</p>
    <p>Online fact-checking services have limited topical coverage and long delay.</p>
    <p>Existing models use feature engineering  over simplistic; or recently deep neural networks  ignore propagation structures; Kernel-based method  develop based on tree structure but cannot learn high-level feature representations automatically.</p>
  </div>
  <div class="page">
    <p>Observation &amp; Hypothesis</p>
    <p>IDEA: Combining the two models, leveraging propagation structure by representation learning algorithm</p>
    <p>(a) RNN-based model (Ma et al. 2016)</p>
    <p>Doubt</p>
    <p>Support</p>
    <p>Neutral</p>
    <p>(b) Tree kernel-based model (Ma et al. 2017)</p>
    <p>Existing works: Consider post representation or propagation structure</p>
  </div>
  <div class="page">
    <p>Observation &amp; Hypothesis</p>
    <p>A reply usually respond to its immediate ancestor rather than the root tweet.</p>
    <p>Repliers tend to disagree with (or question) who support a false rumor or deny a true rumor; repliers tend to agree with who deny a false rumor or support a true rumor.</p>
    <p>(a) False rumor (b) True rumor Polarity stances</p>
    <p>Why such model do better?</p>
    <p>Local characteristic:</p>
  </div>
  <div class="page">
    <p>Contributions  The first study that deeply integrates both structure and</p>
    <p>content semantics based on tree-structured recursive neural networks for detecting rumors from microblog posts</p>
    <p>Propose two variants of RvNN models based on bottom-up and top-down tree structures, to generate better integrated representations for a claim by capturing both structural and textural properties signaling rumors.</p>
    <p>Our experiments based on two real-world Twitter datasets achieve superior improvements over state-of-the-art baselines on both rumor classification and early detection tasks.</p>
    <p>We make the source codes in our experiments publicly accessible at https://github.com/majingCUHK/Rumor_RvNN</p>
  </div>
  <div class="page">
    <p>Introduction  Related Work  Problem Statement  RvNN-based Rumor Detection  Evaluation  Conclusion and Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Related Work  Systems based on common sense and investigative journalism,</p>
    <p>e.g.,  snopes.com  factcheck.org</p>
    <p>Learning-based models for rumor detection  Information credibility: Castillo et al. (2011), Yang et al. (2012)  Using handcrafted and temporal features: Liu et al. (2015), Ma et al.</p>
    <p>(2015), Kwon et al. (2013, 2017)  Using cue terms: Zhao et al. (2015)  Using recurrent neural networks: Ma et al. (2016, 2018)  Tree-kernel-based model:</p>
    <p>Ma et al. (2017), Wu et al. (2015)  RvNN-based works</p>
    <p>images segmentation (Socher et al, 2011)  phrase representation from word vectors (Socher et al, 2012)  Sentiment analysis (Socher et al, 2013)  etc</p>
    <p>Without handcrafted features</p>
  </div>
  <div class="page">
    <p>Introduction  Related Work  Problem Statement  RvNN-based Rumor Detection  Evaluation  Conclusion and Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Problem Statement</p>
    <p>Given a set of microblog posts R = {}, model each source tweet as a tree structure T  = &lt; , &gt;, where each node  provide the text content of each post. And  is directed edges corresponding to response relation.</p>
    <p>Task 1  finer-grained classification for each source post false rumor, true rumor, non-rumor, unverified rumor</p>
    <p>Task 2  detect rumor as early as possible</p>
  </div>
  <div class="page">
    <p>Tweet Structure</p>
    <p>Root tweet</p>
    <p>replies</p>
    <p>: #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling</p>
    <p>: NEED SOURCE. have a feeling this is just hearsay ...</p>
    <p>: I agree. I have been hearing this all day but</p>
    <p>no source 1:12</p>
    <p>: Exactly, i don't think Wal-Mart would let everyone know this if they did!! 2:21</p>
    <p>: 1:30 Idc if they killed a mf foreal. Ima always</p>
    <p>shop with @Walmart. I'm just bein honest</p>
    <p>: #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling</p>
    <p>: NEED SOURCE. have a feeling this is just hearsay ...</p>
    <p>: I agree. I have been hearing this all day but</p>
    <p>no source 1:12</p>
    <p>: Exactly, i don't think Wal-Mart would let everyone know this if they did!! 2:21</p>
    <p>: 1:30 Idc if they killed a mf foreal. Ima always</p>
    <p>shop with @Walmart. I'm just bein honest</p>
    <p>bottom-up tree</p>
    <p>top-down tree</p>
  </div>
  <div class="page">
    <p>Introduction  Related Work  Problem Statement  RvNN-based Rumor Detection  Evaluation  Conclusion and Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Standard Recursive Neural Networks</p>
    <p>RvNN (tree-structured neural networks) utilize sentence parse trees: representation associated with each node of a parse tree is computed from its direct children, computed by</p>
    <p>= ( 9 ;;= + )  p: the feature vector of a parent node whose children are ; and =  computation is done recursively over all tree nodes</p>
  </div>
  <div class="page">
    <p>Bottom-up RvNN</p>
    <p>Input: bottom-up tree (node: a post represented as a vector of words )</p>
    <p>Structure: recursively visit every node from the leaves at the bottom to the root at the top (a natural extension to the original RvNN )</p>
    <p>Intuition: local rumor indicative features are aggregated along different branches (e.g., subtrees having a denial parent and a set of supportive children) (generate a feature vector for each subtree)</p>
    <p>: #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling</p>
    <p>: NEED SOURCE. have a feeling this is just hearsay ...</p>
    <p>: I agree. I have been hearing this all day but</p>
    <p>no source 1:12</p>
    <p>: Exactly, i don't think Wal-Mart would let everyone know this if they did!! 2:21</p>
    <p>: 1:30 Idc if they killed a mf foreal. Ima always</p>
    <p>shop with @Walmart. I'm just bein honest</p>
    <p>GRU equation at node</p>
    <p>Own input</p>
    <p>Children node</p>
  </div>
  <div class="page">
    <p>Top-down RvNN</p>
    <p>: #Walmart donates $10,000 to #DarrenWilson fund to continue police racial profiling</p>
    <p>: NEED SOURCE. have a feeling this is just hearsay ...</p>
    <p>: I agree. I have been hearing this all day but</p>
    <p>no source 1:12</p>
    <p>: Exactly, i don't think Wal-Mart would let everyone know this if they did!! 2:21</p>
    <p>: 1:30 Idc if they killed a mf foreal. Ima always</p>
    <p>shop with @Walmart. I'm just bein honest</p>
    <p>Input: top-down tree</p>
    <p>Structure: recursively visit from the root node to its children until reaching all leaf nodes. (reverse Bottom-up RvNN)</p>
    <p>Intuition: rumor-indicative features are aggregated along the propagation path (e.g., if a post agree with its parents stance, the parents stance should be reinforced) (models how information flows from source post to the current node)</p>
    <p>GRU transition equation at node  Own input Parent node</p>
  </div>
  <div class="page">
    <p>Model Training</p>
    <p>Comparison: both of the two RvNN models aim to capture the structural properties by recursively</p>
    <p>visiting all nodes Bottom-up RvNN: the state of root node (i.e., source tweet) can be regard as the</p>
    <p>representation of the whole tree (can be used for supervised classification). Top-down RvNN: the representation of each path are eventually embedded into the hidden</p>
    <p>vector of all the leaf nodes.</p>
    <p>Output Layer Bottom-up RvNN:  =  K +  Top-down RvNN:  =  L +</p>
    <p>Objective Function:  =   O  QO =ROS; + T US;   =</p>
    <p>=</p>
    <p>Training Procedure parameters are updated using efficient back-propagation through structure (Goller and</p>
    <p>Kuchler, 1996; Socher et al., 2013)</p>
    <p>learned vector of root node</p>
    <p>the pooling vector over all leaf nodes</p>
    <p>prediction Ground truth</p>
  </div>
  <div class="page">
    <p>Introduction  Related Work  Problem Statement  RvNN-based Rumor Detection  Evaluation  Conclusion and Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Data Collection</p>
    <p>Use two reference Tree datasets:</p>
    <p>URL of the datasets: https://www.dropbox.com/s/0jhsfwep3ywvpca/rumdetect2017.zip?dl=0</p>
  </div>
  <div class="page">
    <p>Approaches to compare with</p>
    <p>DTR: Decision tree-based ranking model using enquiry phrases to identify trending rumors (Zhao et al., 2015)</p>
    <p>DTC: Twitter information credibility model using Decision Tree Classifier (Castillo et al., 2011);</p>
    <p>RFC: Random Forest Classifier using three parameters to fit the temporal tweets volume curve (Kwon et al., 2013)</p>
    <p>SVM-TS: Linear SVM classifier using time-series structures to model the variation of social context features. (Ma et al., 2015)</p>
    <p>SVM-BOW: linear SVM classifier using bag-of-words.  SVM-TK and SVM-HK: SVM classifier uses a Tree Kernel</p>
    <p>(Ma et al., 2017) and that uses a Hybrid Kernel (Wu et al., 2015), both model propagation structures with kernels.</p>
    <p>GRU-RNN: The RNN-based rumor detection model. (Ma et al., 2016)</p>
    <p>Ours (BU-RvNN and TD-RvNN): Our bottom-up and topdown recursive models.</p>
  </div>
  <div class="page">
    <p>Results on Twitter15</p>
    <p>Method Accu. NR FR TR UR F1 F1 F1 F1</p>
    <p>DTR 0.409 0.501 0.311 0.364 0.473 DTC 0.454 0.733 0.355 0.317 0.415 RFC 0.565 0.810 0.422 0.401 0.543</p>
    <p>SVM-TS 0.544 0.796 0.472 0.404 0.483 SVM-BOW 0.548 0.564 0.524 0.582 0.512 SVM-HK 0.493 0.650 0.439 0.342 0.336 SVM-TK 0.667 0.619 0.669 0.772 0.645</p>
    <p>GRU-RNN 0.641 0.684 0.634 0.688 0.571 BU-RvNN 0.708 0.695 0.728 0.759 0.653 TD-RvNN 0.723 0.682 0.758 0.821 0.654</p>
    <p>NR: Non-Rumor; FR: False Rumor; TR: True Rumor; UR: Unverified Rumor;</p>
    <p>hand-crafted features (e.g., user info  NR vs others)</p>
    <p>Structural info Linear chain input More info loss</p>
  </div>
  <div class="page">
    <p>Results on Twitter16</p>
    <p>NR: Non-Rumor; FR: False Rumor; TR: True Rumor; UR: Unverified Rumor;</p>
    <p>Method Accu. NR FR TR UR F1 F1 F1 F1</p>
    <p>DTR 0.414 0.394 0.273 0.630 0.344 DTC 0.465 0.643 0.393 0.419 0.403 RFC 0.585 0.752 0.415 0.547 0.563</p>
    <p>SVM-TS 0.574 0.755 0.420 0.571 0.526 SVM-BOW 0.585 0.553 0.556 0.655 0.578 SVM-HK 0.511 0.648 0.434 0.473 0.451 SVM-TK 0.662 0.643 0.623 0.783 0.655</p>
    <p>GRU-RNN 0.633 0.617 0.715 0.577 0.527 BU-RvNN 0.718 0.723 0.712 0.779 0.659 TD-RvNN 0.737 0.662 0.743 0.835 0.708</p>
    <p>models without hand-crafted features</p>
  </div>
  <div class="page">
    <p>Results on Early Detection  In the first few hours, the</p>
    <p>accuracy of the RvNNbased methods climbs more rapidly and stabilize more quickly</p>
    <p>TD-RvNN and BURvNN only need around 8 hours or about 90 tweets to achieve the comparable performance of the best baseline model.</p>
    <p>(a) Twitter15 DATASET</p>
    <p>(b) Twitter16 DATASET</p>
  </div>
  <div class="page">
    <p>Early Detection Example</p>
    <p>Example subtree of a rumor captured by the algorithm at early stage of propagation</p>
    <p>Bottom-up RvNN: a set of responses supporting the parent posts that deny or question the source post.</p>
    <p>Top-down RvNN: some patterns of propagation from the root to leaf nodes like supportdenysupport</p>
    <p>Baselines: sequential models may be confused because the supportive key terms such as be right, yeah, exactly! dominate the responses, and the SVM-TK may miss similar subtrees by just comparing the surface words.</p>
  </div>
  <div class="page">
    <p>Introduction  Related Work  Problem Statement  RvNN-based Rumor Detection  Evaluation  Conclusion and Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Conclusion and future work</p>
    <p>Propose a bottom-up and a top-down tree-structured model based on recursive neural networks for rumor detection on Twitter.</p>
    <p>Using propagation tree to guide the learning of representations from tweets content, such as embedding various indicative signals hidden in the structure, for better identifying rumors.</p>
    <p>Results on two public Twitter datasets show that our method improves rumor detection performance in very large margins as compared to state-of-the-art baselines.</p>
    <p>Future work:  Integrate other types of information such as user properties</p>
    <p>into the structured neural models to further enhance representation learning</p>
    <p>Develop unsupervised models due to massive unlabeled data from social media.</p>
  </div>
  <div class="page">
    <p>Jing Ma (CUHK) 2018/7/15 28</p>
    <p>Q &amp; A</p>
  </div>
</Presentation>
