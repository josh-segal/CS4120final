<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Advanced Flow-control Mechanisms</p>
    <p>for the Sockets Direct Protocol</p>
    <p>over InfiniBand</p>
    <p>P. Balaji, S. Bhagvat, D. K. Panda, R. Thakur, and W. Gropp</p>
    <p>Mathematics and Computer Science, Argonne National Laboratory</p>
    <p>High Performance Cluster Computing, Dell Inc.</p>
    <p>Computer Science and Engineering, Ohio State University</p>
  </div>
  <div class="page">
    <p>High-speed Networking with InfiniBand</p>
    <p>High-speed Networks  A significant driving force for ultra-large scale systems  High performance and scalability are key  InfiniBand is a popular choice as a high-speed network</p>
    <p>What does InfiniBand provide?  High raw performance (low latency and high bandwidth)  Rich features and capabilities</p>
    <p>Hardware offloaded protocol stack (data integrity, reliability, routing)  Zero-copy communication (memory-to-memory)  Remote direct memory access (read/write data to remote memory)  Hardware flow-control (sender ensures receiver is not overrun)  Atomic operations, multicast, QoS and several others</p>
  </div>
  <div class="page">
    <p>TCP/IP on High-speed Networks  TCP/IP unable to keep pace with high-speed networks</p>
    <p>Implemented purely in software (hardware TCP/IP incompatible)  Utilizes the raw network capability (e.g., faster network link)  Performance limited by the TCP/IP stack</p>
    <p>On a 16Gbps network, TCP/IP achieves 2-3 Gbps</p>
    <p>Reason: Does NOT fully utilize network features  Hardware offloaded protocol stack  RDMA operations  Hardware flow-control</p>
    <p>Advanced features of InfiniBand  Great for new applications!  How should existing TCP/IP applications use them?</p>
  </div>
  <div class="page">
    <p>Sockets Direct Protocol (SDP)</p>
    <p>Industry standard high</p>
    <p>performance sockets</p>
    <p>Defined for two purposes:</p>
    <p>Maintain compatibility for</p>
    <p>existing applications</p>
    <p>Deliver the performance of</p>
    <p>networks to the applications</p>
    <p>Many implementations:</p>
    <p>OSU, OpenFabrics,</p>
    <p>Mellanox, Voltaire</p>
    <p>High-speed Network</p>
    <p>Device Driver</p>
    <p>IP</p>
    <p>TCP</p>
    <p>Sockets</p>
    <p>Sockets Direct Protocol</p>
    <p>(SDP)</p>
    <p>Sockets Applications or Libraries</p>
    <p>Advanced Features</p>
    <p>Offloaded Protocol</p>
    <p>SDP allows applications to utilize the</p>
    <p>network performance and capabilities</p>
    <p>with ZERO modifications</p>
  </div>
  <div class="page">
    <p>SDP State-of-Art  SDP standard specifies different communication designs</p>
    <p>Large Messages: Synchronous Zero-copy design using RDMA</p>
    <p>Small Messages: Buffer-copy design with credit-based flow-control using send-recv operations</p>
    <p>These designs are often times not the best !</p>
    <p>Previously, we proposed Asynchronous Zero-copy SDP to improve the performance of large messages [balaji07:azsdp]</p>
    <p>In this paper, we propose new flow-control techniques  Utilizing RDMA and hardware flow-control</p>
    <p>Improve the performance of small messages</p>
    <p>[balaji07:azsdp] Asynchronous Zero-copy Communication for Synchronous Sockets in the Sockets Direct Protocol over InfiniBand. P. Balaji, S. Bhagvat, H. W. Jin and D. K. Panda. Workshop on Communication Archictecture for Clusters (CAC), with IPDPS 2007.</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Existing Credit-based Flow-control design</p>
    <p>RDMA-based Flow-control</p>
    <p>NIC-assisted RDMA-based Flow-control</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Credit-based Flow-control  Flow-control needed to ensure sender does not overrun</p>
    <p>the receiver</p>
    <p>Popular flow-control for many programming models  SDP, MPI (MPICH2, OpenMPI), File-systems (PVFS2, Lustre)</p>
    <p>Generic to many networks  does not utilize many exotic features</p>
    <p>TCP/IP like behavior  Receiver presents N credits; ensures buffering for N segments</p>
    <p>Sender sends N message segments before waiting for an ACK</p>
    <p>When receiver application reads out data and receive buffer is free, an acknowledgment is sent out</p>
    <p>SDP credit-based flow-control uses static compile-time decided credits (unlike TCP/IP)</p>
  </div>
  <div class="page">
    <p>ACK</p>
    <p>Sockets Buffers</p>
    <p>Application Buffer</p>
    <p>Sender</p>
    <p>Application Buffer</p>
    <p>Receiver</p>
    <p>Sockets Buffers</p>
    <p>Credit-based Flow-control</p>
    <p>Receiver has to pre-specify buffers in which data should be received  InfiniBand requirement for Send-receive communication</p>
    <p>Sender manages send buffers and receiver manages receive buffers  Coordination between sender-receiver through explicit acknowledgments</p>
    <p>Credits = 4</p>
  </div>
  <div class="page">
    <p>Sockets Buffers</p>
    <p>Application Buffers</p>
    <p>Sender</p>
    <p>Application Buffers Not Posted</p>
    <p>Receiver</p>
    <p>Sockets Buffers Credits = 4</p>
    <p>Application Buffer</p>
    <p>ACK</p>
    <p>Receiver controls buffers  Statically sized temporary buffers  Two primary disadvantages:</p>
    <p>Inefficient resource usage  excessive wastage of buffers  Small messages pushed directly to network</p>
    <p>Network performance is under-utilized for small messages</p>
    <p>Limitations with Credit-based Flow-control</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Existing Credit-based Flow-control design</p>
    <p>RDMA-based Flow-control</p>
    <p>NIC-assisted RDMA-based Flow-control</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>InfiniBand RDMA capabilities</p>
    <p>Remote Direct Memory Access  Receiver transparent data placement  can help provide a shared</p>
    <p>memory like illusion  Sender-side buffer management  sender can dictate which</p>
    <p>position in the receive buffer the data should be placed</p>
    <p>RDMA with Immediate-data  Requires receiver to explicitly check for the receipt of data  Allows receiver to know when the data has arrived  Loses receiver transparency!  Still retains sender-side buffer management</p>
    <p>In this design, we utilize RDMA with immediate data</p>
  </div>
  <div class="page">
    <p>Sockets Buffers</p>
    <p>Application Buffers</p>
    <p>Sender Receiver</p>
    <p>Sockets Buffers</p>
    <p>Utilizes InfiniBand RDMA with Immediate Data feature  Sender side buffer management</p>
    <p>Avoids buffer wastage for small-medium messages</p>
    <p>Uses an immediate send threshold to improve the throughput for small-medium messages using message coalescing</p>
    <p>Immediate Send Threshold = 4</p>
    <p>Application Buffers Not PostedApplication Buffer</p>
    <p>ACK</p>
    <p>RDMA-based Flow-control</p>
  </div>
  <div class="page">
    <p>Sockets Buffers</p>
    <p>Application Buffers</p>
    <p>Sender Receiver</p>
    <p>Sockets Buffers Immediate Send Threshold = 4</p>
    <p>Application Buffers Not PostedApplication Buffer</p>
    <p>Limitations of RDMA-based Flow-control</p>
    <p>Application is computing</p>
    <p>Remote credits are available, data is present in sockets buffer</p>
    <p>Communication progress does not take place</p>
    <p>ACK</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Existing Credit-based Flow-control design</p>
    <p>RDMA-based Flow-control</p>
    <p>NIC-assisted RDMA-based Flow-control</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>InfiniBand hardware provides a nave message level flow control mechanism</p>
    <p>Guarantees that a message is not sent out till the receiver is ready</p>
    <p>Hardware takes care of progress even if application is busy with other computation</p>
    <p>Does not guarantee that the receiver has posted a sufficiently large buffer  buffer overruns are errors!</p>
    <p>Does not provide message coalescing capabilities</p>
    <p>Software Flow control schemes are more intelligent  Message coalescing, segmentation and reassembly</p>
    <p>No progress if application is busy with other computation</p>
    <p>Hardware vs. Software Flow-control</p>
  </div>
  <div class="page">
    <p>NIC-Assisted Flow Control</p>
    <p>Hybrid Hardware/Software</p>
    <p>Takes the best of IB hardware flow-control and the software</p>
    <p>features of RDMA-based flow-control</p>
    <p>Contains two main mechanisms:</p>
    <p>Virtual window mechanism</p>
    <p>Mainly for correctness  avoid buffer overflows</p>
    <p>Asynchronous interrupt mechanism</p>
    <p>Enhancement to virtual window mechanism</p>
    <p>Improves performance by coalescing data</p>
    <p>NIC-assisted RDMA-based Flow-control</p>
  </div>
  <div class="page">
    <p>Sockets Buffers</p>
    <p>Application Buffers</p>
    <p>Sender Receiver</p>
    <p>Sockets Buffers</p>
    <p>N / W = 4</p>
    <p>Application Buffers Not PostedApplication Buffer</p>
    <p>Virtual Window Mechanism</p>
    <p>Application is computing</p>
    <p>ACK</p>
    <p>For a virtual window size of W, the receiver posts N/W work queue entries, i.e., it is ready to receive N/W messages</p>
    <p>Sender always sends message segments smaller than W</p>
    <p>The first N/W messages are directly transmitted by the NIC</p>
    <p>The later send requests are queued by the hardware</p>
    <p>NIC-handled Buffers</p>
  </div>
  <div class="page">
    <p>Sockets Buffers</p>
    <p>Application Buffers</p>
    <p>Sender Receiver</p>
    <p>Sockets Buffers</p>
    <p>N / W = 4</p>
    <p>Application Buffers Not PostedApplication Buffer</p>
    <p>Asynchronous Interrupt Mechanism</p>
    <p>Application is computing</p>
    <p>ACK</p>
    <p>After the NIC gives the interrupt, it still has some messages to send</p>
    <p>allows us to effectively utilize the interrupt time without wasting it</p>
    <p>We can coalesce small amounts of data  sufficient to reach the</p>
    <p>performance of RDMA-based flow control</p>
    <p>IB Interrupt</p>
    <p>Software handled Buffers</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Existing Credit-based Flow-control design</p>
    <p>RDMA-based Flow-control</p>
    <p>NIC-assisted RDMA-based Flow-control</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Testbed</p>
    <p>16-node cluster</p>
    <p>Dual Intel Xeon 3.6GHz EM64T processors (single core, dual</p>
    <p>processor)</p>
    <p>Each processor has 2MB L2 cache</p>
    <p>The system has 1GB of 533MHz DDR SDRAM</p>
    <p>Connected using Mellanox MT25208 InfiniBand DDR</p>
    <p>adapters (3rd generation adapters)</p>
    <p>Mellanox MTS-2400 24-port fully non-blocking switch</p>
  </div>
  <div class="page">
    <p>SDP Latency and Bandwidth</p>
    <p>RDMA-based and NIC-assisted flow-control designs outperform credit-based flow-control by almost 10X for some message sizes</p>
  </div>
  <div class="page">
    <p>SDP Buffer Utilization</p>
    <p>RDMA-based and NIC-assisted flow-control designs utilize the SDP buffers in a much better manner, which eventually leads to their better performance</p>
  </div>
  <div class="page">
    <p>Communication Progress</p>
    <p>Computati on</p>
    <p>Good communication</p>
    <p>Progress</p>
    <p>Bad communication</p>
    <p>Progress</p>
  </div>
  <div class="page">
    <p>Component Framework for Combined Task/Data Parallelism  Developed by U. Maryland</p>
    <p>Popular model for data-intensive applications</p>
    <p>User defines sequence of pipelined components (filters and filter groups)  Data parallelism</p>
    <p>Stream based communication</p>
    <p>User tells the runtime system to generate/instantiate copies of filters  Task parallelism</p>
    <p>Flow control between filter copies</p>
    <p>Transparent: single stream illusion</p>
    <p>Data-cutter Library</p>
    <p>host1</p>
    <p>R0</p>
    <p>R1</p>
    <p>host2</p>
    <p>R2</p>
    <p>host3</p>
    <p>Ra0</p>
    <p>host1</p>
    <p>E0</p>
    <p>EK</p>
    <p>host2</p>
    <p>EK+1</p>
    <p>EN</p>
    <p>host4</p>
    <p>Ra1</p>
    <p>host5</p>
    <p>Ra2</p>
    <p>host1</p>
    <p>M</p>
    <p>Cluster 1</p>
    <p>Cluster 3</p>
    <p>Cluster 2</p>
    <p>Virtual Microscope Application</p>
  </div>
  <div class="page">
    <p>Evaluating the Data-cutter Library</p>
    <p>RDMA-based and NIC-assisted flow-control designs achieve about 10-15% better performance</p>
    <p>No difference between RDMA-based and NIC-assisted designs  application makes regular progress</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Existing Credit-based Flow-control design</p>
    <p>RDMA-based Flow-control</p>
    <p>NIC-assisted RDMA-based Flow-control</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>SDP is an industry standard to allow sockets applications</p>
    <p>to transparently utilize the performance and features of IB  Previous designs allow SDP to utilize some of the features of IB  Capabilities of features such as hardware flow-control and RDMA</p>
    <p>for small messages have not been studied so far</p>
    <p>In this paper we present two flow-control mechanisms</p>
    <p>which utilizes these features of IB</p>
    <p>Shown that our designs can improve performance by up to</p>
    <p>Future Work: Integrate our designs in the OpenFabrics</p>
    <p>SDP implementation. Study MPI flow-control techniques.</p>
  </div>
  <div class="page">
    <p>Thank You !</p>
    <p>Contacts:</p>
    <p>P. Balaji: balaji@mcs.anl.gov</p>
    <p>S. Bhagvat: sitha_bhagvat@dell.com</p>
    <p>D. K. Panda: panda@cse.ohio-state.edu</p>
    <p>R. Thakur: thakur@mcs.anl.gov</p>
    <p>W. Gropp: gropp@mcs.anl.gov</p>
    <p>Web links:</p>
    <p>http://www.mcs.anl.gov/~balaji</p>
    <p>http://nowlab.cse.ohio-state.edu</p>
  </div>
</Presentation>
