<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Minuet  Rethinking Concurrency Control in Storage Area Networks</p>
    <p>Andrey Ermolinskiy (U. C. Berkeley) Daekyeong Moon (U. C. Berkeley) Byung-Gon Chun (Intel Research, Berkeley) Scott Shenker (U. C. Berkeley and ICSI)</p>
    <p>FAST 09</p>
  </div>
  <div class="page">
    <p>Storage Area Networks (SANs) are gaining widespread adoption in data centers.</p>
    <p>An attractive architecture for clustered services and data-intensive clustered applications that require a scalable and highly-available storage backend. Examples:  Online transaction processing</p>
    <p>Data mining and business intelligence</p>
    <p>Digital media production and streaming media delivery</p>
    <p>Storage Area Networks  an Overview</p>
  </div>
  <div class="page">
    <p>One of the main design challenges: ensuring safe and efficient coordination of concurrent access to shared state on disk.</p>
    <p>Clustered SAN applications and services</p>
    <p>Traditional techniques for shared-disk applications: distributed locking, leases.</p>
    <p>Need mechanisms for distributed concurrency control.</p>
  </div>
  <div class="page">
    <p>Limitations of distributed locking</p>
    <p>Distributed locking semantics do not suffice to guarantee correct serialization of disk requests and hence do not ensure application-level data safety.</p>
  </div>
  <div class="page">
    <p>Data integrity violation: an example Client 1  updating resource R</p>
    <p>Client 2  reading resource R</p>
    <p>DLM SAN</p>
    <p>X X X X X X X X X X</p>
    <p>Shared resource R</p>
  </div>
  <div class="page">
    <p>Data integrity violation: an example Client 1  updating resource R</p>
    <p>Client 2  reading resource R</p>
    <p>DLM</p>
    <p>Shared resource R</p>
    <p>SAN</p>
    <p>X X X X X X X X X X</p>
    <p>Lock(R)</p>
    <p>owns lock on R</p>
    <p>Lock(R)</p>
    <p>waiting for lock on R</p>
    <p>- OK</p>
    <p>Write(B, offset=3, data= ) Y Y Y Y Y Y Y Y</p>
    <p>CRASH!</p>
    <p>Client 1</p>
    <p>Client 2 owns lock on R</p>
    <p>- OK Read(R, offset=0, data= )</p>
    <p>X X X X X</p>
    <p>Read(R, offset=5, data= )</p>
    <p>X X X Y Y X X X</p>
  </div>
  <div class="page">
    <p>Data integrity violation: an example</p>
    <p>Client 2  reading resource R</p>
    <p>X X X Y Y Y Y X X X</p>
    <p>X X X X X Y Y X X X</p>
    <p>Both clients obey the locking protocol, but Client 1 observes only partial effects of Client 2s update.</p>
    <p>Update atomicity is violated.</p>
    <p>Shared resource R</p>
  </div>
  <div class="page">
    <p>The lock service represents an additional point of failure.</p>
    <p>DLM failure  loss of lock management state  application downtime.</p>
    <p>Availability limitations of distributed locking</p>
  </div>
  <div class="page">
    <p>Standard fault tolerance techniques can be applied to mitigate the effects of DLM failures  State machine replication</p>
    <p>Dynamic election</p>
    <p>These techniques necessitate some form of global agreement.</p>
    <p>Agreement requires an active majority  Makes it difficult to tolerate network-level failures and large</p>
    <p>scale node failures.</p>
    <p>Availability limitations of distributed locking</p>
  </div>
  <div class="page">
    <p>DLM1 DLM2</p>
    <p>DLM3</p>
    <p>SAN</p>
    <p>C1</p>
    <p>C2</p>
    <p>C3</p>
    <p>C4</p>
    <p>Application cluster</p>
    <p>DLM replicas C3 and C4 stop making process</p>
    <p>Example: a partitioned network</p>
  </div>
  <div class="page">
    <p>Minuet overview</p>
    <p>Minuet is a new synchronization primitive for shareddisk applications and middleware that seeks to address these limitations.  `Guarantees safe access to shared state in the face of</p>
    <p>arbitrary asynchrony  Unbounded network transfer delays  Unbounded clock drift rates</p>
    <p>Improves application availability  Resilience to network partitions and large-scale node failures.</p>
  </div>
  <div class="page">
    <p>Our approach</p>
    <p>We focus on ensuring safe ordering of disk requests at target storage devices.</p>
    <p>A traditional cluster lock service provides the guarantees of mutual exclusion and focuses on preventing conflicting lock assignments.</p>
    <p>Lock(R) Read(R, offset=0, data= )</p>
    <p>Read(R, offset=5, data= ) Unlock(R)</p>
    <p>Client 2  reading resource R</p>
  </div>
  <div class="page">
    <p>Session isolation</p>
    <p>Read1.1(R) C1</p>
    <p>Lock(R, Shared)</p>
    <p>Read1.2(R)</p>
    <p>UpgradeLock(R, Excl)</p>
    <p>Write1.1(R)</p>
    <p>Write1.2(R)</p>
    <p>DowngradeLock(R, Shared)</p>
    <p>Read1.3(R)</p>
    <p>Unlock(R)</p>
    <p>C2</p>
    <p>Lock(R, Shared)</p>
    <p>Read2.1(R)</p>
    <p>UpgradeLock(R, Excl)</p>
    <p>Write2.1(R)</p>
    <p>Write2.2(R) Unlock(R)</p>
    <p>Shared session</p>
    <p>Shared session</p>
    <p>Excl session</p>
    <p>Excl session</p>
    <p>Session isolation: R.owner must observe the prefixes of all sessions to R in strictly serial order, such that</p>
    <p>R Owner</p>
    <p>No two requests in a shared session are interleaved by an exclusive-session request from another client.</p>
  </div>
  <div class="page">
    <p>Session isolation</p>
    <p>Read1.1(R) C1</p>
    <p>Lock(R, Shared)</p>
    <p>Read1.2(R)</p>
    <p>UpgradeLock(R, Excl)</p>
    <p>Write1.1(R)</p>
    <p>Write1.2(R)</p>
    <p>DowngradeLock(R, Shared)</p>
    <p>Read1.3(R)</p>
    <p>Unlock(R)</p>
    <p>C2</p>
    <p>Lock(R, Shared)</p>
    <p>Read2.1(R)</p>
    <p>UpgradeLock(R, Excl)</p>
    <p>Write2.1(R)</p>
    <p>Write2.2(R) Unlock(R)</p>
    <p>Shared session</p>
    <p>Shared session</p>
    <p>Excl session</p>
    <p>Excl session</p>
    <p>Session isolation: R.owner must observe the prefixes of all sessions to R in strictly serial order, such that</p>
    <p>R Owner</p>
    <p>No two requests in an exclusive session are interleaved by a shared- or exclusive-session request from another client.</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation  Each session to a shared resource is assigned a</p>
    <p>globally-unique session identifier (SID) at the time of lock acquisition.</p>
    <p>Client annotates its outbound disk commands with its current SID for the respective resource.</p>
    <p>SAN-attached storage devices are extended with a small application-independent logical component (guard), which:  Examines the client-supplied session annotations</p>
    <p>Rejects commands that violate session isolation.</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>R.clientSID = &lt;TS, TX&gt;</p>
    <p>R.curSType = {Excl / Shared / None}</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>R.clientSID = &lt;TS, TX&gt;</p>
    <p>R.curSType = {Excl / Shared / None}</p>
    <p>Establishing a session to resource R:</p>
    <p>R.clientSID  unique session ID</p>
    <p>Lock(R, Shared / Excl) { R.curSType  Shared / Excl</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>R.clientSID = &lt;TS, TX&gt;</p>
    <p>R.curSType = {Excl / Shared / None}</p>
    <p>Submitting a remote disk command:</p>
    <p>READ / WRITE (LUN, Offset, Length, )</p>
    <p>verifySID = &lt;Ts, Tx&gt; updateSID = &lt;Ts, Tx&gt; R</p>
    <p>command</p>
    <p>session annotation</p>
    <p>Initialize the session annotation: IF (R.curSType = Excl) {</p>
    <p>}</p>
    <p>verifySID  R.clientSID updateSID  R.clientSID</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>R.clientSID = &lt;TS, TX&gt;</p>
    <p>R.curSType = {Excl / Shared / None}</p>
    <p>Submitting a remote disk command:</p>
    <p>READ / WRITE (LUN, Offset, Length, )</p>
    <p>verifySID = &lt;Ts, Tx&gt; updateSID = &lt;Ts, Tx&gt; R</p>
    <p>command</p>
    <p>session annotation</p>
    <p>Initialize the session annotation: IF (R.curSType = Shared) {</p>
    <p>}</p>
    <p>verifySID.Ts  EMPTY verifySID.Tx  R.clientSID.TX updateSID  R.clientSID</p>
  </div>
  <div class="page">
    <p>Enforcing session isolation Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>R.clientSID = &lt;TS, TX&gt;</p>
    <p>R.curSType = {Excl / Shared / None}</p>
    <p>Submitting a remote disk command:</p>
    <p>READ / WRITE (LUN, Offset, Length, )</p>
    <p>verifySID = &lt;Ts, Tx&gt; updateSID = &lt;Ts, Tx&gt; R</p>
    <p>command</p>
    <p>session annotation</p>
    <p>Initialize the session annotation: IF (R.curSType = Shared) {</p>
    <p>}</p>
    <p>verifySID.Ts  EMPTY verifySID.Tx  R.clientSID.TX updateSID  R.clientSID</p>
    <p>disk cmd.</p>
    <p>annotation</p>
  </div>
  <div class="page">
    <p>Client node</p>
    <p>Enforcing session isolation Client node</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>R.clientSID = &lt;TS, TX&gt;</p>
    <p>R.curSType = {Excl / Shared / None}</p>
    <p>disk cmd.</p>
    <p>annotation</p>
    <p>R</p>
    <p>Guard module</p>
  </div>
  <div class="page">
    <p>Client node</p>
    <p>Enforcing session isolation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>disk cmd.</p>
    <p>annotation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>Guard logic at the storage controller: R.ownerSID = &lt;Ts, Tx&gt;</p>
    <p>IF (verifySID.Tx &lt; R.ownerSID.Tx)</p>
    <p>decision  REJECT</p>
    <p>ELSE IF ((verifySID.Ts  EMPTY) AND (verifySID.Ts &lt; R.ownerSID.Ts))</p>
    <p>decision  REJECT</p>
    <p>ELSE decision  ACCEPT</p>
  </div>
  <div class="page">
    <p>Client node</p>
    <p>Enforcing session isolation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN</p>
    <p>disk cmd.</p>
    <p>annotation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>Guard logic at the storage controller: R.ownerSID = &lt;Ts, Tx&gt;</p>
    <p>IF (decision = ACCEPT) {</p>
    <p>Drop the command</p>
    <p>R.ownerSID.Ts  MAX(R.ownerSID.Ts, updateSID.Ts)</p>
    <p>} ELSE {</p>
    <p>Respond to client with</p>
    <p>R.ownerSID.TX  MAX(R.ownerSID.TX, updateSID.TX)</p>
    <p>Enqueue and process the command</p>
    <p>}</p>
    <p>Status = BADSESSION</p>
    <p>R.ownerSID</p>
  </div>
  <div class="page">
    <p>Client node</p>
    <p>Enforcing session isolation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN annotation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>Guard logic at the storage controller: R.ownerSID = &lt;Ts, Tx&gt;</p>
    <p>IF (decision = ACCEPT) {</p>
    <p>Drop the command</p>
    <p>R.ownerSID.Ts  MAX(R.ownerSID.Ts, updateSID.Ts)</p>
    <p>} ELSE {</p>
    <p>Respond to client with</p>
    <p>R.ownerSID.TX  MAX(R.ownerSID.TX, updateSID.TX)</p>
    <p>Enqueue and process the command</p>
    <p>}</p>
    <p>ACCEPT disk cmd.</p>
    <p>Status = BADSESSION</p>
    <p>R.ownerSID</p>
  </div>
  <div class="page">
    <p>Client node</p>
    <p>Enforcing session isolation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN R</p>
    <p>Guard module</p>
    <p>R.ownerSID = &lt;Ts, Tx&gt; Guard logic at the storage controller:</p>
    <p>IF (decision = ACCEPT) {</p>
    <p>Drop the command</p>
    <p>R.ownerSID.Ts  MAX(R.ownerSID.Ts, updateSID.Ts)</p>
    <p>} ELSE {</p>
    <p>Respond to client with</p>
    <p>R.ownerSID.TX  MAX(R.ownerSID.TX, updateSID.TX)</p>
    <p>Enqueue and process the command</p>
    <p>Status = BADSESSION</p>
    <p>R.ownerSID</p>
    <p>}</p>
    <p>REJECT Status = BADSESSION</p>
    <p>R.ownerSID</p>
  </div>
  <div class="page">
    <p>Client node</p>
    <p>Enforcing session isolation</p>
    <p>R</p>
    <p>Guard module</p>
    <p>SAN R</p>
    <p>Guard module</p>
    <p>R.ownerSID = &lt;Ts, Tx&gt;</p>
    <p>REJECT Status = BADSESSION</p>
    <p>R.ownerSID</p>
    <p>Client node</p>
    <p>Upon command rejection:  Storage device responds to the client with a special status code</p>
    <p>(BADSESSION) and the most recent value of R.ownerSID.</p>
    <p>Application at the client node  Observes a failed disk request and forced lock revocation.</p>
    <p>Re-establishes its session to R under a new SID and retries.</p>
  </div>
  <div class="page">
    <p>The guard module addresses the safety problems arising from delayed disk request delivery and inconsistent failure observations.</p>
    <p>Enforcing safe ordering of requests at the storage device lessens the demands on the lock service.  Lock acquisition state need not be kept consistent at all</p>
    <p>times.</p>
    <p>Flexibility in the choice of mechanism for coordination.</p>
    <p>Assignment of session identifiers</p>
  </div>
  <div class="page">
    <p>Assignment of session identifiers</p>
    <p>Looselyconsistent</p>
    <p>Traditional DLM Enabled by Minuet</p>
    <p>Optimistic</p>
    <p>- Clients choose their SIDs independently and do not coordinate their choices.</p>
    <p>- Resilient to network partitions and massive node failures.</p>
    <p>- Performs well under low rates of resource contention.</p>
    <p>- Minimizes latency overhead of synchronization.</p>
    <p>Strong</p>
    <p>- Strict serialization of Lock/ Unlock requests.</p>
    <p>- Disk command rejection does not occur.</p>
    <p>- SIDs are assigned by a central lock manager.</p>
    <p>- Performs well under high rates of resource contention.</p>
  </div>
  <div class="page">
    <p>Supporting distributed transactions  Session isolation provides a building block for more</p>
    <p>complex and useful semantics.</p>
    <p>Serializable transactions can be supported by extending Minuet with ARIES-style logging and recovery facilities.</p>
    <p>Minuet guard logic:  Ensures safe access to the log and the snapshot during</p>
    <p>recovery.</p>
    <p>Enables the use of optimistic concurrency control, whereby conflicts are detected and resolved at commit time.</p>
    <p>(See paper for details)</p>
  </div>
  <div class="page">
    <p>Minuet implementation  We have implemented a proof-of-concept Linux-based</p>
    <p>prototype and several sample applications.</p>
    <p>iSCSI</p>
    <p>TCP/IP</p>
    <p>Storage cluster - Linux</p>
    <p>- iSCSI Enterprise Target [2]</p>
    <p>[2] http://iscsitarget.sourceforge.net/ [1] http://www.open-iscsi.org/</p>
    <p>Application cluster</p>
    <p>- Linux</p>
    <p>- Open-iSCSI initiator [1] - Minuet client library</p>
    <p>TCP/IP</p>
    <p>- Linux</p>
    <p>- Minuet lock manager process</p>
    <p>Lock manager</p>
  </div>
  <div class="page">
    <p>Sample applications</p>
    <p>Client performs a sequence of read-modify-write operations on randomly-selected blocks.</p>
    <p>Each operation is performed under the protection of an exclusive Minuet lock on the respective block.</p>
  </div>
  <div class="page">
    <p>Sample applications</p>
    <p>Transactional Insert, Delete, and Lookup operations.</p>
    <p>Client caches recently accessed tree blocks in local memory.</p>
    <p>Shared Minuet locks (and content of the block cache) are retained across transactions.</p>
    <p>With optimistic coordination, stale cache entries are detected and invalidated at transaction commit time.</p>
  </div>
  <div class="page">
    <p>Emulab deployment and evaluation  Experimental setup:</p>
    <p>32-node application cluster  850MHz Pentium III, 512MB DRAM, 7200 RPM IDE disk</p>
    <p>4-node storage cluster  3.0GHz 64-bit Xeon, 2GB DRAM, 10K RPM SCSI disk</p>
    <p>3 Minuet lock manager nodes  850MHz Pentium III, 512MB DRAM, 7200 RPM IDE disk</p>
    <p>100Mbps Ethernet</p>
  </div>
  <div class="page">
    <p>Emulab deployment and evaluation  Measure application performance with two methods</p>
    <p>of concurrency control:  Strong</p>
    <p>Application clients coordinate through one Minuet lock manager process that runs on a dedicated node.</p>
    <p>Traditional distributed locking.</p>
    <p>Weak-own  Each client process obtains locks from a local Minuet</p>
    <p>lock manager instance.  No direct inter-client coordination.  Optimistic technique enabled by our approach.</p>
  </div>
  <div class="page">
    <p>Parallel chunkmap: Uniform workload  250,000 data chunks striped across [1-4] storage nodes.</p>
    <p>8KB chunk size, 32 chunkmap client nodes</p>
    <p>Uniform workload: clients select chunks uniformly at random.</p>
  </div>
  <div class="page">
    <p>Parallel chunkmap: Hotspot workload  250,000 data chunks striped across 4 storage nodes.</p>
    <p>8KB chunk size, 32 chunkmap client nodes</p>
    <p>Hotspot(x) workload: x% of operations touch a hotspot region of the chunkmap.</p>
    <p>Hotspot size = 0.1% = 2MB.</p>
  </div>
  <div class="page">
    <p>Experiment 2: Parallel key-value store</p>
    <p>SmallTree</p>
    <p>Block size</p>
    <p>Fanout</p>
    <p>Depth</p>
    <p>Initial leaf occupancy</p>
    <p>Number of keys</p>
    <p>Total dataset size</p>
    <p>LargeTree</p>
  </div>
  <div class="page">
    <p>Experiment 2: Parallel key-value store  [1-4] storage nodes.</p>
    <p>32 application client nodes.</p>
    <p>Each client performs a series of random keyvalue insertions.</p>
  </div>
  <div class="page">
    <p>Challenges  Practical feasibility and barriers to adoption</p>
    <p>Extending storage arrays with guard logic</p>
    <p>Medatada storage overhead (table of ownerSIDs).</p>
    <p>SAN bandwidth overhead due to session annotations</p>
    <p>Changes to the programming model  Dealing with I/O command rejection and forced lock</p>
    <p>revocations</p>
  </div>
  <div class="page">
    <p>Related Work  Optimistic concurrency control (OCC) in database</p>
    <p>management systems.</p>
    <p>Device-based locking for shared-disk environments (Dlocks, Device Memory Export Protocol).</p>
    <p>Storage protocol mechanisms for failure fencing (SCSI-3 Persistent Reserve).</p>
    <p>New synchronization primitives for datacenter applications (Chubby, Zookeeper).</p>
  </div>
  <div class="page">
    <p>Summary  Minuet is a new synchronization primitive for clustered</p>
    <p>shared-disk applications and middleware.</p>
    <p>Augments shared storage devices with guard logic.</p>
    <p>Enables the use of OCC as an alternative to conservative locking.</p>
    <p>Guarantees data safety in the face of arbitrary asynchrony.  Unbounded network transfer delays</p>
    <p>Unbounded clock drift rates</p>
    <p>Improves application availability.  Resilience to large-scale node failures and network partitions</p>
  </div>
  <div class="page">
    <p>Thank you !</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Related Work  Optimistic concurrency control (OCC)</p>
    <p>Well-known technique from the database field.</p>
    <p>Minuet enables the use of OCC in clustered SAN applications as an alternative to conservative distributed locking.</p>
  </div>
  <div class="page">
    <p>Related Work  Device-based synchronization</p>
    <p>(Dlocks, Device Memory Export Protocol)  Minuet revisits this idea from a different angle; provides a</p>
    <p>more general primitive that supports both OCC and traditional locking.</p>
    <p>We extend storage devices with guard logic  a minimal functional component that enables both approaches.</p>
  </div>
  <div class="page">
    <p>Related Work  Storage protocol mechanisms for failure fencing</p>
    <p>(SCSI-3 Persistent Reserve)  PR prevents out-of-order delivery of delayed disk commands</p>
    <p>from (suspected) faulty nodes.</p>
    <p>Ensures safety but not availability in a partitioned network; Minuet provides both.</p>
  </div>
  <div class="page">
    <p>Related Work  New synchronization primitives for datacenter</p>
    <p>applications (Chubby, Zookeeper).  Minuet focuses on fine-grained synchronization for clustered</p>
    <p>SAN applications.</p>
    <p>Minuets session annotations are conceptually analogous to Chubbys lock sequencers.  We extend this mechanism to shared-exclusive locking.  Given the ability to reject out-of-order requests at the</p>
    <p>destination, global consistency on the state of locks and use of an agreement protocol may be more than necessary.</p>
    <p>Minuet attains improved availability by relaxing these consistency constraints.</p>
  </div>
  <div class="page">
    <p>Clustered SAN applications and services</p>
    <p>SAN</p>
    <p>Application cluster Disk drive arrays</p>
    <p>FCP, iSCSI,</p>
  </div>
  <div class="page">
    <p>Clustered SAN applications and services</p>
    <p>HBA</p>
    <p>OS</p>
    <p>Clustered storage middleware</p>
    <p>Application</p>
    <p>SAN</p>
    <p>File systems (Lustre, GFS, OCFS, GPFS)</p>
    <p>Relational databases (Oracle RAC)</p>
    <p>Hardware</p>
    <p>Block device driver</p>
    <p>FCP, iSCSI,</p>
    <p>Storage stack</p>
  </div>
  <div class="page">
    <p>Minuet implementation: application node</p>
    <p>SCSI disk driver drivers/scsi/sd.c</p>
    <p>SCSI mid level</p>
    <p>SCSI lower level Open-iSCSI initiator</p>
    <p>v.2.0-869.2</p>
    <p>User</p>
    <p>Linux kernel</p>
    <p>Block device driver</p>
    <p>iSCSI target</p>
    <p>Minuet lock manager</p>
    <p>TCP / IP</p>
    <p>iSCSI / TCP / IP</p>
    <p>Application</p>
    <p>Minuet client library</p>
    <p>SCSI disk driver drivers/scsi/sd.c</p>
    <p>SCSI mid level</p>
    <p>SCSI lower level Open-iSCSI initiator</p>
    <p>v.2.0-869.2</p>
    <p>User</p>
    <p>Linux kernel</p>
    <p>Block device driver</p>
    <p>iSCSI target</p>
    <p>Minuet lock manager</p>
    <p>TCP / IP</p>
    <p>iSCSI / TCP / IP</p>
  </div>
  <div class="page">
    <p>Minuet API</p>
    <p>MinuetUpgradeLock(resource_id, lock_mode);  MinuetDowngradeLock(resource_id, lock_mode);</p>
    <p>MinueDiskRead(lun_id, resource_id, start_sector, length, data_buf);  MinueDiskWrite(lun_id, resource_id, start_sector, length, data_buf);</p>
    <p>MinuetXactBegin();  MinuetXactLogUpdate(lun_id, resource_id, start_sector, length, data_buf);  MinuetXactCommit(readset_resource_ids[], writeset_resource_ids[]);  MinuetXactAbort();  MinuetXactMarkSynched();</p>
    <p>Lock service</p>
    <p>Remote disk I/O</p>
    <p>Transaction service</p>
  </div>
  <div class="page">
    <p>Experiment 2: B+ Tree</p>
  </div>
  <div class="page">
    <p>Five stages of a transaction (T): (see paper for details) 1) READ</p>
    <p>Acquire shared Minuet locks on T.ReadSet; Read these resources from shared disk.</p>
    <p>Acquire exclusive Minuet locks on the elements of T.WriteSet; Apply updates locally; Append description of updates to the log.</p>
    <p>Contact the storage devices to verify validity of all sessions in T and lock T.WriteSet in preparation for commit.</p>
    <p>Force-append a Commit record to the log.</p>
    <p>Supporting serializable transactions</p>
  </div>
  <div class="page">
    <p>Extensions to the storage stack:  Open-iSCSI Initiator on application nodes:</p>
    <p>Minuet session annotations are attached to outbound command PDUs using the Additional Header Segment (AHS) protocol feature of iSCSI.</p>
    <p>iSCSI Enterprise Target on storage nodes:  Guard logic (350 LoC; 2% increase in complexity).</p>
    <p>ownerSIDs are maintained in main memory using a hash table.</p>
    <p>Command rejection is signaled to the initiator via a Reject PDU.</p>
    <p>Minuet implementation</p>
  </div>
</Presentation>
