<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Zh-Ja Patent MT based on Syntactic Pre-ordering</p>
    <p>Chinese-to-Japanese Patent Machine Translation based on Syntactic Pre-ordering for WAT 2015</p>
    <p>Katsuhito Sudoh Masaaki Nagata NTT Communication Science Laboratories, Japan sudoh.katsuhito@lab.ntt.co.jp</p>
    <p>Abstract</p>
    <p>This paper presents our Chinese-toJapanese patent machine translation system for WAT 2015 (Group ID: ntt) that uses syntactic pre-ordering over Chinese dependency structures. A head word and its modifier words are reordered by hand-written rules or a learning-to-rank model. Our system outperforms baseline phrase-based machine translations and competes with baseline tree-to-string machine translations.</p>
    <p>Patent documents, which well-structured written documents that describe the technical details of inventions, are expected to have almost no semantic ambiguities caused by indirect or rhetorical expressions. Due to this aspect, patent documents are good candidates for literal translation, which most machine translation (MT) approaches aim to do.</p>
    <p>One technical challenge for patent machine translation is the complex syntactic structure of patent documents, which typically have long sentences that complicate MT reordering, especially for the word order in distant languages. Chinese and Japanese have similar word order in noun modifiers but different subject-verb-object order, requiring long distance reordering in translation. In this years WAT evaluation campaign (Nakazawa et al., 2015), we tackle long distance reordering by syntactic pre-ordering based on Chinese dependency structures (Sudoh et al., 2014) in a Chinese-to-Japanese patent translation task.</p>
    <p>Our system basically consists of three components: Chinese syntactic analysis (word segmentation, part-of-speech (POS) tagging, and dependency parsing) adapted to patent documents; dependency-based syntactic pre-ordering</p>
    <p>Chinese word segmentation and part-of-speech tagging</p>
    <p>Chinese dependency parsing</p>
    <p>Dependency-based syntactic pre-ordeirng</p>
    <p>Phrase-based statistical MT</p>
    <p>Chinese sentence</p>
    <p>Japanese sentence</p>
    <p>Supplied parallel text</p>
    <p>MT models</p>
    <p>Pre-ordering model</p>
    <p>Pre-ordering rules</p>
    <p>Chinese language resource (patent)</p>
    <p>Dep. models</p>
    <p>Word/POS models</p>
    <p>Chinese language resource (CTB)</p>
    <p>Figure 1: Brief workflow of our Chinese-toJapanese MT system. Gray resources are in-house ones.</p>
    <p>with hand-written rules or a learning-to-rank model; and a standard phrase-based statistical MT. This paper describes our systems details and discusses our evaluation results.</p>
    <p>Figure 1 shows a brief workflow of our Chineseto-Japanese MT system. Its basic architecture is standard with syntactic pre-ordering. Input sentences are first applied to word segmentation and POS tagging, parsed into dependency trees, reordered using pre-ordering rules or a pre-ordering model, and finally translated into Japanese by a phrase-based statistical MT.</p>
    <p>Word segmentation and POS tagging are solved jointly (Suzuki et al., 2012) for better Chinese word segmentation based on the POS tag sequences. The dependency parser produces un</p>
    <p>Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 9598, Kyoto, Japan, 16th October 2015.</p>
    <p>[Semi-supervised learning] (Suzuki+ 2009)</p>
    <p>~1% positive effect in RIBES/BLEU</p>
    <p>[Syntactic pre-ordering] (Rule-based)</p>
    <p>Modified Head Finalization (Han+ 2012) basically head-final w/ exceptions</p>
    <p>(Data-driven; learning-to-rank) Reorder head/modifier in dep. tree</p>
    <p>by RankingSVM (Yang+ 2012)</p>
    <p>~3-4% positive effect in RIBES ~2% positive effect in BLEU</p>
    <p>PBMT Data-driven Rulebased T2S</p>
    <p>RIBES 0.781 0.812 0.822 0.814</p>
    <p>Human n/a 8.00 16.25 20.75</p>
  </div>
</Presentation>
