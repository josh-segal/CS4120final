<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Semantics-based Distributed I/O with the ParaMEDIC</p>
    <p>Framework</p>
    <p>P. Balaji, W. Feng, H. Lin</p>
    <p>Math. and Computer Science, Argonne National Laboratory</p>
    <p>Computer Science and Engg., Virginia Tech Dept. of Computer Sci., North Carolina State University</p>
  </div>
  <div class="page">
    <p>Distributed Computation and I/O  Growth of combined compute and I/O requirements</p>
    <p>E.g., Genomic sequence search, Large-scale data mining, data visual analytics and communication profiling</p>
    <p>Commonality: Require a lot of compute power and use and generate a lot of data  Data has to be managed for later processing or archival</p>
    <p>Managing large data volumes: Distributed I/O  Non-local access to large compute systems</p>
    <p>Data generated remotely and transferred to local systems</p>
    <p>Resource locality: Applications need compute and storage  Data generated at one site and moved to another</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Distributed I/O: The Necessary Evil  Lot of prior research tries to improve distributed I/O</p>
    <p>Continues to be the elusive holy grail  Difficult to achieve high performance for real data [1]</p>
    <p>Bandwidth is not everything  Real software requires synchronization (milliseconds)</p>
    <p>High-speed TCP eats up memory  slows down applications</p>
    <p>Data encryption or endianness conversion required in some cases</p>
    <p>Not everyone has a lambda grid  Scientists run jobs on large centers from their local</p>
    <p>system</p>
    <p>There is just too much data!</p>
    <p>Solution: FEDEX !</p>
    <p>HPDC '08</p>
    <p>[1] Wide Area Filesystem Performance Using Lustre on the Teragrid, S. Simms, G. Pike, D. Balog. Teragrid Conference, 2007</p>
  </div>
  <div class="page">
    <p>Case Study: mpiBLAST on the TeraGrid</p>
    <p>HPDC '08</p>
    <p>I/O Time</p>
    <p>Compute Time</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>c u</p>
    <p>ti o</p>
    <p>n T</p>
    <p>im e</p>
    <p>( s</p>
    <p>e c</p>
    <p>)</p>
    <p>On a local-area network, mpiBLAST I/O time is less than 5%</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>ParaMEDIC: Framework to Decouple Compute</p>
    <p>and I/O</p>
    <p>Case Studies with mpiBLAST and MPE</p>
    <p>Experimental Results</p>
    <p>Glimpses of Follow-on Work</p>
    <p>Concluding Remarks</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>ParaMEDIC Overview  Parallel Meta-data Environment for Distributed I/O</p>
    <p>and Computing</p>
    <p>New way of programming distributed I/O  Application generates output data  ParaMEDIC takes over:</p>
    <p>Transforms output to (orders-of-magnitude smaller) application-specific meta-data at the compute site</p>
    <p>Transports meta-data over the WAN to the storage site  Transforms meta-data back to the original data at the</p>
    <p>storage site (host site for the global file-system)  Similar to compression, yet different</p>
    <p>Deals with data as abstract objects, not as a bytestream</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>The ParaMEDIC Framework</p>
    <p>HPDC '08</p>
    <p>Applications</p>
    <p>mpiBLAST Communication</p>
    <p>Profiling Remote</p>
    <p>Visualization</p>
    <p>ParaMEDIC Data Tools</p>
    <p>Data Encryption</p>
    <p>Data Integrity</p>
    <p>Communication Services</p>
    <p>Direct Network</p>
    <p>Global Filesystem</p>
    <p>Application Plugins</p>
    <p>mpiBLAST Plugin</p>
    <p>Communication Profiling Plugin</p>
    <p>Basic Compression</p>
    <p>ParaMEDIC API (PMAPI)</p>
    <p>Other Utilities</p>
    <p>Column Parsing</p>
    <p>Data Sorting</p>
  </div>
  <div class="page">
    <p>Tradeoffs in the ParaMEDIC Framework  Trading Computation and I/O</p>
    <p>More computation: Converting output to meta-data and back requires extra work</p>
    <p>Lesser I/O: Only meta-data is transferred over the WAN, so lesser bandwidth usage on the WAN</p>
    <p>But, computation is free; I/O is not !</p>
    <p>Trading Portability and Performance  Utility functions help develop application plugins, but</p>
    <p>will always need non-zero effort  Data is dealt has high-level objects: Better chance of</p>
    <p>improved performance</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>Case Studies with mpiBLAST and MPE</p>
    <p>Experimental Results</p>
    <p>Glimpses of Follow-on Work</p>
    <p>Concluding Remarks</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Sequence Search with mpiBLAST</p>
    <p>HPDC '08</p>
    <p>Query Sequences</p>
    <p>Database Sequences</p>
    <p>Output</p>
    <p>Sequential Search of Queries Parallel Search of Queries</p>
    <p>Query Sequences</p>
    <p>Database Sequences</p>
    <p>Output</p>
  </div>
  <div class="page">
    <p>mpiBLAST Meta-Data</p>
    <p>HPDC '08</p>
    <p>Query Sequences</p>
    <p>Database Sequences</p>
    <p>Output</p>
    <p>Alignment information for</p>
    <p>a bunch of sequences</p>
    <p>Alignment of two sequences is</p>
    <p>independent of the remaining sequences</p>
    <p>Meta-data (IDs of matched sequences)</p>
    <p>Communicate over the WAN</p>
    <p>Query Sequences</p>
    <p>Temporary Database</p>
    <p>Sequences</p>
  </div>
  <div class="page">
    <p>ParaMEDIC-powered mpiBLAST</p>
    <p>Compute Master I/O Master</p>
    <p>mpiBLAST Master</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Master</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>Query Raw Metadata Query</p>
    <p>Write Results</p>
    <p>Generate Temp Database</p>
    <p>Read Temp Database</p>
    <p>I/O Work e rs</p>
    <p>Com pute Work e rs</p>
    <p>I/O Se rve rs hos ting file</p>
    <p>s ys te m</p>
    <p>The ParaMEDIC Framework</p>
    <p>Compute Sites Storage SiteWAN</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>MPE: A Profiling Library for MPI</p>
    <p>MPE: MPI Profiling Environment  Suite of performance analysis tools and libraries  Shipped as a part of the MPICH2 implementation of</p>
    <p>MPI</p>
    <p>Relies on the MPI Profiling Interface  Application is run regularly, MPE automagically logs</p>
    <p>communication calls and time taken</p>
    <p>Generates lots of data  A large-scale application such as FLASH can generate</p>
    <p>about 2.5MB of data per second per process  A 16K process run for an hour generates 150 TB of</p>
    <p>data</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Example MPE Profiling Log (GROMACS)</p>
    <p>HPDC '08</p>
    <p>Identify periodicity using Fourier transforms and only store the diffs in each period Can give about 3-5X improvement</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>Case Studies with mpiBLAST and MPE</p>
    <p>Experimental Results</p>
    <p>Glimpses of Follow-on Work</p>
    <p>Concluding Remarks</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>LAN Emulating a 10Gbps WAN</p>
    <p>HPDC '08</p>
    <p>mpiBLAST ParaMEDIC</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>c u</p>
    <p>tio n</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c )</p>
    <p>Number of Requested Sequences</p>
    <p>E xe</p>
    <p>c u</p>
    <p>tio n</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c )</p>
  </div>
  <div class="page">
    <p>Performance on Real Systems</p>
    <p>HPDC '08</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>c u</p>
    <p>tio n</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c )</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>c u</p>
    <p>tio n</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c )</p>
  </div>
  <div class="page">
    <p>Performance Breakup on the TeraGrid</p>
    <p>HPDC '08</p>
    <p>I/O Time</p>
    <p>Compute Time</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>c u</p>
    <p>tio n</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c )</p>
    <p>Post-processing Time + I/O Time Compute Time</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>c u</p>
    <p>tio n</p>
    <p>t im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c )</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>Case Studies with mpiBLAST and MPE</p>
    <p>Experimental Results</p>
    <p>Glimpses of Follow-on Work</p>
    <p>Concluding Remarks</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Evaluation on a Worldwide Supercomputer</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
    <p>Absolute Time</p>
    <p>Number of Query Sequence Sets</p>
    <p>I/ O</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>s )</p>
    <p>Number of Query Sequence Sets</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Microbial Genome Database Search</p>
    <p>Semantic-aware metadata gives scientists 2.5*1014 searches at their finger-tips  All metadata results from all searches can fit on iPod</p>
    <p>Nano  Semantically compressed 1 PB into 4 GB (106X)</p>
    <p>Usual compression results in 1 PB into 300 TB (3X)</p>
    <p>Semantic Compression</p>
    <p>HPDC '08</p>
    <p>ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing, P. Balaji, W. Feng, J. Archuleta and H. Lin. Storage Challenge Award, SC 2007.</p>
    <p>Distributed I/O with ParaMEDIC: Experiences with a Worldwide Supercomputer, P. Balaji, W. Feng, H. Lin, J. Archuleta, S. Matsuoka, A. Warren, J. Setubal, E. Lusk, R. Thakur, I. Foster, D. S. Katz, S. Jha, K. Shinpaugh, S. Coghlan and D. Reed. Best Paper Award, ISC 2008.</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>Case Studies with mpiBLAST and MPE</p>
    <p>Experimental Results</p>
    <p>Glimpses of Follow-on Work</p>
    <p>Concluding Remarks</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Concluding Remarks  Distributed I/O is a necessary evil</p>
    <p>Difficult to get high performance for real data</p>
    <p>Traditional approaches deal with data as a stream</p>
    <p>of bytes (allows for portability across any type of</p>
    <p>data)</p>
    <p>We propose ParaMEDIC</p>
    <p>Semantics-based meta-data transformation of data</p>
    <p>Trade Portability for Performance</p>
    <p>Evaluated on emulated and real systems</p>
    <p>Order-of-magnitude benefits in performance</p>
    <p>HPDC '08</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Contact Information</p>
    <p>Email: balaji@mcs.anl.gov Web: http://www.mcs.anl.gov/~balaji</p>
  </div>
</Presentation>
