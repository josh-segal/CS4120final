<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Young Jin Yu, Dong In Shin, Woong Shin, Nae Young Song,</p>
    <p>Hyeonsang Eom, and Heon Young Yeom</p>
    <p>Seoul National University,  Taejin Infotech 2012. 06. 14. Young Jin Yu</p>
    <p>HOTSTORAGE12</p>
  </div>
  <div class="page">
    <p>I/O demand is very high.  Social Network Services  Cloud Platform  Desktop users</p>
    <p>Storage system has suffered from small random I/O accesses  Random throughput of a disk &lt; 1 MB/s</p>
    <p>Fortunately, Fast Storage Devices are coming.</p>
    <p>Access Mechanism: Magnetics  Electronics  Low-latency  Good for random I/O performance  Flash-SSD, DRAM-SSD, PCM-SSD,</p>
  </div>
  <div class="page">
    <p>SSD</p>
    <p>OSs Storage Stack</p>
    <p>Application</p>
    <p>What matters us ! (Application throughput)</p>
    <p>What vendors give to us (Device throughput)</p>
    <p>Different Performance Number!</p>
  </div>
  <div class="page">
    <p>SSD</p>
    <p>OSs Storage Stack</p>
    <p>Application</p>
    <p>Synchronous I/O Path</p>
    <p>MSST10, High Perf. SSD . MICRO10, Moneta:</p>
    <p>HotStorage11, Onyx:  FAST12, When Poll is better</p>
    <p>(e.g. I/O scheduler, SoftIRQ handler)</p>
    <p>We will call it Sync+Poll</p>
  </div>
  <div class="page">
    <p>DRAM-SSD (provided by TAEJIN Infotech)</p>
    <p>7 usecs for reading/writing a 4KB page</p>
    <p>Peak device throughput: 700 MB/s</p>
    <p>DDR2 64 GB, PCI-Express type</p>
  </div>
  <div class="page">
    <p>poll</p>
    <p>No kblockd No SoftIRQ</p>
  </div>
  <div class="page">
    <p>Peak Device Throughput</p>
    <p>Performance Wall (75000 IOPS13us/4KB)</p>
    <p>Cant merge writes under Sync+Poll</p>
  </div>
  <div class="page">
    <p>Lesson  Large data transfer is still important !</p>
    <p>How to make a large request ?</p>
    <p>Read-ahead under sequential read pattern</p>
    <p>Still effective on (Sync+Poll)</p>
    <p>Request merge under sequential write pattern  (Sync+Poll) cannot accumulate I/O requests</p>
    <p>No way to make a large request under random</p>
    <p>access pattern !</p>
  </div>
  <div class="page">
    <p>High Throughput</p>
    <p>Minimize Per-Request</p>
    <p>Latency</p>
    <p>Mitigate Per-Request</p>
    <p>Latency</p>
  </div>
  <div class="page">
    <p>Temporal Merge</p>
    <p>Combines multiple (even non-sequential) requests within a short time window, and</p>
    <p>Dispatches them by using a new I/O interface</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Each thread submits a block request.</p>
    <p>Only one thread becomes a winner.</p>
    <p>The winner combines concurrent block requests into one and dispatches it by using the new interface.</p>
    <p>The losing threads yield CPU and sleep until the completion of their requests.</p>
    <p>Synchronous Temporal Merge  No plugging/unplugging is</p>
    <p>required during merge operation.</p>
  </div>
  <div class="page">
    <p>Advantage  Balance of Synchronous I/O path and Batching  Low-latency (No sleep/wakeup for a winner)</p>
    <p>High-throughput (Oblivious to block access pattern)</p>
    <p>Disadvantage  Merge Count (i.e. Benefit) is limited by Concurrency.  Concurrency: the maximum number of threads entering into</p>
    <p>I/O subsystem</p>
    <p>Due to delayed write semantics, the concurrency is usually lower than the number of user threads that issued write requests.</p>
  </div>
  <div class="page">
    <p>How to merge I/O requests even when the number of I/O threads is very low?  Utilize I/O scheduler again,</p>
    <p>But this time, do it with the extended I/O interface</p>
    <p>The result would depend on tradeoff betn  The advantage of large data transfer</p>
    <p>The disadvantage of increased latency</p>
  </div>
  <div class="page">
    <p>Each thread piles up I/O requests in a request queue.</p>
    <p>kblockd or user process 1) fetches all the block requests, 2) merges them, 3) dispatches the merged request</p>
    <p>Cache-friendly request retirement by using SoftIRQ (instead of Inter-ProcessorInterrupt used in MSST10)</p>
    <p>Tune a few parameters</p>
    <p>unplug_thresh, scheduler,</p>
    <p>Asynchronous Temporal Merge</p>
    <p>Use plugging/unplugging</p>
    <p>Effective even when the concurrency is low</p>
  </div>
  <div class="page">
    <p>Advantage</p>
    <p>It could maximize the accumulation of block requests in a queue when the concurrency is low.</p>
    <p>Disadvantage</p>
    <p>Existing I/O schedulers (in Linux) are not designed to accumulate read requests.  If a device is idle, a newly-arriving read request is</p>
    <p>immediately dispatched by an unplug invocation with holding a queuelock spinlock.</p>
  </div>
  <div class="page">
    <p>Environment</p>
    <p>CPU: 8 Cores (Xeon E5630@2.5GHz)</p>
    <p>RAM: 2 GB (out of 16 GB) is used.</p>
    <p>I/O subsystems (see next slides)</p>
    <p>Async+Intr, Sync+Poll, STM+Poll, ATM+Poll</p>
    <p>Benchmarks</p>
    <p>Iozone, Postmark</p>
  </div>
  <div class="page">
    <p>Typical Storage Stack</p>
    <p>File System</p>
    <p>Generic Block Layer</p>
    <p>SCSI Subsystem</p>
    <p>Upper level</p>
    <p>Mid level</p>
    <p>Lower level</p>
    <p>SSD</p>
    <p>Application</p>
    <p>I/O Scheduler</p>
    <p>Async+Intr</p>
    <p>Customize this layer to Translate SCSI-command into Device-specific command.</p>
  </div>
  <div class="page">
    <p>Typical Storage Stack</p>
    <p>File System</p>
    <p>Generic Block Layer</p>
    <p>SCSI Subsystem</p>
    <p>Upper level</p>
    <p>Mid level</p>
    <p>Lower level</p>
    <p>SSD</p>
    <p>Application</p>
    <p>I/O Scheduler</p>
    <p>STM</p>
    <p>ATM</p>
    <p>Sync</p>
  </div>
  <div class="page">
    <p>Peak Device Throughput</p>
  </div>
  <div class="page">
    <p>STM achieves 85%~100% of the peak device throughput.  ATM achieves 95%~100% of the peak device</p>
    <p>throughput except for the Random-Read access pattern.</p>
    <p>Seq.R Seq.W Rand.R Rand.W</p>
    <p>Async+Intr 82% 68% 22% 28%</p>
    <p>Sync+Poll 93% 44% 46% 45%</p>
    <p>STM+Poll 100% 85% 88% 92%</p>
    <p>ATM+Poll 95% 100% 43% 96%</p>
  </div>
  <div class="page">
    <p>Peak Device Throughput</p>
  </div>
  <div class="page">
    <p>Temporal Merge  Enables I/O subsystem to dispatch discontiguous</p>
    <p>block requests by using an extended I/O interface  Helps to achieve near-peak device throughput from</p>
    <p>random access workload</p>
    <p>Future works  Real-world benchmarks  Standardization issue (NVMHCI specification)  Reliability issue (atomic update)  Parallelism across devices (RAID)  Parallelism across servers (Networked Storage)</p>
  </div>
</Presentation>
