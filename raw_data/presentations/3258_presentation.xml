<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Benchmarking Sparse Matrix-Vector Multiply</p>
    <p>In 5 Minutes</p>
    <p>Benchmarking Sparse Matrix-Vector Multiply</p>
    <p>In 5 Minutes</p>
    <p>Hormozd Gahvari, Mark Hoemmen, James Demmel, and Kathy Yelick</p>
    <p>January 21, 2007</p>
    <p>Hormozd Gahvari, Mark Hoemmen, James Demmel, and Kathy Yelick</p>
    <p>January 21, 2007</p>
  </div>
  <div class="page">
    <p>OutlineOutline</p>
    <p>What is Sparse Matrix-Vector Multiply (SpMV)? Why benchmark it?</p>
    <p>How to benchmark it? Past approaches Our approach</p>
    <p>Results Conclusions and directions for future</p>
    <p>work</p>
    <p>What is Sparse Matrix-Vector Multiply (SpMV)? Why benchmark it?</p>
    <p>How to benchmark it? Past approaches Our approach</p>
    <p>Results Conclusions and directions for future</p>
    <p>work</p>
  </div>
  <div class="page">
    <p>SpMVSpMV</p>
    <p>Sparse Matrix-(dense)Vector Multiply Multiply a dense vector by a sparse matrix</p>
    <p>(one whose entries are mostly zeroes) Why do we need a benchmark?</p>
    <p>SpMV is an important kernel in scientific computation</p>
    <p>Vendors need to know how well their machines perform it</p>
    <p>Consumers need to know which machines to buy</p>
    <p>Existing benchmarks do a poor job of approximating SpMV</p>
    <p>Sparse Matrix-(dense)Vector Multiply Multiply a dense vector by a sparse matrix</p>
    <p>(one whose entries are mostly zeroes) Why do we need a benchmark?</p>
    <p>SpMV is an important kernel in scientific computation</p>
    <p>Vendors need to know how well their machines perform it</p>
    <p>Consumers need to know which machines to buy</p>
    <p>Existing benchmarks do a poor job of approximating SpMV</p>
  </div>
  <div class="page">
    <p>Existing BenchmarksExisting Benchmarks</p>
    <p>The most widely used method for ranking computers is still the LINPACK benchmark, used exclusively by the Top 500 supercomputer list</p>
    <p>Benchmark suites like the High Performance Computing Challenge (HPCC) Suite seek to change this by including other benchmarks</p>
    <p>Even the benchmarks in HPCC do not model SpMV however</p>
    <p>This work is proposed for inclusion into the HPCC suite</p>
    <p>The most widely used method for ranking computers is still the LINPACK benchmark, used exclusively by the Top 500 supercomputer list</p>
    <p>Benchmark suites like the High Performance Computing Challenge (HPCC) Suite seek to change this by including other benchmarks</p>
    <p>Even the benchmarks in HPCC do not model SpMV however</p>
    <p>This work is proposed for inclusion into the HPCC suite</p>
  </div>
  <div class="page">
    <p>Benchmarking SpMV is hard!Benchmarking SpMV is hard!</p>
    <p>Issues to consider: Matrix formats Memory access patterns Performance optimizations and why</p>
    <p>we need to benchmark them</p>
    <p>Preexisting benchmarks that perform SpMV do not take all of this into account</p>
    <p>Issues to consider: Matrix formats Memory access patterns Performance optimizations and why</p>
    <p>we need to benchmark them</p>
    <p>Preexisting benchmarks that perform SpMV do not take all of this into account</p>
  </div>
  <div class="page">
    <p>Matrix FormatsMatrix Formats</p>
    <p>We store only the nonzero entries in sparse matrices</p>
    <p>This leads to multiple ways of storing the data, based on how we index it Coordinate, CSR, CSC, ELLPACK,</p>
    <p>Use Compressed Sparse Row (CSR) as our baseline format as it provides best overall unoptimized performance across many architectures</p>
    <p>We store only the nonzero entries in sparse matrices</p>
    <p>This leads to multiple ways of storing the data, based on how we index it Coordinate, CSR, CSC, ELLPACK,</p>
    <p>Use Compressed Sparse Row (CSR) as our baseline format as it provides best overall unoptimized performance across many architectures</p>
  </div>
  <div class="page">
    <p>CSR SpMV ExampleCSR SpMV Example</p>
    <p>(M,N) = (4,5)</p>
    <p>NNZ = 8</p>
    <p>row_start:</p>
    <p>(0,2,4,6,8)</p>
    <p>col_idx:</p>
    <p>(0,1,0,2,1,3,2,4)</p>
    <p>values:</p>
    <p>(1,2,3,4,5,6,7,8)</p>
  </div>
  <div class="page">
    <p>Memory Access PatternsMemory Access Patterns</p>
    <p>Unlike dense case, memory access patterns differ for matrix and vector elements  Matrix elements: unit stride  Vector elements: indirect access for the source vector</p>
    <p>(the one multiplied by the matrix)  This leads us to propose three categories for</p>
    <p>SpMV problems:  Small: everything fits in cache  Medium: source vector fits in cache, matrix does not  Large: source vector does not fit in cache</p>
    <p>These categories will exercise the memory hierarchy differently and so may perform differently</p>
    <p>Unlike dense case, memory access patterns differ for matrix and vector elements  Matrix elements: unit stride  Vector elements: indirect access for the source vector</p>
    <p>(the one multiplied by the matrix)  This leads us to propose three categories for</p>
    <p>SpMV problems:  Small: everything fits in cache  Medium: source vector fits in cache, matrix does not  Large: source vector does not fit in cache</p>
    <p>These categories will exercise the memory hierarchy differently and so may perform differently</p>
  </div>
  <div class="page">
    <p>Examples from Three Platforms</p>
    <p>Examples from Three Platforms</p>
    <p>Intel Pentium 4 2.4 GHz 512 KB cache</p>
    <p>Intel Itanium 2 1 GHz 3 MB cache</p>
    <p>AMD Opteron 1.4 GHz 1 MB cache</p>
    <p>Intel Pentium 4 2.4 GHz 512 KB cache</p>
    <p>Intel Itanium 2 1 GHz 3 MB cache</p>
    <p>AMD Opteron 1.4 GHz 1 MB cache</p>
    <p>Data collected using a test suite of 275 matrices taken from the University of Florida Sparse Matrix Collection</p>
    <p>Performance is graphed vs. problem size</p>
    <p>Data collected using a test suite of 275 matrices taken from the University of Florida Sparse Matrix Collection</p>
    <p>Performance is graphed vs. problem size</p>
  </div>
  <div class="page">
    <p>horizontal axis = matrix dimension or vector length</p>
    <p>vertical axis = density in nnz/row</p>
    <p>colored dots represent unoptimized performance of real matrices</p>
  </div>
  <div class="page">
    <p>Performance Optimizations Performance Optimizations</p>
    <p>Many different optimizations possible  One family of optimizations involves blocking the matrix to</p>
    <p>improve reuse at a particular level of the memory hierarchy  Register blocking - very often useful  Cache blocking - not as useful</p>
    <p>Which optimizations to use?  HPCC framework allows significant optimization by the user - we</p>
    <p>dont want to go as far  Automatic tuning at runtime permits a reasonable comparison of</p>
    <p>architectures, by trying the same optimizations on each one  We will use only the register-blocking optimization (BCSR), which</p>
    <p>is implemented in the OSKI automatic tuning system for sparse matrix kernels developed at Berkeley</p>
    <p>Prior research has found register blocking to be applicable to a number of real-world matrices, particularly ones from finite element applications</p>
    <p>Many different optimizations possible  One family of optimizations involves blocking the matrix to</p>
    <p>improve reuse at a particular level of the memory hierarchy  Register blocking - very often useful  Cache blocking - not as useful</p>
    <p>Which optimizations to use?  HPCC framework allows significant optimization by the user - we</p>
    <p>dont want to go as far  Automatic tuning at runtime permits a reasonable comparison of</p>
    <p>architectures, by trying the same optimizations on each one  We will use only the register-blocking optimization (BCSR), which</p>
    <p>is implemented in the OSKI automatic tuning system for sparse matrix kernels developed at Berkeley</p>
    <p>Prior research has found register blocking to be applicable to a number of real-world matrices, particularly ones from finite element applications</p>
  </div>
  <div class="page">
    <p>Both unoptimized and optimized SpMV matter Both unoptimized and</p>
    <p>optimized SpMV matter Why we need to measure optimized SpMV:</p>
    <p>Some platforms benefit more from performance tuning than others</p>
    <p>In the case of the tested platforms, Itanium 2 and Opteron gain vs. P4 when we tune using OSKI</p>
    <p>Why we need to measure unoptimized SpMV:  Some SpMV problems are more resistant to optimization  To be effective, register blocking needs a matrix with a</p>
    <p>dense block structure  Not all sparse matrices have one</p>
    <p>Graphs on next slide illustrate this</p>
    <p>Why we need to measure optimized SpMV:  Some platforms benefit more from performance tuning than</p>
    <p>others  In the case of the tested platforms, Itanium 2 and Opteron</p>
    <p>gain vs. P4 when we tune using OSKI</p>
    <p>Why we need to measure unoptimized SpMV:  Some SpMV problems are more resistant to optimization  To be effective, register blocking needs a matrix with a</p>
    <p>dense block structure  Not all sparse matrices have one</p>
    <p>Graphs on next slide illustrate this</p>
  </div>
  <div class="page">
    <p>horizontal axis = matrix dimension or vector length</p>
    <p>vertical axis = density in nnz/row</p>
    <p>blank dots represent real matrices that OSKI could not tune due to lack of a dense block structure</p>
    <p>colored dots represent speedups obtained by OSKIs tuning</p>
  </div>
  <div class="page">
    <p>So what do we do?So what do we do?</p>
    <p>We have a large search space of matrices to examine</p>
    <p>We could just do lots of SpMV on real-world matrices. However  Its not portable. Several GB to store and transport. Our</p>
    <p>test suite takes up 8.34 GB of space  Appropriate set of matrices is always changing as</p>
    <p>machines grow larger</p>
    <p>Instead, we can randomly generate sparse matrices that mirror real-world matrices by matching certain properties of these matrices</p>
    <p>We have a large search space of matrices to examine</p>
    <p>We could just do lots of SpMV on real-world matrices. However  Its not portable. Several GB to store and transport. Our</p>
    <p>test suite takes up 8.34 GB of space  Appropriate set of matrices is always changing as</p>
    <p>machines grow larger</p>
    <p>Instead, we can randomly generate sparse matrices that mirror real-world matrices by matching certain properties of these matrices</p>
  </div>
  <div class="page">
    <p>Matching Real Matrices With Synthetic Ones</p>
    <p>Matching Real Matrices With Synthetic Ones</p>
    <p>Randomly generated matrices for each of 275 matrices taken from the Florida collection</p>
    <p>Matched real matrices in dimension, density (measured in NNZ/row), blocksize, and distribution of nonzero entries</p>
    <p>Nonzero distribution was measured for each matrix by looking at what fraction of nonzero entries are in bands a certain percentage away from the main diagonal</p>
    <p>Randomly generated matrices for each of 275 matrices taken from the Florida collection</p>
    <p>Matched real matrices in dimension, density (measured in NNZ/row), blocksize, and distribution of nonzero entries</p>
    <p>Nonzero distribution was measured for each matrix by looking at what fraction of nonzero entries are in bands a certain percentage away from the main diagonal</p>
  </div>
  <div class="page">
    <p>Band Distribution Illustration</p>
    <p>Band Distribution Illustration</p>
    <p>What proportion of the nonzero entries fall into each of these bands 1-5?</p>
    <p>We use 10 bands instead of 5, but have shown 5 for simplicity.</p>
  </div>
  <div class="page">
    <p>In these graphs, real matrices are denoted by a red R, and synthetic matrices by a green S. Real matrices are connected by a line whose color indicates which matrix was faster to the synthetic matrices created to approximate them.</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Remaining IssuesRemaining Issues  Weve found a reasonable way to model real</p>
    <p>matrices, but benchmark suites want less output. HPCC requires its benchmarks to report only a few numbers, preferably just one</p>
    <p>Challenges in getting there  As weve seen, SpMV performance depends greatly on</p>
    <p>the matrix, and there is a large range of problem sizes. How do we capture this all? Stats on Florida matrices:</p>
    <p>Dimension ranges from a few hundred to over a million NNZ/row ranges from 1 to a few hundred</p>
    <p>How to capture performance of matrices with small dense blocks that benefit from register blocking?</p>
    <p>What well do:  Bound the set of synthetic matrices we generate  Determine which numbers to report that we feel capture</p>
    <p>the data best</p>
    <p>Weve found a reasonable way to model real matrices, but benchmark suites want less output. HPCC requires its benchmarks to report only a few numbers, preferably just one</p>
    <p>Challenges in getting there  As weve seen, SpMV performance depends greatly on</p>
    <p>the matrix, and there is a large range of problem sizes. How do we capture this all? Stats on Florida matrices:</p>
    <p>Dimension ranges from a few hundred to over a million NNZ/row ranges from 1 to a few hundred</p>
    <p>How to capture performance of matrices with small dense blocks that benefit from register blocking?</p>
    <p>What well do:  Bound the set of synthetic matrices we generate  Determine which numbers to report that we feel capture</p>
    <p>the data best</p>
  </div>
  <div class="page">
    <p>Bounding the Benchmark SetBounding the Benchmark Set  Limit to square matrices  Look over only a certain range of problem dimensions</p>
    <p>and NNZ/row  Since dimension range is so huge, restrict dimension to</p>
    <p>powers of 2  Limit blocksizes tested to ones in {1,2,3,4,6,8} x</p>
    <p>{1,2,3,4,6,8}  These were the most common ones encountered in prior</p>
    <p>research with matrices that mostly had dense block structures</p>
    <p>Here are the limits based on the matrix test suite:  Dimension &lt;= 2^20 (a little over one million)  24 &lt;= NNZ/row &lt;= 34 (avg. NNZ/row for real matrix test</p>
    <p>suite is 29)  Generate matrices with nonzero entries distributed</p>
    <p>(band distribution) based on statistics for the test suite as a whole</p>
    <p>Limit to square matrices  Look over only a certain range of problem dimensions</p>
    <p>and NNZ/row  Since dimension range is so huge, restrict dimension to</p>
    <p>powers of 2  Limit blocksizes tested to ones in {1,2,3,4,6,8} x</p>
    <p>{1,2,3,4,6,8}  These were the most common ones encountered in prior</p>
    <p>research with matrices that mostly had dense block structures</p>
    <p>Here are the limits based on the matrix test suite:  Dimension &lt;= 2^20 (a little over one million)  24 &lt;= NNZ/row &lt;= 34 (avg. NNZ/row for real matrix test</p>
    <p>suite is 29)  Generate matrices with nonzero entries distributed</p>
    <p>(band distribution) based on statistics for the test suite as a whole</p>
  </div>
  <div class="page">
    <p>Condensing the DataCondensing the Data</p>
    <p>This is a lot of data  11 x 12 x 36 = 4752 matrices to run</p>
    <p>Tuned and untuned cases are separated, as they highlight differences between platforms  Untuned data will only come from unblocked matrices  Tuned data will come from the remaining (blocked)</p>
    <p>matrices</p>
    <p>In each case (blocked and unblocked), report the maximum and median MFLOP rates to capture small/medium/large behavior</p>
    <p>When forced to report one number, report the blocked median</p>
    <p>This is a lot of data  11 x 12 x 36 = 4752 matrices to run</p>
    <p>Tuned and untuned cases are separated, as they highlight differences between platforms  Untuned data will only come from unblocked matrices  Tuned data will come from the remaining (blocked)</p>
    <p>matrices</p>
    <p>In each case (blocked and unblocked), report the maximum and median MFLOP rates to capture small/medium/large behavior</p>
    <p>When forced to report one number, report the blocked median</p>
  </div>
  <div class="page">
    <p>OutputOutput</p>
    <p>Unblocked Blocked Max Median Max Median</p>
    <p>Pentium 4 699 307 1961 530 Itanium 2 443 343 2177 753 Opteron 396 170 1178 273</p>
    <p>(all numbers MFLOP/s)</p>
    <p>Unblocked Blocked Max Median Max Median</p>
    <p>Pentium 4 699 307 1961 530 Itanium 2 443 343 2177 753 Opteron 396 170 1178 273</p>
    <p>(all numbers MFLOP/s)</p>
  </div>
  <div class="page">
    <p>How well does the benchmark approximate real SpMV performance?</p>
    <p>These graphs show the benchmark numbers as horizontal lines versus the real matrices which are denoted by circles.</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>OutputOutput</p>
    <p>Matrices generated by the benchmark fall into small/medium/large categories as follows:</p>
    <p>Matrices generated by the benchmark fall into small/medium/large categories as follows:</p>
    <p>Pentium 4 Itanium 2 Opteron</p>
    <p>Small 17% 33% 23%</p>
    <p>Medium 42% 50% 44%</p>
    <p>Large 42% 17% 33%</p>
  </div>
  <div class="page">
    <p>One More ProblemOne More Problem</p>
    <p>Takes too long to run: Pentium 4: 150 minutes Itanium 2: 128 minutes Opteron: 149 minutes</p>
    <p>How to cut down on this? HPCC would like our benchmark to run in 5 minutes</p>
    <p>Takes too long to run: Pentium 4: 150 minutes Itanium 2: 128 minutes Opteron: 149 minutes</p>
    <p>How to cut down on this? HPCC would like our benchmark to run in 5 minutes</p>
  </div>
  <div class="page">
    <p>Test fewer problem dimensions The largest ones do not give any extra</p>
    <p>information Test fewer NNZ/row</p>
    <p>Once dimension gets large enough, small variations in NNZ/row have little effect</p>
    <p>These decisions are all made by a runtime estimation algorithm</p>
    <p>Benchmark SpMV data supports this</p>
    <p>Test fewer problem dimensions The largest ones do not give any extra</p>
    <p>information Test fewer NNZ/row</p>
    <p>Once dimension gets large enough, small variations in NNZ/row have little effect</p>
    <p>These decisions are all made by a runtime estimation algorithm</p>
    <p>Benchmark SpMV data supports this</p>
    <p>Cutting RuntimeCutting Runtime</p>
  </div>
  <div class="page">
    <p>Sample graphs of benchmark SpMV for 1x1 and 3x3 blocked matrices</p>
  </div>
  <div class="page">
    <p>Output ComparisonOutput Comparison</p>
    <p>Unblocked Blocked Max Median Max Median</p>
    <p>Pentium 4 692 362 1937 555 (699) (307) (1961) (530)</p>
    <p>Itanium 2 442 343 2181 803 (443) (343) (2177) (753)</p>
    <p>Opteron 394 188 1178 286 (396) (170) (1178) (273)</p>
    <p>Unblocked Blocked Max Median Max Median</p>
    <p>Pentium 4 692 362 1937 555 (699) (307) (1961) (530)</p>
    <p>Itanium 2 442 343 2181 803 (443) (343) (2177) (753)</p>
    <p>Opteron 394 188 1178 286 (396) (170) (1178) (273)</p>
  </div>
  <div class="page">
    <p>Runtime ComparisonRuntime Comparison</p>
    <p>Full Shortened</p>
    <p>Pentium 4 150 min 3 min</p>
    <p>Itanium 2 128 min 3 min</p>
    <p>Opteron 149 min 3 min</p>
    <p>Full Shortened</p>
    <p>Pentium 4 150 min 3 min</p>
    <p>Itanium 2 128 min 3 min</p>
    <p>Opteron 149 min 3 min</p>
  </div>
  <div class="page">
    <p>Conclusions and Directions for the Future</p>
    <p>Conclusions and Directions for the Future</p>
    <p>SpMV is hard to benchmark because performance varies greatly depending on the matrix</p>
    <p>Carefully chosen synthetic matrices can be used to approximate SpMV</p>
    <p>A benchmark that reports one number and runs quickly is harder, but we can do reasonably well by looking at the median</p>
    <p>In the future:  Tighter maximum numbers  Parallel version</p>
    <p>Software available at http://bebop.cs.berkeley.edu</p>
    <p>SpMV is hard to benchmark because performance varies greatly depending on the matrix</p>
    <p>Carefully chosen synthetic matrices can be used to approximate SpMV</p>
    <p>A benchmark that reports one number and runs quickly is harder, but we can do reasonably well by looking at the median</p>
    <p>In the future:  Tighter maximum numbers  Parallel version</p>
    <p>Software available at http://bebop.cs.berkeley.edu</p>
  </div>
</Presentation>
