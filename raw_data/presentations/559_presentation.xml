<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Markov Network Structure with Decision Trees</p>
    <p>Daniel Lowd University of Oregon</p>
    <p>&lt;lowd@cs.uoregon.edu&gt;</p>
    <p>Jesse Davis Katholieke Universiteit Leuven &lt;jesse.davis@cs.kuleuven.be&gt;</p>
    <p>Joint work with:</p>
  </div>
  <div class="page">
    <p>Problem Definition</p>
    <p>Applications: Diagnosis, prediction, recommendations, and much more!</p>
    <p>Smoke</p>
    <p>Wheeze Asthma</p>
    <p>Cancer</p>
    <p>Flu</p>
    <p>F W A S C T T T F F F F T T F F T T F F T F F T T F F F T T</p>
    <p>Training DataTraining Data Markov Network StructureMarkov Network Structure</p>
    <p>P(F,W,A,S,C)</p>
    <p>SLOW</p>
    <p>SEARCH</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>Result: Similar accuracy, orders of magnitude faster!</p>
    <p>S</p>
    <p>W A</p>
    <p>C</p>
    <p>F</p>
    <p>F W A S C</p>
    <p>T T T F F</p>
    <p>F F T T F</p>
    <p>F T T F F</p>
    <p>T F F T T</p>
    <p>F F F T T</p>
    <p>Training DataTraining Data Markov Network StructureMarkov Network Structure</p>
    <p>P(F,W,A,S,C)</p>
    <p>F=?F=?</p>
    <p>S=?S=?</p>
    <p>P(C|F,S)</p>
    <p>Decision TreesDecision Trees</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background  Markov networks  Weight learning  Structure learning</p>
    <p>DTSL: Decision Tree Structure Learning  Experiments</p>
  </div>
  <div class="page">
    <p>Markov Networks: Representation</p>
    <p>Smoke</p>
    <p>Wheeze Asthma</p>
    <p>Cancer</p>
    <p>Flu</p>
    <p>(aka Markov random fields, Gibbs distributions, log-linear models, exponential models, maximum entropy models)</p>
    <p>Smoke  Cancer</p>
    <p>Feature iWeight of Feature i</p>
    <p>P(x) = exp wi fi(x) i</p>
    <p>(1Z</p>
    <p>)</p>
    <p>Variables</p>
  </div>
  <div class="page">
    <p>Markov Networks: Learning</p>
    <p>Smoke  Cancer</p>
    <p>Weight Learning  Given: Features, Data  Learn: Weights</p>
    <p>Structure Learning  Given: Data  Learn: Features</p>
    <p>Two Learning Tasks</p>
    <p>Feature iWeight of Feature i</p>
    <p>P(x) = exp wi fi(x) i (1Z</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>)()()(log xnExnxP w</p>
    <p>iwiw i</p>
    <p>Markov Networks: Weight Learning</p>
    <p>Maximum likelihood weights</p>
    <p>Pseudo-likelihood</p>
    <p>No. of times feature i is true in data</p>
    <p>Expected no. times feature i is true according to model</p>
    <p>i</p>
    <p>ii xneighborsxPxPL ))(|()(</p>
    <p>Slow: Requires inference at each step</p>
    <p>No inference: More tractable to compute</p>
  </div>
  <div class="page">
    <p>Markov Networks: Structure Learning [Della Pietra et al., 1997]</p>
    <p>Given: Set of variables = {F, W, A, S, C}  At each step</p>
    <p>{F  W, F  A, , A  C, F  S  C, , A  S  C}</p>
    <p>Current model = {F, W, A, S, C, S  C}</p>
    <p>New model = {F, W, A, S, C, S  C, F  W}</p>
    <p>Candidate features: Conjoin variables to features in model</p>
    <p>Select best candidate</p>
    <p>Iterate until no feature improves score</p>
    <p>Downside: Weight learning at each step  very slow!</p>
  </div>
  <div class="page">
    <p>Bottom-up Learning of Markov Networks (BLM)</p>
    <p>F1: F  W  A F2: A  S F3: W  A F4: F  S  C F5: S  C</p>
    <p>F W A S C</p>
    <p>T T T F F</p>
    <p>F F T T F</p>
    <p>F T T F F</p>
    <p>T F F T T</p>
    <p>F F F T T</p>
    <p>F1: W  A F2: A  S F3: W  A F4: F  S  C F5: S  C</p>
    <p>Initial Model Revised Model</p>
    <p>[Davis and Domingos, 2010]</p>
    <p>Downside: Weight learning at each step  very slow!</p>
  </div>
  <div class="page">
    <p>L1 Structure Learning [Ravikumar et al., 2009]</p>
    <p>Given: Set of variables= {F, W, A, S, C} Do: L1 logistic regression to predict each variable</p>
    <p>Construct pairwise features between target and each variable with non-zero weight</p>
    <p>CC</p>
    <p>WW</p>
    <p>AA</p>
    <p>SS</p>
    <p>FF</p>
    <p>WW</p>
    <p>AA</p>
    <p>SS</p>
    <p>CC</p>
    <p>Model = {S  C,  , W  F} Downside: Algorithm restricted to pairwise features</p>
  </div>
  <div class="page">
    <p>General Strategy: Local Models</p>
    <p>Learn a local model to predict each variable given the others:</p>
    <p>AA</p>
    <p>BB</p>
    <p>CCBB</p>
    <p>AA BB DD</p>
    <p>DD</p>
    <p>CC</p>
    <p>Combine the models into a Markov network</p>
    <p>CCAA BB DD</p>
    <p>c.f. Ravikumar et al., 2009</p>
  </div>
  <div class="page">
    <p>DTSL: Decision Tree Structure Learning [Lowd and Davis, ICDM 2010]</p>
    <p>Given: Set of variables= {F, W, A, S, C} Do: Learn a decision tree to predict each variable</p>
    <p>F=?F=?</p>
    <p>false</p>
    <p>false S=?S=?</p>
    <p>tru e</p>
    <p>tru eP(C|F,S) = P(F|C,S) =</p>
    <p>Construct a feature for each leaf in each tree: F  C F  S  C F  S  C</p>
    <p>F  C F  S  C F  S  C</p>
  </div>
  <div class="page">
    <p>DTSL Feature Pruning [Lowd and Davis, ICDM 2010]</p>
    <p>F=?F=?</p>
    <p>false</p>
    <p>false S=?S=?</p>
    <p>tru e</p>
    <p>tru eP(C|F,S) =</p>
    <p>Original: F  C F  S  C F  S  C F  C F  S  C F  S  C</p>
    <p>Pruned: All of the above plus F, F  S, F  S, F</p>
    <p>Nonzero: F, S, C, F  C, S  C</p>
  </div>
  <div class="page">
    <p>Empirical Evaluation</p>
    <p>Algorithms  DSTL [Lowd and Davis, ICDM 2010]  DP [Della Pietra et al., 1997]  BLM [Davis and Domingos, 2010]  L1 [Ravikumar et al., 2009]  All parameters were tuned on held-out data</p>
    <p>Metrics  Running time (structure learning only)  Per variable conditional marginal log-likelihood</p>
    <p>(CMLL)</p>
  </div>
  <div class="page">
    <p>Conditional Marginal Log Likelihood</p>
    <p>Measures ability to predict each variable separately, given evidence.</p>
    <p>Split variables into 4 sets: Use 3 as evidence (E), 1 as query (Q) Rotate through all variables appear in queries</p>
    <p>Probabilities estimated using MCMC (specifically MC-SAT [Poon and Domingos, 2006])</p>
    <p>CMLL( x,e) = log P(X i = xi | E = e) i</p>
    <p>[Lee et al., 2007]</p>
  </div>
  <div class="page">
    <p>Domains</p>
    <p>Compared across 13 different domains  2,800 to 290,000 train examples  600 to 38,000 tune examples  600 to 58,000 test examples  16 to 900 variables</p>
  </div>
  <div class="page">
    <p>Run time (minutes) N</p>
    <p>LT CS</p>
    <p>M SN</p>
    <p>BC</p>
    <p>KD D</p>
    <p>Cu p2</p>
    <p>Pl an</p>
    <p>ts</p>
    <p>A ud</p>
    <p>io</p>
    <p>Je ste</p>
    <p>r</p>
    <p>N et</p>
    <p>lix</p>
    <p>M SW</p>
    <p>eb</p>
    <p>Bo ok</p>
    <p>Ea ch</p>
    <p>M ov</p>
    <p>ie</p>
    <p>W eb</p>
    <p>KB</p>
    <p>ew sg</p>
    <p>ro up</p>
    <p>s</p>
    <p>Re ut</p>
    <p>er s</p>
    <p>DTSL L1 BLM DLP</p>
  </div>
  <div class="page">
    <p>Run time (minutes) N</p>
    <p>LT CS</p>
    <p>M SN</p>
    <p>BC</p>
    <p>KD D</p>
    <p>Cu p2</p>
    <p>Pl an</p>
    <p>ts</p>
    <p>A ud</p>
    <p>io</p>
    <p>Je ste</p>
    <p>r</p>
    <p>N et</p>
    <p>lix</p>
    <p>M SW</p>
    <p>eb</p>
    <p>Bo ok</p>
    <p>Ea ch</p>
    <p>M ov</p>
    <p>ie</p>
    <p>W eb</p>
    <p>KB</p>
    <p>ew sg</p>
    <p>ro up</p>
    <p>s</p>
    <p>Re ut</p>
    <p>er s</p>
    <p>DTSL L1 BLM DLP</p>
  </div>
  <div class="page">
    <p>Accuracy: DTSL vs. BLM</p>
    <p>DTSL Better</p>
    <p>BLM Better</p>
  </div>
  <div class="page">
    <p>Accuracy: DTSL vs. L1</p>
    <p>DTSL Better</p>
    <p>L1 Better</p>
  </div>
  <div class="page">
    <p>Do long features matter?</p>
    <p>M SN</p>
    <p>BC</p>
    <p>Pl an</p>
    <p>ts</p>
    <p>Bo ok</p>
    <p>Ea ch</p>
    <p>M ov</p>
    <p>ie</p>
    <p>N LT</p>
    <p>CS</p>
    <p>KD D</p>
    <p>C up</p>
    <p>ew sg</p>
    <p>ro up</p>
    <p>s</p>
    <p>W eb</p>
    <p>KB</p>
    <p>M SW</p>
    <p>eb</p>
    <p>Re ut</p>
    <p>er s</p>
    <p>A ud</p>
    <p>io</p>
    <p>Je st</p>
    <p>er</p>
    <p>N et</p>
    <p>lix 0</p>
    <p>DTSL Better L1 Better</p>
    <p>D TS</p>
    <p>L Fe</p>
    <p>at ur</p>
    <p>e Le</p>
    <p>ng th</p>
  </div>
  <div class="page">
    <p>Results Summary</p>
    <p>Accuracy  DTSL wins on 5 domains  L1 wins on 6 domains  BLM wins on 2 domains</p>
    <p>Speed  DTSL is 16 times faster than L1  DTSL is 100-10,000 times faster than BLM and DP  For DTSL, weight learning becomes the bottleneck</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>DTSL uses decision trees to learn MN structures much faster with similar accuracy.</p>
    <p>Using local models to build a global model is an effective strategy.</p>
    <p>L1 and DTSL have different strengths  L1 can combine many independent influences  DTSL can handle complex interactions  Can we get the best of both worlds?</p>
    <p>(Ongoing work)</p>
  </div>
</Presentation>
