<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Markov Logic Network Structure</p>
    <p>Via Hypergraph Lifting</p>
    <p>Stanley Kok Dept. of Computer Science and Eng.</p>
    <p>University of Washington Seattle, USA</p>
    <p>Joint work with Pedro Domingos</p>
  </div>
  <div class="page">
    <p>Synopsis of LHL</p>
    <p>Input: Relational DB</p>
    <p>Advises Pete Sam Pete Saul Paul Sara</p>
    <p>TAs Sam CS1 Sam CS2 Sara CS1</p>
    <p>Teaches Pete CS1 Pete CS2 Paul CS2</p>
    <p>Output: Probabilistic KB</p>
    <p>Input: Relational DB Advises</p>
    <p>Pete Sam</p>
    <p>Pete Saul</p>
    <p>Paul Sar</p>
    <p>TAs</p>
    <p>Sam CS1</p>
    <p>Sam CS2</p>
    <p>Sara CS1</p>
    <p>Teaches</p>
    <p>Pete CS1</p>
    <p>Pete CS2</p>
    <p>Paul CS2</p>
    <p>Output: Probabilistic KB</p>
    <p>Sam</p>
    <p>Pete CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>CS4</p>
    <p>CS5</p>
    <p>CS6</p>
    <p>CS7</p>
    <p>CS8</p>
    <p>Paul</p>
    <p>Pat</p>
    <p>Phil</p>
    <p>Sara</p>
    <p>Saul</p>
    <p>Sue TAs</p>
    <p>Advises Teaches</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Advises</p>
    <p>Professor</p>
    <p>Student</p>
    <p>Course</p>
    <p>Goal of LHL</p>
  </div>
  <div class="page">
    <p>Area under Prec Recall Curve (AUC)</p>
    <p>Conditional Log-Likelihood (CLL)</p>
    <p>LHL</p>
    <p>BUSL MSL</p>
    <p>LHL BUSL MSL</p>
    <p>Experimental Results</p>
    <p>LHL LHL</p>
    <p>BUSL MSLBUSL MSL</p>
  </div>
  <div class="page">
    <p>Background  Learning via Hypergraph Lifting  Experiments  Future Work</p>
    <p>Background  Learning via Hypergraph Lifting  Experiments  Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>A logical KB is a set of hard constraints on the set of possible worlds</p>
    <p>Lets make them soft constraints: When a world violates a formula, it becomes less probable, not impossible</p>
    <p>Give each formula a weight (Higher weight  Stronger constraint)</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>A Markov logic network (MLN) is a set of pairs (F,w)  F is a formula in first-order logic  w is a real number</p>
    <p>vector of truth assignments to ground atoms</p>
    <p>partition function</p>
    <p>weight of ith formula</p>
    <p>#true groundings of ith formula</p>
    <p>P (x) = 1Z exp  P F</p>
    <p>i =1 wi ni</p>
  </div>
  <div class="page">
    <p>Challenging task  Few approaches to date</p>
    <p>[Kok &amp; Domingos, ICML05; Mihalkova &amp; Mooney, ICML07; Biba et. al. ECAI08; Huynh &amp; Mooney, ICML08]</p>
    <p>Most MLN structure learners  Greedily and systematically enumerate formulas  Computationally expensive; large search space  Susceptible to local optima</p>
    <p>MLN Structure Learning</p>
  </div>
  <div class="page">
    <p>While beam not empty Add unit clauses to beam While beam has changed For each clause c in beam</p>
    <p>c  add a literal to c newClauses  newClauses [ c</p>
    <p>beam  k best clauses in beam [ newClauses Add best clause in beam to MLN</p>
    <p>MSL [Kok &amp; Domingos, ICML05]</p>
  </div>
  <div class="page">
    <p>Find paths of linked ground atoms !formulas  Path  conjunction that is true at least once  Exponential search space of paths  Restricted to short paths</p>
    <p>Relational Pathfinding [Richards &amp; Mooney, AAAI92]</p>
    <p>Sam</p>
    <p>Pete CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>CS4</p>
    <p>CS5</p>
    <p>CS6</p>
    <p>CS7</p>
    <p>CS8</p>
    <p>Paul</p>
    <p>Pat</p>
    <p>Phil</p>
    <p>Sara</p>
    <p>Saul</p>
    <p>Sue TAs</p>
    <p>Advises</p>
    <p>Pete CS1</p>
    <p>Sam</p>
    <p>Advises(Pete, Sam)  Teaches(Pete, CS1)  TAs(Sam, CS1) Advises( p , s )  Teaches( p , c )  TAs( s , c )</p>
  </div>
  <div class="page">
    <p>Find short paths with a form of relational pathfinding  Path ! Boolean variable ! Node in Markov network  Greedily tries to link the nodes with edges  Cliques ! clauses</p>
    <p>Form disjunctions of atoms in cliques nodes  Greedily adds clauses to an empty MLN</p>
    <p>BUSL [Mihalkova &amp; Mooney, ICML07]</p>
    <p>Advises( p,s)</p>
    <p>Teaches(p,c)</p>
    <p>TAs(s,c)</p>
    <p>Advises(p,s) V Teaches(p,c) V TAs(s,c) :Advises(p,s) V : Teaches(p,c) V TAs(s,c)</p>
  </div>
  <div class="page">
    <p>Background  Learning via Hypergraph Lifting  Experiments  Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Uses relational pathfinding to fuller extent  Induces a hypergraph over clusters of constants</p>
    <p>Learning via Hypergraph Lifting (LHL)</p>
    <p>Sam</p>
    <p>Pete CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>CS4</p>
    <p>CS5</p>
    <p>CS6</p>
    <p>CS7</p>
    <p>CS8</p>
    <p>Paul</p>
    <p>Pat</p>
    <p>Phil</p>
    <p>Sara</p>
    <p>Saul</p>
    <p>Sue TAs</p>
    <p>Advises</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2</p>
    <p>CS3 CS4</p>
    <p>CS5 CS6</p>
    <p>CS7 CS8</p>
    <p>Advises</p>
    <p>Lift</p>
  </div>
  <div class="page">
    <p>Uses a hypergraph (V,E)  V : set of nodes  E : set of labeled, non-empty, ordered</p>
    <p>subsets of V  Find paths in a hypergraph</p>
    <p>Path: set of hyperedges s.t. for any two e0 and en, 9 sequence of hyperedges in set that leads from e0  en</p>
    <p>Learning via Hypergraph Lifting (LHL)</p>
  </div>
  <div class="page">
    <p>Relational DB can be viewed as hypergraph  Nodes  Constants  Hyperedges  True ground atoms</p>
    <p>DB Advises</p>
    <p>Pete Sam Pete Saul Paul Sara</p>
    <p>TAs</p>
    <p>Sam CS1 Sam CS2 Sara CS1</p>
    <p>Teache s</p>
    <p>Pete CS1 Pete CS2 Paul CS2</p>
    <p>Learning via Hypergraph Lifting (LHL)</p>
    <p>Sam</p>
    <p>Pete CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>CS4</p>
    <p>CS5</p>
    <p>CS6</p>
    <p>CS7</p>
    <p>CS8</p>
    <p>Paul</p>
    <p>Pat</p>
    <p>Phil</p>
    <p>Sara</p>
    <p>Saul</p>
    <p>Sue TAs</p>
    <p>Advises</p>
  </div>
  <div class="page">
    <p>LHL lifts hypergraph into more compact rep.  Jointly clusters nodes into higher-level concepts  Clusters hyperedges</p>
    <p>Traces paths in lifted hypergraph</p>
    <p>LHL = Clustering + Relational Pathfinding</p>
    <p>Sam</p>
    <p>Pete CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>CS4</p>
    <p>CS5</p>
    <p>CS6</p>
    <p>CS7</p>
    <p>CS8</p>
    <p>Paul</p>
    <p>Pat</p>
    <p>Phil</p>
    <p>Sara</p>
    <p>Saul</p>
    <p>Sue TAs</p>
    <p>Advises</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Advises</p>
    <p>Lift</p>
  </div>
  <div class="page">
    <p>LHL has three components  LiftGraph: Lifts hypergraph</p>
    <p>FindPaths: Finds paths in lifted hypergraph</p>
    <p>CreateMLN: Creates rules from paths, and</p>
    <p>adds good ones to empty MLN 16</p>
    <p>Learning via Hypergraph Lifting</p>
  </div>
  <div class="page">
    <p>Defined using Markov logic  Jointly clusters constants in bottom-up</p>
    <p>agglomerative manner  Allows information to propagate from one</p>
    <p>cluster to another  Ground atoms also clustered  #Clusters need not be specified in advance  Each lifted hyperedge contains  one true</p>
    <p>ground atom 17</p>
    <p>LiftGraph</p>
  </div>
  <div class="page">
    <p>Find cluster assignment C that maxmizes posterior prob. P(C | D) / P(D | C) P(C)</p>
    <p>Learning Problem in LiftGraph</p>
    <p>Truth values of ground atoms Defined with</p>
    <p>an MLN Defined with another MLN</p>
  </div>
  <div class="page">
    <p>For each predicate r and each cluster combination containing a true ground atom of r, we have an atom prediction rule</p>
    <p>LiftGraphs P(D|C) MLN</p>
    <p>Pete Paul Pat Phil</p>
    <p>Professor</p>
    <p>Student</p>
    <p>Sam Sara Saul Sue</p>
    <p>Teach es</p>
    <p>TAs</p>
    <p>Advises</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Course Pete Paul Pat Phil</p>
    <p>Professor CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Course</p>
    <p>Teach es</p>
  </div>
  <div class="page">
    <p>For each predicate r and each cluster combination containing a true ground atom of r, we have an atom prediction rule</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Course</p>
    <p>LiftGraphs P(D|C) MLN</p>
    <p>Pete Paul Pat Phil</p>
    <p>Professor</p>
    <p>Teach es</p>
    <p>Teaches(p,c)p 2  c 2 )</p>
  </div>
  <div class="page">
    <p>For each predicate r, we have a default atom prediction rule</p>
    <p>LiftGraphs P(D|C) MLN</p>
    <p>Pete Paul Pat Phil</p>
    <p>Professor CS1 CS2</p>
    <p>CS3 CS4</p>
    <p>CS5 CS6</p>
    <p>CS7 CS8</p>
    <p>Course</p>
    <p>Sam Sara Saul Sue</p>
    <p>x 2</p>
    <p>y 2x 2</p>
    <p>Pete Paul Pat Phil</p>
    <p>Professor</p>
    <p>y 2</p>
    <p>Default Cluster Combination</p>
    <p>) Teaches(x,y)</p>
    <p>Student</p>
  </div>
  <div class="page">
    <p>Each symbol belongs to exactly one cluster  Infinite weight</p>
    <p>Exponential prior on #cluster combinations  Negative weight -</p>
    <p>LiftGraphs P(C) MLN</p>
  </div>
  <div class="page">
    <p>Hard assignments of constants to clusters  Weights and log-posterior computed in closed</p>
    <p>form  Searches for cluster assignment with highest</p>
    <p>log-posterior</p>
    <p>LiftGraph</p>
  </div>
  <div class="page">
    <p>LiftGraphs Search Algm</p>
    <p>Pete</p>
    <p>Paul</p>
    <p>CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>Sam</p>
    <p>Sara</p>
    <p>Tea che</p>
    <p>s</p>
    <p>Advises</p>
    <p>Pete</p>
    <p>Paul</p>
    <p>Pete Paul</p>
  </div>
  <div class="page">
    <p>LiftGraphs Search Algm</p>
    <p>CS1</p>
    <p>CS2</p>
    <p>CS3</p>
    <p>Sam</p>
    <p>Sara</p>
    <p>Tea che</p>
    <p>s</p>
    <p>Advises</p>
    <p>Pete Paul</p>
    <p>CS1</p>
    <p>CS2</p>
    <p>CS1 CS2</p>
    <p>CS3</p>
    <p>CS1 CS2 CS3</p>
    <p>Sam</p>
    <p>Sara</p>
    <p>Sam Sara</p>
    <p>Teaches</p>
    <p>Advises</p>
  </div>
  <div class="page">
    <p>FindPaths</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Teache s</p>
    <p>TAs</p>
    <p>Advises</p>
    <p>Paths Found</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Advises( , )</p>
    <p>Advises( , ) , Teaches ( , )</p>
    <p>Advises( , ) , Teaches ( , ), TAs( , )</p>
  </div>
  <div class="page">
    <p>Advises( , ) ,</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>Teaches( , ) ,</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Pete Paul Pat Phil</p>
    <p>TAs( , )</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>:Advises(p, s) V :Teaches(p, c) V :TAs(s, c)</p>
    <p>Clause Creation</p>
    <p>Advises( , )</p>
    <p>Pete Paul Pat Phil</p>
    <p>Sam Sara Saul Sue</p>
    <p>Teaches( , )</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Pete Paul Pat Phil</p>
    <p>TAs( , )</p>
    <p>Sam Sara Saul Sue</p>
    <p>CS1 CS2 CS3 CS4 CS5 CS6 CS7 CS8</p>
    <p>Advises( , )</p>
    <p>Teaches( , )</p>
    <p>TAs( , )</p>
    <p>p</p>
    <p>p</p>
    <p>s</p>
    <p>s</p>
    <p>c</p>
    <p>c</p>
    <p>Advises(p, s)  Teaches(p, c)  TAs(s, c)</p>
    <p>Advises(p, s) V :Teaches(p, c) V :TAs(s, c)</p>
    <p>Advises(p, s) V Teaches(p, c) V :TAs(s, c)</p>
  </div>
  <div class="page">
    <p>Clause Pruning</p>
    <p>: Advises(p, s) V :Teaches(p, c) V TAs(s, c)</p>
    <p>Advises(p, s) V :Teaches(p, c) V TAs(s, c)</p>
    <p>: Advises(p, s) V :Teaches(p, c)</p>
    <p>: Advises(p, s) V TAs(s, c)</p>
    <p>: Advises(p, s)</p>
    <p>: Teaches(p, c)</p>
    <p>:Teaches(p, c) V TAs(s, c)</p>
    <p>TAs(s, c)</p>
    <p>Score</p>
    <p>`</p>
  </div>
  <div class="page">
    <p>Clause Pruning</p>
    <p>: Advises(p, s) V :Teaches(p, c) V TAs(s, c)</p>
    <p>Advises(p, s) V :Teaches(p, c) V TAs(s, c)</p>
    <p>: Advises(p, s) V :Teaches(p, c)</p>
    <p>: Advises(p, s) V TAs(s, c)</p>
    <p>: Advises(p, s)</p>
    <p>: Teaches(p, c)</p>
    <p>:Teaches(p, c) V TAs(s, c)</p>
    <p>TAs(s, c)</p>
    <p>Score</p>
    <p>Compare each clause against its sub-clauses (taken individually)</p>
  </div>
  <div class="page">
    <p>Add clauses to empty MLN in order of decreasing score</p>
    <p>Retrain weights of clauses each time clause is added</p>
    <p>Retain clause in MLN if overall score improves</p>
    <p>MLN Creation</p>
  </div>
  <div class="page">
    <p>Background  Learning via Hypergraph Lifting  Experiments  Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>IMDB  Created from IMDB.com DB  Movies, actors, etc., and relationships  17,793 ground atoms; 1224 true ones</p>
    <p>UW-CSE  Describes academic department  Students, faculty, etc., and relationships  260,254 ground atoms; 2112 true ones</p>
    <p>Datasets</p>
  </div>
  <div class="page">
    <p>Cora  Citations to computer science papers  Papers, authors, titles, etc., and their</p>
    <p>relationships  687,422 ground atoms; 42,558 true ones</p>
    <p>Datasets</p>
  </div>
  <div class="page">
    <p>Five-fold cross validation  Inferred prob. true for groundings of each</p>
    <p>predicate  Groundings of all other predicates as evidence</p>
    <p>Evaluation measures  Area under precision-recall curve (AUC)  Average conditional log-likelihood (CLL)</p>
    <p>Methodology</p>
  </div>
  <div class="page">
    <p>MCMC inference algms in Alchemy to evaluate the test atoms  1 million samples  24 hours</p>
    <p>Methodology</p>
  </div>
  <div class="page">
    <p>Compared with  MSL [Kok &amp; Domingos, ICML05]  BUSL [Mihalkova &amp; Mooney, ICML07]</p>
    <p>Lesion study  NoLiftGraph: LHL with no hypergraph lifting</p>
    <p>Find paths directly from unlifted hypergraph  NoPathFinding: LHL with no pathfinding</p>
    <p>Use MLN representing LiftGraph</p>
    <p>Methodology</p>
  </div>
  <div class="page">
    <p>LHL vs. BUSL vs. MSL Area under Prec-Recall Curve</p>
    <p>LHL BUSL MSL</p>
    <p>IMDB UW-CSE</p>
    <p>Cora</p>
    <p>LHL BUSL MSL</p>
    <p>LHL BUSL MSL</p>
  </div>
  <div class="page">
    <p>LHL vs. BUSL vs. MSL Conditional Log-likelihood</p>
    <p>IMDB UW-CSE</p>
    <p>Cora</p>
    <p>LHL BUSL MSL LHL BUSL MSL</p>
    <p>LHL BUSL MSL</p>
  </div>
  <div class="page">
    <p>LHL vs. BUSL vs. MSL Runtime</p>
    <p>UW-CSEIMDB</p>
    <p>Cora</p>
    <p>m in</p>
    <p>h r</p>
    <p>h r</p>
    <p>LHL BUSL MSL LHL BUSL MSL</p>
    <p>LHL BUSL MSL</p>
  </div>
  <div class="page">
    <p>LHL vs. NoLiftGraph Area under Prec-Recall Curve</p>
    <p>Cora</p>
    <p>NoLift Graph</p>
    <p>LHL NoLift GraphLHL</p>
    <p>NoLift Graph</p>
    <p>LHL</p>
  </div>
  <div class="page">
    <p>LHL vs. NoLiftGraph Conditional Log-likelihood</p>
    <p>IMDB UW-CSE</p>
    <p>Cora</p>
    <p>NoLift GraphLHL</p>
    <p>NoLift GraphLHL</p>
    <p>NoLift GraphLHL</p>
  </div>
  <div class="page">
    <p>LHL vs. NoLiftGraph Runtime</p>
    <p>IMDB UW-CSE</p>
    <p>Cora</p>
    <p>m in</p>
    <p>h r</p>
    <p>h r</p>
    <p>NoLift GraphLHL</p>
    <p>NoLift GraphLHL</p>
    <p>NoLift GraphLHL</p>
  </div>
  <div class="page">
    <p>LHL vs. NoPathFinding</p>
    <p>A U</p>
    <p>C</p>
    <p>A U</p>
    <p>C</p>
    <p>C L</p>
    <p>L</p>
    <p>C L</p>
    <p>L</p>
    <p>IMDB UW-CSE</p>
    <p>NoPath FindingLHL</p>
    <p>NoPath FindingLHL</p>
    <p>NoPath FindingLHL</p>
    <p>NoPath FindingLHL</p>
  </div>
  <div class="page">
    <p>if a is an actor</p>
    <p>and d is a director,</p>
    <p>and they both worked in the same movie,</p>
    <p>then a probably worked under d</p>
    <p>if p is a professor,</p>
    <p>and p co-authored a paper with s,</p>
    <p>then s is likely a student</p>
    <p>if papers x and y have same author</p>
    <p>then x and y are likely to be same paper 44</p>
    <p>Examples of Rules Learned</p>
  </div>
  <div class="page">
    <p>Motivation  Background  Learning via Hypergraph Lifting  Experiments  Future Work</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Integrate the components of LHL  Integrate LHL with lifted inference [Singla &amp;</p>
    <p>Domingos, AAAI08]</p>
    <p>Construct ontology simultaneously with probabilistic KB</p>
    <p>Further scale LHL up  Apply LHL to larger, richer domains</p>
    <p>e.g., the Web</p>
    <p>Future Work</p>
  </div>
  <div class="page">
    <p>LHL = Clustering + Relational Pathfinding  Lifts data into more compact form</p>
    <p>Essential for speeding up relational pathfinding  LHL outperforms state-of-the-art structure</p>
    <p>learners</p>
    <p>Conclusion</p>
  </div>
</Presentation>
