<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Probabilistic Clustering-Projection Model</p>
    <p>for Discrete Data</p>
    <p>Shipeng Yu1,2, Kai Yu2, Volker Tresp2, Hans-Peter Kriegel1</p>
    <p>October 2005</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Previous Work  The PCP Model  Learning in PCP Model  Experiments  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>We model discrete data in this work  Fundamental problem for data mining and machine learning  In bag-of-words document modelling: document-word pairs  In collaborative filtering: item-rating pairs</p>
    <p>Properties  The data can be described as a big matrix with integer entries  The data matrix is normally very sparse (&gt;90% are zeros)</p>
    <p>w1 w2  wV d1 2 0  1 d2 0 1  2 ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>dD 1 1  0</p>
    <p>Documents</p>
    <p>Words</p>
    <p>Occurrences</p>
  </div>
  <div class="page">
    <p>Data Clustering</p>
    <p>Goal: Group similar documents together  For continuous data: Distance-based similarity (k-means)</p>
    <p>Iteratively minimize a distance-based cost function  Equivalent to a Gaussian mixture model</p>
    <p>For discrete data: Occurrence-based similarity</p>
    <p>Similar documents should have similar occurrences of words  No Gaussianity holds for discrete data</p>
    <p>w1 w2  wV d1 2 0  1 d2 0 1  2 ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>dD 1 1  0</p>
  </div>
  <div class="page">
    <p>Data Projection</p>
    <p>Goal: Find a low-dimensional feature mapping  For continuous data: Principal Component Analysis</p>
    <p>Find orthogonal dimensions to explain data covariance  For discrete data: Topic detection</p>
    <p>Topics explain the co-occurrences of words  Topics are not orthogonal, but independent</p>
    <p>w1 w2  wV d1 2 0  1 d2 0 1  2 ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>dD 1 1  0</p>
    <p>z1  zK</p>
  </div>
  <div class="page">
    <p>Projection versus Clustering</p>
    <p>They are normally modelled separately  But why not jointly?</p>
    <p>More informative projection</p>
    <p>better document clusters  Better clustering structure</p>
    <p>better projection for words  There should be a stable situation</p>
    <p>And how? PCP Model  Well-defined generative model for the data  Standard ways for learning and inference  Generalizable to new data</p>
    <p>w1 w2  wV d1 2 0  1 d2 0 1  2 ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>dD 1 1  0</p>
    <p>z1  zK</p>
  </div>
  <div class="page">
    <p>Two-sided clustering [Hofmann &amp; Puzicha 98]: Same problem as PLSI  Discrete-PCA [Buntine &amp; Perttu 03]: Similar to LDA in spirit  TTMM [Keller &amp; Bengio 04]: Lack a full Bayesian explanation</p>
    <p>Previous Work for Discrete Data</p>
    <p>PLSI [Hofmann 99]  First topic model  Not well-defined generative model</p>
    <p>LDA [Blei et al 03]  State-of-the-art topic model  Generalize PLSI with Dirichlet prior  No clustering effect is modelled</p>
    <p>NMF [Lee &amp; Seung 99]  Factorize the data matrix  Can be explained as a cluster</p>
    <p>ing model  No projection of words is dire</p>
    <p>ctly modelled</p>
    <p>Projection model Clustering model</p>
    <p>w1 w2  wV d1 2 0  1 d2 0 1  2 ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>dD 1 1  0</p>
    <p>z1  zK</p>
    <p>w1 w2  wV d1 2 0  1 d2 0 1  2 ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>dD 1 1  0</p>
    <p>Joint Projection-Clustering model</p>
  </div>
  <div class="page">
    <p>PCP Model: Overview</p>
    <p>Probabilistic Clustering-Projection Model  A probabilistic model for discrete data  A clustering model using projected features  A projection model with structural data</p>
    <p>Learning in PCP model: Variational EM  Exactly equivalent to iteratively performing cluste</p>
    <p>ring and projection operations  Guaranteed convergence</p>
  </div>
  <div class="page">
    <p>PCP Model: Sampling Process</p>
    <p>... ... ... ...</p>
    <p>Clustering Projection</p>
    <p>cluster 1</p>
    <p>Weights</p>
    <p>Cluster centers</p>
    <p>P ro</p>
    <p>jectio n</p>
    <p>document wD</p>
    <p>word</p>
    <p>...</p>
    <p>word wD ;N D</p>
    <p>wD ;N 1</p>
    <p>word</p>
    <p>...</p>
    <p>word</p>
    <p>w1;1</p>
    <p>w1;N 1</p>
    <p>cluster D</p>
    <p>topic</p>
    <p>topic zD ;N D</p>
    <p>zD ;N 1</p>
    <p>...</p>
    <p>topic</p>
    <p>topic</p>
    <p>z1;1</p>
    <p>z1;N 1</p>
    <p>...</p>
    <p>Clustering model using projected featuresProjection model with structural data D documents M clusters K topics V words</p>
    <p>Dirichlet</p>
    <p>Dirichlet</p>
    <p>Multinomial</p>
    <p>Multinomial Multinomial</p>
    <p>document w1</p>
  </div>
  <div class="page">
    <p>PCP Model: Plate Model</p>
    <p>Likelihood</p>
    <p>Model Parameters</p>
    <p>Latent Variables Observations</p>
    <p>Clustering Model</p>
    <p>Projection Model</p>
  </div>
  <div class="page">
    <p>Learning in PCP Model</p>
    <p>We are interested in the posterior distribution</p>
    <p>The integral is intractable  Variational EM learning</p>
    <p>Approximate the posterior with a variational distribution</p>
    <p>Minimize the KL-divergence  Variational E-step: Minimize w.r.t. variational parameters  Variational M-step: Minimize w.r.t. model parameters</p>
    <p>Iterate until convergence</p>
    <p>DK L (qjjp)</p>
    <p>Variational Parameters</p>
    <p>Dirichlet Dirichlet Multinomial Multinomial</p>
  </div>
  <div class="page">
    <p>Update Equations</p>
    <p>Equations can be separated to clustering updates and projecti on updates</p>
    <p>Variational EM learning corresponds to iteratively performing c lustering and projection until convergence</p>
    <p>Clustering Updates</p>
    <p>Projection Updates</p>
  </div>
  <div class="page">
    <p>Clustering Updates Update soft cluster assignments, P (cd = m)</p>
    <p>Update cluster centers Update cluster weights</p>
    <p>Prior term</p>
    <p>Likelihood term</p>
    <p>Prior term Likelihood term</p>
    <p>Sufficient Projection term</p>
  </div>
  <div class="page">
    <p>Projection Updates Update word projection, P (zd;n = k)</p>
    <p>Update projection matrix Empirical estimate</p>
    <p>Sufficient Clustering term</p>
  </div>
  <div class="page">
    <p>PCP Learning Algorithm</p>
    <p>Clustering Updates</p>
    <p>Projection Updates</p>
    <p>Sufficient Clustering term</p>
    <p>Sufficient Projection term</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Methodology  Document Modelling: Compare model generalization  Word Projection: Evaluate topic space  Document Clustering: Evaluate clustering results</p>
    <p>Data sets  5 categories in Reuters-21578: 3948 docs, 7665 words  4 categories in 20Newsgroup: 3888 docs, 8396 words</p>
    <p>Preprocessing  Stemming and stop-word removing  Pick up words that occur at least in 5 documents</p>
  </div>
  <div class="page">
    <p>Case Study  Run on a 4-group subset of 20Newsgroup data</p>
    <p>Car</p>
    <p>Bike</p>
    <p>Baseball</p>
    <p>Hockey</p>
  </div>
  <div class="page">
    <p>Exp1: Document Modelling</p>
    <p>Goal: Evaluate generalization performance  Methods to compare</p>
    <p>PLSI: A pseudo form for generalization  LDA: State-of-the-art method</p>
    <p>Metric: Perplexity</p>
    <p>90% for training and 10% for testing</p>
    <p>Perp(Dtest) = exp( P</p>
    <p>d lnp(wd)= P</p>
    <p>d jwdj)</p>
  </div>
  <div class="page">
    <p>Exp2: Word Projection</p>
    <p>Goal: Evaluate the projection matrix  Methods to compare: PLSI, LDA  We train SVMs on the 10-dimensional space after projection  Test classification accuracy on leave-out data</p>
    <p>Reuters Newsgroup</p>
  </div>
  <div class="page">
    <p>Exp3: Document Clustering</p>
    <p>Goal: Evaluate clustering for documents  Methods to compare</p>
    <p>NMF: Do factorization for clustering  LDA+k-means: Do clustering on the projected space</p>
    <p>Metric: normalized mutual information</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>PCP is a well-defined generative model  PCP models clustering and projection jointly  Learning in PCP corresponds to an iterative</p>
    <p>process of clustering and projection  PCP learning guarantees convergence  Future work</p>
    <p>Large scale experiments  Build a probabilistic model with more factors</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
  </div>
</Presentation>
