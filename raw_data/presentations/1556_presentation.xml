<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>LightNVM: The Linux Open-Channel SSD Subsystem</p>
    <p>Matias Bjrling, Javier Gonzlez, and Philippe Bonnet</p>
  </div>
  <div class="page">
    <p>I/O Predictability and Isolation</p>
    <p>latency</p>
    <p>spinning drives...</p>
    <p>I/O Performance is unpredictable due</p>
    <p>to writes being buffered</p>
  </div>
  <div class="page">
    <p>Log-on-log, Indirection, and Narrow I/O</p>
    <p>Log-on-Log Write Indirection &amp; Lost State</p>
    <p>Solid-State Drive</p>
    <p>Block Layer</p>
    <p>Log-structured File-system</p>
    <p>HW</p>
    <p>Kernel</p>
    <p>Space</p>
    <p>User</p>
    <p>Space</p>
    <p>Log-structured Database (e.g., RocksDB)</p>
    <p>Read/Write/Trim</p>
    <p>pread/pwrite</p>
    <p>Metadata Mgmt. Garbage CollectionAddress Mapping</p>
    <p>Metadata Mgmt. Garbage CollectionAddress Mapping</p>
    <p>Metadata Mgmt. Garbage CollectionAddress Mapping</p>
    <p>VFS</p>
    <p>Solid-State Drive Pipeline</p>
    <p>die2 die3die0 die1</p>
    <p>NAND Controller</p>
    <p>Reads</p>
    <p>Write Buffer</p>
    <p>Writes</p>
    <p>Even if Writes and Reads does not collide from application Indirection and loss of information due to a Narrow I/O interface</p>
    <p>FTL-like implementation at various layers</p>
    <p>FTL-like implementation at various layers</p>
    <p>FTL-like implementation</p>
    <p>at multiple layers</p>
    <p>Not able to align data on media = Write amplification</p>
    <p>increase + extra GC</p>
    <p>Writes Decoupled from Reads</p>
    <p>Read/Write Interface makes Data placement + Buffering = Best Effort</p>
    <p>Host does not know SSD state due to the narrow</p>
    <p>I/O Interface</p>
  </div>
  <div class="page">
    <p>Solid-State Drive</p>
    <p>Parallel Units</p>
    <p>Flash Translation Layer Channel X</p>
    <p>Media Error Handling</p>
    <p>Media Retention Management</p>
    <p>M e</p>
    <p>d ia</p>
    <p>C o</p>
    <p>n tr</p>
    <p>o ll e</p>
    <p>rResponsibilities</p>
    <p>Host Interface</p>
    <p>Channel Y</p>
    <p>Solid-State Drives and Non-Volatile Media</p>
    <p>Read/Write/Erase</p>
    <p>Read/Write</p>
    <p>Tens of Parallel Units!</p>
    <p>Transform R/W/E to R/W</p>
    <p>Manage Media Constraints</p>
    <p>ECC, RAID, Retention</p>
    <p>Read (50-100us)</p>
    <p>Write (1-10ms)</p>
    <p>Erase (3-15ms)</p>
  </div>
  <div class="page">
    <p>New Storage Interface that provides</p>
    <p>Predictable I/O</p>
    <p>I/O Isolation</p>
    <p>Reduces Write Amplication</p>
    <p>Removal of multiple log-structured data structures</p>
    <p>Intelligent data placement and I/O scheduling decisions</p>
    <p>Make the host aware of the SSD state to make those decisions</p>
  </div>
  <div class="page">
    <p>Contributions</p>
  </div>
  <div class="page">
    <p>Physical Page Addressing (PPA) Interface</p>
    <p>Expose geometry of the SSD  Logical/Physical geometry</p>
    <p>Performance</p>
    <p>Media-specific metadata (if needed)</p>
    <p>Controller functionalities</p>
    <p>Hierarchical Address Space  Encode geometry into the address space</p>
    <p>Vector I/Os  Read/Write/Erase</p>
    <p>Up to the SSD vendor</p>
    <p>Encode parallel units into the address space</p>
    <p>Efficient access to the given this new address</p>
    <p>space</p>
  </div>
  <div class="page">
    <p>Encode Geometry in Address Space Channels -&gt; Parallel Units -&gt; Planes -&gt; Blocks -&gt; Pages -&gt; Sectors</p>
    <p>CH0PU0 CH0PU1 CHxPUy... 0</p>
    <p>Max LBA -1</p>
    <p>Encode Geometry into the Address Space</p>
    <p>OCSSD</p>
    <p>Solid State Drive</p>
    <p>PU 1..M</p>
    <p>Channel 1</p>
    <p>M e</p>
    <p>d ia</p>
    <p>C o</p>
    <p>n tr</p>
    <p>o ll e</p>
    <p>r</p>
    <p>Channel N</p>
    <p>Planes</p>
    <p>Pages Block0 Block1</p>
    <p>BlockZ</p>
    <p>Sectors</p>
    <p>D D D D</p>
    <p>Pages Block0 Block1</p>
    <p>BlockZ</p>
    <p>Sectors</p>
    <p>D D D D</p>
    <p>Linear Address Space</p>
  </div>
  <div class="page">
    <p>PU2</p>
    <p>PU3</p>
    <p>PU0</p>
    <p>PU1</p>
    <p>Controller</p>
    <p>Vector I/O Access</p>
    <p>Obtain higher throughput through parallel units</p>
    <p>Large overhead if I/Os is separately issued</p>
    <p>Introduce vector I/O interface to enable host to submit I/Os to multiple PUs using one command</p>
    <p>Vector Read/Write/Erase using scatter/gather address list</p>
    <p>Vector I/O</p>
    <p># LBA</p>
  </div>
  <div class="page">
    <p>LightNVM Architecture</p>
    <p>liblightnvm</p>
    <p>Open-Channel SSD</p>
    <p>NVMe Device Driver</p>
    <p>LightNVM Subsystem</p>
    <p>pblk</p>
    <p>Hardware</p>
    <p>Kernel</p>
    <p>Space</p>
    <p>User</p>
    <p>Space Application(s)</p>
    <p>File System</p>
    <p>PPA Addressing</p>
    <p>S c a</p>
    <p>la r</p>
    <p>R e</p>
    <p>a d</p>
    <p>/W ri te</p>
    <p>(o p</p>
    <p>ti o</p>
    <p>n a</p>
    <p>l)</p>
    <p>G e</p>
    <p>o m</p>
    <p>e tr</p>
    <p>y</p>
    <p>V e</p>
    <p>c to</p>
    <p>re d</p>
    <p>R /W</p>
    <p>/E</p>
    <p>(2)</p>
    <p>(1)</p>
    <p>(3)</p>
  </div>
  <div class="page">
    <p>Host-side Flash Translation Layer - pblk</p>
    <p>Mapping table  Sector-granularity</p>
    <p>Write buffering  Lockless circular buffer  Multiple producers  Single consumer (Write Thread)</p>
    <p>Error Handling  Media write/erase errors</p>
    <p>Garbage Collection  Refresh data  Rewrite blocks</p>
    <p>Open-Channel SSD</p>
    <p>NVMe Device Driver</p>
    <p>LightNVM Subsystem</p>
    <p>Hardware</p>
    <p>Linux</p>
    <p>Kernel File System</p>
    <p>make_rq make_rq</p>
    <p>Write Thread Error Handling</p>
    <p>L2P Table</p>
    <p>Write Context</p>
    <p>Write Buffer Write Entry</p>
    <p>Write</p>
    <p>Lookup Cache Hit</p>
    <p>Read</p>
    <p>Add Entry</p>
    <p>GC/Rate-limiting Thread</p>
    <p>Read Path Write Path</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>CNEX Labs Open-Channel SSD  NVMe  PCIe Gen3x8  2TB MLC NAND</p>
    <p>Geometry  16 channels  8 PUs per channel (Total: 128 PUs)</p>
    <p>Parallel Unit Characteristics  Page size: 16K + 64B user OOB  Planes: 4, Blocks: 1.067, Block Size: 256 Pages</p>
    <p>Performance:  Write: Single PU 47MB/s  Read: Single 108MB/s, 280MB/s (64K)</p>
    <p>Evaluation  Sanity check &amp; Base  Interface Flexibility</p>
    <p>Limit # Active Parallel Write Units  Predictable Latency</p>
  </div>
  <div class="page">
    <p>Base Performance using Vector I/O</p>
    <p>Grows with parallelism</p>
    <p>RR slightly lower due to scheduling</p>
    <p>conflicts</p>
    <p>Throughput &amp; Latency</p>
    <p>Request I/O Size</p>
  </div>
  <div class="page">
    <p>Limit # Active Writers</p>
    <p>A priori knowledge of workload. E.g., limit to 400MB/s Write  Limit number of Active PU Writers, and achieve better read latency</p>
    <p>Single Read and Write Perf.</p>
    <p>Mixed Read/Write</p>
    <p>Write throughput 400MB/s</p>
    <p>Write latency increases, and read latency</p>
    <p>reduces</p>
  </div>
  <div class="page">
    <p>Predictable Latency</p>
    <p>4K reads during 64K concurrent writes</p>
    <p>Consistent low latency at 99.99, 99.999, 99.9999</p>
  </div>
  <div class="page">
    <p>Lessons Learned</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Contributions  Physical Page Addressing (PPA) I/O Interface  The LightNVM Subsystem  pblk: A host-side Flash Translation Layer for Open-Channel SSDs  Demonstrate the effectiveness of the interface</p>
    <p>Linux kernel subsystem for Open-Channel SSDs  Initial release in Linux kernel 4.4.  User-space library (liblightnvm) support with Linux kernel 4.11.  Pblk upstream with Linux kernel 4.12.</p>
    <p>Physical Page Addressing Specification is available  The right time to dive into Open-Channel SSDs</p>
    <p>More information available at: http://lightnvm.io</p>
  </div>
</Presentation>
