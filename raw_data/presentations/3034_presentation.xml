<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Neural Question Answering</p>
    <p>at BioASQ 5B Georg Wiese, Dirk Weissenborn, Mariana Neves</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Neural question answering (QA) systems are end-to-end trainable machine learning models which achieve top performance in domains with large training datasets</p>
    <p>We apply an extractive neural QA system (FastQA [1]) to BioASQ 5B Phase B (list &amp; factoid questions)</p>
    <p>Extractive QA: Answer is given as start and end pointers in the context (snippets)</p>
  </div>
  <div class="page">
    <p>Network Architecture</p>
  </div>
  <div class="page">
    <p>Network Architecture</p>
    <p>Input Layer  GloVe &amp; character embeddings</p>
    <p>(like the original FastQA)  Biomedical embeddings [3]  Question type features</p>
  </div>
  <div class="page">
    <p>Network Architecture</p>
    <p>Original FastQA [1] Our Architecture</p>
    <p>Output Layer  Change start probability activation</p>
    <p>from softmax to sigmoid  -&gt; Multiple starts can be selected for</p>
    <p>list questions  For each selected start, select the</p>
    <p>corresponding end pointer via softmax</p>
  </div>
  <div class="page">
    <p>Network Architecture</p>
  </div>
  <div class="page">
    <p>Training Procedure</p>
    <p>Problem: Neural QA typically requires ~105 questions to train</p>
    <p>Datasets of such scale exist in the open domain, e.g. SQuAD [2] with ~105 factoid questions on Wikipedia articles</p>
    <p>We train in two steps:</p>
  </div>
  <div class="page">
    <p>Systems</p>
    <p>We trained five models using 5-fold cross validation on all available training data</p>
    <p>We submitted two systems:  Single: Best single model according to its respective development set  Ensemble: Ensemble of all five models (averaging scores before</p>
    <p>sigmoid/softmax activation)</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Factoid Results:  Our system won 3/5 batches  Averaged over the five</p>
    <p>batches, our system (ensemble) was 1.5 percentage points above the best competitor</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>List Results:  Our system won 2/5</p>
    <p>batches  On average, the best</p>
    <p>competitor performed 3.4 percentage points better than our ensemble model</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Strengths: Competitive performance, despite:</p>
    <p>Less feature engineering than traditional QA systems  A less domain-dependent architecture, because we dont rely on</p>
    <p>domain-specific structured resources</p>
    <p>Limitations:</p>
    <p>Extractive QA cannot generate answer which are not explicitly mentioned in the snippets -&gt; No yes/no &amp; summary questions</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[1] Weissenborn et al.: Making Neural QA as Simple as Possible but not Simpler</p>
    <p>[2] Rajpurkar et al.: SQuAD: 100,000+ Questions for Machine Comprehension of Text</p>
    <p>[3] Pavlopoulos et al.: Continuous Space Word Vectors Obtained by Applying Word2Vec to Abstracts of Biomedical Articles</p>
  </div>
  <div class="page">
    <p>Thank You. Questions?</p>
    <p>Related CONLL paper:</p>
    <p>Neural Domain Adaptation for Biomedical Question Answering</p>
    <p>Contact:</p>
    <p>georg.wiese@student.hpi.de</p>
  </div>
</Presentation>
