<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>When Cache Blocking of Sparse Matrix Vector Multiply Works and Why</p>
    <p>By Rajesh Nishtala, Richard W. Vuduc, James W. Demmel, and Katherine A. Yelick</p>
    <p>BeBOP Project, U.C. Berkeley June, 2004</p>
    <p>http://bebop.cs.berkeley.edu rajeshn@eecs.berkeley.edu</p>
  </div>
  <div class="page">
    <p>General Approach</p>
    <p>Sparse kernels are prevalent in many applications</p>
    <p>Automatically tune kernels to get optimum performance  Nave performance is typically less than 10% of machine peak  Performance dependent on matrix and platform  Numerous parameters that can be adjusted</p>
    <p>Create analytic performance models</p>
    <p>Use the performance models as a basis for a system that will automatically pick the correct parameters</p>
    <p>Verify performance models using built-in hardware counters</p>
  </div>
  <div class="page">
    <p>Introduction and Overview</p>
    <p>Sparse Matrix Vector Multiply (SpM x V): y  y+Ax  x, y are dense vectors</p>
    <p>We will call x the source vector and y the destination vector</p>
    <p>A is a sparse matrix (&lt;1% of entries are nonzero)</p>
    <p>Nave Performance of Pitfalls  High memory bandwidth requirements  Poor locality (indirect and irregular memory accesses)  Poor instruction mix (low flops to memory operations ratio)</p>
    <p>matrix: A</p>
    <p>vector: x</p>
    <p>vector: y</p>
  </div>
  <div class="page">
    <p>Cache Blocking</p>
    <p>Assume a Compressed Sparse Row (CSR) format  Cache blocking breaks the CSR matrix into multiple smaller CSR matrices</p>
    <p>Improves temporal locality in accesses to source vector  Adds extra set of row pointers for each block</p>
    <p>Goal: Given a matrix and a machine combination, pick the correct cache block size</p>
    <p>Key Tradeof  Does the benefit of the added temporal locality outweigh the costs associated</p>
    <p>with accessing the added overhead  An extension of the work done by Eun-Jin Im in the work on Sparsity</p>
    <p>r</p>
    <p>c</p>
  </div>
  <div class="page">
    <p>New Cache Blocking Optimizations</p>
    <p>Row Start/End  Band matrices may lead to many rows with all zeros  New optimization avoids iterating over these rows by adding</p>
    <p>starting row and ending row on which nonzero elements exist  Example:</p>
    <p>Exploiting Cache Block Structure  Unlike previous versions of the code, treat each of the sub</p>
    <p>matrices as an individual sparse matrix  Allow for easier incorporation of diferent kernels and</p>
    <p>multiple levels of cache blocking  Negligible performance costs because of few number of</p>
    <p>cache blocks</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Traditional dense methods to find tile sizes cannot be used because of indirect and irregular memory accesses.</p>
    <p>Similar work in sparse case  Temam and Jalby, Heras et al., Fraguela et al. have developed</p>
    <p>probabilistic models that predict cache misses.  Distinguish themselves by their ability to account for self and</p>
    <p>cross interference misses but assume uniform distribution of non-zeros</p>
    <p>Our models account for multiple levels in the memory system including the TLB and explicitly model the execution time</p>
    <p>Other areas  Gropp et al. and Herber et. al use similar techniques to tune</p>
    <p>applications in other domains</p>
  </div>
  <div class="page">
    <p>Analytic Models of the Memory System</p>
    <p>Execution Time Model  Cache misses are modeled and verified with</p>
    <p>hardware counters  Charge i for hits at each cache level</p>
    <p>T = (L1 hits) 1 + (L2 hits) 2 + (mem hits) mem + (TLB misses) TLB</p>
    <p>Cache Models  Lower bound on cache misses assume only compulsory</p>
    <p>misses  Upper bound is same as lower bound except that every</p>
    <p>access to the source and destination vectors miss  Based on models by Vuduc et al. in Super Computing 2002</p>
  </div>
  <div class="page">
    <p>Analytic Models of Memory System (cont.)</p>
    <p>Models are too simple to show performance advantages of Cache Blocking  According to models, Cache</p>
    <p>Blocking has no benefit since the blocking adds over head to the data structure</p>
    <p>Need to model one of the levels of the memory system more accurately to expose advantages of locality</p>
    <p>Empirical evidence suggests largest performance gains using cache blocking come from minimizing TLB misses</p>
    <p>Empirical evidence suggests two categories of block sizes that work</p>
    <p>For each row and column block size shown above, the value in the cell contains the number of matrices out of our 14 test matrices whose performance was within 90% of peak if that block size was chosen.</p>
  </div>
  <div class="page">
    <p>Analytic Model Verification</p>
    <p>14 test matrices in our suite with varying density and structure  Except the dense matrix in sparse format, all the matrices</p>
    <p>are too sparse to be aided by register blocking  Source vectors for all matrices are much larger than</p>
    <p>largest level of cache  Wide range of applications including, Linear Programming,</p>
    <p>Web connectivity, and Latent Semantic Indexing.</p>
    <p>3 Platforms  Intel Itanium 2  IBM Power 4  Intel Pentium 3</p>
    <p>Where available we used the hardware counters through PAPI</p>
  </div>
  <div class="page">
    <p>Model Verification  Analytic Model</p>
    <p>Over predicts performance by more than a factor of 2</p>
    <p>Relative performance is well predicted implying that it can be used as the basis for a heuristic</p>
    <p>Cache Blocking improves performance of certain matrices and doesnt yield improvements on others</p>
    <p>PAPI Model  Instead of lower and upper</p>
    <p>bound models for cache and TLB misses true hardware counter values are used</p>
    <p>Still over predict performance implying that there is time that we are not accounting for.</p>
  </div>
  <div class="page">
    <p>L3 Data Cache Miss Reduction</p>
    <p>Matrices with largest speedups experience largest decreases in the number of cache misses</p>
    <p>More than 1 order of magnitude less misses in some cases</p>
  </div>
  <div class="page">
    <p>TLB Miss Reduction</p>
    <p>Matrices with largest speedups experience largest decreases in the number of cache misses</p>
    <p>More than 2 orders of magnitude less misses in some cases</p>
  </div>
  <div class="page">
    <p>Speedup  For the same matrix</p>
    <p>speedup varies.</p>
    <p>Platforms with larger processor memory gaps are improved by cache blocking</p>
    <p>Matrices (5 and 6 experience the best speedups on all the platforms</p>
    <p>Matrices (12-14) which are extremely sparse show very little speedup</p>
  </div>
  <div class="page">
    <p>Sparse Band Matrices</p>
    <p>Row Start/End optimization greatly reduces sensitivity to block size</p>
    <p>Best Performance insensitive to row block size</p>
    <p>Cache blocking is not useful for a banded matrix because there is less cache thrashing</p>
    <p>RSE optimization forgives the choice of a bad block size</p>
  </div>
  <div class="page">
    <p>When and Why Cache Blocking Works</p>
    <p>Matrix Structure  Matrices that have a large column dimension aided with</p>
    <p>cache blocking  The gains through temporal locality in accesses to the source</p>
    <p>vector x far outweigh the costs of the added data structure</p>
    <p>If the matrix is too sparse cache blocking does not help  There is not enough temporal locality to amortize the added</p>
    <p>overhead</p>
    <p>Band matrices are not aided by cache blocking  Matrix already has optimal access pattern</p>
  </div>
  <div class="page">
    <p>Platform Evaluation</p>
    <p>Efectiveness of cache blocking varies across platforms for same matrix  As average number of cycles to access main memory goes</p>
    <p>up cache blocking helps since it reduces expensive accesses to main memory</p>
    <p>TLB misses account for a non trivial part of the execution time  Larger page sizes would mitigate this problem</p>
    <p>In SpM x V there are two types of access  Streaming access to matrix (only spatial locality exploited)</p>
    <p>Keeping matrix in cache wastes space for source vector  Random access to source vector (both spatial and temporal</p>
    <p>locality exploited)  The more of the source vector that can stay in cache the better</p>
    <p>Future work for machines without caches (SX1) or machines with uncached loads (Cray X1)</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Cache Blocking of SpM x V works when the benefits of the added temporal locality in the source vector x outweighs the costs of the added data structure overhead</p>
    <p>We use analytic models to select cache block size  Models are good at predicting the relative performance</p>
    <p>Cache Blocking appears to be most efective on matrices in which the column dimension is a lot greater than the row dimension</p>
    <p>Banded matrices are not aided by cache blocking since the structure already lends itself to the optimal access pattern.</p>
    <p>Row Start/End optimization forgives a bad block size choice</p>
    <p>Full Technical Report  Performance Modeling and Analysis of Cache Blocking in Sparse</p>
    <p>Matrix Vector Multiply (UCB/CSD-04-1335, June, 2004.) Rajesh Nishtala, Richard W. Vuduc, James W. Demmel, Katherine A. Yelick</p>
    <p>Project website: http://bebop.cs.berkeley.edu</p>
    <p>My website: http://www.cs.berkeley.edu/~rajeshn</p>
  </div>
  <div class="page">
    <p>References (1/3)</p>
    <p>Full Technical Report  Performance Modeling and Analysis of Cache Blocking in Sparse</p>
    <p>Matrix Vector Multiply (UCB/CSD-04-1335, June, 2004.) Rajesh Nishtala, Richard W. Vuduc, James W. Demmel, Katherine A. Yelick</p>
    <p>Other References  1. S. Browne, J. Dongarra, N. Garner, K. London, and P. Mucci. A scalable</p>
    <p>crossplatform infrastructure for application performance tuning using hardware counters. In Proceedings of Supercomputing, November 2000.</p>
    <p>2. B. B. Fraguela, R. Doallo, and E. L. Zapata. Memory hierarchy performance prediction for sparse blocked algorithms. Parallel Processing Letters, 9(3), 1999.</p>
    <p>3. W. D. Gropp, D. K. Kasushik, D. E. Keyes, and B. F. Smith. Towards realistic bounds for implicit CFD codes. In Proceedings of Parallel Computational Fluid Dynamics, pages 241{248, 1999.</p>
  </div>
  <div class="page">
    <p>References (2/3)</p>
    <p>4. G. Heber, A. J. Dolgert, M. Alt, K. A. Mazurkiewicz, and L. Stringer. Fracture mechanics on the Intel Itanium architecture: A case study. In Workshop on EPIC Architectures and Compiler Technology (ACM MICRO 34), Austin, TX, 2001.</p>
    <p>5. D. B. Heras, V. B. Perez, J. C. C. Dominguez, and F. F. Rivera. Modeling and improving locality for irregular problems: sparse matrixvector product on cache memories as a case study. In HPCN Europe, pages 201{210, 1999.</p>
    <p>6. E.-J. Im. Optimizing the performance of sparse matrix-vector multiplication. PhD thesis, University of California, Berkeley, May 2000.</p>
    <p>7. Y. Saad. SPARSKIT: A basic toolkit for sparse matrix computations, 1994. www.cs.umn.edu/Research/arpa/SPARSKIT/sparskit.html.</p>
    <p>8. R. H. Saavedra-Barrera. CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking. PhD thesis, University of California, Berkeley, February 1992.</p>
  </div>
  <div class="page">
    <p>References (3/3)</p>
    <p>9. A. Snavely, L. Carrington, and N. Wolter. Modeling application performance by convolving machine signatures with application proles. 2001.</p>
    <p>10. O. Temam and W. Jalby. Characterizing the behavior of sparse algorithms on caches. In Proceedings of Supercomputing '92, 1992.</p>
    <p>11. R. Vuduc, J. W. Demmel, K. A. Yelick, S. Kamil, R. Nishtala, and B. Lee. Performance optimizations and bounds for sparse matrix-vector multiply. In Proceedings of Supercomputing, Baltimore, MD, USA, November 2002.</p>
    <p>12. R. W. Vuduc. Automatic performance tuning of sparse matrix kernels. PhD thesis, University of California, Berkeley, 2003.</p>
    <p>This work was supported in part by the National Science Foundation under ACI-9619020 ACI0090127 and gifts from: Intel Corporation, Hewlett Packard, and Sun Microsystems.</p>
    <p>The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.</p>
  </div>
</Presentation>
