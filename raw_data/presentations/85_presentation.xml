<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Active Feedback in Ad Hoc IR</p>
    <p>Xuehua Shen, ChengXiang Zhai</p>
    <p>Department of Computer Science</p>
    <p>University of Illinois, Urbana-Champaign</p>
  </div>
  <div class="page">
    <p>Normal Relevance Feedback (RF)</p>
    <p>Feedback</p>
    <p>Judgments: d1 + d2  dk</p>
    <p>Query Retrieval System</p>
    <p>Top K Results d1 3.5 d2 2.4  dk 0.5</p>
    <p>User</p>
    <p>Document Collection</p>
  </div>
  <div class="page">
    <p>Document Selection in RF</p>
    <p>Feedback</p>
    <p>Judgments: d1 + d2  dk</p>
    <p>Query Retrieval System</p>
    <p>Which k docs</p>
    <p>to present ?</p>
    <p>User</p>
    <p>Document Collection</p>
    <p>Can we do better than just presenting top-K? (Consider diversity)</p>
  </div>
  <div class="page">
    <p>Active Feedback (AF)</p>
    <p>An IR system actively selects documents for obtaining relevance judgments</p>
    <p>If a user is willing to judge K documents,</p>
    <p>which K documents should we present</p>
    <p>in order to maximize learning effectiveness?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Framework and specific methods  Experiment design and results  Summary and future work</p>
  </div>
  <div class="page">
    <p>A Framework for Active Feedback</p>
    <p>Consider active feedback as a decision problem  Decide K documents (D) for relevance judgment</p>
    <p>Formalize it as an optimization problem  Optimize the expected learning benefits (loss) by</p>
    <p>requesting relevance judgments on D from the user</p>
    <p>Consider two cases of loss function according to the interaction between documents</p>
  </div>
  <div class="page">
    <p>Formula of the Framework</p>
    <p>* arg min ( , ) ( | , , ) D</p>
    <p>D L D p U q C d</p>
    <p>( , ) ( , , ) ( | , , )</p>
    <p>( , , ) ( | , , )</p>
    <p>j</p>
    <p>k</p>
    <p>i i i</p>
    <p>j</p>
    <p>L D l D j p j D U</p>
    <p>l D j p j d U</p>
    <p>Value of documents for learning</p>
    <p>Independent judgment</p>
    <p>Different judgments</p>
  </div>
  <div class="page">
    <p>Independent Loss</p>
    <p>( , ) ( , , ) ( | , , ) k</p>
    <p>i i i</p>
    <p>j</p>
    <p>L D l D j p j d U</p>
    <p>( , , ) ( , , ) k</p>
    <p>i i i</p>
    <p>l D j l d j</p>
    <p>r</p>
    <p>Independent Loss</p>
    <p>( ) ( , , ) ( | , , ) ( | , , ) i</p>
    <p>i i i i i j</p>
    <p>r d l d j p j d U p U q C d</p>
    <p>*</p>
    <p>arg min ( , , ) ( | , , ) ( | , , ) i</p>
    <p>k</p>
    <p>i i i i D i j</p>
    <p>D l d j p j d U p U q C d</p>
    <p>( , ) ( , , ) ( | , , ) kk</p>
    <p>i i i i i i</p>
    <p>j</p>
    <p>L D l d j p j d U</p>
    <p>Expected loss of each document</p>
  </div>
  <div class="page">
    <p>Independent Loss (cont.)</p>
    <p>Uncertainty Sampling</p>
    <p>( ,1, ) log ( 1 | , )</p>
    <p>( , 0, ) log ( 0 | , ) i i i</p>
    <p>i i i</p>
    <p>l d p R d d C</p>
    <p>l d p R d d C</p>
    <p>( ) ( | , ) ( | , , )i ir d H R d p U q C d</p>
    <p>( ) ( , , ) ( | , , ) ( | , , ) i</p>
    <p>i i i i i j</p>
    <p>r d l d j p j d U p U q C d</p>
    <p>Top K</p>
    <p>, 0 1 0</p>
    <p>, ( ,1, ) ,</p>
    <p>( 0, ) , i i</p>
    <p>i</p>
    <p>d C l d C</p>
    <p>l d C C C</p>
    <p>Relevant docs more useful than non-relevant docs</p>
    <p>More uncertain, more useful</p>
  </div>
  <div class="page">
    <p>Dependent Loss</p>
    <p>First select Top N docs of baseline retrieval</p>
    <p>Cluster N docs into K clusters</p>
    <p>K Cluster Centroid</p>
    <p>MMR</p>
    <p>Gapped Top K</p>
    <p>Pick one doc every G+1 docs</p>
    <p>( , , ) ( 1 | , , ) ( , ) k</p>
    <p>i i i</p>
    <p>L D U p j d U D</p>
    <p>More relevant, more useful</p>
    <p>More diverse, more useful</p>
  </div>
  <div class="page">
    <p>Illustration of Three AF Methods</p>
    <p>Top-K (normal feedback)</p>
    <p>Gapped Top-K</p>
    <p>K-Cluster Centroid</p>
    <p>Aiming at high diversity</p>
  </div>
  <div class="page">
    <p>Evaluating Active Feedback</p>
    <p>Query Select K</p>
    <p>Docs</p>
    <p>K docs</p>
    <p>Judgment File</p>
    <p>+</p>
    <p>Judged Docs</p>
    <p>+ +</p>
    <p>+</p>
    <p>Initial ResultsNo Feedback</p>
    <p>(Top-k, Gapped, Clustering)</p>
    <p>FeedbackFeedback Results</p>
  </div>
  <div class="page">
    <p>Retrieval Methods (Lemur toolkit)</p>
    <p>Query Q</p>
    <p>DDocument D</p>
    <p>Q</p>
    <p>)||( DQD  Results</p>
    <p>KL Divergence</p>
    <p>Feedback Docs F={d1, , dn}</p>
    <p>Active Feedback</p>
    <p>Default parameter settings unless otherwise stated</p>
    <p>FQQ   )1(' F</p>
    <p>Mixture Model Feedback</p>
    <p>Only learn from relevant docs</p>
  </div>
  <div class="page">
    <p>Comparison of Three AF Methods</p>
    <p>Collection</p>
    <p>Active FB Method</p>
    <p>#AFRel</p>
    <p>Per topic</p>
    <p>Include judged docs</p>
    <p>MAP Pr@10doc</p>
    <p>HARD</p>
    <p>Baseline / 0.301 0.501</p>
    <p>Pseudo FB / 0.320 0.515</p>
    <p>Top-K 3.0 0.325 0.527</p>
    <p>Gapped 2.6 0.330** 0.548 * Clustering 2.4 0.332 0.565</p>
    <p>AP88-89</p>
    <p>Baseline / 0.201 0.326</p>
    <p>Pseudo FB / 0.218 0.343</p>
    <p>Top-K 2.2 0.228 0.351</p>
    <p>Gapped 1.5 0.234 * 0.389 ** Clustering 1.3 0.237 ** 0.393 **</p>
    <p>Top-K is the worst!Clustering uses fewest relevant docs</p>
  </div>
  <div class="page">
    <p>Appropriate Evaluation of Active Feedback</p>
    <p>New DB (AP88-89, AP90)</p>
    <p>Original DB with judged docs (AP88-89, HARD)</p>
    <p>+ +</p>
    <p>Original DB without judged docs</p>
    <p>+ +</p>
    <p>Cant tell if the ranking of un-judged documents is improved</p>
    <p>Different methods have different test documents</p>
    <p>See the learning effect more explicitly</p>
    <p>But the docs must be similar to original docs</p>
  </div>
  <div class="page">
    <p>Retrieval Performance on AP90 Dataset</p>
    <p>Method Baseline Pseudo</p>
    <p>FB</p>
    <p>Top K Gapped Top K</p>
    <p>K Cluster Centroid</p>
    <p>MAP 0.203 0.220 0.220 0.222 0.223</p>
    <p>pr@10 0.295 0.317 0.321 0.326** 0.325</p>
    <p>Top-K is consistently the worst!</p>
  </div>
  <div class="page">
    <p>Mixture Model Parameter  Factor</p>
    <p>FQQ   )1('</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Introduce the active feedback problem  Propose a preliminary framework and three</p>
    <p>methods (Top-k, Gapped Top-k, Clustering)</p>
    <p>Study the evaluation strategy  Experiment results show that</p>
    <p>Presenting the top-k is not the best strategy</p>
    <p>Clustering can generate fewer, higher quality feedback examples</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Explore other methods for active feedback  Develop a general framework  Combine pseudo feedback and active feedback</p>
  </div>
  <div class="page">
    <p>Thank you !</p>
    <p>The End</p>
  </div>
</Presentation>
