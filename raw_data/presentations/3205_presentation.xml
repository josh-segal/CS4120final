<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Automatically Detecting Vulnerable Sites Before They Turn Malicious</p>
    <p>Kyle Soska Carnegie Mellon University ECE / Cylab ksoska@cmu.edu</p>
    <p>Nicolas Chris&gt;n Carnegie Mellon University ECE / Cylab nicolasc@cmu.edu</p>
    <p>Sponsors: NSF (CCF-0424422, CNS-1223762) and DHS S&amp;T/CSD (N66001-13-C-0131)</p>
  </div>
  <div class="page">
    <p>Problem Setting</p>
    <p>Adversaries compromise websites  Economically Ra&gt;onal  mone&gt;ze compromises</p>
    <p>Neutral to vic&gt;ms  Maximize volume, efficiency, profits</p>
    <p>Hack&gt;vist  promote social, poli&gt;cal, or religious agenda  Targeted aYacks  Low volume</p>
  </div>
  <div class="page">
    <p>Economically Rational Adversary</p>
    <p>Decisions always maximize profit  Probabilis&gt;c Polynomial Time</p>
    <p>Cannot break standard crypto, session cookies, hashes, etc.</p>
    <p>Does not control significant por&gt;on of web  Cannot perform adversarial machine learning aYacks by poisoning a random sample of the web</p>
    <p>Able to exploit vulnerable web so]ware</p>
  </div>
  <div class="page">
    <p>Mode of Operation Step 1: Find bug or vulnerability in popular web so]ware or content management system (CMS) Step 2: Enumerate sites containing vulnerability</p>
    <p>Step 3: Exploit vulnerable sites Step 4: Mone&gt;ze and profit</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Problem and Goal</p>
    <p>Exis&gt;ng approaches detect if a webpage is already malicious</p>
    <p>Is it possible to predict if a non-malicious website will become malicious in the future?  What would such a system look like?  What requirements are imposed on such a system?  What are the fundamental limita&gt;ons?</p>
  </div>
  <div class="page">
    <p>System Design</p>
    <p>Blacklists</p>
    <p>Zone Files</p>
    <p>Malicious Sites</p>
    <p>Safe Sites</p>
    <p>Template Filter</p>
    <p>Feature Extrac&gt;on</p>
    <p>Stream Classifier</p>
    <p>Archive.org Alexa.com</p>
  </div>
  <div class="page">
    <p>System Properties  Efficiency</p>
    <p>Internet dataset  Interpretability</p>
    <p>Need to build intui&gt;on about why the site will become compromised</p>
    <p>Robustness to Imbalanced Data  Far more benign examples than malicious ones</p>
    <p>Robustness To Mislabeled Data  Blacklists may contain errors or be incomplete</p>
    <p>Adap&gt;ve  Internet is a concept dri]ing, requires ac&gt;ve adapta&gt;on</p>
  </div>
  <div class="page">
    <p>Dataset</p>
    <p>Blacklists</p>
    <p>Zone Files</p>
    <p>Malicious Sites</p>
    <p>Safe Sites</p>
    <p>Template Filter</p>
    <p>Feature Extrac&gt;on</p>
    <p>Stream Classifier</p>
    <p>Archive.org Alexa.com</p>
  </div>
  <div class="page">
    <p>Dataset Type Instances Archived</p>
    <p>Instances % Archived</p>
    <p>PhishTank 91,555 34,922 38.1 Search Redirec&gt;on*</p>
    <p>.com Zone Files 336,671 336,671 N/A</p>
    <p>PhishTank: Feb 2013  Dec 2013  Search Redirec&gt;on: Oct 2011  Sept 2013  Zone Files: Feb 2010  Sept 2013</p>
    <p>* Leon&gt;adis et al., 2014</p>
  </div>
  <div class="page">
    <p>Filtering</p>
    <p>Blacklists</p>
    <p>Zone Files</p>
    <p>Malicious Sites</p>
    <p>Safe Sites</p>
    <p>Template Filter</p>
    <p>Feature Extrac&gt;on</p>
    <p>Stream Classifier</p>
    <p>Archive.org Alexa.com</p>
  </div>
  <div class="page">
    <p>Filtering</p>
    <p>Naviga&gt;on</p>
    <p>User Content</p>
    <p>Social Media Links</p>
  </div>
  <div class="page">
    <p>Filtering  Based on [Yi et al., 2003]  Compute entropy-like heuris&gt;c Composite Importance (CmpImp [0, 1]) for each element on a page</p>
    <p>Remove elements above a fixed threshold</p>
  </div>
  <div class="page">
    <p>Original Page</p>
  </div>
  <div class="page">
    <p>Threshold = 0.99</p>
  </div>
  <div class="page">
    <p>Threshold = 0.1</p>
  </div>
  <div class="page">
    <p>Feature Extraction</p>
    <p>Blacklists</p>
    <p>Zone Files</p>
    <p>Malicious Sites</p>
    <p>Safe Sites</p>
    <p>Template Filter</p>
    <p>Feature Extrac&gt;on</p>
    <p>Stream Classifier</p>
    <p>Archive.org Alexa.com</p>
  </div>
  <div class="page">
    <p>Feature Set</p>
    <p>Traffic Features  Site Rank  Links into site  Load Percen&gt;le  more</p>
    <p>Content Features  HTML Tags (type, content, aYributes)</p>
  </div>
  <div class="page">
    <p>Dynamic Features  Millions of unique HTML tags (including content)</p>
    <p>Solu&gt;on: order tags by some sta&gt;s&gt;c, select top N  ACC2 based on [Foreman, 2003]  Let , denote the set of benign and malicious sites respec&gt;vely,  the set of tags from a site, then ACC2 for a tag x can be defined as:</p>
    <p>()=||:|/||  |:|/|| |</p>
  </div>
  <div class="page">
    <p>Prominent Features After 90,000 Samples</p>
    <p>Feature Sta&gt;s&gt;c Value</p>
    <p>meta{content: Wordpress 3.2.1, name: generator}</p>
    <p>ul{class: [xoxo, blogroll]} 0.0446</p>
    <p>You can start edi&gt;ng here. 0.0421</p>
    <p>meta{content: Wordpress 3.3.1, name: generator}</p>
    <p>/all in one seo pack 0.0252</p>
    <p>span{class: [breadcrumbs, pathway]} 0.0226</p>
    <p>If Comments are open, but there are no comments.</p>
    <p>div{id: content_disclaimer} 0.0039</p>
  </div>
  <div class="page">
    <p>Varying Window Sizes</p>
    <p>Low Transient High Variance</p>
    <p>High Transient Low Variance</p>
  </div>
  <div class="page">
    <p>CMS Evolution</p>
    <p>AYack Campaign</p>
    <p>AYack Campaign</p>
    <p>AYack Campaign Low Ac&gt;vity</p>
  </div>
  <div class="page">
    <p>Parking Page Feature</p>
    <p>Similar value over 1 year later!</p>
  </div>
  <div class="page">
    <p>Classification</p>
    <p>Blacklists</p>
    <p>Zone Files</p>
    <p>Malicious Sites</p>
    <p>Safe Sites</p>
    <p>Template Filter</p>
    <p>Feature Extrac&gt;on</p>
    <p>Stream Classifier</p>
    <p>Archive.org Alexa.com</p>
  </div>
  <div class="page">
    <p>Classification</p>
    <p>Largely based on [Gao et al., 2007]  Break input data stream into blocks  Resample input blocks  Train ensemble C4.5 decision tree classifiers using Hoeffding bounds [Domingos et al., 2000]</p>
    <p>Retrain periodically using new dynamic features</p>
  </div>
  <div class="page">
    <p>Classification Results</p>
    <p>Good Opera&gt;ng Point</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>Only makes sense when page content and traffic sta&gt;s&gt;cs are risk factors of malice  Sites hacked via weak passwords or via social engineering aYacks violate this</p>
    <p>Sites that are maliciously hosted may violate this</p>
    <p>Requires some sites to become compromised in order to make predic&gt;ons</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Predic&gt;ng websites that become malicious in the future is possible!</p>
    <p>Acceptable performance can be achieved even on our modest dataset</p>
    <p>Kyle Soska  ksoska@cmu.edu</p>
  </div>
</Presentation>
