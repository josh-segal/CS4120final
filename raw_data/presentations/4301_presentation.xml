<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>YING LIN 1, SHENGQI YANG 2, VESELIN STOYANOV 3, HENG JI 1</p>
    <p>A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling</p>
  </div>
  <div class="page">
    <p>MOTIVATION</p>
    <p>Most high-performance data-driven models rely on a large amount of labeled training data. However, a model trained on one language usually performs poorly on another language.</p>
    <p>Extend existing services to more languages:  Collect, select, and pre-process data  Compile guidelines for new languages  Train annotators to qualify for annotation tasks  Annotate data  Adjudicate annotations and assess the annotation quality and inter-annotator agreement</p>
  </div>
  <div class="page">
    <p>MOTIVATION</p>
    <p>Most high-performance data-driven models rely on a large amount of labeled training data. However, a model trained on one language usually performs poorly on another language.</p>
    <p>Extend existing services to more languages:  Collect, select, and pre-process data  Compile guidelines for new languages  Train annotators to qualify for annotation tasks  Annotate data  Adjudicate annotations and assess inter-annotator agreement</p>
    <p>Rapid and low-cost development of capabilities for low-resource languages.  Disaster response and recovery</p>
  </div>
  <div class="page">
    <p>TRANSFER LEARNING &amp; MULTI-TASK LEARNING</p>
    <p>Leverage existing data of related languages and tasks and transfer knowledge to our target task.</p>
    <p>The Tasman Sea lies between Australia and New Zealand.</p>
    <p>English French</p>
    <p>Multi-task Learning (MTL) is an effective solution for knowledge transfer across tasks.  In the context of neural network architectures, we usually perform MTL by sharing parameters</p>
    <p>across models.</p>
    <p>Model A</p>
    <p>Model B</p>
    <p>Task A Data</p>
    <p>Task B Data</p>
    <p>Parameter Sharing: When optimizing model A , we update and hence . In this way, we can partially train model B as .</p>
    <p>lAustralie est spare de lAsie par les mers dArafuraet de Timor et de la Nouvelle-Zlande par la mer de Tasman</p>
  </div>
  <div class="page">
    <p>SEQUENCE LABELING</p>
    <p>To illustrate our idea, we take sequence labeling as a case study.  In the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., Part-of-speech</p>
    <p>tag) to each token in a sentence.  It underlies a range of fundamental NLP tasks, including POS Tagging, Name Tagging, and Chunking.</p>
    <p>B-, I-, E-, S-: beginning of a mention, inside of a mention, the end of a mention and a single-token mention  O: not part of any mention  Although we only focus on sequence labeling in this work, our architecture can be adapted for many NLP tasks</p>
    <p>with slight modification.</p>
    <p>PER GPE GPE</p>
    <p>PER ORG GPE</p>
    <p>Itamar Rabinovich, who as Israel's ambassador to Washington conducted unfruitful negotiations with</p>
    <p>Syria, told Israel Radio it looked like Damascus wated to talk rather than fight.</p>
    <p>NAME TAGGING</p>
    <p>POS TAGGING Koalas are largely sedentary and sleep up to 20 hours a day.</p>
    <p>NNS VBP RB JJ CC VB IN TO CD NNS DT NN</p>
    <p>B-PER E-PER</p>
  </div>
  <div class="page">
    <p>BASE MODEL: LSTM-CRF (CHIU AND NICHOLS, 2016)</p>
    <p>Input Sentence</p>
    <p>Features</p>
    <p>Tagger</p>
    <p>Each token in the given sentence is represented as the combination of its word embedding and character feature vector.</p>
    <p>The Bidirectional LSTM (long-short term memory) processes the input sentence from both directional, encodeing each token and its context into a vector (hidden states).</p>
    <p>The linear layer projects hidden states to label space.</p>
    <p>The CRF layer models the dependencies between labels.</p>
    <p>CRF</p>
    <p>Linear Layer</p>
    <p>Bi-LSTM</p>
    <p>Word Embedding Character Embedding</p>
    <p>Characterlevel CNN</p>
  </div>
  <div class="page">
    <p>PREVIOUS TRANSFER MODELS FOR SEQUENCE LABELING</p>
    <p>Yang et al. (2017) proposed three transfer learning architectures for different use cases. * Above figures are adapted from (Yang et al., 2017)</p>
    <p>T-B: Cross-domain transfer With disparate label sets</p>
    <p>T-A: Cross-domain transfer T-C: Cross-lingual Transfer</p>
  </div>
  <div class="page">
    <p>OUR MODEL: MULTI-LINGUAL MULTI-TASK ARCHITECTURE</p>
    <p>Our model  combines multi-lingual transfer and multi-task transfer  is able to transfer knowledge from multiple sources</p>
  </div>
  <div class="page">
    <p>OUR MODEL: MULTI-LINGUAL MULTI-TASK MODEL</p>
    <p>LSTM-CRF LSTM-CRFLSTM-CRF LSTM-CRF</p>
    <p>Cross-task Transfer POS Tagging ! Name</p>
    <p>Tagging</p>
    <p>Cross-lingual Transfer English ! Spanish</p>
  </div>
  <div class="page">
    <p>OUR MODEL: MULTI-LINGUAL MULTI-TASK MODEL</p>
    <p>The bidirectional LSTM, character embeddings and character-level networks serve as the basis of the architecture. This level of parameter sharing aims to provide universal word representation and feature extraction capability for all tasks and languages</p>
  </div>
  <div class="page">
    <p>OUR MODEL: MULTI-LINGUAL MULTI-TASK MODEL - CROSS-LINGUAL TRANSFER</p>
    <p>For the same task, most components are shared between languages.  Although our architecture does not require aligned cross-lingual word embeddings, we also evaluate it with</p>
    <p>aligned embeddings generated using MUSEs unsupervised model (Conneau et al. 2017).</p>
  </div>
  <div class="page">
    <p>OUR MODEL: MULTI-LINGUAL MULTI-TASK MODEL - LINEAR LAYER</p>
    <p>English: improvement, development, payment,  French: vraiment, compltement, immdiatement We combine the output of the shared linear layer and the output of the language-specific linear layer using</p>
    <p>=    + (1  )</p>
    <p>where . and are optimized during training. is the LSTM hidden states. As is a square matrix, , , and have the same dimension</p>
    <p>We add a language-specific linear layer to allow the model to behave differently towards some features for different languages.</p>
  </div>
  <div class="page">
    <p>OUR MODEL: MULTI-LINGUAL MULTI-TASK MODEL - CROSS-TASK TRANSFER</p>
    <p>Linear layers and CRF layers are not shared between different tasks.  Tasks of the same language use the same embedding matrix: mutually enhance word representations</p>
  </div>
  <div class="page">
    <p>ALTERNATING TRAINING</p>
    <p>() =</p>
    <p>To optimize multiple tasks within one model, we adopt the alternating training approach in (Luong et al., 2016).</p>
    <p>At each training step, we sample a task with probability:</p>
    <p>d1 d3 d2 d2d3</p>
    <p>In our experiments, instead of tuning mixing rate , we estimate it by:</p>
    <p>=</p>
    <p>where is the task coefficient, is the language coefficient, and is the number of training examples. (or ) takes the value 1 if the task (or language) of is the same as that of the target task; Otherwise it takes the value 0.1.</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - DATA SETS</p>
    <p>Name Tagging  English: CoNLL 2003  Spanish and Dutch: CoNLL 2002  Russian: LDC2016E95 (Russian Representative Language Pack)  Chechen: TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus</p>
    <p>Part-of-speech Tagging: CoNLL 2017 (Universal Dependencies)</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - SETUP</p>
    <p>50-dimensional pre-trained word embeddings  English, Spanish and Dutch: Wikipedia  Russian: LDC2016E95  Chechen: TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus</p>
    <p>Cross-lingual word embedding: we aligned mono-lingual pre-trained word embeddings with MUSE (https://github.com/facebookresearch/MUSE).</p>
    <p>50-dimensional randomly initialized character embeddings  Optimization: SGD with momentum (), gradient clipping (threshold: 5.0) and exponential learning rate</p>
    <p>decay.</p>
    <p>CharCNN Filter Number 20 Highway Layer Number 2 Highway Activation Function SeLU LSTM Hidden State Size 171</p>
    <p>LSTM Dropout Rate 0.6 Learning Rate 0.02 Batch Size 19</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - COMPARISON OF DIFFERENT MODELS</p>
    <p>Target task: Dutch Name Tagging  Auxiliary task: Dutch POS Tagging, English Name Tagging, English POS Tagging</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - COMPARISON OF DIFFERENT MODELS</p>
    <p>Target task: Spanish Name Tagging  Auxiliary task: Spanish POS Tagging, English Name Tagging, English POS Tagging</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - COMPARISON OF DIFFERENT MODELS</p>
    <p>Target task: Chechen Name Tagging  Auxiliary task: Russian POS Tagging + Name Tagging or English POS Tagging + Name Tagging</p>
    <p>All training data: Baseline: 78.9% Our Model : 82.3%</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - COMPARISON WITH STATE-OF-THE-ART MODELS</p>
    <p>Language Model F-score</p>
    <p>Dutch Glilick et al. (2016) 82.84</p>
    <p>Lample et al. (2016) 81.74</p>
    <p>Yang et al. (2017) 85.19</p>
    <p>Baseline 85.14</p>
    <p>Cross-task 85.69</p>
    <p>Cross-lingual 85.71</p>
    <p>Our Model 86.55</p>
    <p>Spanish Glilick et al. (2016) 82.95</p>
    <p>Lample et al. (2016) 85.75</p>
    <p>Yang et al. (2017) 85.77</p>
    <p>Baseline 85.44</p>
    <p>Cross-task 85.37</p>
    <p>Cross-lingual 85.02</p>
    <p>Our Model 85.88  We also compared our model with state-of-the-art models with all training data.</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - COMPARISON WITH STATE-OF-THE-ART MODELS</p>
    <p>Baseline</p>
    <p>Our Model</p>
    <p>Incorrect</p>
    <p>Correct</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - CROSS-TASK TRANSFER VS CROSS-LINGUAL TRANSFER</p>
    <p>With 100 Dutch training sentences:  The baseline model misses the name</p>
    <p>Ingeborg Marx.  The cross-task transfer model finds the name</p>
    <p>but assigns a wrong tag to Marx.  The cross-lingual transfer model correctly</p>
    <p>identifies the whole name.  The task-specific knowledge that B-PER !</p>
    <p>S-PER is an invalid transition will not be learned in the POS Tagging model.</p>
    <p>The cross-lingual transfer model transfers such knowledge through the shared CRF layer.</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - ABLATION STUDIES</p>
    <p>Model 0 10 100 200 All</p>
    <p>Basic 2.06 20.03 47.98 51.52 77.63</p>
    <p>+C 1.69 24.22 48.53 56.26 83.38</p>
    <p>+CL 9.62 25.97 49.54 56.29 83.37</p>
    <p>+CLS 3.21 25.43 50.67 56.34 84.02</p>
    <p>+CLSH 7.70 30.48 53.73 58.09 84.68</p>
    <p>+CLSHD 12.12 35.82 57.33 63.27 86.00</p>
    <p>C: Character embedding; L: Shared LSTM; S: Language-specific H: Highway Networks; D: Dropout</p>
    <p>Generally, all components improve the performance.  Sharing the LSTM layer slightly hurts the performance in the high-resource setting.  Language-specific Layer can impair the performance in extreme low-resource settings because this layer is trained only on the target task</p>
    <p>data.</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - EFFECT OF THE AMOUNT OF AUXILIARY TASK DATA</p>
    <p>Does our model heavily rely on the amount of auxiliary task data?  The performance goes up when we increase the sample rate from 0 to 0.2 for auxiliary task data.  However, we do not observe substantial improvement when we further increase the sample rate.</p>
    <p>Using only 1% auxiliary data, our model already obtains 3.7%-9.7% absolute F-score gains.</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS - EFFECT OF THE AMOUNT OF AUXILIARY TASK DATA</p>
    <p>Does our model heavily rely on the amount of auxiliary task data?  The performance goes up when we increase the sample rate from 0 to 0.2 for auxiliary task data.  However, we do not observe substantial improvement when we further increase the sample rate.</p>
    <p>Using only 1% auxiliary data, our model already obtains 3.7%-9.7% absolute F-score gains.</p>
  </div>
  <div class="page">
    <p>REFERENCES</p>
    <p>Jason P. C. Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. TACL, 4:357370  Alexis Conneau, Guillaume Lample, MarcAurelio Ranzato, Ludovic Denoyer, and Herve J  egou. 2017.  Word</p>
    <p>translation without parallel data. arXiv preprint arXiv:1710.04087  Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amar nag Subramanya. 2016. Multilingual language processing from</p>
    <p>bytes. In NAACL HLT  Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural</p>
    <p>architectures for named entity recognition. In NAACL HLT  Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. 2017. Transfer lear ning for sequence tagging with</p>
    <p>hierarchical recurrent networks. In ICLR</p>
  </div>
  <div class="page">
    <p>Thank you</p>
    <p>YING LIN, SHENGQI YANG, VESELIN STOYANOV, HENG JI</p>
    <p>A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling</p>
  </div>
</Presentation>
