<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Camdoop Exploiting In-network Aggregation for Big Data Applications</p>
    <p>Paolo Costa costa@imperial.ac.uk</p>
    <p>joint work with Austin Donnelly, Antony Rowstron, and Greg OShea (MSR Cambridge)</p>
  </div>
  <div class="page">
    <p>MapReduce Overview</p>
    <p>Map  Processes input data and generates (key, value) pairs</p>
    <p>Shuffle  Distributes the intermediate pairs to the reduce tasks</p>
    <p>Reduce  Aggregates all values associated to each key</p>
    <p>Chunk 0</p>
    <p>Chunk 1</p>
    <p>Chunk 2</p>
    <p>Input file</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Intermediate results Final results</p>
  </div>
  <div class="page">
    <p>Problem</p>
    <p>Shuffle phase is challenging for data center networks  All-to-all traffic pattern with O(N2) flows  Led to proposals for full-bisection bandwidth</p>
    <p>Split 0</p>
    <p>Split 1</p>
    <p>Split 2</p>
    <p>Input file</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Intermediate results Final results</p>
  </div>
  <div class="page">
    <p>Data Reduction</p>
    <p>The final results are typically much smaller than the intermediate results</p>
    <p>In most Facebook jobs the final size is 5.4 % of the intermediate size</p>
    <p>In most Yahoo jobs the ratio is 8.2 %</p>
    <p>Split 0</p>
    <p>Split 1</p>
    <p>Split 2</p>
    <p>Input file</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Intermediate results Final results</p>
  </div>
  <div class="page">
    <p>Data Reduction</p>
    <p>The final results are typically much smaller than the intermediate results</p>
    <p>In most Facebook jobs final size is 5.4 % of the intermediate size</p>
    <p>In most Yahoo jobs the ratio is 8.2 %</p>
    <p>Split 0</p>
    <p>Split 1</p>
    <p>Split 2</p>
    <p>Input file</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Intermediate results Final results</p>
    <p>How can we exploit this to reduce the traffic and improve the performance of the shuffle phase?</p>
  </div>
  <div class="page">
    <p>Background: Combiners</p>
    <p>Split 0</p>
    <p>Split 1</p>
    <p>Split 2</p>
    <p>Input file</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Intermediate results Final results</p>
    <p>To reduce the data transferred in the shuffle, users can specify a combiner function  Aggregates the local intermediate pairs</p>
    <p>Server-side only =&gt; limited aggregation 6/52</p>
  </div>
  <div class="page">
    <p>Background: Combiners</p>
    <p>Split 0</p>
    <p>Split 1</p>
    <p>Split 2</p>
    <p>Input file</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Map Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Reduce Task</p>
    <p>Intermediate results Final results</p>
    <p>Combiner</p>
    <p>Combiner</p>
    <p>Combiner</p>
    <p>To reduce the data transferred in the shuffle, users can specify a combiner function  Aggregates the local intermediate pairs</p>
    <p>Server-side only =&gt; limited aggregation 7/52</p>
  </div>
  <div class="page">
    <p>Distributed Combiners</p>
    <p>It has been proposed to use aggregation trees in MapReduce to perform multiple steps of combiners  e.g., rack-level aggregation [Yu et al., SOSP09]</p>
  </div>
  <div class="page">
    <p>What happens when we map the tree to a typical data center topology?</p>
    <p>The server link is the bottleneck Full-bisection bandwidth does not help here</p>
    <p>Mismatch between physical and logical topology Two logical links are mapped onto the same physical link</p>
    <p>Logical and Physical Topology</p>
    <p>Logical topology Physical topology</p>
    <p>ToR Switch</p>
  </div>
  <div class="page">
    <p>Logical and Physical Topology</p>
    <p>Logical topology Physical topology</p>
    <p>ToR Switch</p>
    <p>Only 500 Mbps per child</p>
    <p>What happens when we map the tree to a typical data center topology?</p>
    <p>The server link is the bottleneck Full-bisection bandwidth does not help here</p>
    <p>Mismatch between physical and logical topology Two logical links are mapped onto the same physical link</p>
  </div>
  <div class="page">
    <p>Logical and Physical Topology</p>
    <p>Logical topology Physical topology</p>
    <p>ToR Switch</p>
    <p>Only 500 Mbps per child</p>
    <p>What happens when we map the tree to a typical data center topology?</p>
    <p>The server link is the bottleneck Full-bisection bandwidth does not help here</p>
    <p>Mismatch between physical and logical topology Two logical links are mapped onto the same physical link</p>
    <p>Camdoop Goal Perform the combiner functions within the network as</p>
    <p>opposed to application-level solutions</p>
    <p>Reduce shuffle time by aggregating packets on path</p>
  </div>
  <div class="page">
    <p>How Can We Perform In-network Processing?</p>
    <p>We exploit CamCube  Direct-connect topology  3D torus  Uses no switches / routers</p>
    <p>for internal traffic</p>
    <p>Servers intercept, forward and process packets</p>
    <p>Nodes have (x,y,z)coordinates  This defines a key-space (=&gt; key-based routing)  Coordinates are locally re-mapped in case of failures</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>How Can We Perform In-network Processing?</p>
    <p>We exploit CamCube  Direct-connect topology  3D torus  Uses no switches / routers</p>
    <p>for internal traffic</p>
    <p>Servers intercept, forward and process packets</p>
    <p>Nodes have (x,y,z)coordinates  This defines a key-space (=&gt; key-based routing)  Coordinates are locally re-mapped in case of failures</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>How Can We Perform In-network Processing?</p>
    <p>We exploit CamCube  Direct-connect topology  3D torus  Uses no switches / routers</p>
    <p>for internal traffic</p>
    <p>Servers intercept, forward and process packets</p>
    <p>Nodes have (x,y,z)coordinates  This defines a key-space (=&gt; key-based routing)  Coordinates are locally re-mapped in case of failures</p>
    <p>y</p>
    <p>z</p>
    <p>(1,2,2)</p>
    <p>x</p>
    <p>(1,2,1)</p>
  </div>
  <div class="page">
    <p>How Can We Perform In-network Processing?</p>
    <p>We exploit CamCube  Direct-connect topology  3D torus  Uses no switches / routers</p>
    <p>for internal traffic</p>
    <p>Servers intercept, forward and process packets</p>
    <p>Nodes have (x,y,z)coordinates  This defines a key-space (=&gt; key-based routing)  Coordinates are locally re-mapped in case of failures</p>
    <p>y</p>
    <p>z</p>
    <p>(1,2,2)</p>
    <p>x</p>
    <p>(1,2,1)</p>
    <p>Key property No distinction between network and computation devices</p>
    <p>Servers can perform arbitrary packet processing on-path</p>
  </div>
  <div class="page">
    <p>Mapping a tree</p>
    <p>on a switched topology  on CamCube</p>
    <p>The 1 Gbps link becomes the bottleneck</p>
    <p>Packets are aggregated on path (=&gt; less traffic)</p>
    <p>1:1 mapping btw. logical and physical topology</p>
  </div>
  <div class="page">
    <p>Camdoop Design</p>
    <p>Goals</p>
  </div>
  <div class="page">
    <p>Design Goal #1</p>
    <p>Programming Model</p>
    <p>Camdoop adopts the same MapReduce model</p>
    <p>GFS-like distributed file-system  Each server runs map tasks on local chunks</p>
    <p>We use a spanning tree  Combiners aggregate map tasks and children results (if any)</p>
    <p>and stream the results to the parents  The root runs the reduce task and generates the final output</p>
  </div>
  <div class="page">
    <p>Design Goal #2 Network locality</p>
    <p>How to map the tree nodes to servers?</p>
  </div>
  <div class="page">
    <p>Design Goal #2 Network locality</p>
    <p>Map task outputs are always read from the local disk</p>
  </div>
  <div class="page">
    <p>Design Goal #2 Network locality</p>
    <p>The parent-children are mapped on physical neighbors</p>
    <p>(1,2,1)</p>
    <p>(1,2,2) (1,1,1)</p>
  </div>
  <div class="page">
    <p>Design Goal #2 Network locality</p>
    <p>How to map the tree nodes to servers?</p>
    <p>Map task outputs are always read from the local disk</p>
    <p>The parent-children are mapped on physical neighbors</p>
    <p>(1,2,1)</p>
    <p>(1,2,2) (1,1,1)</p>
    <p>This ensures maximum locality and optimizes network transfer</p>
  </div>
  <div class="page">
    <p>Network Locality Logical View Physical View (3D Torus)</p>
    <p>One physical link is used by one and only one logical link</p>
  </div>
  <div class="page">
    <p>Design Goal #3 Load Distribution</p>
  </div>
  <div class="page">
    <p>Design Goal #3 Load Distribution</p>
    <p>Poor server load distribution</p>
    <p>Only 1 Gbps (instead of 6)</p>
    <p>Different in-degree</p>
  </div>
  <div class="page">
    <p>Design Goal #3 Load Distribution</p>
    <p>Only 1 Gbps (instead of 6)</p>
    <p>Poor bandwidth utilization</p>
  </div>
  <div class="page">
    <p>Design Goal #3 Load Distribution</p>
    <p>Poor server load distribution Poor bandwidth utilization</p>
    <p>Solution: stripe the data across disjoint trees Different links are used Improves load distribution</p>
  </div>
  <div class="page">
    <p>Design Goal #3</p>
    <p>First issue: Poor load distribution Second issue: Poor bandwidth utilization</p>
    <p>Solution: stripe the data across 6 disjoint trees All links are used =&gt; (Up to) 6 Gbps / server</p>
    <p>Good load distribution</p>
  </div>
  <div class="page">
    <p>Design Goal #4</p>
    <p>Fault-tolerance</p>
    <p>The tree is built in the coordinate space  CamCube remaps coordinates in case of failures</p>
    <p>Details in the paper</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Testbed  27-server CamCube (3 x 3 x 3)  Quad-core Intel Xeon 5520 2.27 Ghz  12GB RAM  6 Intel PRO/1000 PT 1 Gbps ports  Runtime &amp; services implemented in user-space</p>
    <p>Simulator  Packet-level simulator (CPU overhead not modelled)  512-server (8x8x8) CamCube</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Design and implementation recap</p>
    <p>Camdoop</p>
    <p>Shuffle &amp; reduce parallelized</p>
    <p>Reduce phase is parallelized with the shuffle phase  Since all streams are ordered, as soon as the root receive at least</p>
    <p>one packet from all children, it can start the reduce function  No need to store to disk intermediate results on reduce servers</p>
    <p>Reduce Shuffle Map</p>
    <p>Shuffle Map</p>
    <p>Reduce</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Design and implementation recap</p>
    <p>Camdoop</p>
    <p>Shuffle &amp; reduce parallelized</p>
    <p>CamCube</p>
    <p>Six disjoint trees</p>
    <p>In - network aggregation</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Design and implementation recap</p>
    <p>Camdoop TCP Camdoop (switch)</p>
    <p>Shuffle &amp; reduce parallelized</p>
    <p>CamCube</p>
    <p>Six disjoint trees</p>
    <p>In - network aggregation</p>
    <p>Reduce Shuffle Map Shuffle Map</p>
    <p>TCP Camdoop (switch)  27 CamCube servers attached to a ToR switch  TCP is used to transfer data in the shuffle phase</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Design and implementation recap</p>
    <p>Camdoop TCP Camdoop (switch) Camdoop (no agg )</p>
    <p>Shuffle &amp; reduce parallelized</p>
    <p>CamCube</p>
    <p>Six disjoint trees</p>
    <p>In - network aggregation</p>
    <p>Reduce Shuffle Map Shuffle Map</p>
    <p>TCP Camdoop (switch)  27 CamCube servers attached to a ToR switch  TCP is used to transfer data in the shuffle phase</p>
    <p>Camdoop (no agg)  Like Camdoop but without in-network aggregation  Shows the impact of just running on CamCube</p>
  </div>
  <div class="page">
    <p>Validation against Hadoop &amp; Dryad</p>
    <p>Sort and WordCount</p>
    <p>Camdoop baselines are competitive against Hadoop and Dryad</p>
    <p>Several reasons:  Shuffle and reduce parellized  Fine-tuned implementation</p>
    <p>Sort WordCount T</p>
    <p>im e</p>
    <p>l o</p>
    <p>g sc</p>
    <p>a le</p>
    <p>( s)</p>
    <p>Hadoop Dryad/DryadLINQ</p>
    <p>TCP Camdoop (switch) Camdoop (no agg)</p>
    <p>Worse</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Validation against Hadoop &amp; Dryad</p>
    <p>Sort and WordCount</p>
    <p>Camdoop baselines are competitive against Hadoop and Dryad</p>
    <p>Several reasons:  Shuffle and reduce parellized  Fine-tuned implementation</p>
    <p>Sort WordCount T</p>
    <p>im e</p>
    <p>l o</p>
    <p>g sc</p>
    <p>a le</p>
    <p>( s)</p>
    <p>Hadoop Dryad/DryadLINQ</p>
    <p>TCP Camdoop (switch) Camdoop (no agg)</p>
    <p>We consider these as our</p>
    <p>baselines</p>
    <p>Worse</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Parameter Sweep</p>
    <p>Output size / intermediate size (S)  S=1 (no aggregation)</p>
    <p>o Every key is unique  S=1/N  0 (full aggregation)</p>
    <p>o Every key appears in all map task outputs</p>
    <p>We use synthetic workloads to explore different value of S o Intermediate data size is 22.2 GB (843 MB/server)</p>
    <p>Reduce tasks (R)  R= 1 (all-to-one)</p>
    <p>o E.g., Interactive queries, top-K jobs  R=N (all-to-all)</p>
    <p>o Common setup in MapReduce jobs o N output files are generated</p>
  </div>
  <div class="page">
    <p>All-to-one (R=1)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p>Output size/ intermediate size (S)</p>
    <p>TCP Camdoop (switch)</p>
    <p>Camdoop (no agg)</p>
    <p>Camdoop</p>
    <p>Worse</p>
    <p>Better</p>
    <p>Full aggregation</p>
    <p>No aggregation</p>
  </div>
  <div class="page">
    <p>All-to-one (R=1)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p>Output size/ intermediate size (S)</p>
    <p>TCP Camdoop (switch)</p>
    <p>Camdoop (no agg)</p>
    <p>Camdoop</p>
    <p>Worse</p>
    <p>Better</p>
    <p>Full aggregation</p>
    <p>No aggregation</p>
    <p>Performance independent of S</p>
    <p>Impact of in-network</p>
    <p>aggregation</p>
    <p>Impact of running on CamCube</p>
  </div>
  <div class="page">
    <p>All-to-one (R=1)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p>Output size/ intermediate size (S)</p>
    <p>TCP Camdoop (switch)</p>
    <p>Camdoop (no agg)</p>
    <p>Camdoop</p>
    <p>Worse</p>
    <p>Better</p>
    <p>Full aggregation</p>
    <p>No aggregation</p>
    <p>Performance independent of S</p>
    <p>Impact of in-network</p>
    <p>aggregation</p>
    <p>Facebook reported aggregation ratio</p>
    <p>Impact of running on CamCube</p>
  </div>
  <div class="page">
    <p>All-to-all (R=27)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p>Output size / intermediate size (S)</p>
    <p>TCP Camdoop (switch) Camdoop (no agg)</p>
    <p>Camdoop Switch 1 Gbps (bound)</p>
    <p>Worse</p>
    <p>Better</p>
    <p>Full aggregation</p>
    <p>No aggregation</p>
  </div>
  <div class="page">
    <p>All-to-all (R=27)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p>Output size / intermediate size (S)</p>
    <p>TCP Camdoop (switch) Camdoop (no agg)</p>
    <p>Camdoop Switch 1 Gbps (bound)</p>
    <p>Impact of running on CamCube</p>
    <p>Worse</p>
    <p>Better</p>
    <p>Full aggregation</p>
    <p>No aggregation</p>
    <p>Impact of in-network</p>
    <p>aggregation</p>
    <p>Maximum theoretical performance over the switch</p>
  </div>
  <div class="page">
    <p>Number of reduce tasks (S=0)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p># reduce tasks (R)</p>
    <p>TCP Camdoop (switch) Camdoop (no agg) Camdoop Worse</p>
    <p>Better</p>
    <p>All-to-one All-to-all</p>
  </div>
  <div class="page">
    <p>Number of reduce tasks (S=0)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p># reduce tasks (R)</p>
    <p>TCP Camdoop (switch) Camdoop (no agg) Camdoop Worse</p>
    <p>Better</p>
    <p>R does not (significantly) impact performance</p>
    <p>All-to-one All-to-all</p>
    <p>Performance depends on R</p>
    <p>Implementation bottleneck</p>
  </div>
  <div class="page">
    <p>Number of reduce tasks (S=0)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p># reduce tasks (R)</p>
    <p>TCP Camdoop (switch) Camdoop (no agg) Camdoop Worse</p>
    <p>Better</p>
    <p>R does not (significantly) impact performance</p>
    <p>All-to-one All-to-all</p>
    <p>Performance depends on R</p>
    <p>Camdoop decouples the job execution time from the number of output files generated</p>
  </div>
  <div class="page">
    <p>Behavior at scale (simulated)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p># reduce tasks (R) logscale</p>
    <p>Switch 1 Gbps (bound) Camdoop</p>
    <p>N=512, S= 0</p>
    <p>Worse</p>
    <p>Better</p>
    <p>All-to-one All-to-all</p>
  </div>
  <div class="page">
    <p>Behavior at scale (simulated)</p>
    <p>T im</p>
    <p>e (</p>
    <p>s) l</p>
    <p>o g</p>
    <p>sc a</p>
    <p>le</p>
    <p># reduce tasks (R) logscale</p>
    <p>Switch 1 Gbps (bound) Camdoop</p>
    <p>N=512, S= 0</p>
    <p>Worse</p>
    <p>Better</p>
    <p>All-to-one All-to-all</p>
    <p>This assumes full-bisection</p>
    <p>bandwidth</p>
  </div>
  <div class="page">
    <p>More experiments (failures, multiple jobs,) in the paper</p>
    <p>Beyond MapReduce</p>
  </div>
  <div class="page">
    <p>Beyond MapReduce  The partition-aggregate model</p>
    <p>also common in interactive services  e.g., Bing Search, Google Dremel</p>
    <p>Small-scale data  10s to 100s of KB returned per server</p>
    <p>Typically, these services use one reduce task (R=1)  Single result must</p>
    <p>be returned to the user</p>
    <p>Full aggregation is common (S  0)  E.g., N servers generate their best k responses each</p>
    <p>and the final result contains the best k responses</p>
    <p>Web server</p>
    <p>requests</p>
    <p>Cache</p>
    <p>Leaf servers</p>
    <p>Parent servers</p>
  </div>
  <div class="page">
    <p>Small-scale data (R=1, S=0) Worse</p>
    <p>Better 0</p>
    <p>T im</p>
    <p>e (</p>
    <p>m s)</p>
    <p>Input data size / server</p>
    <p>TCP Camdoop (switch)</p>
    <p>Camdoop (no agg)</p>
    <p>Camdoop</p>
  </div>
  <div class="page">
    <p>Small-scale data (R=1, S=0) Worse</p>
    <p>Better 0</p>
    <p>T im</p>
    <p>e (</p>
    <p>m s)</p>
    <p>Input data size / server</p>
    <p>TCP Camdoop (switch)</p>
    <p>Camdoop (no agg)</p>
    <p>Camdoop</p>
    <p>In-network aggregation can be beneficial also for (small-scale data) interactive services</p>
  </div>
  <div class="page">
    <p>Camdoop  Explores the benefits of in-network processing by</p>
    <p>running combiners within the network  No change in the programming model  Achieves lower shuffle and reduce time  Decouples performance from the # of output files</p>
    <p>A final thought: how would Camdoop run on this?  AMD SeaMicro  a 512-core cluster</p>
    <p>for data centers using a 3D torus  Fast interconnect: 5 Gbps / link</p>
    <p>Conclusions</p>
  </div>
</Presentation>
