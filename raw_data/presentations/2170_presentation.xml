<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hyeong-Jun Kim, Jin-Soo Kim</p>
    <p>Sungkyunkwan University</p>
    <p>Young-Sik Lee</p>
    <p>KAIST</p>
    <p>HotStorage 16 June 20, 2016</p>
  </div>
  <div class="page">
    <p>HDD 10s of milliseconds</p>
    <p>NAND 10s of microseconds</p>
    <p>~1/1000</p>
    <p>~1/1000</p>
    <p>DRAM nanoseconds</p>
  </div>
  <div class="page">
    <p>User Apps</p>
    <p>USER</p>
    <p>KERNEL</p>
    <p>VFS/File System</p>
    <p>Block Layer</p>
    <p>SAS Driver</p>
    <p>Request Queue</p>
    <p>SCSI XLAT</p>
    <p>HDD (~10ms)</p>
    <p>M in</p>
    <p>im iz</p>
    <p>e d</p>
    <p>S ta</p>
    <p>ck</p>
    <p>User Apps</p>
    <p>VFS/File System</p>
    <p>Block Layer</p>
    <p>NVMe Driver</p>
    <p>NVMe SSD (&lt;100s)</p>
    <p>User Apps</p>
    <p>VFS/File System</p>
    <p>Block Layer</p>
    <p>SAS Driver</p>
    <p>Request Queue</p>
    <p>SCSI XLAT</p>
    <p>SAS SSD (~150s)</p>
    <p>O p</p>
    <p>ti m</p>
    <p>iz e</p>
    <p>d S</p>
    <p>ta ck</p>
  </div>
  <div class="page">
    <p>Use of polling for the fast I/O completion [Yang et al. FAST 2012]</p>
    <p>Optimization of a low-level hardware abstraction layer</p>
    <p>[Shin et al. ATC 2014]</p>
    <p>Reducing the translation overhead between abstraction layers</p>
    <p>Optimizations to fully exploit the performance of fast storage devices</p>
    <p>[Yu et al. ACM TOCS 2014]</p>
    <p>Polling, request merging, double buffering and reducing context switches</p>
  </div>
  <div class="page">
    <p>Kernel should be general to provide an abstraction layer</p>
    <p>Kernel cannot implement any policy that favors a certain application</p>
    <p>Updating kernel requires a constant effort to port application</p>
    <p>specific optimizations</p>
  </div>
  <div class="page">
    <p>Direct access to the special storage device [Caulfield et al. ASPLOS 2012]</p>
    <p>Special hardware is required</p>
    <p>Direct access to NVMe device</p>
    <p>Intel Storage Performance Development Kit  SPDK (Sep 2015)</p>
    <p>Micron Userspace NVMe driver project  UNVMe (Feb 2016)</p>
    <p>Device dedicated to a single user process</p>
    <p>Provides just simple read &amp; write interface based on polling</p>
    <p>Not sufficient to port existing applications</p>
  </div>
  <div class="page">
    <p>USER</p>
    <p>KERNEL</p>
    <p>User Apps</p>
    <p>VFS/File System</p>
    <p>Block Layer</p>
    <p>NVMe Driver</p>
    <p>NVMe Device</p>
    <p>H/W</p>
    <p>NVMe SSD</p>
    <p>NVMe Device</p>
    <p>User Apps</p>
    <p>NVMeDirect Framework</p>
    <p>NVMe I/O Queue</p>
    <p>NVMe Driver</p>
    <p>Permission management  Queue management</p>
    <p>NVMe SSD</p>
    <p>co n</p>
    <p>tr o</p>
    <p>l</p>
    <p>d a</p>
    <p>ta</p>
  </div>
  <div class="page">
    <p>Allows user-space applications to directly access NVMe SSDs without any hardware modifications</p>
    <p>Achieves high performance by avoiding storage stack overhead</p>
    <p>Supports various I/O policies</p>
    <p>Applications can be optimized according to their I/O characteristics</p>
    <p>Selective use of block cache, I/O scheduler, or I/O completion thread</p>
    <p>Asynchronous I/O vs. Synchronous I/O</p>
    <p>Buffered I/O vs. Direct I/O</p>
    <p>Designed to maximize performance for trusted applications</p>
    <p>Storage appliance, private clouds, etc.</p>
  </div>
  <div class="page">
    <p>N V</p>
    <p>M e</p>
    <p>D ir</p>
    <p>e ct</p>
    <p>L ib</p>
    <p>ra ry</p>
    <p>NVMe Controller</p>
    <p>I/ O</p>
    <p>H a</p>
    <p>n d</p>
    <p>le s</p>
    <p>I/ O</p>
    <p>Q u</p>
    <p>e u</p>
    <p>e s</p>
    <p>Block Cache</p>
    <p>I/O Scheduler</p>
    <p>I/O Completion</p>
    <p>Thread</p>
    <p>Handle Handle</p>
    <p>Admin Tool</p>
    <p>NVMeDirect API</p>
    <p>U se</p>
    <p>r K</p>
    <p>e rn</p>
    <p>e l</p>
    <p>H W</p>
    <p>N V</p>
    <p>M e</p>
    <p>D ri</p>
    <p>ve r</p>
    <p>D ef</p>
    <p>au lt</p>
    <p>Q</p>
    <p>u eu</p>
    <p>es</p>
    <p>U se</p>
    <p>r C re</p>
    <p>at ed</p>
    <p>Q</p>
    <p>u eu</p>
    <p>es</p>
    <p>NVMeDirect Management</p>
    <p>Kernel driver</p>
    <p>Admin tool</p>
    <p>NVMeDirect I/O</p>
    <p>I/O Handles</p>
    <p>User-space I/O Queues</p>
    <p>NVMeDirect I/O Framework</p>
    <p>Block Cache</p>
    <p>I/O Scheduler</p>
    <p>I/O Completion Thread</p>
  </div>
  <div class="page">
    <p>User-space I/O Queues</p>
    <p>Memory-mapped address space for NVMe I/O Queues created in the kernel address space</p>
    <p>I/O Handles</p>
    <p>Used to send I/O requests to NVMe I/O Queue(s)</p>
    <p>A thread can create one or more I/O Handles</p>
    <p>Each Handle can be configured to use different features : caching, I/O scheduling, I/O completion, etc.</p>
    <p>I/ O</p>
    <p>H a</p>
    <p>n d</p>
    <p>le s</p>
    <p>I/ O</p>
    <p>Q u</p>
    <p>e u</p>
    <p>e s</p>
    <p>Handle Handle Handle Handle</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>N V</p>
    <p>M e</p>
    <p>D ir</p>
    <p>e ct</p>
    <p>L ib</p>
    <p>ra ry</p>
    <p>NVMe Controller</p>
    <p>I/ O</p>
    <p>H a</p>
    <p>n d</p>
    <p>le s</p>
    <p>I/ O</p>
    <p>Q u</p>
    <p>e u</p>
    <p>e s</p>
    <p>Block Cache</p>
    <p>I/O Scheduler</p>
    <p>I/O Completion</p>
    <p>Thread</p>
    <p>Handle</p>
    <p>Admin Tool</p>
    <p>NVMeDirect API</p>
    <p>U se</p>
    <p>r K</p>
    <p>e rn</p>
    <p>e l</p>
    <p>H W</p>
    <p>N V</p>
    <p>M e</p>
    <p>D ri</p>
    <p>ve r</p>
    <p>D ef</p>
    <p>au lt</p>
    <p>Q</p>
    <p>u eu</p>
    <p>e</p>
    <p>U se</p>
    <p>r C re</p>
    <p>at ed</p>
    <p>Q</p>
    <p>u eu</p>
    <p>e</p>
    <p>nvmed = nvmed_open(/proc/nvme0/n1);</p>
    <p>queue = nvmed_queue_create(nvmed);</p>
    <p>handle = nvmed_handle_create(queue);</p>
    <p>size = nvmed_read(handle, buf, len);</p>
    <p>ret = nvmed_set_param (handle, BUFFERED_IO, TRUE);</p>
    <p>Block Cache</p>
  </div>
  <div class="page">
    <p>Enables high performance I/O</p>
    <p>Low latency and high throughput</p>
    <p>Easy to support new interfaces</p>
    <p>Weighted queue, multi-stream, etc.</p>
    <p>Easy to develop and debug</p>
    <p>Provides various I/O policies</p>
    <p>Free from kernel update</p>
    <p>Co-exists with legacy kernel I/O</p>
  </div>
  <div class="page">
    <p>Implementation on the Linux kernel 4.3.3</p>
    <p>Experimental setup  Ubuntu 14.04 LTS</p>
    <p>3.3GHz Intel Core i7 CPU (6 cores) &amp; 64GB of DRAM</p>
    <p>Intel 750 Series 400GB NVMe SSD</p>
    <p>Comparison with  Kernel I/O</p>
    <p>SPDK</p>
    <p>NVMeDirect</p>
  </div>
  <div class="page">
    <p>Asynchronous random I/O performance using FIO</p>
    <p>IO P</p>
    <p>S (</p>
    <p>B )</p>
    <p>x 1</p>
    <p>,0 0</p>
    <p>Queue Depth</p>
    <p>Random Read</p>
    <p>Queue Depth</p>
    <p>Random Write</p>
  </div>
  <div class="page">
    <p>Polling is not efficient on bandwidth sensitive workload due to the significant increase in the CPU load</p>
    <p>Significant performance degradation occurs in a certain polling period</p>
    <p>Control Polling Period dynamically based on I/O size or hints from applications</p>
    <p>C P</p>
    <p>U U</p>
    <p>ti li</p>
    <p>za ti</p>
    <p>o n</p>
    <p>( %</p>
    <p>)</p>
    <p>N o</p>
    <p>rm a</p>
    <p>li ze</p>
    <p>d R</p>
    <p>e a</p>
    <p>d I</p>
    <p>O P</p>
    <p>S</p>
    <p>Polling Period (s)</p>
  </div>
  <div class="page">
    <p>Redis: in-memory data structure store</p>
    <p>Logging every operation for persistency</p>
    <p>Logs are 10 to 100 bytes in size</p>
    <p>Write buffer is required due to small-size data</p>
    <p>Difficult to run on SPDK without significant code modification</p>
    <p>M e</p>
    <p>D ir</p>
    <p>e ct</p>
    <p>L ib</p>
    <p>ra ry</p>
    <p>I/ O</p>
    <p>H a</p>
    <p>n d</p>
    <p>le s</p>
    <p>I/ O</p>
    <p>Q u</p>
    <p>e u</p>
    <p>e s</p>
    <p>Block Cache</p>
    <p>I/O Scheduler</p>
    <p>I/O Completion</p>
    <p>Thread</p>
    <p>Handle</p>
    <p>NVMeDirect API</p>
    <p>U se</p>
    <p>r</p>
    <p>Redis</p>
  </div>
  <div class="page">
    <p>Using workload-A in YCSB on Redis</p>
    <p>Update-heavy workload with Zipf distribution</p>
    <p>Throughput</p>
    <p>(o p</p>
    <p>s/ s)</p>
    <p>x 1</p>
    <p>,0 0</p>
    <p>Kernel I/O NVMeDirect</p>
    <p>Read Update</p>
    <p>La te</p>
    <p>n cy</p>
    <p>(</p>
    <p>s)</p>
    <p>Kernel I/O NVMeDirect</p>
  </div>
  <div class="page">
    <p>NVMeDirect supports prioritized I/O without H/W features</p>
    <p>Prioritized I/O without a weighted round-robin scheduler</p>
    <p>Using flexible binding between Handles and Queues</p>
    <p>Sharing a single Queue with multiple Handles</p>
    <p>N V</p>
    <p>M e</p>
    <p>D ir</p>
    <p>e ct</p>
    <p>L ib</p>
    <p>ra ry</p>
    <p>I/ O</p>
    <p>H</p>
    <p>a n</p>
    <p>d le</p>
    <p>s I/</p>
    <p>O</p>
    <p>Q u</p>
    <p>e u</p>
    <p>e s</p>
    <p>Block Cache</p>
    <p>I/O Scheduler</p>
    <p>I/O Completion Thread</p>
    <p>NVMeDirect API</p>
    <p>Handle Handle HandleHandle</p>
  </div>
  <div class="page">
    <p>One prioritized thread with a dedicated queue, Three threads with a shared queue</p>
    <p>Each thread performs 4KB random write</p>
    <p>Kernel I/O NVMeDirect</p>
    <p>with Dedicated Queue</p>
    <p>B I</p>
    <p>O P</p>
    <p>S</p>
  </div>
  <div class="page">
    <p>NVMeDirect</p>
    <p>First full framework for I/O in the user-space based on stock NVMe devices</p>
    <p>Can be easily applied to many applications</p>
    <p>Useful for emerging storage devices, e.g. 3D XPointTM, etc.</p>
    <p>Available as open-source at https://github.com/nvmedirect (July 2016)</p>
    <p>Future work</p>
    <p>User-level file systems</p>
    <p>Porting diverse data-intensive applications over NVMeDirect</p>
    <p>Protecting the system from illegal access</p>
  </div>
  <div class="page">
    <p>Thank you hjkim@csl.skku.edu</p>
  </div>
</Presentation>
