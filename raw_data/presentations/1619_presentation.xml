<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Saurabh Kadekodi</p>
    <p>Gotta have HeART Improving storage ef7iciency by exploiting</p>
    <p>disk-reliability heterogeneity</p>
    <p>Greg GangerRashmi Vinayak</p>
  </div>
  <div class="page">
    <p>Cluster storage systems</p>
    <p>2</p>
    <p>Storage subsystem of distributed systems</p>
    <p>Thousands to millions of disks in primary storage tier</p>
    <p>Built incrementally according to demand</p>
  </div>
  <div class="page">
    <p>Reliability heterogeneity in disks</p>
    <p>3</p>
    <p>Disk 7leet has heterogeneous collection of disks</p>
    <p>Different in reliability Manufacturing differences across makes/models</p>
    <p>Different vibration / temperature experiences</p>
    <p>I/O churn</p>
  </div>
  <div class="page">
    <p>Overview of exploiting reliability heterogeneity</p>
    <p>4</p>
    <p>Data redundancy typically same across disk 7leet  E.g., 3-replication: 3 copies of data on independent devices</p>
    <p>Disks from same storage tier vary a lot in failure rates  E.g., HDDs from different makes/models fail differently</p>
    <p>Explicitly consider reliability heterogeneity in deciding redundancy</p>
    <p>HeART: Heterogeneity Aware Redundancy Tuner  Tailors redundancy to disk failure rate heterogeneity</p>
    <p>A safe, accurate and online framework</p>
    <p>Reduces storage overhead, and thus cost</p>
    <p>HeART could save 11-33% disk space on a production dataset</p>
  </div>
  <div class="page">
    <p>Cluster storage system reliability</p>
    <p>5</p>
    <p>Failures common in todays cluster storage systems  Disk failures measured as annualized failure rates (AFR)</p>
    <p>AFR expected % of disk failures in a year</p>
    <p>Popular fault tolerance mechanism redundancy  Full data replication</p>
    <p>Erasure coding</p>
    <p>Redundancy con7igurations ignore disk AFR differences</p>
  </div>
  <div class="page">
    <p>Reliability heterogeneity</p>
    <p>6</p>
    <p>Backblaze dataset 5 yrs of HDD reliability,</p>
  </div>
  <div class="page">
    <p>Reliability heterogeneity</p>
    <p>6</p>
    <p>HDD failure rates vary a lot in the 7ield  Also shown by Schroeder et al. for SSDs in FAST 2016</p>
    <p>No single redundancy scheme is good enough for all disks  Conservative redundancy overprotection for strong disk types</p>
    <p>Lower redundancy subset of disks risk data loss</p>
  </div>
  <div class="page">
    <p>Exploiting reliability heterogeneity</p>
    <p>7</p>
    <p>Redundancy decisions informed by AFR differences</p>
    <p>Challenges 1. Has to be monitored in the Gield</p>
    <p>Redundancy tailoring mechanism needs to be:  Safe: prevent under-redundancy from causing data loss</p>
    <p>Accurate: identify different reliability phases correctly</p>
    <p>Online: bene7its only realizable during disks low failure rate</p>
  </div>
  <div class="page">
    <p>The bathtub curve (each disk group)</p>
    <p>Infancy</p>
    <p>A F R ( % )</p>
    <p>Age of disk0 0</p>
    <p>WearoutUseful life</p>
    <p>Lower failure rate</p>
    <p>lower AFR lower redundancy lower storage cost</p>
  </div>
  <div class="page">
    <p>T im e</p>
    <p>T im ey</p>
    <p>Two disk groups over time</p>
    <p>9</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>T im e</p>
    <p>Disk group x infant-mortality end</p>
    <p>Disk group x wearout start</p>
    <p>ftdefau ltftdefau lt ftx</p>
    <p>Disk group y Infant-mortality end</p>
    <p>Disk group y wearout start</p>
    <p>T im eftdefau ltftdefau lt fty</p>
    <p>Two disk groups over time</p>
    <p>9</p>
    <p>Deployment (start monitoring)</p>
    <p>ftdefau lt = default fault tolerance scheme</p>
    <p>y</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>start of wearout</p>
    <p>end of infancy</p>
    <p>decommissioning age</p>
    <p>ftdefau lt ftdefau ltftdiskg ro u p</p>
    <p>T im e</p>
    <p>How to detect?</p>
    <p>Is AFR well-behaved?</p>
    <p>What should the redundancy be?</p>
    <p>Disk-group reliability timeline</p>
  </div>
  <div class="page">
    <p>Heteretogeneity-Aware Redundancy Tuner</p>
    <p>Reliability requirement (MTTDL)</p>
    <p>Anomaly detector</p>
    <p>or</p>
    <p>HeART</p>
    <p>Disk health monitoring data</p>
    <p>Change point detector Redundancy Tuner</p>
  </div>
  <div class="page">
    <p>start of wearout</p>
    <p>end of infancy</p>
    <p>decommissioning age</p>
    <p>ftdefau lt ftdefau ltftdiskgrou p</p>
    <p>Ti m e</p>
    <p>How to detect?</p>
    <p>Is AFR well-behaved?</p>
    <p>What should the redundancy be?</p>
    <p>Disk-group reliability timeline</p>
  </div>
  <div class="page">
    <p>AFR in useful life: stability &amp; anomalies</p>
    <p>13</p>
    <p>Useful life AFR is typically stable, within reasonable bounds</p>
    <p>External factors can cause simultaneous bulk failures  Rack power failure, accidents, human error, etc.</p>
    <p>Anomalies appear like (premature) wearout  Bene7its proportional to length of useful life</p>
    <p>Bulk failures may not re7lect true HDD failure rate</p>
    <p>true wearout</p>
    <p>end of infancy</p>
    <p>decommissioning age</p>
    <p>ftdefau lt ftdefau ltftdiskg ro u p Time</p>
    <p>premature wearout</p>
    <p>ftdefau lt</p>
    <p>Anomalous failures</p>
  </div>
  <div class="page">
    <p>AFR in useful life: stability &amp; anomalies</p>
    <p>14</p>
    <p>Spikes due to simultaneous bulk failures</p>
    <p>Need for anomaly detector</p>
  </div>
  <div class="page">
    <p>AFR in useful life: 7iltering anomalies</p>
    <p>15</p>
    <p>A and B: 300+ disks failed simultaneously</p>
    <p>C D E</p>
  </div>
  <div class="page">
    <p>disk group old age start</p>
    <p>disk group infant mortality end</p>
    <p>disk group decommissioned</p>
    <p>ftdefau lt ftdefau ltftdiskgrou p</p>
    <p>T im e</p>
    <p>How to detect?</p>
    <p>Is AFR well-behaved?</p>
    <p>What should the redundancy be?</p>
    <p>Disk-group reliability timeline</p>
  </div>
  <div class="page">
    <p>Change point detection</p>
    <p>17</p>
    <p>Reliability target can be missed if:  Hasty declaration of end of infancy</p>
    <p>Delayed declaration of onset of wearout</p>
    <p>Tradeoff between extracting bene7its and safety</p>
    <p>Use online change point detectors to identify change points</p>
    <p>Infancy</p>
    <p>A F R ( % )</p>
    <p>Age of disk0, 0</p>
    <p>WearoutUseful life</p>
    <p>Lower failure rate</p>
  </div>
  <div class="page">
    <p>Original signal</p>
    <p>Discrepancy curve</p>
    <p>Leveraging existing algorithms</p>
    <p>18</p>
    <p>Online anomaly detection (RRCF) [3]:</p>
    <p>Ruptures compares discrepancy in adjacent sliding windows</p>
    <p>Window length is one month by default</p>
    <p>weekly or fortnightly AFRs are too jumpy</p>
    <p>monthly AFRs balance reaction time with accuracy of AFR</p>
    <p>Online change point detection (Ruptures) [4]:</p>
    <p>Original signal</p>
    <p>Discrepancy curve</p>
  </div>
  <div class="page">
    <p>start of wearout</p>
    <p>end of infancy</p>
    <p>decommissioning age</p>
    <p>ftdefau lt ftdefau ltftdiskgrou p</p>
    <p>T im e</p>
    <p>How to detect?</p>
    <p>Is AFR well-behaved?</p>
    <p>What should the redundancy be?</p>
    <p>Disk-group reliability timeline</p>
  </div>
  <div class="page">
    <p>The Backblaze dataset</p>
    <p>20</p>
    <p>100K+ HDDs belonging to Backblaze: a backup company  Daily reliability statistics from mid 2013 - mid 2018</p>
    <p>Open sourced</p>
    <p>6 drive makes/models with signi7icant number of disks to test:</p>
    <p>Disk Grp Num Drives Num Failed Age so far (yrs)</p>
    <p>S-4 37015 9539 5</p>
    <p>H-4A 8715 3939 4.77</p>
    <p>H-4B 15048 1276 4.2</p>
    <p>S-8C 9885 186 1.99</p>
    <p>S-8E 14395 162 1.2</p>
    <p>S-12E 21581 148 0.64</p>
  </div>
  <div class="page">
    <p>Methodology to evaluate cost reductions</p>
    <p>21</p>
    <p>Need target reliability (MTTDL)  Higher the MTTDL, the more resilient the data</p>
    <p>Reliability target decided by disk group with highest AFR  Currently on all disks provided acceptable MTTDL</p>
    <p>MTTDL is only higher for disks with lower AFR</p>
    <p>Find cheaper that meets MTTDL</p>
    <p>Cheaper redundancy lower storage cost</p>
    <p>is decided with the following constraints:</p>
    <p>Tolerate at least as many failures as</p>
    <p>Have an upper bound on stripe width</p>
    <p>ftdefau lt</p>
    <p>ftdefau lt</p>
    <p>ftdiskg ro u p</p>
    <p>ftdiskg ro u p</p>
  </div>
  <div class="page">
    <p>Evaluation on Backblaze dataset</p>
    <p>22</p>
    <p>S-4 disks have the highest AFR (4.01%) in Backblaze  Reliability target is MTTDL of on S-4 HDDs</p>
    <p>Upper bound on stripe width = 2x</p>
    <p>options evaluated:</p>
    <p>3-replication</p>
    <p>6-of-9 erasure code</p>
    <p>10-of-14 erasure code</p>
    <p>ftdefau lt</p>
    <p>ftdefau lt ftdefau lt</p>
  </div>
  <div class="page">
    <p>Disk group AFR</p>
    <p>23</p>
    <p>H-4A HDDs S-8E HDDs</p>
  </div>
  <div class="page">
    <p>HeART in action: H-4A HDDs</p>
    <p>24</p>
    <p>Infant mortality end Old age</p>
    <p>x x</p>
    <p>End of infancy AFR + buffer =</p>
    <p>HeART determined AFR 1.82%</p>
  </div>
  <div class="page">
    <p>HeART in action: S-8E HDDs</p>
    <p>25</p>
    <p>Only infant mortality end detected</p>
    <p>x HeART determined AFR 2.48%</p>
    <p>End of infancy AFR + buffer =</p>
  </div>
  <div class="page">
    <p>Storage savings: 6-of-9</p>
    <p>26</p>
    <p>Small storage overhead of only 1.5x</p>
    <p>ft6of9 ft6of9ftdiskgrou p Time</p>
    <p>infancy useful life wearout</p>
    <p>Useful life savings13%</p>
    <p>Lifelong savings</p>
    <p>D</p>
    <p>is k</p>
    <p>sp a</p>
    <p>ce s</p>
    <p>a vi</p>
    <p>n g</p>
    <p>s (%</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Storage savings: 6-of-9</p>
    <p>26</p>
    <p>Small storage overhead of only 1.5x</p>
    <p>ft6of9 ft6of9ftdiskgrou p Time</p>
    <p>infancy useful life wearout</p>
    <p>Useful life savings 11%10%</p>
    <p>Lifelong savings</p>
    <p>D</p>
    <p>is k</p>
    <p>sp a</p>
    <p>ce s</p>
    <p>a vi</p>
    <p>n g</p>
    <p>s (%</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Storage savings: 10-of-14</p>
    <p>27</p>
    <p>Even smaller storage overhead of only 1.4x</p>
    <p>ft10of14 ft10of14ftdiskgrou p Time</p>
    <p>infancy useful life wearout</p>
    <p>Lifelong savings</p>
    <p>D</p>
    <p>is k</p>
    <p>sp a</p>
    <p>ce s</p>
    <p>a vi</p>
    <p>n g</p>
    <p>s (%</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Storage savings: 10-of-14</p>
    <p>27</p>
    <p>Even smaller storage overhead of only 1.4x</p>
    <p>ft10of14 ft10of14ftdiskgrou p Time</p>
    <p>infancy useful life wearout</p>
    <p>Useful life savings</p>
    <p>Lifelong savings</p>
    <p>D</p>
    <p>is k</p>
    <p>sp a</p>
    <p>ce s</p>
    <p>a vi</p>
    <p>n g</p>
    <p>s (%</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Carnegie Mellon Saurabh Kadekodi  January 2019</p>
    <p>Conclusion</p>
    <p>28</p>
    <p>Exploiting reliability heterogeneity reduces storage cost</p>
    <p>11-33% space savings observed on production dataset</p>
    <p>HeART: an online heterogeneity-aware redundancy tuner  actively engages with disk bathtub curves</p>
    <p>built-in online anomaly and change point detector</p>
    <p>suggests cheap redundancy schemes that meet reliability</p>
    <p>Questions?</p>
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>My heart is in the work</p>
    <p>My work is in the HeART</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>30</p>
  </div>
</Presentation>
