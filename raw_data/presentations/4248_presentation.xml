<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Perplexity on Reduced Corpora  Analysis of Cutoff by Power Law</p>
    <p>Hayato Kobayashi</p>
    <p>Yahoo Japan Corporation</p>
  </div>
  <div class="page">
    <p>Cutoff</p>
    <p>Removing low-frequency words from a corpus</p>
    <p>Common practice to save computational costs in learning</p>
    <p>Language modeling</p>
    <p>Needed even in a distributed environment, since the feature</p>
    <p>space of k-grams is quite large [Brants+ 2007]</p>
    <p>Topic modeling</p>
    <p>Enough for roughly analyzing topics, since low-frequency words</p>
    <p>have a small impact on the statistics [Steyvers&amp;Griffiths 2007]</p>
  </div>
  <div class="page">
    <p>Question</p>
    <p>How many low-frequency words can we remove while</p>
    <p>maintaining sufficient performance?</p>
    <p>More generally, how much can we reduce a corpus/model using</p>
    <p>a certain strategy?</p>
    <p>Many experimental studies addressing the question</p>
    <p>[Stoleke 1998], [Buchsbaum+ 1998], [Goodman&amp;Gao 2000],</p>
    <p>[Gao&amp;Zhang 2002], [Ha+ 2006], [Hirsimaki 2007], [Church+ 2007]</p>
    <p>Discussing trade-off relationships between the size of reduced</p>
    <p>corpus/model and its performance</p>
    <p>No theoretical study!</p>
  </div>
  <div class="page">
    <p>This work</p>
    <p>First address the question from a theoretical standpoint</p>
    <p>Derive the trade-off formulae of the cutoff strategy for k</p>
    <p>gram models and topic models</p>
    <p>Perplexity vs. reduced vocabulary size</p>
    <p>Verify the correctness of our theory on synthetic corpora</p>
    <p>and examine the gap between theory and practice on</p>
    <p>several real corpora</p>
  </div>
  <div class="page">
    <p>Approach</p>
    <p>Assume a corpus follows Zipfs law (power law)</p>
    <p>Empirical rule representing a long-tail property in a corpus</p>
    <p>Essentially the same approach as in physics</p>
    <p>Constructing a theory while believing experimentally observed</p>
    <p>results (e.g., gravity acceleration g)</p>
    <p>We can derive the landing point of a ball by believing g.</p>
    <p>Similarly, we try to clarify the trade-off relationships by</p>
    <p>believing Zipfs law.</p>
    <p>),( 0 v</p>
    <p>g</p>
    <p>v )2sin( 2</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Preliminaries</p>
    <p>Zipfs law</p>
    <p>Perplexity (PP)</p>
    <p>Cutoff and restoring</p>
    <p>PP of unigram models</p>
    <p>PP of k-gram models</p>
    <p>PP of topic models</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Zipfs law</p>
    <p>Empirical rule discovered on real corpora [Zipf, 1935]</p>
    <p>Word frequency f(w) is inversely proportional to its frequency</p>
    <p>ranking r(w)</p>
    <p>)( )(</p>
    <p>wr</p>
    <p>C wf</p>
    <p>f( w</p>
    <p>)</p>
    <p>r(w)</p>
    <p>Log-log graph</p>
    <p>Real corpora roughly follow Zipfs law</p>
    <p>(Linear on a log-log graph)</p>
    <p>Frequency ranking</p>
    <p>Frequency</p>
    <p>Max. frequency</p>
    <p>Zipf random</p>
  </div>
  <div class="page">
    <p>Perplexity (PP)</p>
    <p>Widely used evaluation measure of statistical models</p>
    <p>Geometric mean of the inverse of the per-word likelihood on</p>
    <p>the held-out test corpus</p>
    <p>PP means how many possibilities one has for estimating the</p>
    <p>next word</p>
    <p>Lower perplexity means better generalization performance</p>
    <p>Corpus size</p>
    <p>Test corpus</p>
  </div>
  <div class="page">
    <p>Cutoff</p>
    <p>Removing low frequency words</p>
    <p>f(remaining word)  f(removed word) holds</p>
    <p>f( w</p>
    <p>)</p>
    <p>r(w)</p>
    <p>Reduced</p>
    <p>corpus w</p>
    <p>L e a rn</p>
    <p>e d p</p>
    <p>ro b .</p>
    <p>Probability ranking</p>
    <p>Learn from w</p>
    <p>Need to infer</p>
  </div>
  <div class="page">
    <p>Constant restoring</p>
    <p>Infer the prob. of the removed words as a constant</p>
    <p>Approximate the result learned from the original corpus</p>
    <p>In fe</p>
    <p>rr e d p</p>
    <p>ro b .</p>
    <p>Probability ranking</p>
    <p>Reduced corpus</p>
    <p>Learned from w</p>
    <p>Constant</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Preliminaries</p>
    <p>Zipfs law</p>
    <p>Perplexity (PP)</p>
    <p>Cutoff and restoring</p>
    <p>PP of unigram models</p>
    <p>PP of k-gram models</p>
    <p>PP of topic models</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Perplexity of unigram models</p>
    <p>Predictive distribution of unigram models</p>
    <p>Optimal restoring constant</p>
    <p>Obtained by minimizing PP w.r.t. a constant , after substituting the restored probability into PP</p>
    <p>N</p>
    <p>wf wp</p>
    <p>)( )(</p>
    <p>Reduced corpus size</p>
    <p>Corpus size</p>
    <p>Vocab. size Reduced vocab. size</p>
    <p>p(w)</p>
  </div>
  <div class="page">
    <p>Theorem (PP of unigram models)</p>
    <p>For any reduced vocabulary size W, the perplexity PP1 of</p>
    <p>the optimal restored distribution of a unigram model is</p>
    <p>calculated as</p>
    <p>Bertrand series (special form)</p>
    <p>Harmonic series</p>
  </div>
  <div class="page">
    <p>Approximation of PP of unigrams</p>
    <p>H(X) and B(X) can be approximated by definite integrals</p>
    <p>Approximate formula o is obtained as</p>
    <p>is quasi polynomial (quadratic)</p>
    <p>Behaves as a quadratic function on a log-log graph</p>
    <p>Reduced vocab. size</p>
    <p>Euler-Mascheroni const.</p>
  </div>
  <div class="page">
    <p>PP of unigrams vs. reduced vocab. size</p>
    <p>Log-log graph Real (Reuters)</p>
    <p>Theory</p>
    <p>Zipf random</p>
    <p>same size as Reuters</p>
    <p>Maximum f(w)</p>
    <p>Zipf rand: 234,705</p>
    <p>Reuters: 136,371</p>
    <p>Our theory is suited for inferring the growth rate of perplexity</p>
    <p>rather than the perplexity value itself</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Preliminaries</p>
    <p>Zipfs law</p>
    <p>Perplexity (PP)</p>
    <p>Cutoff and restoring</p>
    <p>PP of unigram models</p>
    <p>PP of k-gram models</p>
    <p>PP of topic models</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Perplexity of k-gram models</p>
    <p>Simple model where k-grams are calculated from a</p>
    <p>random word sequence based on Zipfs law</p>
    <p>The model is stupid</p>
    <p>Bigram is is is quite frequent</p>
    <p>Two bigrams is a and a is have the same frequency</p>
    <p>Later experiment will uncover the fact that the model can</p>
    <p>roughly capture the behavior of real corpora</p>
    <p>)&quot;is a(&quot;)&quot;(&quot;)&quot;(&quot;)&quot;a is(&quot; papispp</p>
    <p>)&quot;(&quot;)&quot;(&quot;)&quot;is is(&quot; ispispp</p>
  </div>
  <div class="page">
    <p>Frequency of a k-gram</p>
    <p>Frequency fk of a k-gram wk is defined by</p>
    <p>Decay function g2 of bigrams is as follows</p>
    <p>Decay function gk of k-grams is defined through its</p>
    <p>inverse:</p>
    <p>Decay function</p>
    <p>Piltz divisor function that</p>
    <p>represents # of divisors of n</p>
  </div>
  <div class="page">
    <p>Exponent of k-gram distributions</p>
    <p>Assume k-gram frequencies follow a power law</p>
    <p>[Ha+ 2006] found k-gram frequencies roughly follow a power</p>
    <p>law, whose exponent k is smaller than 1 (k&gt;1)</p>
    <p>Optimal exponent in our model based on the assumption</p>
    <p>By minimizing the sum of squared errors between the inverse</p>
    <p>gradients gk -1(r) and r1/k on a log-log graph</p>
  </div>
  <div class="page">
    <p>Exponent of k-grams vs. gram size</p>
    <p>Normal graph</p>
    <p>Real (Reuters)</p>
    <p>Theory</p>
  </div>
  <div class="page">
    <p>Corollary (PP of k-gram models)</p>
    <p>For any reduced vocabulary size W, the perplexity of the</p>
    <p>optimal restored distribution of a k-gram model is</p>
    <p>calculated as</p>
    <p>X</p>
    <p>x aa x XH</p>
    <p>X</p>
    <p>x aa x</p>
    <p>xa XB</p>
    <p>ln :)(</p>
    <p>Bertrand series (another special form)</p>
    <p>Hyper harmonic series</p>
  </div>
  <div class="page">
    <p>PP of k-grams vs. reduced vocab. size</p>
    <p>Log-log graph</p>
    <p>Theory (Bigram)</p>
    <p>Unigram</p>
    <p>Theory (Trigram)</p>
    <p>Zipf (Bigram)</p>
    <p>Zipf (Trigram)Due to</p>
    <p>Sparseness</p>
    <p>We need to make assumptions that include</p>
    <p>backoff and smoothing for higher order k-grams</p>
  </div>
  <div class="page">
    <p>Additional properties by power-law</p>
    <p>Treat as a variant of the coupon collectors problem</p>
    <p>How many trials are needed for collecting all coupons whose</p>
    <p>occurrence probabilities follow some stable distribution</p>
    <p>There exists several works about power law distributions</p>
    <p>Corpus size for collecting all of the k-grams, according to</p>
    <p>[Boneh&amp;Papanicolaou 1996]</p>
    <p>When k = 1, , otherwise,</p>
    <p>Lower and upper bound of the number of k-grams from</p>
    <p>the corpus size N and vocab. size W, according to</p>
    <p>[Atsonios+ 2011]</p>
    <p>k</p>
    <p>k kW</p>
    <p>1W ln 2 W</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Preliminaries</p>
    <p>Zipfs law</p>
    <p>Perplexity (PP)</p>
    <p>Cutoff and restoring</p>
    <p>PP of unigram models</p>
    <p>PP of k-gram models</p>
    <p>PP of topic models</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Perplexity of topic models</p>
    <p>Latent Dirichlet Allocation (LDA) [Blei+ 2003]</p>
    <p>Learning with Gibbs sampling</p>
    <p>Obtain a good topic assignment zi for each word wi</p>
    <p>Posterior distributions of two hidden parameters</p>
    <p>)(</p>
    <p>)(</p>
    <p>)(</p>
    <p>)(</p>
    <p>w</p>
    <p>zz</p>
    <p>d</p>
    <p>zd</p>
    <p>nw</p>
    <p>nz</p>
    <p>[Griffiths&amp;Steyvers 2004]</p>
    <p>Document-topic distribution</p>
    <p>Mixture rate of topic z in document d</p>
    <p>Topic-word distribution</p>
    <p>Occurrence rate of word w in topic z</p>
  </div>
  <div class="page">
    <p>Rough assumptions of  and</p>
    <p>Assumption of</p>
    <p>Word distribution z of each topic z follows Zipfs law</p>
    <p>Assumptions of  (two extreme cases)</p>
    <p>Case All: Each document evenly has all topics</p>
    <p>Case One: Each document only has one topic (uniform dist.)</p>
    <p>Case All: PP of a topic model  PP of a unigram</p>
    <p>Marginal predictive distribution is independent of d</p>
    <p>=1/T</p>
    <p>The curve of actual perplexity is expected to be between their values</p>
    <p>It is natural, regarding each topic as a corpus</p>
  </div>
  <div class="page">
    <p>Theorem(PP of LDA models: Case One)</p>
    <p>For any reduced vocabulary size W, the perplexity of the</p>
    <p>optimal restored distribution of a topic model in the Case</p>
    <p>One is calculated as</p>
    <p>T : # of topics in LDA</p>
  </div>
  <div class="page">
    <p>PP of LDA models vs. reduced vocab. size</p>
    <p>Theory (Case One)</p>
    <p>(Case One +</p>
    <p>Case All) / 2</p>
    <p>Zipf Theory (Case All)</p>
    <p>Mix of 20 Zipf</p>
    <p>T=20</p>
    <p>CGS w/ 100 iter.</p>
    <p>==0.1</p>
    <p>Log-log graph Real (Reuters)</p>
  </div>
  <div class="page">
    <p>Time, memory, and PP of LDA learning</p>
    <p>Results of Reuters corpus</p>
    <p>Memory usage of the (1/10)-corpus is only 60% of that of</p>
    <p>the original corpus</p>
    <p>Helps in-memory computing for a larger corpus,</p>
    <p>although the computational time decreased a little</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Preliminaries</p>
    <p>Zipfs law</p>
    <p>Perplexity (PP)</p>
    <p>Cutoff and restoring</p>
    <p>PP of unigram models</p>
    <p>PP of k-gram models</p>
    <p>PP of topic models</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Trade-off formulae of the cutoff strategy for k-gram</p>
    <p>models and topic models based on Zipflaw</p>
    <p>Perplexity vs. reduced vocabulary size</p>
    <p>Experiments on real corpora showed that the estimation</p>
    <p>of the perplexity growth rate is reasonable</p>
    <p>We can get the best cutoff parameter by maximizing the</p>
    <p>reduction rate ensuring an acceptable (relative) perplexity</p>
    <p>Possibility that we can theoretically derive empirical</p>
    <p>parameters, or rules of thumb, for different NLP</p>
    <p>problems</p>
    <p>Can we derive other rules of thumb based on Zipfs law?</p>
  </div>
  <div class="page">
    <p>Thank you</p>
  </div>
</Presentation>
