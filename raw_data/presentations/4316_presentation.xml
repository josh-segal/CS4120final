<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Reasoning with Sarcasm by Reading</p>
    <p>In-between</p>
    <p>Yi Tay1, Luu Anh Tuan2, Siu Cheung Hui1 and Jian Su2 1 Nanyang Technological University, Singapore</p>
  </div>
  <div class="page">
    <p>Background  Sarcasm: o a form of verbal irony that is intended to express contempt or</p>
    <p>ridicule (The Free Dictionary) o commonly manifests on social communities (e.g. Twitter, Reddit)</p>
    <p>Prior work considered sarcasm to be a contrast between a positive and negative sentiment (Riloff et al., 2013)</p>
    <p>I love to be ignored! Perfect movie for people who cant fall asleep</p>
    <p>Scope of this work: sarcasm detection based on documents content and commonsense knowledge but not external knowledge, or users profile and context</p>
    <p>I love to solve math problem everyday Cool. It took me 10 hours to flight from Sydney to Melbourne.</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>State-of-the-art sarcasm detection systems mainly rely on deep and sequential neural networks (Ghosh and Veale, 2016; Zhang et al., 2016): o compositional encoders (GRU, LSTM) are often employed, with</p>
    <p>the input document being parsed one word at a time o no explicit interaction between word pairs ! hampers ability to</p>
    <p>explicitly model contrast, incongruity or juxtaposition of situations o difficult to capture long-range dependencies</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>Proposed approach</p>
    <p>Our idea: modeling contrast in order to reason with sarcasm o either between positive-negative sentiments or between literal</p>
    <p>figurative scenarios</p>
    <p>How? o looking in-between: propose a multi-dimensional intra-attention</p>
    <p>recurrent network ! capture both word-word relationship and long-range dependency</p>
    <p>I absolutely love to be ignored! Perfect movie for people who cant fall asleep</p>
    <p>!4</p>
  </div>
  <div class="page">
    <p>Architecture</p>
    <p>!5</p>
    <p>single-dimensional intra-attention:</p>
    <p>multi-dimensional intra-attention:</p>
    <p>intra-attention weight vector:</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>!6</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>!7</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>!8</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>!9</p>
  </div>
  <div class="page">
    <p>Visualization of attention weights</p>
    <p>!10</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We proposed a new neural network architecture for sarcasm detection o incorporates a multi-dimensional intra-attention component that</p>
    <p>learns an intra-attentive representation of the sentence o enabling it to detect contrastive sentiment, situations and</p>
    <p>incongruity</p>
    <p>outperforms strong state-of-the-art baselines such as GRNN and CNN-LSTM-DNN over six public benchmarks</p>
    <p>Able to learns highly interpretable attention weights ! paving the way for more explainable neural sarcasm detection methods.</p>
    <p>!11</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[1] Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In proceedings of EMNLP, 2013.</p>
    <p>[2] Meishan Zhang, Yue Zhang, and Guohong Fu. 2016. Tweet sarcasm detection using deep neural network. In proceedings of COLING, 2016.</p>
    <p>[3] Aniruddha Ghosh and Tony Veale. 2016. Fracking sarcasm using neural network. In proceedings of NAACL, 2016.</p>
    <p>!12</p>
  </div>
</Presentation>
