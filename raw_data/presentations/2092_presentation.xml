<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards Performance-Portable, Scalable, and Convenient Linear Algebra</p>
    <p>Philippe Tillet1,3, Karl Rupp2, Siegfried Selberherr1, Chin-Teng Lin3</p>
    <p>HotPar13, June 24th, 2013</p>
  </div>
  <div class="page">
    <p>GPUs: Library AspectsGPUs: Library Aspects</p>
    <p>Challenge: The architectural barrier</p>
    <p>Challenge: The memory barrier</p>
    <p>Challenge: The usability barrier</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier Tackling the portability issueTackling the portability issue</p>
    <p>OpenCL - A unified programming model for parallel devices</p>
    <p>Includes: a low-level API a low-level programming language</p>
    <p>Compiled Just-In-Time</p>
    <p>Write once, run everywhere</p>
    <p>Currently available for CPUs, GPUs, MICs...</p>
    <p>Soon available on FPGAs, DSPs...</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier The architecture strikes backThe architecture strikes back</p>
    <p>The good old matrix-matrix multiplication problem</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Matrix Rows/Columns</p>
    <p>GTX 470 - CUBLAS 5.0</p>
    <p>GTX 470 - ViennaCL 1.4.0</p>
    <p>We obtain same performance as CuBLAS 5.0</p>
    <p>What if we use this kernel on AMD Hardware?</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier The architecture strikes backThe architecture strikes back</p>
    <p>The good old matrix-matrix multiplication problem</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Matrix Rows/Columns</p>
    <p>GTX 470 - CUBLAS 5.0</p>
    <p>GTX 470 - ViennaCL 1.4.0</p>
    <p>HD7970 - clAmdBlas 1.10</p>
    <p>HD7970 - ViennaCL 1.4.0</p>
    <p>We get 60% of the clAmdBlas performance!</p>
    <p>What if we use this kernel on a CPU?</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier The architecture strikes backThe architecture strikes back</p>
    <p>The good old matrix-matrix multiplication problem</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Matrix Rows/Columns</p>
    <p>GTX 470 - CUBLAS 5.0</p>
    <p>GTX 470 - ViennaCL 1.4.0</p>
    <p>HD7970 - clAmdBlas 1.10</p>
    <p>HD7970 - ViennaCL 1.4.0</p>
    <p>i7 960 - MKL 11.0.2</p>
    <p>i7 960 - ViennaCL 1.4.0</p>
    <p>We get 20% of the MKL performance!</p>
    <p>Code portability does not imply performance portability</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier Breaking the barrierBreaking the barrier</p>
    <p>The manual approach</p>
    <p>Learn about the architecture</p>
    <p>Write, debug, profile, optimize your code</p>
    <p>Repeat the process whenever a new architecture is released</p>
    <p>The automatic approach</p>
    <p>Build a compute kernel generator</p>
    <p>Build parameterized compute kernels for your algorithms</p>
    <p>Run an automatic tuning procedure</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier The auto-tuning procedureThe auto-tuning procedure</p>
    <p>Potentially huge parameter space</p>
    <p>K</p>
    <p>K</p>
    <p>kl</p>
    <p>kl</p>
    <p>ks</p>
    <p>ks M ml ms</p>
    <p>N</p>
    <p>nl</p>
    <p>ns</p>
    <p>*=</p>
    <p>Big area of future work</p>
    <p>The parameter space is still architecture-dependent</p>
    <p>The brute-force approach executes in 10  4  4 hours...</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier Breaking the barrierBreaking the barrier</p>
    <p>The good old matrix-matrix multiplication problem - revisited</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Matrix Rows/Columns</p>
    <p>INTEL Core i7 960 - MKL 11.0.2</p>
    <p>INTEL Core i7 960 - ViennaCL</p>
    <p>NVIDIA GTX 470 - CUBLAS 5.0</p>
    <p>NVIDIA GTX 470 - ViennaCL</p>
    <p>AMD HD7970 - clAmdBlas 1.10</p>
    <p>AMD HD7970 - ViennaCL</p>
    <p>What about single precision?</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier Breaking the barrierBreaking the barrier</p>
    <p>The good old matrix-matrix multiplication problem - revisited</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Matrix Rows/Columns</p>
    <p>INTEL Core i7 960 - MKL 11.0.2</p>
    <p>INTEL Core i7 960 - ViennaCL</p>
    <p>NVIDIA GTX 470 - CUBLAS 5.0</p>
    <p>NVIDIA GTX 470 - ViennaCL</p>
    <p>AMD HD7970 - clAmdBlas 1.10</p>
    <p>AMD HD7970 - ViennaCL</p>
    <p>What about less compute-intensive tasks?</p>
  </div>
  <div class="page">
    <p>The architectural barrierThe architectural barrier Breaking the barrierBreaking the barrier</p>
    <p>The Matrix-Vector multiplication problem</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Matrix Rows/Columns</p>
    <p>INTEL Core i7 960 - MKL 11.0.2</p>
    <p>INTEL Core i7 960 - ViennaCL</p>
    <p>NVIDIA GTX 470 - CUBLAS 5.0</p>
    <p>NVIDIA GTX 470 - ViennaCL</p>
    <p>AMD HD7970 - clAmdBlas 1.10</p>
    <p>AMD HD7970 - ViennaCL</p>
    <p>OpenCL code can be performance-portable</p>
  </div>
  <div class="page">
    <p>How to runHow to run</p>
    <p>BLAS Level 1 operations</p>
    <p>using namespace viennacl; vector&lt;double&gt; x, y; scalar&lt;double&gt; beta; /* Fill x, y here */ custom_operation op; op.add(beta = inner_prod(x, 2*x + y)); op.execute();</p>
    <p>Multi-Matrix multiplication</p>
    <p>using namespace viennacl; add_all_available_devices(CL_DEVICE_TYPE_GPU); multi_matrix&lt;float&gt; C, A, B; /* Fill A, B here */ C = prod(A + B, A - B); finish();</p>
  </div>
  <div class="page">
    <p>What you getWhat you get BLAS Level 1 operationsBLAS Level 1 operations</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>Vector Size</p>
    <p>INTEL Core i7 960 - MKL 11.0.2</p>
    <p>INTEL Core i7 960 - ViennaCL</p>
    <p>NVIDIA GTX 470 - CUBLAS 5.0</p>
    <p>NVIDIA GTX 470 - ViennaCL</p>
    <p>AMD HD7970 - clAmdBlas 1.10</p>
    <p>AMD HD7970 - ViennaCL</p>
  </div>
  <div class="page">
    <p>What you getWhat you get Distributed SGEMMDistributed SGEMM</p>
    <p>G F</p>
    <p>L O</p>
    <p>P /s</p>
    <p>M e m</p>
    <p>o ry</p>
    <p>U s a g e (</p>
    <p>G B</p>
    <p>)</p>
    <p>Matrix Rows/Columns (Thousands)</p>
    <p>ViennaCL - GTX 470 ViennaCL - Tesla C2050 ViennaCL - GTX 470 + Tesla C2050, Ideal ViennaCL - GFLOP/s - Optimized ViennaCL - GFLOP/s - Default ViennaCL - Memory Usage- Optimized ViennaCL - Memory Usage - Default</p>
  </div>
  <div class="page">
    <p>SummarySummary</p>
    <p>Auto-Tuning Framework</p>
    <p>Adapts the underlying hardware</p>
    <p>The whole BLAS Standard is supported</p>
    <p>At least 75% of vendor-tuned libraries performance</p>
    <p>Supports multiple GPUs</p>
    <p>Future Work</p>
    <p>Improve the auto-tuning procedure</p>
    <p>Fully transparent use</p>
    <p>Open-Source</p>
    <p>Will be released in ViennaCL 1.5.0</p>
    <p>http://viennacl.sourceforge.net</p>
  </div>
  <div class="page">
    <p>Global StructureGlobal Structure</p>
    <p>RAM</p>
    <p>Device Dv1,1</p>
    <p>Vendor 1</p>
    <p>Task 1</p>
    <p>Compute Kernel Generator</p>
    <p>Device Dv1,2 Device Dv2,1</p>
    <p>Task 2</p>
    <p>B</p>
    <p>Task 3</p>
    <p>Task 4</p>
    <p>Scheduler</p>
    <p>Vendor 2</p>
    <p>Vector Addition</p>
    <p>Matrix Product</p>
  </div>
  <div class="page">
    <p>Generator SchemeGenerator Scheme</p>
    <p>Static Optimizations</p>
    <p>Compute Kernels Segmentation</p>
    <p>Template Engine</p>
    <p>Dynamic Optimizations</p>
    <p>Auto-Tuning Environment</p>
    <p>...</p>
    <p>... Dynamic</p>
    <p>Optimizations</p>
    <p>Device 1</p>
    <p>Dynamic Optimizations</p>
    <p>Auto-Tuning Environment</p>
    <p>...</p>
    <p>Dynamic Optimizations</p>
    <p>Device N</p>
  </div>
</Presentation>
