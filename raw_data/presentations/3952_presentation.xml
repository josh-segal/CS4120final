<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Accelerating Parallel Analysis of Scientific Simulation Data via Zazen</p>
    <p>Tiankai Tu, Charles A. Rendleman, Patrick J. Miller, Federico Sacerdoti, Ron O. Dror, and David E. Shaw</p>
    <p>D. E. Shaw Research</p>
  </div>
  <div class="page">
    <p>Motivation Goal: To model biological processes that occur on the millisecond time scale</p>
    <p>Approach: A specialized, massively parallel supercomputer called Anton (2009 ACM Gordon Bell Award for Special Achievement)</p>
  </div>
  <div class="page">
    <p>Millisecond-scale MD Trajectories</p>
    <p>Simulation length: 1x 10-3 s</p>
    <p>Output interval: 10 x 10-12 s</p>
    <p>Number of frames: 100 M frames</p>
    <p>A biomolecular system: 25 K atoms</p>
    <p>Position and velocity: 24 bytes/atom</p>
    <p>Frame size: 0.6 MB/frame</p>
  </div>
  <div class="page">
    <p>Part I: How We Analyze Simulation Data in Parallel</p>
  </div>
  <div class="page">
    <p>An MD Trajectory Analysis Example: Ion Permeation</p>
  </div>
  <div class="page">
    <p>-2</p>
    <p>-1</p>
    <p>Ion A Ion B</p>
    <p>A Hypothetic Trajectory 20,000 atoms in total; two ions of interest</p>
  </div>
  <div class="page">
    <p>Ion State Transition</p>
    <p>Above channel</p>
    <p>Inside channel</p>
    <p>Into channel</p>
    <p>from above</p>
    <p>Below channel</p>
    <p>Into channel</p>
    <p>from below</p>
    <p>S</p>
    <p>S</p>
    <p>S</p>
  </div>
  <div class="page">
    <p>Typical Sequential Analysis</p>
    <p>Maintain a main-memory resident data structure to record states and positions</p>
    <p>Process frames in ascending simulated physical time order</p>
    <p>Strong inter-frame data dependence: Data analysis tightly coupled with data acquisition</p>
  </div>
  <div class="page">
    <p>Problems with Sequential Analysis Millisecond-scale trajectory size : 60 TB</p>
    <p>Local disk read bandwidth : 100 MB / s</p>
    <p>Time to fetch data to memory : 1 week</p>
    <p>Analysis time : Varied</p>
    <p>Time to perform data analysis : Weeks</p>
    <p>Sequential analysis lack the computational, memory, and I/O</p>
    <p>capabilities!</p>
  </div>
  <div class="page">
    <p>A Parallel Data Analysis Model Specify which frames to be accessed</p>
    <p>Trajectory definition</p>
    <p>Stage1: Per-frame data acquisition</p>
    <p>Stage 2: Cross-frame data analysis</p>
    <p>Decouple data acquisition from data analysis</p>
  </div>
  <div class="page">
    <p>Trajectory Definition</p>
    <p>-2</p>
    <p>-1</p>
    <p>Ion A Ion B</p>
    <p>Every other frame in the trajectory</p>
  </div>
  <div class="page">
    <p>-2</p>
    <p>-1</p>
    <p>Ion A Ion B</p>
    <p>Per-frame Data Acquisition (stage 1) P0 P1</p>
  </div>
  <div class="page">
    <p>Cross-frame Data Analysis (stage 2)</p>
    <p>-2</p>
    <p>-1</p>
    <p>Ion A Ion B</p>
    <p>Analyze ion A on P0 and ion B on P1 in parallel</p>
  </div>
  <div class="page">
    <p>Inspiration: Googles MapReduce</p>
    <p>reduce(K1, ...) reduce(K2, ...)</p>
    <p>Google File System</p>
    <p>Input files Input files Input files</p>
    <p>map(...)</p>
    <p>K1: {v1j} K2: {v2j}</p>
    <p>K1: {v1j, v1i, v1k}</p>
    <p>map(...)</p>
    <p>K1: {v1i} K2: {v2i}</p>
    <p>map(...)</p>
    <p>K1: {v1k} K2: {v2k}</p>
    <p>K2: {v2k, v2j, v2i}</p>
    <p>Output file Output file</p>
  </div>
  <div class="page">
    <p>Trajectory Analysis Cast Into MapReduce</p>
    <p>Per-frame data acquisition (stage 1): map()</p>
    <p>Cross-frame data analysis (stage 2): reduce()</p>
    <p>Key-value pairs: connecting stage1 and stage2</p>
    <p>Keys: categorical identifiers or names</p>
    <p>Values: including timestamps</p>
    <p>Examples: (ion_idj, (tk, xik, yjk, zjk))</p>
    <p>Key Value</p>
  </div>
  <div class="page">
    <p>The HiMach Library A MapReduce-style API that allows users to write Python programs to analyze MD trajectories</p>
    <p>A parallel runtime that executes HiMach user programs in parallel on a Linux cluster automatically</p>
    <p>Performance results on a Linux cluster:</p>
  </div>
  <div class="page">
    <p>Typical SimulationAnalysis Storage Infrastructure</p>
    <p>Parallel supercomputer</p>
    <p>File servers</p>
    <p>Analysis node</p>
    <p>Analysis cluster</p>
    <p>I/O node</p>
    <p>Analysis node</p>
    <p>Local disks</p>
    <p>Local disks</p>
    <p>Parallel analysis programs</p>
  </div>
  <div class="page">
    <p>Part II: How We Overcome the I/O Bottleneck in Parallel Analysis</p>
  </div>
  <div class="page">
    <p>Trajectory Characteristics A large number of small frames</p>
    <p>Write once, read many</p>
    <p>Distinguishable by unique integer sequence numbers</p>
    <p>Amenable to out-of-order parallel access in the map phase</p>
  </div>
  <div class="page">
    <p>Our Main Idea At simulation time, actively cache</p>
    <p>frames in the local disks of the analysis nodes as the frames become available</p>
    <p>At analysis time, fetch data from local disk caches in parallel</p>
  </div>
  <div class="page">
    <p>Limitations Require large aggregate disk</p>
    <p>capacity on the analysis cluster</p>
    <p>Assume relatively low average simulation data output rate</p>
  </div>
  <div class="page">
    <p>An Example Analysis node 0</p>
    <p>/</p>
    <p>sim0</p>
    <p>/</p>
    <p>bodhi</p>
    <p>Analysis node 1</p>
    <p>/</p>
    <p>bodhi</p>
    <p>sim0 sim1sim1</p>
    <p>f0 f2 0 2</p>
    <p>seq</p>
    <p>sim1</p>
    <p>f0 f2f1 f3</p>
    <p>sim0</p>
    <p>f1 1</p>
    <p>seq f3</p>
    <p>NFS server</p>
    <p>Local bitmap Remote bitmap Merged bitmap</p>
  </div>
  <div class="page">
    <p>The Zazen Protocol</p>
    <p>How to guarantee that each frame is read by one and only one node in the face of node failure and recovery?</p>
  </div>
  <div class="page">
    <p>The Zazen Protocol Execute a distributed consensus protocol before performing actual disk I/O</p>
    <p>Assign data retrieval tasks in a locationaware manner</p>
    <p>Read data from local disks if the data are already cached</p>
    <p>Fetch missing data from file servers</p>
    <p>No metadata servers to keep record of who has what</p>
  </div>
  <div class="page">
    <p>The Zazen Protocol (contd)</p>
    <p>Bitmaps: a compact structure for recording the presence or non-presence of a cached copy</p>
    <p>All-to-all reduction algorithms: an efficient mechanism for inter-processor collective communications (used an MPI library in practice)</p>
  </div>
  <div class="page">
    <p>Implementation The Bodhi library</p>
    <p>The Bodhi server</p>
    <p>The Zazen protocol</p>
    <p>Parallel supercomputer</p>
    <p>Bodhi server</p>
    <p>Bodhi library</p>
    <p>Bodhi server</p>
    <p>File servers</p>
    <p>Analysis node Analysis node Zazen cluster</p>
    <p>Parallel analysis programs (HiMach jobs)</p>
    <p>Zazen protocol</p>
    <p>Bodhi library</p>
    <p>Bodhi library I/O node</p>
  </div>
  <div class="page">
    <p>Performance Evaluation</p>
  </div>
  <div class="page">
    <p>A Linux cluster with 100 nodes</p>
    <p>Two Intel Xeon 2.33 GHz quad-core processors per node</p>
    <p>Four 500 GB 7200-RPM SATA disks organized in RAID 0 per node</p>
    <p>CentOS 4.6 with a Linux kernel of 2.6.26</p>
    <p>Nodes connected to a Gigabit Ethernet core switch</p>
    <p>Common accesses to NFS directories exported by a number of enterprise storage servers</p>
    <p>Experiment Setup</p>
  </div>
  <div class="page">
    <p>Fixed-Problem-Size Scalability</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>Number of nodes</p>
    <p>Execution time of the Zazen protocol to assign the I/O tasks of reading 1 billion frames</p>
  </div>
  <div class="page">
    <p>Fixed-Cluster-Size Scalability</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>Number of frames</p>
    <p>Execution time of the Zazen protocol on 100 nodes assigning different number of frames</p>
  </div>
  <div class="page">
    <p>Efficiency I: Achieving Better I/O BW One Bodhi daemon</p>
    <p>per user process</p>
    <p>G B</p>
    <p>/s</p>
    <p>Application read processes per node</p>
    <p>One Bodhi daemon per analysis node</p>
    <p>G B</p>
    <p>/s</p>
    <p>Application read processes per node</p>
  </div>
  <div class="page">
    <p>Efficiency II: Comparison w. NFS/PFS NFS (v3) on separate enterprise storage servers  Dual quad-core 2.8-GHz Opteron processors, 16 GB memory, 48 SATA disks organized in RAID 6</p>
    <p>Four 1 GigE connection to the core switch of the 100-node cluster</p>
    <p>PVFS2 (2.8.1) on the same 100 analysis nodes  I/O (data) server and metadata server on all nodes</p>
    <p>File I/O performed via the PVFS2 Linux kernel interface</p>
    <p>Hadoop/HDFS (0.19.1) on the same 100 nodes  Data stored via HDFSs C library interface, block sizes set to be equal to file sizes, three replications per file</p>
    <p>Data accessed via a read-only Hadoop MapReduce Java program (with a number of best-effort optimizations)</p>
  </div>
  <div class="page">
    <p>Efficiency II: Outperforming NFS/PFS</p>
    <p>G B</p>
    <p>/s</p>
    <p>File size for read</p>
    <p>NFS PVFS2 Hadoop/HDFS Zazen</p>
    <p>I/O bandwidth of reading files of different sizes</p>
  </div>
  <div class="page">
    <p>Efficiency II: Outperforming NFS/PFS Time to read one terabyte of data</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>File size for read</p>
    <p>NFS PVFS2 Hadoop/HDFS Zazen</p>
  </div>
  <div class="page">
    <p>Read Perf. under Writes (1GB/s)</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>File size for reads</p>
    <p>File size for writes No writes 1 GB files 256 MB files 64 MB files 2 MB files</p>
  </div>
  <div class="page">
    <p>End-to-End Performance A HiMach analysis program called water residence on 100 nodes</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>Application processes per node</p>
    <p>NFS Zazen Memory</p>
  </div>
  <div class="page">
    <p>Robustness Worst case execution time is T(1 +  (B/b) )</p>
    <p>The water-residence program re-executed with varying number of nodes powered off</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>Node failure rate</p>
    <p>Theoretical worst case</p>
    <p>Actual running time</p>
  </div>
  <div class="page">
    <p>Summary Zazen accelerates order-independent, parallel data access by (1) actively caching simulation output, and (2) executing an efficient distributed consensus protocol.</p>
    <p>Simple and robust</p>
    <p>Scalable on a large number of nodes</p>
    <p>Much higher performance than NFS/PFS * Applicable to a certain class of time</p>
    <p>dependent simulation datasets *</p>
  </div>
</Presentation>
