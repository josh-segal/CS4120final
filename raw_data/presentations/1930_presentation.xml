<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Accelerating Deep Learning Inference via Freezing</p>
    <p>Adarsh Kumar, Arjun Balasubramanian, Shivaram Venkataraman, Aditya Akella</p>
  </div>
  <div class="page">
    <p>Deep Learning  State of affairs Over the years - Top 5 error rate decreasing - Models becoming deeper</p>
    <p>Top Competitors - ImageNet Large Scale Visual Recognition Challenge</p>
  </div>
  <div class="page">
    <p>Deep Learning  State of affairs Over the years - Top 5 error rate decreasing - Models becoming deeper</p>
    <p>Top Competitors - ImageNet Large Scale Visual Recognition Challenge</p>
    <p>Suits goals for ML training</p>
  </div>
  <div class="page">
    <p>Deep Learning  State of affairs</p>
    <p>dddd Over the years - Top 5 error rate decreasing - Models becoming deeper</p>
    <p>Top Competitors - ImageNet Large Scale Visual Recognition Challenge</p>
    <p>Suits goals for ML training</p>
    <p>Not aligned with goals for ML inference</p>
  </div>
  <div class="page">
    <p>Deep Learning - Background</p>
    <p>Input Layer Hidden Layer 1 Hidden Layer 2 Output Layer</p>
    <p>Neural Network - Sequence of layers with each layer dependent on previous layers</p>
  </div>
  <div class="page">
    <p>Deep Learning  State of affairs</p>
    <p>dddd</p>
    <p>Top Competitors - ImageNet Large Scale Visual Recognition Challenge</p>
    <p>Not aligned with goals for ML inference - Requires low latency - Challenge due to deeper models</p>
  </div>
  <div class="page">
    <p>Deep Learning  Reducing Latency</p>
    <p>Prior Solutions</p>
    <p>Model Quantization: Changes precision of computation; Hurts accuracy  Model Distillation: Smaller model is trained to mimic larger/ensemble model; Hurts accuracy  Ensemble Methods: Run multiple models, choose best; Resources wasted  Anytime Predictions: Auxiliary Predictions; Trade-off b/w accuracy and latency  Custom Hardware: TPUs, FPGAs; Hardware dependent</p>
  </div>
  <div class="page">
    <p>Freeze Inference</p>
    <p>Provides low-latency inference by caching intermediate layer outputs</p>
    <p>Goals  No trade-off on accuracy  Resource efficient  Hardware agnostic</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Key Insight Input Layer BackgroundSubtraction</p>
    <p>Edge Detection</p>
    <p>Output Layer</p>
    <p>JUMBO</p>
    <p>JUMBO</p>
    <p>Input to layer is not same for both images Input to layer is same for both images 9</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Basic Mechanism</p>
    <p>Prior to Inference - Cache intermediate layer outputs</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Basic Mechanism</p>
    <p>During Inference  Look-up from cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Basic Mechanism</p>
    <p>During Inference  Look-up from cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Basic Mechanism</p>
    <p>During Inference  Look-up from cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Basic Mechanism</p>
    <p>During Inference  Look-up from cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Basic Mechanism</p>
    <p>During Inference  Look-up from cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely</p>
    <p>Challenge #2: Curse of dimensionality</p>
    <p>Challenge #3: Memory and computational overheads</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely</p>
    <p>Challenge #2: Curse of dimensionality</p>
    <p>Challenge #3: Memory and computational overheads</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely Why? - High dimensions - Floating point precision</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely Why? - High Dimensions - Floating point precision</p>
    <p>Approach? - Points close by in feature space have high probability of having same prediction</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely Towards Approximate Caching Instead of exact matches, find k nearest points in the cache</p>
    <p>Prediction is the majority label among the k nearest neighbors</p>
    <p>P AA</p>
    <p>B</p>
    <p>P Point under consideration</p>
    <p>APrediction is</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely Need confidence to infer the quality of a prediction</p>
    <p>Prediction can be more confident if: (i) More neighbors agree on the same label (ii) Neighbors are closer to the input point</p>
    <p>A P</p>
    <p>A A</p>
    <p>P A</p>
    <p>A B</p>
    <p>A P</p>
    <p>A A</p>
    <p>P A</p>
    <p>A A</p>
    <p>&gt;</p>
    <p>&gt;</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely Need confidence to infer the quality of a prediction</p>
    <p>Prediction can be more confident if: (i) More neighbors agree on the same label (ii) Neighbors are closer to the input point</p>
    <p>A P</p>
    <p>A A</p>
    <p>P A</p>
    <p>A B</p>
    <p>A P</p>
    <p>A A</p>
    <p>PA</p>
    <p>A A</p>
    <p>&gt;</p>
    <p>&gt;</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely Need confidence to infer the quality of a prediction</p>
    <p>Prediction can be more confident if: (i) More neighbors agree on the same label (ii) Neighbors are closer to the input point</p>
    <p>Confidence - Heuristic based on the above</p>
    <p>A P</p>
    <p>A A</p>
    <p>P A</p>
    <p>A B</p>
    <p>A P</p>
    <p>A A</p>
    <p>PA</p>
    <p>A A</p>
    <p>&gt;</p>
    <p>&gt;</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #1: Exact cache hit is unlikely</p>
    <p>Towards Approximate Caching How much confidence is good enough? - Need to establish a threshold per layer.</p>
    <p>Validation dataset</p>
    <p>Each point Make prediction at layer k</p>
    <p>Correct prediction</p>
    <p>?</p>
    <p>No Threshold at layer k</p>
    <p>= Max. such observed confidence</p>
  </div>
  <div class="page">
    <p>Freeze Inference  System Design</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Results</p>
    <p>Evaluation against - Datasets: CIFAR-10 and CIFAR-100 - Models: ResNet-18 and ResNet-50</p>
    <p>For each test, - Use 35,000 points for cache construction - Use 5,000 points for threshold computation - Apply Freeze Inference for 10,000 requests</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Results</p>
    <p>ResNet-18 results Upper Bound Actual</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Results</p>
    <p>ResNet-18 results</p>
    <p>Block 5  k-NN: ~25% Upper bound: ~90% Bridging this gap is an interesting research problem</p>
    <p>Upper Bound Actual</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Discussion</p>
    <p>Discussion Point #1  Managing memory requirement - Storing each point incurs memory overheads - Can use k-means to reduce memory overheads - Given a fixed cache budget M, choose points to constitute cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Discussion</p>
    <p>Discussion Point #2  Cache placement - To be placed closed to region of compute for low latency - Cache placement on GPUs</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Can use caching of intermediate layer outputs to reduce inference latency</p>
    <p>Open research challenges to fully realize the potential</p>
    <p>Adaptation to custom hardware like GPUs  Computational and memory overheads  Online cache construction mechanism  Better cache look-up schemes</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #2: Curse of Dimensionality - Distance based similarity measures do not work well in high dimension - Impact: Cache look-up will not be accurate</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #2: Curse of Dimensionality - Distance based similarity measures do not work well in high dimension - Impact: Cache look-up will not be accurate</p>
    <p>Solution? - Inspired by metric learning, use a one layer neural network for supervised</p>
    <p>dimensionality reduction</p>
    <p>High Dimen.</p>
    <p>Low Dimen.</p>
    <p># classes</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #3: Memory and Computational Overheads k-nearest neighbors necessitates - Compute: Distance to be computed against each point in cache - Memory: To hold the cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Challenges</p>
    <p>Challenge #3: Memory and Computational Overheads k-nearest neighbors necessitates - Compute: Distance to be computed against each point in cache - Memory: To hold the cache</p>
    <p>Solution? Can use k-means to cluster points in cache Store only cluster centers and associated labels in cache</p>
  </div>
  <div class="page">
    <p>Freeze Inference - Results</p>
    <p>Memory overheads depend on  (i) # layers in model (ii) Lower dimension size (d) (iii) Value of k in k-NN</p>
    <p>Model Memory (d=1024 and k=100) ResNet-18 12.5 MB ResNet-50 25 MB</p>
  </div>
  <div class="page">
    <p>Freeze Inference  Discussion</p>
    <p>Discussion Point #3  Online Cache Updates - Incorporating inference points into cache - Handling frequent inference queries</p>
  </div>
</Presentation>
