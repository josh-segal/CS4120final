<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Yiming Zhang, Rui Chu</p>
    <p>@ NUDT</p>
    <p>Chuanxiong Guo, Guohan Lu, Yongqiang Xiong, Haitao Wu</p>
    <p>@ MSRA</p>
    <p>June, 2012</p>
    <p>RAMCube: Exploiting Network Proximity</p>
    <p>for RAM-Based Key-Value Store</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Disk-based storage is problematic</p>
    <p>I/O latency and bandwidth</p>
    <p>Current typical storage system</p>
    <p>Using RAM as a cache</p>
    <p>App servers + storage servers + cache servers</p>
    <p>Facebook keeps more than 75% data in memcached</p>
  </div>
  <div class="page">
    <p>Why Cache is NOT Preferred</p>
    <p>Consistency  App is responsible for consistency (e.g. memcached)</p>
    <p>Error-prone</p>
    <p>E.g., the failure of Facebook Automated Verifying System</p>
    <p>Efficiency  RAM is 1000X faster than disk</p>
  </div>
  <div class="page">
    <p>Using RAM as a Persistent Store</p>
    <p>RAMCloud: RAM-based key-value store</p>
    <p>Data is kept entirely in the RAM of servers</p>
    <p>Primary-backup for durability</p>
    <p>Multiple copies for each key-value pair</p>
    <p>Primary node + backup node + recovery node</p>
    <p>Write/read/recover procedure</p>
    <p>Data center network</p>
    <p>X recover</p>
  </div>
  <div class="page">
    <p>RAMCloud (cont.)</p>
    <p>Fast failure recovery</p>
    <p>Availability: MTTF / (MTTF + MTTR)</p>
    <p>Not specifically designed for DCN</p>
    <p>Temporary network problems cause false</p>
    <p>failure detection</p>
    <p>Parallel unarranged recovery flows</p>
    <p>ToR switch failures</p>
  </div>
  <div class="page">
    <p>RAMCube: Exploiting DCN Proximity</p>
    <p>Goal of RAM-based storage</p>
    <p>High performance</p>
    <p>Low latency</p>
    <p>10+ s read/write latency</p>
    <p>Large scale</p>
    <p>Hundreds of TB data</p>
    <p>High throughput  Millions of read ops per server per sec</p>
    <p>Hundreds of thousands of write ops / server / sec</p>
    <p>Fast failure recovery for high availability</p>
    <p>1~2 sec</p>
  </div>
  <div class="page">
    <p>RAMCube Design Choices</p>
    <p>Network hardware</p>
    <p>- InfiniBand is high-bandwidth, low-latency</p>
    <p>But expensive and not common in DCN</p>
    <p>We use commodity Ethernet-based BCube</p>
    <p>Primary-backup vs. symmetric replication</p>
    <p>- Symmetric replication easy to achieve high</p>
    <p>availability</p>
    <p>But uses more RAM</p>
    <p>We use primary-recovery-backup like</p>
    <p>RAMCloud</p>
  </div>
  <div class="page">
    <p>Basic Idea of RAMCube</p>
  </div>
  <div class="page">
    <p>Using BCube</p>
  </div>
  <div class="page">
    <p>Mapping Key Space onto BCube</p>
    <p>Multi-layer logical rings</p>
    <p>Primary ring  All servers in BCube are</p>
    <p>primary servers.</p>
    <p>The whole key space is mapped onto primary ring  Each primary server for a</p>
    <p>subset of the key space</p>
  </div>
  <div class="page">
    <p>Multiple Rings in RAMCube (cont.)</p>
    <p>Each primary server P has a recovery ring  Composed of 1-hop neighbors to P</p>
    <p>Map the ps key space to recovery servers</p>
    <p>Each recovery server R has a backup ring  Composed of servers 1-hop to R and</p>
    <p>Map the rs key space to backup servers</p>
  </div>
  <div class="page">
    <p>RAMCube Property</p>
    <p>MultiRing of RAMCube for BCube(n, k)</p>
    <p># of primary servers</p>
    <p>P = nk+1</p>
    <p># of recovery servers for each primary p</p>
    <p>R = (n-1)(k+1)</p>
    <p># of backup servers for each primary server</p>
    <p>B = (n-1)2k(k+1)/2</p>
    <p>Each primary node has plenty of recovery</p>
    <p>and backup servers</p>
    <p>BCube(16,2),</p>
    <p>P = 4096, R = 45, B = 675</p>
    <p>BCube(4,1),</p>
    <p>P = 16, R = 6, B = 9</p>
  </div>
  <div class="page">
    <p>Failure detection</p>
    <p>Heartbeats</p>
    <p>Primary node periodically sends heartbeats to each of its recovery nodes</p>
    <p>Confirmation of failure</p>
    <p>Recovery node uses source routing to issue additional pings to suspicious primary node</p>
  </div>
  <div class="page">
    <p>Failure Recovery</p>
    <p>Primary node failure</p>
    <p>Recovery nodes fetch dominant copies to their RAM from directly-connected backup nodes</p>
    <p>Given 10Gbps network bandwidth and 100MB/s disk bandwidth  BCube (16,2) can easily recover 64GB data in 1~2 sec</p>
    <p>Recovery/backup node failure</p>
    <p>Not as critical as primary node failure</p>
  </div>
  <div class="page">
    <p>Prototype Implementation</p>
    <p>Platform &amp; language</p>
    <p>Windows 2008R2,</p>
    <p>C++, libevent</p>
    <p>Code size: 3000+ lines</p>
    <p>RAMCube components</p>
    <p>Connection manager</p>
    <p>maintains the status of directly connected neighbors and</p>
    <p>handles network events</p>
    <p>memory manager</p>
    <p>uses a slab-based mechanism and handles set/get/delete</p>
    <p>requests</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Evaluation configuration</p>
    <p>BCube(4,1) with 16 servers</p>
    <p>Eight 2.4GHz cores &amp; 16G RAM per node</p>
    <p>Single-threaded server and client</p>
    <p>Both with a busy loop</p>
    <p>15B key + 100B value</p>
    <p>Experiments</p>
    <p>Throughput</p>
    <p>Server failure recovery</p>
    <p>Switch failures</p>
  </div>
  <div class="page">
    <p>Experimental Results -1</p>
  </div>
  <div class="page">
    <p>Experimental Results -2</p>
    <p>Ideal:</p>
    <p>Total Disk I/O (backup)</p>
    <p>Total net I/O (recovery)</p>
  </div>
  <div class="page">
    <p>Experimental Results -3</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>SSD &amp; Low-latency Ethernet</p>
    <p>Rich data model</p>
    <p>Table/tablet support, column family</p>
    <p>Utilizing multi-core</p>
    <p>Improve read/write ops throughput</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>RAMCube: RAM-based persistent k-v store</p>
    <p>Exploiting network proximity of BCube</p>
    <p>Failure detection  1-hop  Recovery nodes directly watch their primary nodes</p>
    <p>Recovery traffic  1-hop  No unexpected traffic overlap</p>
    <p>Switch failures  multi-path  Graceful performance degradation</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
  </div>
</Presentation>
