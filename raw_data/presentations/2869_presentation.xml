<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Prototype-Driven Grammar Induction</p>
    <p>Aria Haghighi and Dan Klein</p>
    <p>Computer Science Division</p>
    <p>University of California Berkeley</p>
  </div>
  <div class="page">
    <p>Grammar Induction</p>
    <p>Grammar Engineer</p>
    <p>EMData Output</p>
  </div>
  <div class="page">
    <p>Central Questions</p>
    <p>How do we specify what we want to learn?</p>
    <p>How do we fix observed errors?</p>
    <p>NPs are things like DT NN</p>
    <p>Thats not quite it!</p>
  </div>
  <div class="page">
    <p>Experimental Set-up</p>
    <p>Binary Grammar</p>
    <p>{ X1, X2,  Xn} plus POS tags</p>
    <p>Data  WSJ-10 [7k sentences]  Evaluate on Labeled F1  Grammar Upper Bound: 86.1</p>
    <p>Xj</p>
    <p>Xi</p>
    <p>Xk</p>
  </div>
  <div class="page">
    <p>Unconstrained PCFG Induction</p>
    <p>Learn PCFG with EM  Inside-Outside Algorithm  Lari &amp; Young [90]</p>
    <p>Results 0 i j n</p>
    <p>(Inside)</p>
    <p>(Outside)</p>
    <p>PCFG</p>
    <p>Upper Bound</p>
    <p>Labeled F1</p>
  </div>
  <div class="page">
    <p>Encoding Knowledge</p>
    <p>Whats an NP?</p>
    <p>Semi-Supervised Learning</p>
  </div>
  <div class="page">
    <p>Encoding Knowledge</p>
    <p>Whats an NP?</p>
    <p>For instance, DT NN JJ NNS NNP NNP</p>
    <p>Prototype Learning</p>
  </div>
  <div class="page">
    <p>Prototype List</p>
  </div>
  <div class="page">
    <p>DT The</p>
    <p>NN koala</p>
    <p>How to use prototypes?</p>
    <p>NP</p>
    <p>PP</p>
    <p>VP</p>
    <p>VBD sat</p>
    <p>IN in</p>
    <p>DT the</p>
    <p>NN tree</p>
    <p>DT The</p>
    <p>NN koala</p>
    <p>JJ hungry</p>
    <p>NP</p>
    <p>Phrase Prototype</p>
    <p>NP DT NN</p>
    <p>PP IN DT NN</p>
    <p>VP VBD IN DT NN</p>
  </div>
  <div class="page">
    <p>Distributional Similarity</p>
    <p>(DT NN)</p>
    <p>(JJ NNS)</p>
    <p>(NNP NNP) NP</p>
    <p>(VBD DT NN)</p>
    <p>(MD VB NN)</p>
    <p>(VBN IN NN) VP</p>
    <p>(IN NN)</p>
    <p>(IN PRP)</p>
    <p>(TO CD CD) PP</p>
    <p>(DT JJ NN)</p>
    <p>{  __ VBD : 0.3, VBD __  : 0.2, IN __ VBD: 0.1, .. }</p>
    <p>proto=NP</p>
    <p>Clark 01 Klein &amp; Manning 01</p>
  </div>
  <div class="page">
    <p>Distributional Similarity</p>
    <p>(DT NN)</p>
    <p>(JJ NNS)</p>
    <p>(NNP NNP) NP</p>
    <p>(VBD DT NN)</p>
    <p>(MD VB NN)</p>
    <p>(VBN IN NN) VP</p>
    <p>(IN NN)</p>
    <p>(IN PRP)</p>
    <p>(TO CD CD) PP</p>
    <p>(IN DT)</p>
    <p>proto=NONE</p>
  </div>
  <div class="page">
    <p>DT The</p>
    <p>NN koala</p>
    <p>VBD sat</p>
    <p>IN in</p>
    <p>DT the</p>
    <p>NN tree</p>
    <p>JJ hungry</p>
    <p>Prototype CFG+ Model</p>
    <p>proto=NP proto=NONE</p>
  </div>
  <div class="page">
    <p>Prototype CFG+ Model</p>
    <p>NP</p>
    <p>PP</p>
    <p>VP</p>
    <p>DT The</p>
    <p>NN koala</p>
    <p>VBD sat</p>
    <p>IN in</p>
    <p>DT the</p>
    <p>NN tree</p>
    <p>S</p>
    <p>JJ hungry</p>
    <p>NP</p>
    <p>NP</p>
    <p>P (DT NP | NP)</p>
    <p>P (proto=NP | NP)</p>
    <p>CFG Rule</p>
    <p>Proto Feature</p>
    <p>PCFG+(T,F) = X(i,j)! 2 T P(| X) P(pi,j | X)</p>
  </div>
  <div class="page">
    <p>Prototype CFG+ Induction</p>
    <p>Experimental Set-Up  Add Prototypes  BLIPP corpus</p>
    <p>Results</p>
    <p>PCFG</p>
    <p>PROTO</p>
    <p>Labeled F1</p>
    <p>Unlabeled F1 66.9</p>
  </div>
  <div class="page">
    <p>Constituent-Context Model</p>
    <p>DT The</p>
    <p>NN koala</p>
    <p>VBD sat</p>
    <p>IN in</p>
    <p>DT the</p>
    <p>NN tree</p>
    <p>JJ hungry</p>
    <p>Klein &amp; Manning 02</p>
    <p>+</p>
    <p>P(VBD IN DT NN | +)</p>
    <p>P(NN __  | +)</p>
    <p>Yield</p>
    <p>Context</p>
    <p>P(NN VBD| -) P(JJ __ IN | -)</p>
  </div>
  <div class="page">
    <p>Constituent-Context Model</p>
    <p>Bracketing Matrix</p>
    <p>PCCM (B, F) = (i,j) P(Bi,j) P(yi,j | Bi,j) P(ci,j | Bi,j )</p>
    <p>DT The</p>
    <p>NN koala</p>
    <p>VBD sat</p>
    <p>IN in</p>
    <p>DT the</p>
    <p>NN tree</p>
    <p>JJ hungry</p>
    <p>Yield Context</p>
  </div>
  <div class="page">
    <p>Intersected Model</p>
    <p>Different Aspects of Syntax</p>
    <p>Intersected EM [Klein 2005, Liang et. al. 06]</p>
    <p>P(T, F, F) = PCCM(B(T), F) PCFG+(T,F)</p>
    <p>CCM CFG</p>
    <p>PP</p>
    <p>NP</p>
    <p>NP</p>
  </div>
  <div class="page">
    <p>Grammar Induction Experiments</p>
    <p>Intersected CFG+ and CCM Add CCM brackets</p>
    <p>Results</p>
    <p>PROTO</p>
    <p>PROTO CCM</p>
    <p>Labeled F1</p>
  </div>
  <div class="page">
    <p>Reacting to Errors</p>
    <p>Possessive NPs</p>
    <p>Our Tree Target Tree</p>
  </div>
  <div class="page">
    <p>Reacting to Errors</p>
    <p>Add Category: NP-POS NN POS</p>
    <p>New Analysis</p>
  </div>
  <div class="page">
    <p>Error Analysis</p>
    <p>Modal VPs</p>
    <p>Our Tree Target Tree</p>
  </div>
  <div class="page">
    <p>Reacting to Errors</p>
    <p>Add Category: VP-INF VB NN</p>
    <p>New Analysis</p>
  </div>
  <div class="page">
    <p>Fixing Errors</p>
    <p>Supplement Prototypes  NP-POS and VP-INF</p>
    <p>Results</p>
    <p>PROTO CCM</p>
    <p>BEST</p>
    <p>Labeled F1</p>
    <p>Unlabeled F1 78.2</p>
  </div>
  <div class="page">
    <p>Results Summary</p>
    <p>PCFG</p>
    <p>PROTO</p>
    <p>PROTO CCM</p>
    <p>Best</p>
    <p>Upper Bound</p>
    <p>Labeled F1</p>
    <p>50% Error reduction From PCFG to BEST</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Prototype-Driven Learning Flexible Weakly Supervised Framework</p>
    <p>Merged distributional clustering techniques with structured models</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>http://www.cs.berkeley.edu/~aria42</p>
  </div>
  <div class="page">
    <p>Lots of numbers</p>
  </div>
  <div class="page">
    <p>More numbers</p>
  </div>
  <div class="page">
    <p>Yet more numbers</p>
  </div>
</Presentation>
