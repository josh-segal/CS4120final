<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>1 Supported by European Unions Seventh Framework Programme (FP7) under grant agreement no. 615688 (PRIME)</p>
    <p>On the Practical Computational Power of Finite Precision RNNs for</p>
    <p>Language Recognition Gail Weiss, Yoav Goldberg, Eran Yahav</p>
    <p>GRU &lt; LSTM (!?)</p>
  </div>
  <div class="page">
    <p>Current State</p>
    <p>RNNs are everywhere</p>
    <p>We dont know too much about the differences between them:</p>
    <p>Gated RNNs are shown to train better, beyond that:</p>
    <p>RNNs are Turing Complete?</p>
    <p>!2</p>
  </div>
  <div class="page">
    <p>Turing Complete?</p>
    <p>!3</p>
  </div>
  <div class="page">
    <p>Turing Complete?</p>
    <p>!4</p>
    <p>unreasonable assumptions!</p>
  </div>
  <div class="page">
    <p>Turing Complete?</p>
    <p>!5</p>
    <p>unreasonable assumptions!</p>
    <p>TUR ING</p>
    <p>TAR PIT!</p>
  </div>
  <div class="page">
    <p>What happens on real hardware</p>
    <p>and real use-cases?</p>
    <p>!6</p>
  </div>
  <div class="page">
    <p>Real Use</p>
    <p>Gated architectures have the best performance</p>
    <p>LSTM and GRU are most popular</p>
    <p>Of these, the choice between them is unclear</p>
    <p>!7</p>
  </div>
  <div class="page">
    <p>Main Result</p>
    <p>!8</p>
    <p>We accept all RNN types can simulate DFAs</p>
    <p>We show that LSTMs and IRNNs can also count</p>
    <p>And that the GRU and SRNN cannot</p>
  </div>
  <div class="page">
    <p>Power of Counting</p>
    <p>!9</p>
    <p>In NMT: LSTM better at capturing target length</p>
    <p>Practical</p>
  </div>
  <div class="page">
    <p>Power of Counting</p>
    <p>!10</p>
    <p>In NMT: LSTM better at capturing target length</p>
    <p>Practical</p>
    <p>Theoretical</p>
    <p>Finite State Machines vs Counter Machines</p>
  </div>
  <div class="page">
    <p>Similar to finite automata, but also maintain k counters</p>
    <p>A counter has 4 operations: inc/dec by one, do nothing, reset</p>
    <p>Counters are observed by comparison to zero</p>
    <p>K-Counter Machines (SKCMs)</p>
    <p>!11</p>
    <p>Fischer, Meyer, Rosenberg - 1968</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Counting Machines and Chomsky Hierarchy</p>
    <p>!12</p>
    <p>Regular Languages (RL)</p>
    <p>Context Free Languages (CFL)</p>
    <p>Context Sensitive Languages (CSL)</p>
    <p>Recursively Enumerable Languages (RE)</p>
  </div>
  <div class="page">
    <p>Regular Languages (RL)</p>
    <p>Context Free Languages (CFL)</p>
    <p>Context Sensitive Languages (CSL)</p>
    <p>Recursively Enumerable Languages (RE)</p>
    <p>!13</p>
    <p>anbn</p>
    <p>Palindromes</p>
    <p>Chomsky Hierarchy and SKCMs</p>
  </div>
  <div class="page">
    <p>Regular Languages (RL)</p>
    <p>Context Free Languages (CFL)</p>
    <p>Context Sensitive Languages (CSL)</p>
    <p>Recursively Enumerable Languages (RE)</p>
    <p>!14</p>
    <p>anbn anbncn</p>
    <p>Palindromes</p>
    <p>Chomsky Hierarchy and SKCMs</p>
  </div>
  <div class="page">
    <p>!15</p>
    <p>anbn anbncn</p>
    <p>Palindromes</p>
    <p>Regular Languages (RL)</p>
    <p>Context Free Languages (CFL)</p>
    <p>Context Sensitive Languages (CSL)</p>
    <p>Recursively Enumerable Languages (RE)</p>
    <p>Chomsky Hierarchy and SKCMs</p>
  </div>
  <div class="page">
    <p>!16</p>
    <p>anbn anbncn</p>
    <p>Palindromes</p>
    <p>Regular Languages (RL)</p>
    <p>Context Free Languages (CFL)</p>
    <p>Context Sensitive Languages (CSL)</p>
    <p>Recursively Enumerable Languages (RE)</p>
    <p>Chomsky Hierarchy and SKCMs</p>
  </div>
  <div class="page">
    <p>Chomsky Hierarchy and SKCMs</p>
    <p>!17</p>
    <p>anbn anbncn</p>
    <p>Palindromes</p>
    <p>Regular Languages (RL)</p>
    <p>Context Free Languages (CFL)</p>
    <p>Context Sensitive Languages (CSL)</p>
    <p>Recursively Enumerable Languages (RE)</p>
    <p>SKCMs cross the Chomsky Hierarchy!</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Summary so Far</p>
    <p>Counters give additional formal power</p>
    <p>We claimed that LSTM can count and GRU cannot</p>
    <p>Lets see why</p>
    <p>!18</p>
  </div>
  <div class="page">
    <p>Summary so Far</p>
    <p>Counters give additional formal power</p>
    <p>We claimed that LSTM can count and GRU cannot</p>
    <p>Lets see why</p>
    <p>!19</p>
  </div>
  <div class="page">
    <p>Popular Architectures</p>
    <p>!20</p>
    <p>GRU LSTM</p>
    <p>zt = (W zxt + U</p>
    <p>zht1 + b z)</p>
    <p>rt = (W r xt + U</p>
    <p>rht1 + b r)</p>
    <p>ht = tanh(W hxt + U</p>
    <p>h(rt  ht1) + b h)</p>
    <p>ht = zt  ht1 + (1  zt)  ht</p>
    <p>ft = (W f xt + U</p>
    <p>f ht1 + b f )</p>
    <p>it = (W ixt + U</p>
    <p>iht1 + b i)</p>
    <p>ot = (W oxt + U</p>
    <p>oht1 + b o)</p>
    <p>ct = tanh(W cxt + U</p>
    <p>cht1 + b c)</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
  </div>
  <div class="page">
    <p>Popular Architectures</p>
    <p>!21</p>
    <p>GRU LSTM</p>
    <p>zt = (W zxt + U</p>
    <p>zht1 + b z)</p>
    <p>rt = (W r xt + U</p>
    <p>rht1 + b r)</p>
    <p>ht = tanh(W hxt + U</p>
    <p>h(rt  ht1) + b h)</p>
    <p>ht = zt  ht1 + (1  zt)  ht</p>
    <p>ft = (W f xt + U</p>
    <p>f ht1 + b f )</p>
    <p>it = (W ixt + U</p>
    <p>iht1 + b i)</p>
    <p>ot = (W oxt + U</p>
    <p>oht1 + b o)</p>
    <p>ct = tanh(W cxt + U</p>
    <p>cht1 + b c)</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>gates</p>
    <p>candidate vectors</p>
    <p>update functions</p>
  </div>
  <div class="page">
    <p>zt  (0,1) rt  (0,1) ht = tanh(W</p>
    <p>hxt + U h(rt  ht1) + b</p>
    <p>h) ht = zt  ht1 + (1  zt)  ht</p>
    <p>Popular Architectures</p>
    <p>!22</p>
    <p>GRU LSTM</p>
    <p>ft  (0,1)W f xtdfsfsfddgdg</p>
    <p>it  (0,1)W ixtddgdgsfsdfs</p>
    <p>ot  (0,1)W oxtddgdgsdfsfd</p>
    <p>ct = tanh(W cxt + U</p>
    <p>cht1 + b c)</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>gates</p>
    <p>candidate vectors</p>
    <p>update functions</p>
  </div>
  <div class="page">
    <p>!23</p>
    <p>LSTM</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  zt)  ht</p>
    <p>ft  (0,1)W f xtaaaaaaaaaa</p>
    <p>it  (0,1)W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>GRU</p>
    <p>Popular Architectures</p>
    <p>gates</p>
    <p>candidate vectors</p>
    <p>update functions</p>
  </div>
  <div class="page">
    <p>!24</p>
    <p>LSTM</p>
    <p>ft  (0,1)W f xtaaaaaaaaaa</p>
    <p>it  (0,1)W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
  </div>
  <div class="page">
    <p>!25</p>
    <p>LSTM</p>
    <p>ft  (0,1)W f xtaaaaaaaaaa</p>
    <p>it  (0,1)W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>Interpolation</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
  </div>
  <div class="page">
    <p>!26</p>
    <p>LSTM</p>
    <p>ft  (0,1)W f xtaaaaaaaaaa</p>
    <p>it  (0,1)W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>Interpolation</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!27</p>
    <p>LSTM</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>ft  (0,1)W f xtaaaaaaaaaa</p>
    <p>it  (0,1)W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>Interpolation</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!28</p>
    <p>LSTM</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>ft  (0,1)W f xtaaaaaaaaaa</p>
    <p>it  (0,1)W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct = ft  ct1 + it  ct ht = ot  g(ct)</p>
    <p>Interpolation Addition</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!29</p>
    <p>LSTM</p>
    <p>ft  1W f xtaaaaaaaaaa</p>
    <p>it  1W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct  ct1 + ct ht = ot  g(ct)</p>
    <p>Interpolation Addition</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!30</p>
    <p>LSTM</p>
    <p>ft  1W f xtaaaaaaaaaa</p>
    <p>it  1W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  1a c b</p>
    <p>ct  ct1 + 1 ht = ot  g(ct)</p>
    <p>Interpolation Increase by 1</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!31</p>
    <p>LSTM</p>
    <p>ft  1W f xtaaaaaaaaaa</p>
    <p>it  1W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct   1a c b</p>
    <p>ct  ct1  1 ht = ot  g(ct)</p>
    <p>Interpolation Decrease by 1</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!32</p>
    <p>LSTM</p>
    <p>ft  1W f xtaaaaaaaaaa</p>
    <p>it  0W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct  ct1+ct ht = ot  g(ct)</p>
    <p>Interpolation Do Nothing</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!33</p>
    <p>LSTM</p>
    <p>ft  0W f xtaaaaaaaaaa</p>
    <p>it  0W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct  0ct1 + ct ht = ot  g(ct)</p>
    <p>Interpolation Reset</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>!34</p>
    <p>LSTM</p>
    <p>ft  0W f xtaaaaaaaaaa</p>
    <p>it  0W ixtaaaaaaaaaa</p>
    <p>ot  (0,1)W oxt(tanh)aaaa</p>
    <p>ct  (1,1)a c b</p>
    <p>ct  0ct1 + ct ht = ot  g(ct)</p>
    <p>Interpolation Reset</p>
    <p>ct = ft  ct1 + it  ct</p>
    <p>GRU</p>
    <p>zt  (0,1) rt  (0,1) ht  (1,1) ht = zt  ht1 + (1  z)  ht</p>
    <p>Popular Architectures</p>
    <p>Bounde d!</p>
    <p>Can Co unt!</p>
  </div>
  <div class="page">
    <p>Other Architectures</p>
    <p>!35</p>
    <p>SRNN IRNN</p>
    <p>ht = h(Whxt + Uhht1 + bh) ht = max(0,Whxt + Uhht1 + bh)</p>
  </div>
  <div class="page">
    <p>Other Architectures</p>
    <p>!36</p>
    <p>SRNN IRNN</p>
    <p>ht = h(Whxt + Uhht1 + bh)  (0,1) ht = max(0,Whxt + Uhht1 + bh)</p>
    <p>Bounde d!</p>
  </div>
  <div class="page">
    <p>Other Architectures</p>
    <p>!37</p>
    <p>SRNN IRNN</p>
    <p>ht = h(Whxt + Uhht1 + bh)  (0,1) ht = max(0,Whxt + Uhht1 + bh)</p>
    <p>Bounde d!</p>
    <p>keep/reset +0 / +1</p>
    <p>(subtraction in parallel, also increasing, counter)</p>
    <p>{</p>
  </div>
  <div class="page">
    <p>Other Architectures</p>
    <p>!38</p>
    <p>SRNN IRNN</p>
    <p>ht = h(Whxt + Uhht1 + bh)  (0,1) ht = max(0,Whxt + Uhht1 + bh)</p>
    <p>Bounde d!</p>
    <p>(subtraction in parallel, also increasing, counter)</p>
    <p>{</p>
    <p>Can Co unt!</p>
    <p>keep/reset +0 / +1</p>
  </div>
  <div class="page">
    <p>So:</p>
    <p>LSTM can count!</p>
    <p>GRU cannot</p>
    <p>Counting gives greater computational power</p>
    <p>!39</p>
  </div>
  <div class="page">
    <p>Empirically</p>
    <p>!40</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>LSTM GRU</p>
    <p>Trained , (on positive examples up to length 100)anbn</p>
    <p>Activations on :a1000b1000</p>
  </div>
  <div class="page">
    <p>Empirically</p>
    <p>!41</p>
    <p>GRU:  Took much longer to train  Did not generalise even within training domain</p>
    <p>begin failing at n=39 (vs 257 for LSTM)  Did not learn any discernible counting mechanism</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>LSTM GRU</p>
    <p>Trained , (on positive examples up to length 100)anbn</p>
    <p>Activations on :a1000b1000</p>
  </div>
  <div class="page">
    <p>Empirically</p>
    <p>!42</p>
    <p>GRU:  Took much longer to train  Did not generalise even within training domain</p>
    <p>begin failing at n=39 (vs 257 for LSTM)  Did not learn any discernible counting mechanism</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>LSTM GRU</p>
    <p>Trained , (on positive examples up to length 100)anbn</p>
    <p>Activations on :a1000b1000</p>
  </div>
  <div class="page">
    <p>Empirically</p>
    <p>!43</p>
    <p>GRU:  Took much longer to train  Did not generalise even within training domain</p>
    <p>begin failing at n=39 (vs 257 for LSTM)  Did not learn any discernible counting mechanism</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>LSTM GRU</p>
    <p>Trained , (on positive examples up to length 100)anbn</p>
    <p>Activations on :a1000b1000</p>
  </div>
  <div class="page">
    <p>Empirically</p>
    <p>!44</p>
    <p>Activations on :</p>
    <p>LSTM GRU</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>Trained , (on positive examples up to length 50)anbncn</p>
    <p>a100b100c100</p>
  </div>
  <div class="page">
    <p>Empirically</p>
    <p>!45</p>
    <p>Activations on :</p>
    <p>LSTM GRU</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>ACL 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>(a) anbn-LSTM on a1000b1000 (b) anbncn-LSTM on a100b100c100</p>
    <p>(c) anbn-GRU on a1000b1000 (d) anbncn-GRU on a100b100c100</p>
    <p>Figure 1: Activations for LSTM and GRU networks for anbn and anbncn. The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p>
    <p>We show that in the input-bound, finiteprecision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot. This makes the LSTM a variant of a k-counter machine (Fischer et al., 1968), while the GRU remains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p>
    <p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure. Figure 1 shows the activations of 10d LSTM and GRU trained to recognize the languages anbn and anbncn. It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.</p>
    <p>An RNN is a parameterized function R that takes as input an input vector xt and a state vector ht1</p>
    <p>and returns a state vector ht:</p>
    <p>ht = R(xt, ht1) (1)</p>
    <p>The RNN is applied to a sequence x1, ..., xn by starting with an initial vector h0 (often the 0 vector) and applying R repeatedly according to equation (1). Let  be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encoding, an embedding layer, or some other means). Let RNN(x1, ..., xn) denote the state vector h resulting from the application of R to the sequence E(x1), ..., E(xn). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L  if f(RNN(w)) returns 1 for all and only words w = x1, ..., xn 2 L.</p>
    <p>Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), the function R takes the form of an affine transform followed by a tanh nonlinearity:</p>
    <p>ht = tanh(Wxt + Uht1 + b) (2)</p>
    <p>Elman-RNNs are known to be at-least finitestate. Siegelmann (1996) proved that the tanh can</p>
    <p>GRU:  Took much longer to train  Did not generalise well</p>
    <p>begin failing at n=9 (vs 101 for LSTM)  Did not learn any discernible counting mechanism</p>
    <p>Trained , (on positive examples up to length 100)anbncn</p>
    <p>a100b100c100</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>!46</p>
    <p>GRUSRNN Trainability</p>
    <p>LSTMIRNN</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>!47</p>
    <p>GRUSRNN LSTMIRNN</p>
    <p>Practical Expressivity</p>
    <p>Trainability</p>
  </div>
  <div class="page">
    <p>Take Home Message</p>
    <p>!48</p>
    <p>Dont fall in the Turing Tarpit!</p>
    <p>Architectural Choices Matter! and result in actual differences in expressive power</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>!49</p>
    <p>GitHub repository: https://github.com/tech-srl/counting_dimensions</p>
    <p>Google Colab (link through GitHub as well):</p>
    <p>https://tinyurl.com/ybjkumrz</p>
  </div>
</Presentation>
