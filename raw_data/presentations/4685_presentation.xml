<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The TEXTURE Benchmark: Measuring Performance of Text Queries on a Relational DBMS</p>
    <p>Vuk Ercegovac</p>
    <p>David J. DeWitt</p>
    <p>Raghu Ramakrishnan</p>
  </div>
  <div class="page">
    <p>Applications Combining Text and Relational Data</p>
    <p>Query:</p>
    <p>How should such an application be expected to perform?</p>
    <p>Score P.id</p>
    <p>SELECT SCORE, P.id, FROM Products P WHERE P.type = PDA and CONTAINS(P.complaint, short battery life, SCORE) ORDER BY SCORE DESC</p>
    <p>ProductComplaints</p>
  </div>
  <div class="page">
    <p>Possibilities for Benchmarking</p>
    <p>Measure Workload</p>
    <p>Quality Response</p>
    <p>Time/ Throughput</p>
    <p>Relational N/A TPC[3], AS3AP[10], Set Query[8]</p>
    <p>Text TREC[2], VLC2[1] FTDR[4], VLC2[1]</p>
    <p>Relational + Text</p>
    <p>?? TEXTURE</p>
  </div>
  <div class="page">
    <p>Contributions of TEXTURE</p>
    <p>Design micro-benchmark to compare response time using a mixed relational + text query workload</p>
    <p>Develop TextGen to synthetically grow a text collection given a real text collection</p>
    <p>Evaluate TEXTURE on 3 commercial systems</p>
  </div>
  <div class="page">
    <p>Why a Micro-benchmark Design?</p>
    <p>A fine level of control for experiments is needed to differentiate effects due to:  How text data is stored  How documents are assigned a score  Optimizer decisions</p>
  </div>
  <div class="page">
    <p>Why use Synthetic Text?</p>
    <p>Allows for systematic scale-up  Users current data set may be too small</p>
    <p>Users may be more willing to share synthetic data</p>
    <p>Measurements on synthetic data shown empirically by us to be close to same measurements on real data</p>
  </div>
  <div class="page">
    <p>A Note on Quality</p>
    <p>Measuring quality is important!  Easy to quickly return poor results</p>
    <p>We assume that the three commercial systems strive for high quality results  Some participated at TREC  Large overlap between result sets</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>TEXTURE Components  Evaluation  Synthetic Text Generation</p>
  </div>
  <div class="page">
    <p>System A</p>
    <p>TEXTURE Components</p>
    <p>Relational Text Attributes</p>
    <p>DBGen TextGen</p>
    <p>System B Response Time A Response Time B</p>
    <p>num_id num_u num_05 num_5 num_50 txt_short txt_long</p>
    <p>pkey un-clustered indexes display body</p>
    <p>QueryGen</p>
    <p>Query 1 Query 2  Query n</p>
    <p>Query Templates</p>
  </div>
  <div class="page">
    <p>Overview of Data</p>
    <p>Schema based on Wisconsin Benchmark [5]  Used to control relational predicate selectivity</p>
    <p>Relational attributes populated by DBGen [6]  Text attributes populated by TextGen (new)</p>
    <p>Input:  D: document collection, m: scale-up factor</p>
    <p>Output:  D: document collection with |D| x m documents  Goal: Same response times for workloads on D and</p>
    <p>corresponding real collection</p>
  </div>
  <div class="page">
    <p>Overview of Queries</p>
    <p>Query workloads derived from query templates with following parameters</p>
    <p>Text expressions:  Vary number of keywords, keyword selectivity, and</p>
    <p>type of expression (i.e., phrase, Boolean, etc.)  Keywords chosen from text collection</p>
    <p>Relational expression:  Vary predicate selectivity, join condition selectivity</p>
    <p>Sort order:  Choose between relational attribute or score</p>
    <p>Retrieve ALL or TOP-K results</p>
  </div>
  <div class="page">
    <p>Example Queries</p>
    <p>SELECT SCORE, num_id, txt_short FROM R WHERE NUM_5 = 3 and CONTAINS(R.txt_long, foo bar, SCORE) ORDER BY SCORE DESC</p>
    <p>SELECT S.SCORE, S.num_id, S.txt_short FROM R, S WHERE R.num_id = S.num_id and S.NUM_05 = 2 and CONTAINS(S.txt_long, foo bar, S.SCORE) ORDER BY S.SCORE DESC</p>
    <p>Example of a single relation, mixed relational and text query that sorts according to a relevance score.</p>
    <p>Example of a join query, sorting according to a relevance score on S.txt_long.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>TEXTURE Components  Evaluation  Synthetic Text Generation</p>
  </div>
  <div class="page">
    <p>Overview of Experiments</p>
    <p>How is response time affected as the database grows in size?</p>
    <p>How is response time affected by sort order and top-k optimizations?</p>
    <p>How do the results change when input collection to TextGen differs?</p>
  </div>
  <div class="page">
    <p>Data and Query Workloads</p>
    <p>TextGen input is TREC AP Vol.1[1] and VLC2 [2]  Output: relations w/ {1, 2.5, 5, 7.5, 10} x 84,678 tuples  Corresponds to ~250 MB to 2.5 GB of text data</p>
    <p>Text-only queries:  Low (&lt; 0.03%) vs. high selectivity (&lt; 3%)  Phrases, OR, AND</p>
    <p>Mixed, single relation queries:  Low (&lt;0.01%) vs. high selectivity (5%)  Pair with all text-only queries</p>
    <p>Mixed, multi relation queries:  2, 3 relations, vary text attribute used, vary selectivity</p>
    <p>Each query workload consists of 100 queries</p>
  </div>
  <div class="page">
    <p>Methodology for Evaluation</p>
    <p>Setup database and query workloads  Run workload per system multiple times</p>
    <p>to obtain warm numbers  Discard first run, report average of remaining</p>
    <p>Repeat for all systems (A, B, C)  Platform: Microsoft Windows 2003</p>
    <p>Server, dual processor 1.8 GHz AMD, 2 GB of memory, 8 120 GB IDE drives</p>
  </div>
  <div class="page">
    <p>Scaling: Text-Only Workloads</p>
    <p>How does response time vary per system as the data set scales up?  Query workload: low text selectivity (0.03%)  Text data: synthetic based on TREC AP Vol. 1</p>
  </div>
  <div class="page">
    <p>Mixed Text/Relational Workloads</p>
    <p>Workload System</p>
    <p>Low</p>
    <p>A 2.8</p>
    <p>B 30</p>
    <p>C 2.6</p>
    <p>High</p>
    <p>Drill down on scale factor 5 (~450K tuples)  Query workload Low: text selectivity (0.03%)  Query workload High: text selectivity (3%)</p>
    <p>Do the systems take advantage of relational predicate for mixed workload queries?  Query workload Mix: High text, low relational selectivity (0.01%)</p>
    <p>Seconds per system and workload (synthetic TREC)</p>
    <p>Mix</p>
  </div>
  <div class="page">
    <p>Top-k vs. All Results</p>
    <p>Compare retrieving all vs. top-k results  Query workload is Mix from before</p>
    <p>High selectivity text expression (3%)  Low selectivity relational predicate (0.01%)</p>
    <p>Workload System</p>
    <p>All Top-k</p>
    <p>A 69 2.6</p>
    <p>B 97 96</p>
    <p>C 28 2.2</p>
    <p>Seconds per system and workload (450K tuples, synthetic TREC)</p>
  </div>
  <div class="page">
    <p>Varying Sort Order</p>
    <p>Compare sorting by score vs. sorting by relational attribute  When retrieving all, results similar to previous  Results for retrieving top-k shown below</p>
    <p>Workload System</p>
    <p>Score Relational</p>
    <p>A 2.6 2.7</p>
    <p>B 96 715</p>
    <p>C 2.2 2.2</p>
    <p>Seconds per system and workload (450K tuples, synthetic TREC)</p>
  </div>
  <div class="page">
    <p>Varying the Input Collection</p>
    <p>What is the effect of different input text collections on response time?  Query workload: low text selectivity (0.03%)</p>
    <p>All results retrieved</p>
    <p>Text Data: synthetic TREC and VLC2</p>
    <p>Collection System</p>
    <p>Synthetic TREC</p>
    <p>Synthetic VLC2</p>
    <p>A 2.9 1.2</p>
    <p>B 30 3.6</p>
    <p>C 2.5 1.6 Seconds per system and collection (450K tuples)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Benchmark Components  Evaluation  Synthetic Text Generation</p>
  </div>
  <div class="page">
    <p>Synthetic Text Generation</p>
    <p>TextGen:  Input: document collection D, scale-up factor m  Output: document collection D with |D| x m</p>
    <p>documents  Problem: Given documents D, how do we</p>
    <p>add documents to obtain D ?  Goal: Same response times for workloads on D</p>
    <p>and corresponding real collection C, |C|=|D|  Approach: Extract features from D and draw</p>
    <p>|D| samples according to features</p>
  </div>
  <div class="page">
    <p>Document Collection Features</p>
    <p>Features considered  W(w,c) : word distribution  G(n, v) : vocabulary growth  U,L : number of unique, total words per</p>
    <p>document  C(w1, w2, , wn, c) : co-occurrence of word</p>
    <p>groups  Each feature is estimated by a model</p>
    <p>Ex. Zipf[11] or empirical distribution for W  Ex. Heaps Law for G[7]</p>
  </div>
  <div class="page">
    <p>Process to Generate D</p>
    <p>Pre-process: estimate features  Depends on model used for feature</p>
    <p>Generate |D| documents  Generate each document by sampling W</p>
    <p>according to U and L  Grow vocabulary according to G</p>
    <p>Post-process: Swap words between documents in order to satisfy co-occurrence of word groups C</p>
  </div>
  <div class="page">
    <p>Feature-Model Combinations</p>
    <p>Considered 3 instances of TextGen, each a combination of features/models</p>
    <p>Feature TextGen</p>
    <p>W (Word distr.)</p>
    <p>G (Vocab)</p>
    <p>L (Length)</p>
    <p>U (Unique)</p>
    <p>C (co-occur.)</p>
    <p>Synthetic1 Zipf</p>
    <p>Heaps Average</p>
    <p>N/A</p>
    <p>Synthetic2 Empirical Average</p>
    <p>N/A</p>
    <p>Synthetic3 Empirical</p>
  </div>
  <div class="page">
    <p>Which TextGen is a Good Generator?</p>
    <p>Goal: response time measured on synthetic (S) and real (D) should be similar across systems</p>
    <p>Does the use of randomized words in D affect response time accuracy?</p>
    <p>How does the choice of features and models effect response time accuracy as the data set scales?</p>
  </div>
  <div class="page">
    <p>Use of Random Words</p>
    <p>Words are strings composed of a random permutation of letters</p>
    <p>Random words are useful for:  Vocabulary growth  Sharing text collections</p>
    <p>Do randomized words affect measured response times?  What is the affect on stemming, compression, and</p>
    <p>other text processing components?</p>
  </div>
  <div class="page">
    <p>Effect of Randomized Words</p>
    <p>Experiment: create two TEXTURE databases and compare across systems  Database AP based on TREC AP Vol. 1  Database R-AP: randomize each word in AP  Query workload: low &amp; high selectivity keywords</p>
    <p>Result: response times differ on average by &lt; 1%, not exceeding 4.4%</p>
    <p>Conclusion: using random words is reasonable for measuring response time</p>
  </div>
  <div class="page">
    <p>Effect of Features and Models</p>
    <p>Experiment: compare response times over same sized synthetic (S) and real (D) collections  Sample s documents of D  Use TextGen to produce S at several scale factors</p>
    <p>|S| = 10, 25, 50, 75, and 100% of |D|</p>
    <p>Compare response time across systems  Must repeat for each type of text-only query</p>
    <p>workload</p>
    <p>Used as framework for picking features/models</p>
  </div>
  <div class="page">
    <p>TextGen Evaluation Results</p>
    <p>How does response time measured on real data compare to the synthetic TextGen collections?</p>
    <p>Query workload: low selectivity text only query (0.03%)  Graph is for System A</p>
    <p>Similar results obtained for other systems</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>How should quality measurements be incorporated?</p>
    <p>Extend the workload to include updates</p>
    <p>Allow correlations between attributes when generating database</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We propose TEXTURE to fill the gap seen by applications that use mixed relational and text queries</p>
    <p>We can scale-up a text collection through synthetic text generation in such a way that response time is accurately reflected</p>
    <p>Results of evaluation illustrate significant differences between current commercial relational systems</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Kaufman, 2 edition, 1993 5. D. DeWitt. The Wisconsin Benchmark: Past, Present, and Future. The</p>
    <p>Benchmark Handbook, 1991. 6. J. Gray, P. Sundaresan, S. Englert, K. Baclawski, and P. J. Weinberger. Quickly</p>
    <p>Generating Billion-record Synthetic Databases. ACM SIGMOD, 1994 7. H. S. Heaps, Information Retrieval, Computational and Theoretical Aspects.</p>
    <p>Academic Press, 1978. 8. P. ONeil. The Set Query Benchmark. The Benchmark Handbook, 1991 9. K. A. Shoens, A. Tomasic, H. Garcia-Molina. Synthetic Workload Performance</p>
    <p>Analysis of Incremental Updates. In Research and Development in Information Retrieval, 1994.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
