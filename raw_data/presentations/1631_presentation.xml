<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Cesar A. Stuardo, Tanakorn Leesatapornwongsa , Riza O. Suminto, Huan Ke, Jeffrey F. Lukman, Wei-Chiu Chuang , Shan Lu and Haryadi S. Gunawi</p>
    <p>*</p>
    <p>*</p>
    <p>+</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Classic critical bugs  Concurrency  Configuration  Performance  Security</p>
    <p>Scalability bugs?</p>
  </div>
  <div class="page">
    <p>Cassandra  Bug #3831: &quot;start exploding when start reading 100-300 nodes&quot;  Bug #6127: &quot;obvious symptom is with 1000 nodes  Bug #6409: &quot;With &gt;500 nodes,  have trouble&quot;</p>
    <p>Hadoop/HDFS  Bug #1073: &quot;1600 data nodes. randomWriter fails to run&quot;  Bug #3990: &quot;multi-thousand node cluster often results in</p>
    <p>[problem]</p>
    <p>HBase:  Bug #10209: &quot;with 200+ nodes, [cluster] takes 50mins to start&quot;  Bug #12139: &quot;with &gt;200 nodes, StochasticLoadBalancer doesn't</p>
    <p>work&quot;</p>
    <p>Riak:  Bug #926: &quot;1000 node cluster is a larger question</p>
  </div>
  <div class="page">
    <p>Latent bugs whose symptoms surface in large-scale deployments, but not necessarily in small/medium-scale deployments</p>
    <p>ScaleCheck @ FAST 19</p>
    <p>#Nodes</p>
    <p>Sy m</p>
    <p>pt om</p>
  </div>
  <div class="page">
    <p>N=6</p>
    <p>Decommissioning</p>
    <p>ScaleCheck @ FAST 19</p>
    <p>Update Ring</p>
  </div>
  <div class="page">
    <p>N &gt; 200</p>
    <p>A:  B is dead A:  B is alive</p>
    <p>B</p>
    <p>Late Gossip</p>
    <p>CPU intensive 0.1  4 seconds</p>
    <p>Flapping</p>
    <p>In all nodes! (unstable cluster)</p>
    <p>ScaleCheck @ FAST 19</p>
    <p>In every node for(i=0 to N)</p>
    <p>for(j=0 to N) for(k=0 to N)</p>
    <p>Update Ring</p>
  </div>
  <div class="page">
    <p>The bug symptom</p>
  </div>
  <div class="page">
    <p>For Apache Hadoop, testing at thousand-node scale has been one of the most effective ways of finding bugs,</p>
    <p>but its both difficult and expensive. It takes considerable expertise to deploy and operate a large-scale cluster, much less debug the issues.</p>
    <p>Running such a cluster also costs thousands of dollars an hour, making scale testing impossible for the solo contributor.</p>
    <p>As it stands, we are heavily reliant on test clusters operated by large companies to do scale testing.</p>
    <p>A way of finding scalability bugs without requiring running a large-scale cluster would be extremely useful</p>
    <p>Andrew Wang (Cloudera and Apache Hadoop PMC Member and Committer)</p>
    <p># M a c h in e s</p>
    <p>t</p>
    <p>Real-scale Testing</p>
    <p>Time</p>
    <p>...</p>
    <p>N</p>
    <p>ScaleCheck @ FAST 19</p>
    <p>What are the root causes of scalability bugs?</p>
    <p>Q1</p>
    <p>Can we do large-scale testing easily, e.g. in one machine?</p>
    <p>Q2</p>
  </div>
  <div class="page">
    <p>Discovering scalability bugs Democratizing large scale testing</p>
    <p>Real-scale testing</p>
  </div>
  <div class="page">
    <p>SFind</p>
    <p>Loops iterating scale dependent data structures</p>
    <p>Scale Dependent Loops</p>
    <p>What are the root causes of scalability bugs?</p>
    <p>Q1</p>
    <p>Automatically find scale dependent data structures and loops</p>
    <p>by checking their in-memory growth tendency over small scale experiments</p>
  </div>
  <div class="page">
    <p>Can we do large-scale testing easily, e.g. in one machine?</p>
    <p>Q2</p>
    <p>STest</p>
    <p>Many Challenges!!!</p>
    <p>Context switching CPU contention</p>
    <p>Network Overhead</p>
    <p>Memory Overhead</p>
    <p>Bad modularity</p>
    <p>SPC GEDA PIL</p>
    <p>Global Event-Driven Architecture</p>
    <p>Processing Illusion</p>
    <p>Single Process Cluster</p>
    <p>Large scale testing in a single machine with little source code modification</p>
  </div>
  <div class="page">
    <p>Integrated to 4 systems</p>
    <p>Scale tested 18 protocols</p>
    <p>Accurately reproduced 10 known scalability bugs</p>
    <p>Found 4 new bugs</p>
    <p>..in just a single machine</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>SFind</p>
    <p>STest</p>
    <p>Evaluations and Conclusions</p>
  </div>
  <div class="page">
    <p>Scale Dependent Loops iterating scale dependent data structures</p>
    <p>Any distributed system these days has  &gt; 100K LOC  &gt; 100x source files  &gt; 1000x maps, lists, queues, etc</p>
    <p>We can find them manually  /*  holds one entry per peer  */  /*  maps partition to node */  /*  maps blocks to datanodes */</p>
    <p>How can we find them automatically?</p>
  </div>
  <div class="page">
    <p>Size</p>
    <p>Step</p>
    <p>List peers growth tendency</p>
    <p>Step 2 Step N</p>
    <p>N</p>
    <p>N</p>
    <p>List peers = {A} A A peers = {A, B} A peers = {}</p>
    <p>Step 1</p>
    <p>B</p>
    <p>JVMTI and Reflection support</p>
  </div>
  <div class="page">
    <p>As on heap data structures growth tendency after N steps</p>
    <p>Data structures that tend to grow when one or more dimensions grow are scale dependent</p>
    <p>Size</p>
    <p>Step</p>
    <p>List X</p>
    <p>Size</p>
    <p>Step</p>
    <p>Set Y</p>
    <p>Size</p>
    <p>Step</p>
    <p>Map Z</p>
    <p>Size</p>
    <p>Step</p>
    <p>Queue Q</p>
    <p>Size</p>
    <p>Step</p>
    <p>Map R</p>
    <p>Size</p>
    <p>Step</p>
    <p>Set T</p>
    <p>Size</p>
    <p>Step</p>
    <p>Map nodes</p>
    <p>Size</p>
    <p>Step</p>
    <p>List peers</p>
  </div>
  <div class="page">
    <p>Try out different axes of scale  Adding files  Adding partitions  tables, columns, files, directories</p>
    <p>Try out different workloads  Snapshot directories  Removing nodes  migration, rebalance</p>
    <p>inodes tokens</p>
    <p>snapshots deadNodes</p>
  </div>
  <div class="page">
    <p>applyStateLocally</p>
    <p>Priority Method</p>
    <p>? sendToEndpoints</p>
    <p>? applyStateLocally</p>
    <p>#IO</p>
    <p>#Lock</p>
    <p>Complexity</p>
    <p>sendToEndpoints</p>
    <p>Priority Method</p>
    <p>Prioritize harmful scalability bottlenecks</p>
    <p>Map our data structures to loops using data-flow</p>
    <p>Create inter-procedural call graphs for all methods</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>SFind</p>
    <p>STest</p>
    <p>Evaluations and Conclusions</p>
  </div>
  <div class="page">
    <p>Priority Method</p>
    <p>public void testApplyStateLocally(){ //</p>
    <p>}</p>
    <p>TestApplyStateLocally.java</p>
    <p>Many Challenges!!!</p>
    <p>Context switching</p>
    <p>CPU intensity</p>
    <p>Network Overhead</p>
    <p>Memory Overhead</p>
    <p>Bad modularity</p>
  </div>
  <div class="page">
    <p>Deploy nodes as processes in a single machine  One process = one node = one</p>
    <p>systems instance</p>
    <p>Main challenges  Per node runtime overhead  Process context switching</p>
    <p>- Event Lateness (e.g send message)</p>
    <p>Max Colocation factor: 50 nodes</p>
    <p>Expected: Msg every second</p>
    <p>Observed: Msg every 2 seconds Node 1 Node 2</p>
    <p>Node</p>
    <p>Prohibits large scale-testing!</p>
  </div>
  <div class="page">
    <p>Deploy nodes as processes threads in a single process</p>
    <p>Challenges  Lack of modularity (e.g. bad design</p>
    <p>patterns) - Automatic per-node isolation (e.g.</p>
    <p>ClassLoader isolation)</p>
    <p>Large amount of threads - Simplify inter-node communication</p>
    <p>Max Colocation factor: 120 nodes</p>
    <p>Node 1</p>
    <p>Node 2</p>
    <p>Node 2s msg queue</p>
    <p>Node</p>
  </div>
  <div class="page">
    <p>Frequent Design pattern  Service = worker pool + event</p>
    <p>queue</p>
    <p>Node</p>
    <p>service</p>
    <p>But N nodes mean   threads!!!</p>
  </div>
  <div class="page">
    <p>One global event handler per service  total # of threads = # of cores</p>
    <p>Identify event constrains  Multi-threaded versus single</p>
    <p>threaded</p>
    <p>Max Colocation factor: 512 nodes</p>
    <p>Node 1</p>
    <p>Node 2</p>
    <p>Global event handler</p>
    <p>enforce serial processing</p>
  </div>
  <div class="page">
    <p>Namenodes RPC Queue IBR</p>
    <p>RPC Queue full = no other request can be served</p>
    <p>Max size = 1000 FULL!</p>
    <p>processed in serial</p>
    <p>N = 256</p>
    <p>Datanodes</p>
    <p>N = 32</p>
    <p>N = 64</p>
    <p>N = 128</p>
    <p>Accurate!</p>
  </div>
  <div class="page">
    <p>Inaccurate!</p>
  </div>
  <div class="page">
    <p>Replace CPU intensive computation with sleep</p>
    <p>The key to computation is the execution time and eventual output</p>
    <p>What code blocks can be safely replaced by sleep?</p>
    <p>In every node</p>
    <p>In every node</p>
  </div>
  <div class="page">
    <p>function intense() for(){</p>
    <p>for(){ for(){</p>
    <p>write(hello!); }</p>
    <p>} }</p>
    <p>function intense() if(!ScaleCheck){</p>
    <p>for(){ for(){</p>
    <p>for(){ write(hello!);</p>
    <p>} }</p>
    <p>} } else{</p>
    <p>t = getTime() sleep(t)</p>
    <p>}</p>
    <p>Non pertinent operations that do not affect processing semantics</p>
    <p>Offline profiling</p>
  </div>
  <div class="page">
    <p>function relevantAndRelevant(input) for(){</p>
    <p>for(){ for(){</p>
    <p>modifyClusterState() }</p>
    <p>} }</p>
    <p>Pertinent operations that affect processing semantics</p>
    <p>function relevantAndRelevant(input) if(!ScaleCheck){</p>
    <p>for(){ for(){</p>
    <p>for(){ modifyClusterState()</p>
    <p>} }</p>
    <p>} } else{</p>
    <p>t, output = getNextState(input) sleep(t) clusterState = output</p>
    <p>}</p>
    <p>Offline/Online memoization</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Introduction</p>
    <p>SFind</p>
    <p>STest</p>
    <p>Evaluations and Conclusions</p>
  </div>
  <div class="page">
    <p>[see paper for details]</p>
  </div>
  <div class="page">
    <p>applyStateLocally</p>
    <p>Meanwhile in latest Cassandra</p>
    <p>LockIO</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Cassandra HDFS Voldemort</p>
    <p>Naive +SPC +NetStub</p>
    <p>+GEDA</p>
    <p>+PIL</p>
    <p>How many nodes can we colocate in a single machine maintaining accuracy?</p>
    <p>decommission rebalanceIBR</p>
    <p>Need all techniques for a high colocation</p>
    <p>factor</p>
  </div>
  <div class="page">
    <p>Focus on scale dependent CPU/Processing time  E.g. cluster size, number of files, etc  Scale of load, Scale of failure</p>
    <p>Colocation factor limited by a single machine  Akin to unit testing  Multiple machines</p>
    <p>PIL might require real scale memoization  Offline memoization takes too long (days)  Explore other CPU contention reduction techniques</p>
    <p>New code = New maintenance costs  Low cost (about 1% of code base)  Going for zero-effort emulation</p>
  </div>
  <div class="page">
    <p>Discovering scalability bugs Democratizing large scale testing</p>
    <p>Real-scale testing</p>
  </div>
  <div class="page">
    <p>http://ucare.cs.uchicago.edu https://ceres.uchicago.eduhttp://cs.uchicago.edu</p>
  </div>
</Presentation>
