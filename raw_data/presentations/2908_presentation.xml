<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>OpML 20</p>
    <p>Lisa Veiber, Kevin Allix, Yusuf Arslan, Tegawend F. Bissyande, Jacques Klein</p>
    <p>SnT  University of Luxembourg</p>
    <p>Challenges for Explainable Machine Learning in Production</p>
    <p>July 2020</p>
  </div>
  <div class="page">
    <p>Machine learning is increasingly used in practice</p>
    <p>Automation of administrative tasks  Credit scoring  Recommender systems  Automated vehicles  Chatbot</p>
    <p>INTRODUCTION</p>
  </div>
  <div class="page">
    <p>Nevertheless, those algorithms are black box  The inner workings of how the model made a decision given the inputs is</p>
    <p>not readily interpretable  Trade-off between accuracy and interpretability</p>
    <p>BLACK BOXES</p>
    <p>(DARPA, 2017)</p>
  </div>
  <div class="page">
    <p>Legal</p>
    <p>MOTIVATIONS</p>
    <p>EthicalPractical Prudential</p>
  </div>
  <div class="page">
    <p>[GDPR] will also effectively create a right to explanation, whereby a user can ask for an explanation of an algorithmic decision that was made about them. Indeed, articles 13 and 14 state that, when profiling takes place, a data subject has the right to meaningful information about the logic involved.</p>
    <p>- Goodman and Flexman (2016)</p>
    <p>MOTIVATIONS</p>
  </div>
  <div class="page">
    <p>MOTIVATIONS</p>
    <p>In complex tasks, industrials will favour transparent and interpretable models even if they have lower accuracy in performance. However, interpretability frameworks can solve this issue.</p>
    <p>- Ross et al. (2017)</p>
  </div>
  <div class="page">
    <p>INDUSTRIAL PARTNER</p>
    <p>The context of our industrial partner:</p>
    <p>FinTech sector interested in explanations of automated decision-making systems</p>
    <p>Operating in a highly regulated environment, governed mostly by GDPR in Europe</p>
    <p>Different audiences to address (clients, financial analysts, regulators, managers)</p>
    <p>Explainability to be added to pre-existing models (Random Forest [RF])  RF is based on decision trees, but it increases exponentially and become non-interpretable</p>
    <p>High need for security and privacy to be assured</p>
  </div>
  <div class="page">
    <p>EXPLAINABILITY</p>
  </div>
  <div class="page">
    <p>There exists several, sometimes conflicting, definitions of explanability.  Here we refer to explainability as:</p>
    <p>Explainability is used interchangeably with interpretability. It aims to respond to the opacity of the inner workings ofthe model while maintaining the learning performance. It gives machine learning models the ability to explain or to present their behaviours in understandable terms to humans.</p>
    <p>EXPLAINABILITY - DEFINITION</p>
  </div>
  <div class="page">
    <p>TYPES OF EXPLANATION</p>
    <p>There are two main parameters which define the different types of explainability:</p>
    <p>The difference between global and local will define the granularity of the explanations  Inherent model incorporate interpretability in the structure of the initial model  Post-hoc requires another framework to generate explanations</p>
  </div>
  <div class="page">
    <p>TYPES OF EXPLANATION</p>
    <p>Those different types come with an accuracy trade-off</p>
    <p>Inherently explainable models offers accurate explanations but lower performance  Post-hoc explainability is limited in their explanation but keep the initial performance intact  Global explanations increase the model transparency: increase trust in the model  Uncover the mapping for a specific prediction: increase trust in a prediction</p>
  </div>
  <div class="page">
    <p>TYPES OF EXPLANATION</p>
    <p>Those different types come with an accuracy trade-off</p>
    <p>Inherently explainable models offers accurate explanations but lower performance  Post-hoc explainability is limited in their explanation but keep the initial performance intact  Global explanations increase the model transparency. Increase trust in the model  Uncover the mapping for a specific prediction. Increase trust in a prediction</p>
    <p>Here is what to consider in industrial deployment:</p>
    <p>Post-hoc frameworks will need to adapt to the initial ML model  Some post-hoc frameworks generate both local and global explanations</p>
  </div>
  <div class="page">
    <p>FORMATS OF EXPLANATION</p>
    <p>Explanations are usually generated through:</p>
    <p>Visualization-based framework (more frequent)  Producing graphical representations of the predictions</p>
    <p>Experiment using the RRR framework from Ross et al. (2017)</p>
  </div>
  <div class="page">
    <p>FORMATS OF EXPLANATION</p>
    <p>Explanations are usually generated through:</p>
    <p>Visualization-based framework (more frequent)  Producing graphical representations of the predictions</p>
    <p>Text-based framework  Textual explanations of a decision</p>
    <p>Explanation from Park et al. (2018)</p>
  </div>
  <div class="page">
    <p>FORMATS OF EXPLANATION</p>
    <p>Explanations are usually generated through:</p>
    <p>Visualization-based framework (more frequent)</p>
    <p>Producing graphical representations of the predictions</p>
    <p>Text-based framework</p>
    <p>Textual explanations of a decision</p>
    <p>However, visualization-based framework are rarely validated through user study. Applied to other</p>
    <p>tasks than used in the design process, the visualizations can end to be as non-interpretable as the</p>
    <p>initial model.</p>
  </div>
  <div class="page">
    <p>CHALLENGES AND</p>
    <p>RECOMMENDATION</p>
  </div>
  <div class="page">
    <p>Existing frameworks comes with challenges in industrial application, as seen with our industrial partner:</p>
    <p>Data Quality  Task and Model Dependency  Security</p>
    <p>CHALLENGES FOR INDUSTRIAL APPLICATION</p>
  </div>
  <div class="page">
    <p>Challenge:  Explainability framework focus on Computer Vision and NLP tasks  Not addressing tabular data quality (missing values, no clear-cut clusters)  Thus, framework explainability is reduced  If based on tabular data, then rely on optimal data for visualization</p>
    <p>DATA QUALITY</p>
  </div>
  <div class="page">
    <p>Challenge:  Explainability framework focus on Computer Vision and NLP tasks</p>
    <p>Not addressing tabular data quality (missing values, no clear-cut clusters)  Thus, framework explainability is reduced</p>
    <p>If based on tabular data, then rely on optimal data for visualization</p>
    <p>Recommendation:  More consideration to industrial need during framework design  Systematic user validation of interpretability framework on different data formats</p>
    <p>DATA QUALITY</p>
  </div>
  <div class="page">
    <p>Challenge:  Model Dependency: some frameworks are designed for a specific model type</p>
    <p>Resolved by model-agnostic approach (e.g. LIME)  Resolved by surrogate models but they dont provide explanations when model and surrogate differ</p>
    <p>Task Dependency:  Need of different types of explanations for different audiences  Insufficient consideration of different tasks in the different designed framework</p>
    <p>Recommendation:  Clearly define needs and explanation needed before undertaking the tasks  Systematic review of relevant interpretability frameworks for comparison</p>
    <p>MODEL AND TASK DEPENDENCY</p>
  </div>
  <div class="page">
    <p>Challenge:  Robustness:</p>
    <p>If access to model reasoning, can implement adversarial behaviour to alter the model  New research:</p>
    <p>If provided explanations, part of the initial dataset can be recovered</p>
    <p>Recommendation:  Simulate adversarial behaviour and observe alterations in model behaviour  Give on to the exercise of recovering part of the dataset given gradient explanations</p>
    <p>SECURITY</p>
  </div>
  <div class="page">
    <p>CONCLUSION</p>
    <p>Data are crucial for operational decision-making</p>
    <p>Trade-off between explainability and accuracy</p>
    <p>Explainable ML make the model interpretable to users</p>
    <p>Challenges towards explainable ML implementation (data quality, task and model dependency, security)</p>
    <p>Overcoming those challenges can be complex. Still, systematic user review of new interpretable framework and of operational use case can ease those challenges</p>
  </div>
  <div class="page">
    <p>Thank for listening and to everyone who contributed to the paper !</p>
    <p>The authors:  Lisa VEIBER - lisa.veiber@uni.lu  Kevin ALLIX  Yusuf ARSLAN  Tegawend F. BISSYANDE  Jacques KLEIN</p>
  </div>
</Presentation>
