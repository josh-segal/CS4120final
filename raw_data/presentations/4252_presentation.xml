<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An analysis of the user occupational class through Twitter content</p>
    <p>Daniel Preotiuc-Pietro1 Vasileios Lampos2 Nikolaos Aletras2</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>User attribute prediction from text is successful:</p>
    <p>I Age (Rao et al. 2010 ACL) I Gender (Burger et al. 2011 EMNLP) I Location (Eisenstein et al. 2011 EMNLP) I Personality (Schwartz et al. 2013 PLoS One) I Impact (Lampos et al. 2014 EACL) I Political orientation (Volkova et al. 2014 ACL) I Mental illness (Coppersmith et al. 2014 ACL)</p>
    <p>Downstream applications are benefiting from this:</p>
    <p>I Sentiment analysis (Volkova et al. 2013 EMNLP) I Text classification (Hovy 2015 ACL)</p>
  </div>
  <div class="page">
    <p>However...</p>
    <p>Socio-economic factors (occupation, social class, education, income) play a vital role in language use</p>
    <p>(Bernstein 1960, Labov 1972/2006)</p>
    <p>No large scale user level dataset to date</p>
    <p>Applications:</p>
    <p>I sociological analysis of language use I embedding to downstream tasks (e.g. controlling for</p>
    <p>socio-economic status)</p>
  </div>
  <div class="page">
    <p>At a Glance</p>
    <p>Our contributions:</p>
    <p>I Predicting new user attribute: occupation I New dataset: user  occupation I Gaussian Process classification for NLP tasks I Feature ranking and analysis using non-linear methods</p>
  </div>
  <div class="page">
    <p>Standard Occupational Classification</p>
    <p>Standardised job classification taxonomy</p>
    <p>Developed and used by the UK Office for National Statistics (ONS)</p>
    <p>Hierarchical:</p>
    <p>I 1-digit (major) groups: 9 I 2-digit (sub-major) groups: 25 I 3-digit (minor) groups: 90 I 4-digit (unit) groups: 369</p>
    <p>Jobs grouped by skill requirements</p>
  </div>
  <div class="page">
    <p>Standard Occupational Classification</p>
    <p>C1 Managers, Directors and Senior Officials</p>
    <p>I 11 Corporate Managers and Directors I 111 Chief Executives and Senior Officials</p>
    <p>I 1115 Chief Executives and Senior Officials Job: chief executive, bank manager</p>
    <p>I 1116 Elected Officers and Representatives I 112 Production Managers and Directors I 113 Functional Managers and Directors I 115 Financial Institution Managers and Directors I 116 Managers and Directors in Transport and Logistics I 117 Senior Officers in Protective Services I 118 Health and Social Services Managers and Directors I 119 Managers and Directors in Retail and Wholesale</p>
    <p>I 12 Other Managers and Proprietors</p>
  </div>
  <div class="page">
    <p>Standard Occupational Classification</p>
    <p>C2 Professional Occupations Job: mechanical engineer, pediatrist, postdoctoral researcher</p>
    <p>C3 Associate Professional and Technical Occupations Job: system administrator, dispensing optician</p>
    <p>C4 Administrative and Secretarial Occupations Job: legal clerk, company secretary</p>
    <p>C5 Skilled Trades Occupations Job: electrical fitter, tailor</p>
    <p>C6 Caring, Leisure, Other Service Occupations Job: school assistant, hairdresser</p>
    <p>C7 Sales and Customer Service Occupations Job: sales assistant, telephonist</p>
    <p>C8 Process, Plant and Machine Operatives Job: factory worker, van driver</p>
    <p>C9 Elementary Occupations Job: shelf stacker, bartender</p>
  </div>
  <div class="page">
    <p>Data</p>
    <p>Users collected by self-disclosure of job title in profile</p>
    <p>Manually filtered by the authors</p>
  </div>
  <div class="page">
    <p>Data</p>
    <p>Here we classify only at the 1-digit top level group (9 classes)</p>
    <p>Feature representation and labels available online</p>
    <p>Raw data available for research purposes on request (per Twitter TOS)</p>
  </div>
  <div class="page">
    <p>Features</p>
    <p>User Level features (18), such as:</p>
    <p>I number of: I followers I friends I listings I tweets</p>
    <p>I proportion of: I retweets I hashtags I @-replies I links</p>
    <p>I average: I tweets/day I retweets/tweet</p>
  </div>
  <div class="page">
    <p>Features</p>
    <p>Focus on interpretable features for analysis</p>
    <p>Compute over reference corpus of 400M tweets:</p>
    <p>I SVD embeddings and clusters I Word2Vec (W2V) embeddings and clusters</p>
  </div>
  <div class="page">
    <p>SVD Features</p>
    <p>Compute word  word similarity matrix</p>
    <p>Similarity metric is Normalized PMI (Bouma 2009) using the entire tweet as context</p>
    <p>SVD with different number of dimensions (30, 50, 100, 200)</p>
    <p>User is represented by summing its word representations</p>
    <p>The low-dimensional features offer no interpretability</p>
  </div>
  <div class="page">
    <p>SVD Features</p>
    <p>Spectral clustering to get hard clusters of words (30, 50, 100, 200 clusters)</p>
    <p>Each cluster consists of distributionally similar words  topic</p>
    <p>User is represented by the number of times he uses a word from each cluster.</p>
  </div>
  <div class="page">
    <p>Word2Vec Features</p>
    <p>Trained Word2Vec (layer size 50) on our Twitter reference corpus</p>
    <p>Spectral clustering on the word  word similiarity matrix (30, 50, 100, 200 clusters)</p>
    <p>Similarity is cosine similarity of words in the embedding space</p>
  </div>
  <div class="page">
    <p>Gaussian Processes</p>
    <p>Brings together several key ideas in one framework:</p>
    <p>I Bayesian I kernelised I non-parametric I non-linear I modelling uncertainty</p>
    <p>Elegant and powerful framework, with growing popularity in machine learning and application domains</p>
  </div>
  <div class="page">
    <p>Gaussian Process Graphical Model View</p>
    <p>f GP(m, k)</p>
    <p>y N( f (x),2)</p>
    <p>I f : RD &gt; R is a latent function</p>
    <p>I y is a noisy realisation of f (x)</p>
    <p>I k is the covariance function or kernel</p>
    <p>I m and 2 are learnt from data</p>
    <p>k</p>
    <p>f</p>
    <p>yx</p>
    <p>N</p>
  </div>
  <div class="page">
    <p>Gaussian Process Classification</p>
    <p>Pass latent function through logistic function to squash the input from (,) to obtain probability, (x) = p(yi = 1| fi) (similar to logistic regression)</p>
    <p>The likelihood is non-Gaussian and solution is not analytical</p>
    <p>Inference using Expectation propagation (EP)</p>
    <p>FITC approximation for large data</p>
  </div>
  <div class="page">
    <p>Gaussian Process Classification</p>
    <p>ARD kernel learns feature importance  features most discriminative between classes</p>
    <p>We learn 9 one-vs-all binary classifiers</p>
    <p>This way, we find the most predictive features consistent for all classes</p>
  </div>
  <div class="page">
    <p>Gaussian Process Resources</p>
    <p>Free book: http://www.gaussianprocess.org/gpml/chapters/</p>
  </div>
  <div class="page">
    <p>Gaussian Process Resources</p>
    <p>I GPs for Natural Language Processing tutorial (ACL 2014) http://www.preotiuc.ro</p>
    <p>I GP Schools in Sheffield and roadshows in Kampala, Pereira, Nyeri, Melbourne http://ml.dcs.shef.ac.uk/gpss/</p>
    <p>I Annotated bibliography and other materials http://www.gaussianprocess.org</p>
    <p>I GPy Toolkit (Python) https://github.com/SheffieldML/GPy</p>
  </div>
  <div class="page">
    <p>Prediction</p>
    <p>User Level</p>
    <p>LR SVM-RBF GP Baseline</p>
    <p>Stratified 10 fold cross-validation</p>
  </div>
  <div class="page">
    <p>Prediction</p>
    <p>User Level SVD-E (200)</p>
    <p>LR SVM-RBF GP Baseline</p>
    <p>Stratified 10 fold cross-validation</p>
  </div>
  <div class="page">
    <p>Prediction</p>
    <p>User Level SVD-E (200) SVD-C (200)</p>
    <p>LR SVM-RBF GP Baseline</p>
    <p>Stratified 10 fold cross-validation</p>
  </div>
  <div class="page">
    <p>Prediction</p>
    <p>User Level SVD-E (200) SVD-C (200) W2V-E (50)</p>
    <p>LR SVM-RBF GP Baseline</p>
    <p>Stratified 10 fold cross-validation</p>
  </div>
  <div class="page">
    <p>Prediction</p>
    <p>User Level SVD-E (200) SVD-C (200) W2V-E (50) W2V-C (200)</p>
    <p>LR SVM-RBF GP Baseline</p>
    <p>Stratified 10 fold cross-validation</p>
  </div>
  <div class="page">
    <p>Prediction Analysis</p>
    <p>User level features have no predictive value</p>
    <p>Clusters outperform embeddings</p>
    <p>Word2Vec features are better than SVD/NPMI for prediction</p>
    <p>Non-linear methods (SVM-RBF and GP) significantly outperform linear methods</p>
  </div>
  <div class="page">
    <p>Class Comparison</p>
    <p>Jensen-Shannon Divergence between topic distributions across occupational classes</p>
    <p>Some clusters of occupations are observable</p>
  </div>
  <div class="page">
    <p>Feature Analysis Rank Manual Label Topic (most frequent words)</p>
    <p>Most predictive Word2Vec 200 clusters as given by Gaussian Process ARD ranking</p>
  </div>
  <div class="page">
    <p>Feature Analysis</p>
    <p>Rank Manual Label Topic (most frequent words) 7 Football van, foster, cole, winger, terry,</p>
    <p>reckons, youngster, rooney, fielding, kenny</p>
    <p>Most predictive Word2Vec 200 clusters as given by Gaussian Process ARD ranking</p>
  </div>
  <div class="page">
    <p>Feature Analysis - Cumulative density functions</p>
    <p>Topic proportion</p>
    <p>U se</p>
    <p>r p ro</p>
    <p>b a b ili</p>
    <p>ty</p>
    <p>Higher Education (#21)</p>
    <p>C1 C2 C3 C4 C5 C6 C7 C8 C9</p>
    <p>Topic more prevalent  CDF line closer to bottom-right corner</p>
  </div>
  <div class="page">
    <p>Feature Analysis - Cumulative density functions</p>
    <p>Topic proportion</p>
    <p>U se</p>
    <p>r p ro</p>
    <p>b a b ili</p>
    <p>ty</p>
    <p>Arts (#116)</p>
    <p>C1 C2 C3 C4 C5 C6 C7 C8 C9</p>
    <p>Topic more prevalent  CDF line closer to bottom-right corner</p>
  </div>
  <div class="page">
    <p>Feature Analysis - Cumulative density functions</p>
    <p>Topic proportion</p>
    <p>U se</p>
    <p>r p ro</p>
    <p>b a b ili</p>
    <p>ty</p>
    <p>Elongated Words (#164)</p>
    <p>C1 C2 C3 C4 C5 C6 C7 C8 C9</p>
    <p>Topic more prevalent  CDF line closer to bottom-right corner</p>
  </div>
  <div class="page">
    <p>Feature Analysis</p>
    <p>Comparison of mean topic usage between supersets of occupational classes (1-2 vs. 6-9)</p>
  </div>
  <div class="page">
    <p>Take Aways</p>
    <p>User occupation influences language use in social media</p>
    <p>Non-linear methods (Gaussian Processes) obtain significant gains over linear methods</p>
    <p>Topic (clusters) features are both predictive and interpretable</p>
    <p>New dataset available for research</p>
  </div>
</Presentation>
