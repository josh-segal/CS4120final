<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Alex Uta, A.Custura, D. Duplyakin, I. Jimenez, J.S. Rellermeyer, C. Maltzahn, R. Ricci, A. Iosup</p>
    <p>a.uta@vu.nl</p>
    <p>Is Big Data Performance Reproducible in Modern Cloud Networks?</p>
  </div>
  <div class="page">
    <p>Big data infrastructure in the cloud</p>
    <p>Image courtesy of mattturck.com</p>
    <p>Many big data frameworks, infrastructure</p>
    <p>Designed by both industry and academia</p>
    <p>Highly embedded in the cloud</p>
    <p>Every new system/release, research paper means performance evaluation</p>
  </div>
  <div class="page">
    <p>How do we assess performance (in the cloud)?</p>
    <p>Do we understand the underlying conditions?</p>
    <p>Performance evaluation is inherently difficult in systems research.</p>
    <p>Cloud exacerbates performance variability.  How many times do we run experiments in the cloud?</p>
    <p>What do we report? (means, medians, distributions)</p>
  </div>
  <div class="page">
    <p>Cloud performance is variable  Due to:</p>
    <p>Co-location  Virtualization  Network congestion  noisy neighbors  Provider QoS policies</p>
    <p>Focus on networks:</p>
    <p>Ballani et al., SIGCOMM 2011</p>
    <p>Cloud variability affects our experiments.</p>
  </div>
  <div class="page">
    <p>Agenda &amp; Findings</p>
  </div>
  <div class="page">
    <p>Variability is disconsidered in performance evaluations</p>
    <p>Systematic survey -- 4 top systems conferences</p>
    <p>Time frame: 2010  2018</p>
    <p>Articles on: big data, analytics, data science, graph processing, streaming, MapReduce, Hadoop, Spark</p>
    <p>Systems evaluated on clouds</p>
  </div>
  <div class="page">
    <p>Variability is disconsidered in performance evaluations</p>
    <p>Questions asked:</p>
    <p>How many runs/trials?</p>
    <p>Is the article reporting averages/medians over a number of runs/trials?</p>
    <p>Is the article reporting experiment variability? (error-bars, min-max, percentiles)</p>
  </div>
  <div class="page">
    <p>Questions asked:</p>
    <p>How many runs/trials?</p>
    <p>Is the article reporting averages/medians over a number of runs/trials?</p>
    <p>Is the article reporting experiment variability? (error-bars, min-max, percentiles)</p>
    <p>Variability is disconsidered in performance evaluations</p>
  </div>
  <div class="page">
    <p>Questions asked:</p>
    <p>How many runs/trials?</p>
    <p>Is the article reporting averages/medians over a number of runs/trials?</p>
    <p>Is the article reporting variability? (error-bars, min-max, percentiles)</p>
    <p>Variability is disconsidered in performance evaluations</p>
  </div>
  <div class="page">
    <p>Main findings:</p>
    <p>Most articles report 3-10 repetitions, few report &gt; 10</p>
    <p>&gt; 50% of articles have no or poor experiment specification!</p>
    <p>&lt; 50% report only average or median</p>
    <p>~ 40% report variability</p>
    <p>Cited articles &gt; 11,000 citations</p>
    <p>Variability is disconsidered in performance evaluations</p>
  </div>
  <div class="page">
    <p>Is Big Data Performance Reproducible in Modern Cloud Networks?</p>
    <p>Spoiler alert: not so much</p>
  </div>
  <div class="page">
    <p>Experiment Design (1) -- Measuring the Cloud</p>
    <p>2 commercial clouds: Amazon EC2, Google  1 private, research cloud: HPCCloud  SURF, in NL</p>
    <p>Measured bandwidth and latency for multiple instance types for a week  Only some instance types, budget limited</p>
    <p>Multiple communication patterns, to mimic real-world applications:  Continuous streaming  Intermittent communication:</p>
    <p>5 second stream, 30 second pause  10 second stream, 30 second pause</p>
    <p>. . .</p>
  </div>
  <div class="page">
    <p>Performance vs. stochastic noise</p>
    <p>Problem: Underlying cloud performance varies between and during experiments.</p>
    <p>Stochastic noise in Google Cloud.</p>
    <p>Intermittent 5-30 Intermittent 10-30 Continuous</p>
    <p>Mitigation: Establish baselines of platform performance (microbenchmark the platform before running apps) and publish together with application performance.</p>
  </div>
  <div class="page">
    <p>Performance vs. Provider QoS policies  Problem: Experiment might interact with provider policies.</p>
    <p>Token bucket behavior  QoS Policy in Amazon EC2</p>
  </div>
  <div class="page">
    <p>Performance vs. Provider QoS policies  Problem: Experiment might interact with provider policies.</p>
    <p>Mitigation: Detect interaction with the provider (look for behavior that breaks assumptions, i.e., token buckets) and document these.</p>
    <p>Token bucket behavior  QoS Policy in Amazon EC2</p>
  </div>
  <div class="page">
    <p>Main Findings  Modern Cloud Networks</p>
    <p>Large amounts of variability, quantified</p>
    <p>Variability depends on access pattern</p>
    <p>Different behavior between providers</p>
    <p>Two types of variability:  (Similar to) Stochastic noise</p>
    <p>Given by provider QoS policies</p>
    <p>What does this mean for big data experiments?</p>
    <p>First to quantify</p>
  </div>
  <div class="page">
    <p>Experiment Design (2) -- Reproducing App Performance</p>
    <p>Apps = Benchmarking suites: HiBench and TPC-DS</p>
    <p>Platform: Apache Spark</p>
    <p>Stochastic noise: ran directly on Google GCE and HPCCloud, NL</p>
    <p>QoS Policies: emulate AWS Token Bucket on own infrastructure</p>
  </div>
  <div class="page">
    <p>Low confidence Better confidence Good confidence</p>
    <p>How to run repeatable experiments?  Problem: Interference (stochastic noise) affects experiments.</p>
    <p>Mitigation: Stochastic variability is tamed through robust experimentation (repeated experiments and statistics reported). Go beyond 10-15 experiment trials.</p>
    <p>TPC-DS Q65 on Google GCE</p>
    <p>Articles usually run 10-15 reps. median</p>
    <p>Error tolerance</p>
  </div>
  <div class="page">
    <p>How to run repeatable experiments?  Problem: Repeated experiments might influence each other.</p>
    <p>Mitigation: Randomize experiment order, re-use machines sparingly, try different days, rest infrastructure.</p>
    <p>On AWS-like Token Bucket</p>
  </div>
  <div class="page">
    <p>TL;DR: run more experiments</p>
    <p>Network performance variability is a widespread phenomenon</p>
    <p>Systems community often neglects performance variability in evaluations</p>
    <p>Important reproducibility problem in computer systems</p>
    <p>Reliable, repeatable performance == more runs than most articles do</p>
    <p>1st to quantify, but further community-effort needed to tackle it</p>
    <p>DOI: 10.5281/zenodo.3576604 Data released:</p>
  </div>
</Presentation>
