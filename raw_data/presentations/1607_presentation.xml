<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>RAID+: Deterministic and Balanced Data Distribution for Large Disk Enclosures</p>
    <p>Guangyan Zhang, Zican Huang, Xiaosong Ma</p>
    <p>SonglinYang, Zhufan Wang, Weimin Zheng</p>
    <p>Tsinghua University</p>
    <p>Qatar Computing Research Institute, HBKU</p>
  </div>
  <div class="page">
    <p>RAID with Large Disk Pools</p>
    <p>RAID (Redundant Array of Inexpensive Disks) widely used</p>
    <p>Not designed to work directly on large arrays</p>
    <p>HGST 4U60G2 EMC VMAX3 Dell PowerVault MD3060e</p>
  </div>
  <div class="page">
    <p>Existing Ways of Using Large Disk Pools (1): Integrating Homogeneous RAID Groups</p>
    <p>Concatenated (RAID-5C)</p>
    <p>Striped (RAID-50)</p>
    <p>Easy setup Lack of flexibility</p>
    <p>Deterministic addressing</p>
  </div>
  <div class="page">
    <p>Existing Ways of Using Large Disk Pools (2): Building Heterogeneous RAID Groups</p>
    <p>Hot spares</p>
    <p>Separate physical organization</p>
    <p>Low peak performance</p>
    <p>Slow recovery</p>
    <p>Lack of flexibility</p>
    <p>Easy setup</p>
    <p>Strong isolation</p>
    <p>Deterministic addressing</p>
  </div>
  <div class="page">
    <p>Logical</p>
    <p>Physical</p>
    <p>Existing Ways of Using Large Disk Pools (3): Random Block Placement</p>
    <p>Separate logical organization,</p>
    <p>shared physical storage</p>
    <p>Near-balanced data distribution</p>
    <p>Flexible, adaptive setup</p>
    <p>Imbalanced locally</p>
    <p>More bookkeeping</p>
    <p>Randomly map to disks</p>
  </div>
  <div class="page">
    <p>RAID+: Guaranteed Uniform Distribution using 3D templates</p>
    <p>(6+2)-block stripe</p>
    <p>Deterministic addressing</p>
    <p>Uniform data distribution</p>
    <p>Flexible, adaptive setup</p>
    <p>Improved spatial locality</p>
    <p>Logical</p>
    <p>Physical</p>
    <p>(5+1)-block stripe</p>
  </div>
  <div class="page">
    <p>RAID+ 3D Template</p>
    <p>Deterministic mapping of k-block stripes to ndisk pool</p>
    <p>k decoupled from n</p>
    <p>Properties (see paper for details and proofs)</p>
    <p>Fault tolerance of RAID</p>
    <p>No blocks in stripe placed on same disk</p>
    <p>Guaranteed load balance</p>
    <p>Uniform distribution of data blocks, for both application I/O and recovery</p>
    <p>Ease of maintenance</p>
    <p>Deterministic, computable addressing</p>
    <p>Achieved by using Mutually Orthogonal Latin Squares (MOLS)</p>
    <p>Disk pool size n</p>
    <p>Disk IDs for (5+1)-block RAID-5 stripe</p>
    <p>nx(n-1) matrix storing disk IDs</p>
  </div>
  <div class="page">
    <p>Latin square of order n  nn array filled with n different symbols  Each symbol occurring exactly once in each row and column</p>
    <p>2 Latin squares orthogonal, if when superimposed, each of n2</p>
    <p>ordered pairs appears exactly once</p>
    <p>Mutually Orthogonal Latin Squares (MOLS)</p>
    <p>Set of Latin squares, mutually orthogonal pair wise</p>
    <p>Introducing Latin Squares</p>
  </div>
  <div class="page">
    <p>Stacking MOLS into RAID+ Template</p>
    <p>Do we have enough MOLS?  Number of MOLS open question</p>
    <p>n-1 when n is power of prime number</p>
    <p>Valid RAID+ pool sizes</p>
    <p>For the ith square Li, Li[x,y] = ix + y</p>
    <p>Disk pool size n</p>
    <p>Disk IDs for (5+1)-block RAID-5 stripe</p>
    <p>nx(n-1) matrix storing disk IDs</p>
  </div>
  <div class="page">
    <p>D0 D1 D2 D3 D4</p>
    <p>MOLS-based Addressing</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>g</p>
    <p>h</p>
    <p>i</p>
    <p>j</p>
    <p>k</p>
    <p>l</p>
    <p>m</p>
    <p>n</p>
    <p>o</p>
    <p>P</p>
    <p>q</p>
    <p>r</p>
    <p>s</p>
    <p>t</p>
    <p>k=3 MOLS n(n-1)=20 stripes (n-1)k=12 blocks per disk</p>
    <p>a a a</p>
    <p>b b</p>
    <p>b</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>n=5 k=3 Li[x,y] = ix + y</p>
    <p>L2</p>
    <p>L1</p>
    <p>L3</p>
  </div>
  <div class="page">
    <p>d d</p>
    <p>e e e d</p>
    <p>g f f g f</p>
    <p>i h g h h</p>
    <p>j j i j i</p>
    <p>l k l k k</p>
    <p>m m n m l</p>
    <p>o n o o n</p>
    <p>q r p q q</p>
    <p>r s s q q</p>
    <p>s t t t r</p>
    <p>D0 D1 D2 D3 D4</p>
    <p>MOLS-based Addressing (n=5, k=3)</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>d 4 0 1</p>
    <p>e 0 1 2</p>
    <p>f 2 4 1</p>
    <p>g 3 0 2</p>
    <p>h 4 1 3</p>
    <p>i 0 2 4</p>
    <p>j 1 3 0</p>
    <p>k 3 1 4</p>
    <p>l 4 2 0</p>
    <p>m 0 3 1</p>
    <p>n 1 4 2</p>
    <p>o 2 0 3</p>
    <p>P 4 3 2</p>
    <p>q 0 4 3</p>
    <p>r 1 0 4</p>
    <p>s 2 1 0</p>
    <p>t 3 2 1</p>
    <p>a a a</p>
    <p>b b</p>
    <p>b</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>K=3 MOLS n(n-1)=20 stripes (n-1)k=12 blocks per disk</p>
  </div>
  <div class="page">
    <p>Impact of Stripe Ordering</p>
    <p>stripe a: (1, 2, 3)</p>
    <p>stripe b: (2, 3, 4)</p>
    <p>stripe c: (3, 4, 0)</p>
    <p>Sequential read of 6 blocks =&gt;</p>
    <p>Disk access sequence:</p>
    <p>D0 D1 D2 D3 D4</p>
    <p>(n-1)k=12 blocks per disk</p>
    <p>a a a</p>
    <p>b b</p>
    <p>b</p>
    <p>c</p>
    <p>c</p>
    <p>c Parity blocks</p>
    <p>stripe a: (1, 2, 3)</p>
    <p>stripe b: (2, 3, 4)</p>
    <p>stripe c: (3, 4, 0)</p>
    <p>Nave stripe ordering Highly overlapped</p>
  </div>
  <div class="page">
    <p>Sequential read friendly ordering  Intuition: traverse all disks</p>
    <p>(ignoring parity blocks)</p>
    <p>Sequential Access Friendly Stripe Ordering (I)</p>
    <p>a 1 2 3</p>
    <p>b 2 3 4</p>
    <p>c 3 4 0</p>
    <p>d 4 0 1</p>
    <p>e 0 1 2</p>
    <p>f 2 4 1</p>
    <p>g 3 0 2</p>
    <p>h 4 1 3</p>
    <p>i 0 2 4</p>
    <p>j 1 3 0</p>
    <p>k 3 1 4</p>
    <p>l 4 2 0</p>
    <p>m 0 3 1</p>
    <p>n 1 4 2</p>
    <p>o 2 0 3</p>
    <p>p 4 3 2</p>
    <p>q 0 4 3</p>
    <p>r 1 0 4</p>
    <p>s 2 1 0</p>
    <p>t 3 2 1</p>
    <p>+1 +1</p>
    <p>R-friendly ordering: A-C-E-B-D</p>
  </div>
  <div class="page">
    <p>Sequential write friendly ordering  Intuition: traverse all disks</p>
    <p>(including parity blocks)</p>
    <p>Sequential Access Friendly Stripe Ordering (II)</p>
    <p>a 1 2 3</p>
    <p>b 2 3 4</p>
    <p>c 3 4 0</p>
    <p>d 4 0 1</p>
    <p>e 0 1 2</p>
    <p>f 2 4 1</p>
    <p>g 3 0 2</p>
    <p>h 4 1 3</p>
    <p>i 0 2 4</p>
    <p>j 1 3 0</p>
    <p>k 3 1 4</p>
    <p>l 4 2 0</p>
    <p>m 0 3 1</p>
    <p>n 1 4 2</p>
    <p>o 2 0 3</p>
    <p>p 4 3 2</p>
    <p>q 0 4 3</p>
    <p>r 1 0 4</p>
    <p>s 2 1 0</p>
    <p>t 3 2 1</p>
    <p>+1 +1</p>
    <p>W-friendly ordering: A-D-B-E-C</p>
  </div>
  <div class="page">
    <p>When Disk Failure Happens</p>
    <p>a 1 2 3</p>
    <p>b 2 3 4</p>
    <p>c 3 4 0</p>
    <p>d 4 0 1</p>
    <p>e 0 1 2</p>
    <p>f 2 4 1</p>
    <p>g 3 0 2</p>
    <p>h 4 1 3</p>
    <p>i 0 2 4</p>
    <p>j 1 3 0</p>
    <p>k 3 1 4</p>
    <p>l 4 2 0</p>
    <p>m 0 3 1</p>
    <p>n 1 4 2</p>
    <p>o 2 0 3</p>
    <p>p 4 3 2</p>
    <p>q 0 4 3</p>
    <p>r 1 0 4</p>
    <p>s 2 1 0</p>
    <p>t 3 2 1</p>
    <p>Spare square L4</p>
    <p>a 1 2 3 4</p>
    <p>b 2 3 4 0</p>
    <p>c 3 4 0 1</p>
    <p>d 4 0 1 2</p>
    <p>e 0 1 2 3</p>
    <p>f 2 4 1 3</p>
    <p>g 3 0 2 4</p>
    <p>h 4 1 3 0</p>
    <p>i 0 2 4 1</p>
    <p>j 1 3 0 2</p>
    <p>k 3 1 4 2</p>
    <p>l 4 2 0 3</p>
    <p>m 0 3 1 4</p>
    <p>n 1 4 2 0</p>
    <p>o 2 0 3 1</p>
    <p>p 4 3 2 1</p>
    <p>q 0 4 3 2</p>
    <p>r 1 0 4 3</p>
    <p>s 2 1 0 4</p>
    <p>t 3 2 1 0</p>
    <p>a 1 2 3</p>
    <p>b 2 3 4</p>
    <p>c 3 4 0</p>
    <p>d 4 0 1</p>
    <p>e 0 1 2</p>
    <p>f 2 4 1</p>
    <p>g 3 0 2</p>
    <p>h 4 1 3</p>
    <p>i 0 2 4</p>
    <p>j 1 3 0</p>
    <p>k 3 1 4</p>
    <p>l 4 2 0</p>
    <p>m 0 3 1</p>
    <p>n 1 4 2</p>
    <p>o 2 0 3</p>
    <p>p 4 3 2</p>
    <p>q 0 4 3</p>
    <p>r 1 0 4</p>
    <p>s 2 1 0</p>
    <p>t 3 2 1</p>
    <p>Stripe sees a disk only once =&gt; replace with disk ID in L4</p>
    <p>Still orthogonal!</p>
  </div>
  <div class="page">
    <p>RAID+ Interim Layout</p>
    <p>a 1 2 3 4</p>
    <p>b 2 3 4 0</p>
    <p>c 3 4 0 1</p>
    <p>d 4 0 1 2</p>
    <p>e 0 1 2 3</p>
    <p>f 2 4 1 3</p>
    <p>g 3 0 2 4</p>
    <p>h 4 1 3 0</p>
    <p>i 0 2 4 1</p>
    <p>j 1 3 0 2</p>
    <p>k 3 1 4 2</p>
    <p>l 4 2 0 3</p>
    <p>m 0 3 1 4</p>
    <p>n 1 4 2 0</p>
    <p>o 2 0 3 1</p>
    <p>p 4 3 2 1</p>
    <p>q 0 4 3 2</p>
    <p>r 1 0 4 3</p>
    <p>s 2 1 0 4</p>
    <p>t 3 2 1 0</p>
    <p>c a a a b</p>
    <p>d d b b c</p>
    <p>e e e c d</p>
    <p>g f f g f</p>
    <p>i h g h h</p>
    <p>j j i j i</p>
    <p>l k l k k</p>
    <p>m m n m l</p>
    <p>o n o o n</p>
    <p>q r p p p</p>
    <p>r s s q q</p>
    <p>s t t t r</p>
    <p>c a a a b</p>
    <p>d d b b c</p>
    <p>e e e c d</p>
    <p>g f f g f</p>
    <p>i h g h h</p>
    <p>j j i j i</p>
    <p>l k l k k</p>
    <p>m m n m l</p>
    <p>o n o o n</p>
    <p>q r p p p</p>
    <p>r s s q q</p>
    <p>s t t t r</p>
    <p>c d e g</p>
    <p>i j l m</p>
    <p>o q r s5 x12 Normal layout</p>
    <p>(after loss of 1 disk)</p>
  </div>
  <div class="page">
    <p>RAID+ Recovery c a a a b</p>
    <p>d d b b c</p>
    <p>e e e c d</p>
    <p>g f f g f</p>
    <p>i h g h h</p>
    <p>j j i j i</p>
    <p>l k l k k</p>
    <p>m m n m l</p>
    <p>o n o o n</p>
    <p>q r p p p</p>
    <p>r s s q q</p>
    <p>s t t t r</p>
    <p>c d e g</p>
    <p>i j l m</p>
    <p>o q r s</p>
    <p>interim layout (after loss of 1 disk)</p>
    <p>Guaranteed uniform load distribution</p>
    <p>Read and write evenly on all survivors</p>
    <p>Fast all-to-all data migration</p>
    <p>Copy in background to replacement/hotspare</p>
    <p>Repeat for more failures, without hot-spares</p>
    <p>N &gt;&gt; k in most cases!</p>
    <p>Pre-allocated recovery area</p>
    <p>Trading capacity for recovery performance</p>
    <p>Optimized for 1-disk failure</p>
    <p>Maintaining RAID level fault tolerance</p>
    <p>Optimization for 2+ simultaneous failures</p>
    <p>Pre-allocated space</p>
  </div>
  <div class="page">
    <p>Addressing Metadata Management</p>
    <p>c a a a b</p>
    <p>d d b b c</p>
    <p>e e e c d</p>
    <p>g f f g f</p>
    <p>i h g h h</p>
    <p>j j i j i</p>
    <p>l k l k k</p>
    <p>m m n m l</p>
    <p>o n o o n</p>
    <p>q r p p p</p>
    <p>r s s q q</p>
    <p>s t t t r</p>
    <p>RAID+ template (w. reserved recovery area)</p>
    <p>Only records template base address</p>
    <p>Small overhead with typical template sizes</p>
    <p>In-template addressing easily computable</p>
    <p>Implemented in &lt;20 lines of code</p>
  </div>
  <div class="page">
    <p>Evaluation  Implementation</p>
    <p>MD (Multiple Devices) driver for software RAID, in Linux kernel 3.14.35</p>
    <p>Platform  SuperMicro 4U storage server, 2 12-core Intel E5-2650 processors, 128GB DDR4</p>
    <p>2 AOC-S3008L- L8I SAS JBOD adapters, each connected to 30-bay SAS3 expander backplane via 2 channels</p>
    <p>60 Seagate Constellation 7200RPM 2TB HDDs</p>
    <p>Aggregate I/O channel bandwidth: 24GB/s</p>
    <p>Workloads  Synthetic: fio benchmark</p>
    <p>8 public block-level I/O traces, from MSR Cambridge and SPC</p>
    <p>I/O-intensive applications: GridGraph, TPC-C, Facebook-like, MongoDB</p>
  </div>
  <div class="page">
    <p>Systems  Common base RAID setting: (6+1) RAID-5</p>
    <p>RAIDH : CRUSH-style, hashing based random mapping</p>
    <p>RAIDR : Pseudo-random number generation based random mapping</p>
    <p>Metrics  Recovery speed (online and offline), interference to application performance</p>
    <p>Data re-distribution after failures</p>
    <p>Normal performance: synthetic and application, single- and multi-workload</p>
    <p>More results in paper</p>
    <p>Evaluation Methodology</p>
    <p>RAID-5C RAID-50 Partitioned RAID5 Random RAID+</p>
  </div>
  <div class="page">
    <p>RAID50 RAIDR RAIDH RAID+</p>
    <p>R e</p>
    <p>b u</p>
    <p>il d</p>
    <p>t im</p>
    <p>e (s</p>
    <p>)</p>
    <p>Reconstruction Performance: Offline Rebuild</p>
    <p>Two disk pool sizes: (56+3), (28+3)</p>
    <p>Data size per disk: 50GB</p>
    <p>Disk pool size</p>
  </div>
  <div class="page">
    <p>Normal I/O Performance : Multi-workload</p>
    <p>usr_1 prn_0 src1_1 Fin2</p>
    <p>S p</p>
    <p>e e d</p>
    <p>u p</p>
    <p>RAID+ speedup over partitioned RAID-5</p>
    <p>Time (min)</p>
    <p>usr_1</p>
    <p>prn_0</p>
    <p>src1_1</p>
    <p>Fin2</p>
    <p>I/ O</p>
    <p>s</p>
    <p>Time (ms)</p>
    <p>Sample load variation</p>
    <p>Running 4-app mix selected from 8 storage traces</p>
    <p>Evaluating all 70 mixes</p>
    <p>RAID+ brings avg. 2.05 weighted speedup over partitioned RAID-5, vs. 1.83 by RAIDH</p>
    <p>Only 39 out of 280 runs experienced slowdown</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>RAID+: new RAID architecture  Flexible and efficient</p>
    <p>Virtual RAID groups on large disk enclosures</p>
    <p>Enabled by Latin-square based 3D templates  Guaranteed uniform data distribution for both normal and 1-disk failure recovery I/O</p>
    <p>Deterministic addressing</p>
    <p>Utilizing all disks evenly while maintaining data locality</p>
    <p>Especially appealing to shared large disk enclosures  Enterprise server, cloud, and datacenter</p>
  </div>
  <div class="page"/>
</Presentation>
