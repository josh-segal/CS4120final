<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deep Adversarial Learning for NLP</p>
    <p>William Wang Sameer Singh</p>
    <p>With contributions from Jiwei Li1Slides: http://tiny.cc/adversarial</p>
  </div>
  <div class="page">
    <p>Deep Adversarial Learning for NLP</p>
    <p>William Wang Sameer Singh</p>
    <p>With contributions from Jiwei Li.1Slides: http://tiny.cc/adversarial</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Introduction, Background, and GANs (William, 90 mins)</p>
    <p>Adversarial Examples and Rules (Sameer, 75 mins)</p>
    <p>Conclusion and Question Answering (Sameer and William, 15 mins)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background of the Tutorial</p>
    <p>Introduction: Adversarial Learning in NLP</p>
    <p>Adversarial Generation</p>
    <p>A Case Study of GANs in Dialogue Systems</p>
  </div>
  <div class="page">
    <p>Rise of Adversarial Learning in NLP</p>
    <p>Through a simple ACL anthology search, we found that in 2018, there were 20+ times more papers mentioning adversarial, comparing to 2016.</p>
    <p>Meanwhile, the growth of all accepted papers is 1.39 times during this period.</p>
    <p>But if you went to CVPR 2018 in Salt Lake City, there were more than 100 papers on adversarial learning (approximately 1/3 of all adv. learning papers in NLP).</p>
  </div>
  <div class="page">
    <p>Questions Id like to Discuss</p>
    <p>What are the subareas of deep adversarial learning in NLP?</p>
    <p>How do we understand adversarial learning?</p>
    <p>What are some success stories?</p>
    <p>What are the pitfalls that we need to avoid?</p>
  </div>
  <div class="page">
    <p>Opportunities in Adversarial Learning</p>
    <p>Adversarial learning is an interdisciplinary research area, and it is closely related to, but limited to the following fields of study:</p>
    <p>Machine Learning</p>
    <p>Computer Vision</p>
    <p>Natural Language Processing</p>
    <p>Computer Security</p>
    <p>Game Theory</p>
    <p>Economics</p>
  </div>
  <div class="page">
    <p>Adversarial Attack in ML, Vision, &amp; Security</p>
    <p>Goodfellow et al., (2015)</p>
  </div>
  <div class="page">
    <p>Physical-World Adversarial Attack / Examples (Eykholt et al., CVPR 2018)</p>
  </div>
  <div class="page">
    <p>Success of Adversarial Learning</p>
    <p>CycleGAN (Zhu et al., 2017)</p>
  </div>
  <div class="page">
    <p>Failure Cases</p>
    <p>CycleGAN (Zhu et al., 2017) 10</p>
  </div>
  <div class="page">
    <p>Success of Adversarial Learning</p>
    <p>GauGAN (Park et al., 2019)11</p>
  </div>
  <div class="page">
    <p>Deep Adversarial Learning in NLP</p>
    <p>There were some successes of GANs in NLP, but not so much comparing to Vision.</p>
    <p>The scope of Deep Adversarial Learning in NLP includes:</p>
    <p>Adversarial Examples, Attacks, and Rules</p>
    <p>Adversarial Training (w. Noise)</p>
    <p>Adversarial Generation</p>
    <p>Various other usages in ranking, denoising, &amp; domain adaptation.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background of the Tutorial</p>
    <p>Introduction: Adversarial Learning in NLP</p>
    <p>Adversarial Generation</p>
    <p>A Case Study of GANs in Dialogue Systems</p>
  </div>
  <div class="page">
    <p>Adversarial Examples</p>
    <p>One of the more popular areas of adversarial learning in NLP.</p>
    <p>E.g., Alzantot et al., EMNLP 2018</p>
  </div>
  <div class="page">
    <p>Adversarial Attacks (Coavoux et al., EMNLP 2018)</p>
    <p>The main classifier</p>
    <p>predicts a label y from</p>
    <p>a text x, the attacker</p>
    <p>tries to recover some</p>
    <p>private information z</p>
    <p>contained in x from the</p>
    <p>latent representation</p>
    <p>used by the main</p>
    <p>classifier.</p>
  </div>
  <div class="page">
    <p>Adversarial Training</p>
    <p>Main idea:  Adding noise, randomness, or adversarial loss in optimization.</p>
    <p>Goal: make the trained model more robust.</p>
  </div>
  <div class="page">
    <p>Adversarial Training: A Simple Example</p>
    <p>Adversarial Training for Relation Extraction  Wu, Bamman, Russell (EMNLP 2017).</p>
    <p>Task: Relation Classification.</p>
    <p>Interpretation: Regularization in the Feature Space.</p>
  </div>
  <div class="page">
    <p>Adversarial Training for Relation Extraction</p>
    <p>Wu, Bamman, Russell (EMNLP 2017).</p>
  </div>
  <div class="page">
    <p>Adversarial Training for Relation Extraction</p>
    <p>Wu, Bamman, Russell (EMNLP 2017).</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background of the Tutorial</p>
    <p>Introduction: Adversarial Learning in NLP</p>
    <p>Adversarial Generation</p>
    <p>A Case Study of GANs in Dialogue Systems</p>
  </div>
  <div class="page">
    <p>GANs (Goodfellow et al., 2014)</p>
    <p>Two competing neural networks: generator &amp; discriminator</p>
    <p>the classifier trying to detect the fake sample</p>
    <p>forger trying to produce some counterfeit material</p>
    <p>Image: https://ishmaelbelghazi.github.io/ALI/21</p>
  </div>
  <div class="page">
    <p>GAN Objective</p>
    <p>D(x): the probability that x came from the data rather than generator</p>
    <p>Goodfellow, et al., Generative adversarial networks, in NIPS, 2014.</p>
    <p>D</p>
    <p>G</p>
  </div>
  <div class="page">
    <p>GAN Training Algorithm</p>
    <p>Discriminator</p>
    <p>Generator</p>
    <p>Goodfellow, et al., Generative adversarial networks, in NIPS, 2014.</p>
  </div>
  <div class="page">
    <p>GAN Equilibrium</p>
    <p>Global optimality</p>
    <p>Discriminator</p>
    <p>Generator</p>
    <p>D</p>
    <p>G</p>
    <p>s.t.</p>
  </div>
  <div class="page">
    <p>Major Issues of GANs  Mode Collapse (unable to produce diverse samples)</p>
  </div>
  <div class="page">
    <p>Major Issues of GANs in NLP</p>
    <p>Often you need to pre-train the generator and discriminator w. MLE</p>
    <p>But how much?</p>
    <p>Unstable Adversarial Training  We are dealing with two networks / learners / agents</p>
    <p>Should we update them at the same rate?</p>
    <p>The discriminator might overpower the generator.</p>
    <p>With many possible combinations of model choice for generator and discriminator networks in NLP, it could be worse.</p>
  </div>
  <div class="page">
    <p>Major Issues of GANs in NLP</p>
    <p>GANs were originally designed for images  You cannot back-propagate through the generated X</p>
    <p>Image is continuous, but text is discrete (DR-GAN, Tran et al., CVPR 2017).</p>
  </div>
  <div class="page">
    <p>SeqGAN: policy gradient for generating sequences (Yu et al., 2017)</p>
  </div>
  <div class="page">
    <p>Training Language GANs from Scratch</p>
    <p>New Google DeepMind arxiv paper (de Masson dAutume et al., 2019)</p>
    <p>Claims no MLE pre-trainings are needed.</p>
    <p>Uses per time-stamp dense rewards.</p>
    <p>Yet to be peer-reviewed and tested.</p>
  </div>
  <div class="page">
    <p>Why shouldnt NLP give up on GAN?</p>
    <p>Its unsupervised learning.</p>
    <p>Many potential applications of GANs in NLP.</p>
    <p>The discriminator is often learning a metric.</p>
    <p>It can also be interpreted as self-supervised learning (especially with dense rewards).</p>
  </div>
  <div class="page">
    <p>Applications of Adversarial Learning in NLP  Social Media (Wang et al., 2018a; Carton et al., 2018)</p>
    <p>Contrastive Estimation (Cai and Wang, 2018; Bose et al., 2018)</p>
    <p>Domain Adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b)</p>
    <p>Data Cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018)</p>
    <p>Information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018)</p>
    <p>Information retrieval (Li and Cheng, 2018)</p>
    <p>Another 18 papers on Adversarial Learning at NAACL 2019!</p>
  </div>
  <div class="page">
    <p>GANs for Machine Translation</p>
    <p>Yang et al., NAACL 2018</p>
    <p>Wu et al., ACML 2018</p>
  </div>
  <div class="page">
    <p>SentiGAN (Wang and Wan, IJCAI 2018) Idea: use a mixture of generators and a multi-class discriminator.</p>
  </div>
  <div class="page">
    <p>No Metrics Are Perfect: Adversarial Reward Learning (Wang, Chen et al., ACL 2018)</p>
  </div>
  <div class="page">
    <p>AREL Storytelling Evaluation</p>
    <p>Dataset: VIST (Huang et al., 2016).</p>
    <p>XE BLEU-RL CIDEr-RL GAN AREL</p>
    <p>Turing Test</p>
    <p>Win Unsure</p>
  </div>
  <div class="page">
    <p>DSGAN: Adversarial Learning for Distant Supervision IE (Qin et al., ACL 2018)</p>
  </div>
  <div class="page">
    <p>DSGAN: Adversarial Learning for Distant Supervision IE (Qin et al., ACL 2018)</p>
  </div>
  <div class="page">
    <p>KBGAN: Learning to Generate High-Quality Negative Examples (Cai and Wang, NAACL 2018)</p>
    <p>Idea: use adversarial learning to iteratively learn better negative examples.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background of the Tutorial</p>
    <p>Introduction: Adversarial Learning in NLP</p>
    <p>Understanding Adversarial Learning</p>
    <p>Adversarial Generation</p>
    <p>A Case Study of GANs in Dialogue Systems</p>
  </div>
  <div class="page">
    <p>What Should Rewards for Good Dialogue Be Like ?</p>
  </div>
  <div class="page">
    <p>Turing Test</p>
    <p>Reward for Good Dialogue</p>
  </div>
  <div class="page">
    <p>How old are you ?</p>
    <p>I dont know what you are talking about</p>
    <p>Im 25.</p>
    <p>A human evaluator/ judge</p>
    <p>Reward for Good Dialogue</p>
    <p>Jl3</p>
  </div>
  <div class="page">
    <p>How old are you ?</p>
    <p>I dont know what you are talking about</p>
    <p>Im 25.</p>
    <p>Reward for Good Dialogue</p>
    <p>Jl3</p>
  </div>
  <div class="page">
    <p>How old are you ?</p>
    <p>I dont know what you are talking about</p>
    <p>Im 25.</p>
    <p>P= 90% human generated</p>
    <p>P= 10% human generated</p>
    <p>Reward for Good Dialogue</p>
    <p>Jl3</p>
  </div>
  <div class="page">
    <p>Adversarial Learning in Image Generation (Goodfellow et al., 2014)</p>
    <p>Jl3 Jl4</p>
  </div>
  <div class="page">
    <p>Model Breakdown Generative Model (G)</p>
    <p>how are you ?</p>
    <p>Im fine . EOS</p>
    <p>Encoding Decoding</p>
    <p>eos Im fine .</p>
  </div>
  <div class="page">
    <p>Model Breakdown Generative Model (G)</p>
    <p>how are you ?</p>
    <p>Im fine . EOS</p>
    <p>Encoding Decoding</p>
    <p>eos Im fine .</p>
    <p>Discriminative Model (D)</p>
    <p>how are you ? eos Im fine .</p>
    <p>P= 90% human generated</p>
  </div>
  <div class="page">
    <p>Model Breakdown Generative Model (G)</p>
    <p>how are you ?</p>
    <p>Im fine . EOS</p>
    <p>Encoding Decoding</p>
    <p>eos Im fine .</p>
    <p>Discriminative Model (D)</p>
    <p>how are you ? eos Im fine .</p>
    <p>Reward P= 90% human generated</p>
  </div>
  <div class="page">
    <p>Policy Gradient</p>
    <p>REINFORCE Algorithm (William,1992)</p>
    <p>Generative Model (G)</p>
    <p>how are you ?</p>
    <p>Im fine EOS</p>
    <p>Encoding Decoding</p>
    <p>eos Im fine .</p>
  </div>
  <div class="page">
    <p>Adversarial Learning for Neural Dialogue Generation</p>
    <p>Update the Discriminator</p>
    <p>Update the Generator</p>
    <p>The discriminator forces the generator to produce correct responses</p>
  </div>
  <div class="page">
    <p>Human Evaluation</p>
    <p>The previous RL model only perform better on multi-turn conversations</p>
  </div>
  <div class="page">
    <p>Results: Adversarial Learning Improves Response Generation</p>
    <p>Human Evaluator</p>
    <p>vs a vanilla generation model</p>
    <p>Adversarial Win</p>
    <p>Adversarial Lose</p>
    <p>Tie</p>
  </div>
  <div class="page">
    <p>Sample response</p>
    <p>Tell me ... how long have you had this falling sickness ?</p>
    <p>System Response</p>
  </div>
  <div class="page">
    <p>Sample response</p>
    <p>Tell me ... how long have you had this falling sickness ?</p>
    <p>System Response</p>
    <p>Vanilla-Seq2Seq I dont know what you are talking about.</p>
  </div>
  <div class="page">
    <p>Sample response</p>
    <p>Tell me ... how long have you had this falling sickness ?</p>
    <p>System Response</p>
    <p>Vanilla-Seq2Seq I dont know what you are talking about.</p>
    <p>Mutual Information Im not a doctor.</p>
  </div>
  <div class="page">
    <p>Sample response</p>
    <p>Tell me ... how long have you had this falling sickness ?</p>
    <p>System Response</p>
    <p>Vanilla-Seq2Seq I dont know what you are talking about.</p>
    <p>Mutual Information Im not a doctor.</p>
    <p>Adversarial Learning A few months, I guess.</p>
  </div>
  <div class="page">
    <p>Self-Supervised Learning meets Adversarial Learning</p>
    <p>Self-Supervised Dialog Learning (Wu et al., ACL 2019)</p>
    <p>Use of SSL to learn dialogue structure (sequence ordering).</p>
  </div>
  <div class="page">
    <p>Self-Supervised Learning meets Adversarial Learning</p>
    <p>Self-Supervised Dialog Learning (Wu et al., ACL 2019)</p>
    <p>Use of SSN to learn dialogue structure (sequence ordering).</p>
    <p>REGS: Li et al., (2017) AEL: Xu et al., (2017)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Deep adversarial learning is a new, diverse, and interdisciplinary research area, and it is highly related to many subareas in NLP.</p>
    <p>GANs have obtained particular strong results in Vision, but yet there are both challenges and opportunities in GANs for NLP.</p>
    <p>In a case study, we show that adversarial learning for dialogue has obtained promising results.</p>
    <p>There are plenty of opportunities ahead of us with the current advances of representation learning, reinforcement learning, and self-supervised learning techniques in NLP.</p>
  </div>
  <div class="page">
    <p>UCSB Postdoctoral Scientist Opportunities</p>
    <p>Please talk to me at NAACL, or email william@cs.ucsb.edu. 60</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Now we will take an 30 mins break.</p>
  </div>
  <div class="page">
    <p>Adversarial Examples in NLP</p>
    <p>Sameer Singh sameer@uci.edu</p>
    <p>@sameer_</p>
    <p>sameersingh.org</p>
    <p>Slides: http://tiny.cc/adversarial</p>
  </div>
  <div class="page">
    <p>What are Adversarial Examples?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 2</p>
    <p>panda</p>
    <p>gibbon</p>
    <p>[Goodfellow et al, ICLR 2015 ]</p>
  </div>
  <div class="page">
    <p>Whats going on?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 3[Goodfellow et al, ICLR 2015 ]</p>
    <p>Fast Gradient Sign Method</p>
  </div>
  <div class="page">
    <p>Applications of Adversarial Attacks</p>
    <p>Security of ML Models  Should I deploy or not? Whats the worst that can happen?</p>
    <p>Evaluation of ML Models  Held-out test error is not enough</p>
    <p>Finding Bugs in ML Models  What kinds of adversaries might happen naturally?</p>
    <p>(Even without any bad actors)</p>
    <p>Interpretability of ML Models?  What does the model care about, and what does it ignore?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 4</p>
  </div>
  <div class="page">
    <p>Challenges in NLP</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 5</p>
    <p>Change L2 is not really defined for text What is imperceivable? What is a small vs big change? What is the right way to measure this?</p>
    <p>Effect Classification tasks fit in well, but  What about structured prediction? e.g. sequence labeling Language generation? e.g. MT or summarization</p>
    <p>Search Text is discrete,</p>
    <p>cannot use continuous optimization How do we search over sequences?</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries Different ways to address the challenges</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 6</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 7</p>
    <p>What is a small change?</p>
    <p>What does it mean to misbehave?</p>
    <p>How do we find the attack?</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 8</p>
    <p>What is a small change?</p>
  </div>
  <div class="page">
    <p>Change: What is a small change?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 9</p>
    <p>Characters Pros:  Often easy to miss  Easier to search over Cons:  Gibberish, nonsensical words  No useful for interpretability</p>
    <p>Words Pros:  Always from vocabulary  Often easy to miss Cons:  Ungrammatical changes  Meaning also changes</p>
    <p>Phrase/Sentence Pros:  Most natural/human-like  Test long-distance effects Cons:  Difficult to guarantee quality  Larger space to search</p>
    <p>Main Challenge: Defining the distance between x and x</p>
  </div>
  <div class="page">
    <p>Change: A Character (or few)</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 10[ Ebrahimi et al, ACL 2018, COLING 2018 ]</p>
    <p>x = [ I   l o v</p>
    <p>x' = [ I   l i v</p>
    <p>Edit Distance: Flip, Insert, Delete</p>
    <p>x = [ I love movies ]</p>
  </div>
  <div class="page">
    <p>Change: Word-level Changes</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 11</p>
    <p>x = [ I  like this movie  . ]</p>
    <p>x' = [ I  really this movie  . ]Word Embedding?</p>
    <p>x' = [ I  eat this movie  . ]Part of Speech?</p>
    <p>x' = [ I  hate this movie  . ]Language Model?</p>
    <p>x' = [ I  lamp this movie  . ]Random word?</p>
    <p>Lets replace this word</p>
    <p>[ Alzantot et. al. EMNLP 2018 ]</p>
    <p>[Jia and Liang, EMNLP 2017 ]</p>
  </div>
  <div class="page">
    <p>Change: Paraphrasing via Backtranslation</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 12</p>
    <p>This is a good movie</p>
    <p>x</p>
    <p>Este  um bom filme</p>
    <p>cest un bon film</p>
    <p>Translate into multiple languages Use back-translators to score candidates</p>
    <p>S(x, x)  0.5 * P(x | Este  um bom filme) + 0.5 * P(x | cest un bon film)</p>
    <p>This is a good movie This is a good movieS( , ) = 1</p>
    <p>This is a good movie That is a good movieS( , ) = 0.95</p>
    <p>S( , ) = 0This is a good movie Dogs like cats</p>
    <p>x, x should mean the same thing (semantically-equivalent adversaries)</p>
    <p>[Ribeiro et al ACL 2018]</p>
  </div>
  <div class="page">
    <p>Change: Sentence Embeddings</p>
    <p>Deep representations are supposed to encode meaning in vectors  If (x-x) is difficult to compute, maybe we can do (z-z)?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 13</p>
    <p>D</p>
    <p>Decoder (GAN)</p>
    <p>Ez</p>
    <p>Encoder</p>
    <p>z'</p>
    <p>x f y</p>
    <p>x' f y'</p>
    <p>[Zhao et al ICLR 2018]</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 14</p>
    <p>What is a small change?</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 15</p>
    <p>How do we find the attack?</p>
  </div>
  <div class="page">
    <p>Search: How do we find the attack?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 16</p>
    <p>Only access predictions (usually unlimited queries)</p>
    <p>Full access to the model (compute gradients)</p>
    <p>Access probabilities</p>
    <p>Create x and test whether the model misbehaves</p>
    <p>Create x and test whether general direction is correct</p>
    <p>Use the gradient to craft x</p>
    <p>Even this is often unrealistic</p>
  </div>
  <div class="page">
    <p>Search: Gradient-based</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 17</p>
    <p>Or whatever the misbehavior is</p>
    <p>Beam search over the above</p>
    <p>[ Ebrahimi et al, ACL 2018, COLING 2018 ]</p>
  </div>
  <div class="page">
    <p>Search: Sampling</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 18</p>
    <p>[Zhao et al, ICLR 2018 ]</p>
    <p>[ Alzantot et. al. EMNLP 2018 ]</p>
    <p>[Jia and Liang, EMNLP 2017 ]</p>
  </div>
  <div class="page">
    <p>Search: Enumeration (Trial/Error)</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 19</p>
    <p>[Belinkov, Bisk, ICLR 2018 ]</p>
    <p>[Iyyer et al, NAACL 2018 ]</p>
    <p>[Ribeiro et al, ACL 2018 ]</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 20</p>
    <p>How do we find the attack?</p>
  </div>
  <div class="page">
    <p>Choices in Crafting Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 21</p>
    <p>What does it mean to misbehave?</p>
  </div>
  <div class="page">
    <p>Effect: What does it mean to misbehave?</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 22</p>
    <p>Classification Untargeted: any other class</p>
    <p>Targeted: specific other class</p>
    <p>Other Tasks Loss-based: Maximize the loss on the example</p>
    <p>e.g. perplexity/log-loss of the prediction</p>
    <p>Property-based: Test whether a property holds e.g. MT: A certain word is not generated</p>
    <p>NER: No PERSON appears in the output</p>
    <p>No me ataques!MT: Don't attack me!</p>
    <p>NER:</p>
  </div>
  <div class="page">
    <p>Evaluation: Are the attacks good?</p>
    <p>Are they Effective?  Attack/Success rate</p>
    <p>Are the Changes Perceivable? (Human Evaluation)  Would it have the same label?</p>
    <p>Does it look natural?</p>
    <p>Does it mean the same thing?</p>
    <p>Do they help improve the model?  Accuracy after data augmentation</p>
    <p>Look at some examples!</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 23</p>
  </div>
  <div class="page">
    <p>Review of the Choices</p>
    <p>Change  Character level  Word level  Phrase/Sentence level</p>
    <p>Effect  Targeted or Untargeted  Choose based on the task</p>
    <p>Search  Gradient-based  Sampling  Enumeration</p>
    <p>Evaluation</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 24</p>
  </div>
  <div class="page">
    <p>Research Highlights In terms of the choices that were made</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 25</p>
  </div>
  <div class="page">
    <p>Noise Breaks Machine Translation!</p>
    <p>Change Search Tasks</p>
    <p>Random Character Based Passive; add and test Machine Translation</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 26[Belinkov, Bisk, ICLR 2018 ]</p>
  </div>
  <div class="page">
    <p>Hotflip</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 27</p>
    <p>Change Search Tasks</p>
    <p>Character-based (extension to words)</p>
    <p>Gradient-based; beam-search Machine Translation, Classification, Sentiment</p>
    <p>[ Ebrahimi et al, ACL 2018, COLING 2018 ]</p>
    <p>News Classification</p>
    <p>Machine Translation</p>
  </div>
  <div class="page">
    <p>Search Using Genetic Algorithms</p>
    <p>[ Alzantot et. al. EMNLP 2018 ] Sameer Singh, NAACL 2019 Tutorial 28</p>
    <p>Change Search Tasks</p>
    <p>Word-based, language model score</p>
    <p>Genetic Algorithm Textual Entailment, Sentiment Analysis</p>
    <p>Black-box, population-based search of natural adversary</p>
  </div>
  <div class="page">
    <p>Natural Adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 29[Zhao et al, ICLR 2018 ]</p>
    <p>Change Search Tasks</p>
    <p>Sentence, GAN embedding</p>
    <p>Stochastic search Images, Entailment, Machine Translation</p>
    <p>Textual Entailment</p>
  </div>
  <div class="page">
    <p>Semantic Adversaries Semantically-Equivalent Adversary</p>
    <p>(SEA) Semantically-Equivalent Adversarial Rules</p>
    <p>(SEARs)</p>
    <p>color  colour</p>
    <p>x Backtranslation + Enumeration</p>
    <p>x (x, x) Patterns in diffs</p>
    <p>Rules</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 30[Ribeiro et al, ACL 2018 ]</p>
    <p>Change Search Tasks</p>
    <p>Sentence via Backtranslation</p>
    <p>Enumeration VQA, SQuAD, Sentiment Analysis</p>
  </div>
  <div class="page">
    <p>Transformation Rules: VisualQA</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 31[Ribeiro et al, ACL 2018 ]</p>
  </div>
  <div class="page">
    <p>Transformation Rules: SQuAD</p>
  </div>
  <div class="page">
    <p>Transformation Rules: Sentiment Analysis</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 33[Ribeiro et al, ACL 2018 ]</p>
  </div>
  <div class="page">
    <p>Adding a Sentence</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 34[Jia, Liang, EMNLP 2017 ]</p>
    <p>Change Search Tasks</p>
    <p>Add a Sentence Domain knowledge, stochastic search</p>
    <p>Question Answering</p>
  </div>
  <div class="page">
    <p>Some Loosely Related Work Use a broader notions of adversaries</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 35</p>
  </div>
  <div class="page">
    <p>CRIAGE: Adversaries for Graph Embeddings</p>
    <p>[ Pezeshkpour et. al. NAACL 2019 ] Sameer Singh, NAACL 2019 Tutorial 36</p>
    <p>Which link should we add/remove, out of million possible links?</p>
  </div>
  <div class="page">
    <p>Should Not Change / Should Change</p>
    <p>Should Not Change</p>
    <p>like Adversarial Attacks</p>
    <p>Random Swap</p>
    <p>Stopword Dropout</p>
    <p>Paraphrasing</p>
    <p>Grammatical Mistakes</p>
    <p>Should Change</p>
    <p>Overstability Test</p>
    <p>Add Negation</p>
    <p>Antonyms</p>
    <p>Randomize Inputs</p>
    <p>Change Entities</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 37[Niu, Bansal, CONLL 2018 ]</p>
    <p>How do dialogue systems behave when the inputs are perturbed in specific ways?</p>
  </div>
  <div class="page">
    <p>Overstability: Anchors</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 38</p>
    <p>Anchor</p>
    <p>Identify the conditions under which the classifier has the same prediction</p>
    <p>[Ribeiro et al, AAAI 2018 ]</p>
  </div>
  <div class="page">
    <p>Overstability: Input Reduction</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 39[Feng et al, EMNLP 2018 ]</p>
    <p>Remove as much of the input as you can without changing the prediction!</p>
  </div>
  <div class="page">
    <p>Adversarial Examples for NLP</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 40</p>
    <p>Imperceivable changes to the input  Unexpected behavior for the output  Applications: security, evaluation, debugging</p>
    <p>Challenges for NLP  Effect: What is misbehavior?  Change: What is a small change?  Search: How do we find them?  Evaluation: How do we know its good?</p>
  </div>
  <div class="page">
    <p>Sameer Singh, NAACL 2019 Tutorial 41</p>
    <p>More realistic threat models  Give even less access to the model/data</p>
    <p>Defenses and fixes  Spell-check based filtering</p>
    <p>Attack recognition: [Pruthi et al ACL 2019]</p>
    <p>Data augmentation</p>
    <p>Novel losses, e.g. [Zhang, Liang AISTATS 2019]</p>
    <p>Beyond sentences  Paragraphs, documents?</p>
    <p>Semantic equivalency  coherency across sentences</p>
    <p>Future Directions</p>
  </div>
  <div class="page">
    <p>References for Adversarial Examples in NLP Relevant Work (roughly chronological)</p>
    <p>Sentences to QA: [Jia and Liang, EMNLP 2017 ] link</p>
    <p>Noise Breaks MT: [ Belinkov, Bisk, ICLR 2018 ] link</p>
    <p>Natural Adversaries: [Zhao et al, ICLR 2018 ] link</p>
    <p>Syntactic Paraphrases: [Iyyer et al NAACL 2018] link</p>
    <p>Hotflip/Hotflip MT: [ Ebrahimi et al, ACL 2018, COLING 2018 ] link, link</p>
    <p>Surveys</p>
    <p>Adversarial Attacks: [Zhang et al, arXiv 2019] link</p>
    <p>Analysis Methods: [ Belinkov, Glass, TAACL 2019 ] link</p>
    <p>Sameer Singh, NAACL 2019 Tutorial 42</p>
    <p>More Loosely Related Work  Anchors: [Ribeiro et al, AAAI 2018 ] link  Input Reduction: [Feng et al, EMNLP 2018 ] link  Graph Embeddings: [ Pezeshkpour et. al. NAACL 19 ] link</p>
    <p>SEARs: [Ribeiro et al, ACL 2018 ] link  Genetic Algo: [ Alzantot et. al. EMNLP 2018 ] link  Discrete Attacks: [Lei et al SysML 2019] link</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Sameer Singh sameer@uci.edu</p>
    <p>@sameer_</p>
    <p>Sameersingh.org</p>
    <p>Work with Matt Gardner and me</p>
    <p>as part of</p>
    <p>The Allen Institute for Artificial Intelligence</p>
    <p>in Irvine, CA</p>
    <p>All levels: pre-docs, PhD interns, postdocs, and research scientists!</p>
  </div>
</Presentation>
