<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Remote Core Locking</p>
    <p>Migrating Critical-Section Execution to Improve the Performance of Multithreaded Applications</p>
    <p>to appear at USENIX ATC12</p>
    <p>Jean-Pierre Lozi LIP6/INRIA</p>
    <p>Florian David LIP6/INRIA</p>
    <p>Gal Thomas LIP6/INRIA</p>
    <p>Julia Lawall LIP6/INRIA</p>
    <p>Gilles Muller LIP6/INRIA</p>
  </div>
  <div class="page">
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>Memcached/GET Memcached/SET</p>
    <p>Problem: scalability  Many legacy applications dont scale well on multicore architectures  For instance, Memcached (GET/SET requests):</p>
    <p>H ig</p>
    <p>he r i</p>
    <p>s be</p>
    <p>tte r&quot;</p>
  </div>
  <div class="page">
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>Memcached/GET Memcached/SET</p>
    <p>Problem: scalability  Many legacy applications dont scale well on multicore architectures  For instance, Memcached (GET/SET requests):</p>
    <p>Collapses!</p>
    <p>Experiments run on a 48-core, magny-cours x86 AMD machine</p>
    <p>H ig</p>
    <p>he r i</p>
    <p>s be</p>
    <p>tte r&quot;</p>
  </div>
  <div class="page">
    <p>Why?  Critical sections = bottleneck on multicore architectures  High contention  lock acquisition is costly</p>
    <p>More cores  more contention</p>
    <p>SPLASH-2/Radiosity&quot; SPLASH-2/Raytrace&quot; Phoenix2/LG&quot; Phoenix2/SM&quot; Phoenix2/MM&quot; Memcached/GET&quot; Memcached/SET&quot; Berkeley DB/OS&quot; Berkeley DB/SL&quot;</p>
    <p>Number of cores&quot;</p>
    <p>% o</p>
    <p>f t im</p>
    <p>e sp</p>
    <p>en t i</p>
    <p>n cr</p>
    <p>iti ca</p>
    <p>l s ec</p>
    <p>tio n*</p>
  </div>
  <div class="page">
    <p>E xe</p>
    <p>cu tio</p>
    <p>n tim</p>
    <p>e (c</p>
    <p>yc le</p>
    <p>s)</p>
    <p>Delay (cycles)</p>
    <p>Better resistance to contention  No need to redesign the application  Custom microbenchmark to compare locks:</p>
    <p>Solution: designing better locks</p>
    <p>Lo w</p>
    <p>er is</p>
    <p>b et</p>
    <p>te r&quot;</p>
    <p>Higher contention&quot; Lower contention &quot;</p>
    <p>MCS &quot; [Mellor-Crummey91] &quot;</p>
    <p>CAS spinlock &quot;</p>
    <p>Critical sections access 5 cache lines each&quot;</p>
  </div>
  <div class="page">
    <p>E xe</p>
    <p>cu tio</p>
    <p>n tim</p>
    <p>e (c</p>
    <p>yc le</p>
    <p>s)</p>
    <p>Delay (cycles)</p>
    <p>Better resistance to contention  No need to redesign the application  Custom microbenchmark to compare locks:</p>
    <p>Improvement!</p>
    <p>Solution: designing better locks</p>
    <p>Lo w</p>
    <p>er is</p>
    <p>b et</p>
    <p>te r&quot;</p>
    <p>Higher contention&quot; Lower contention &quot;</p>
    <p>MCS &quot; [Mellor-Crummey91] &quot;</p>
    <p>CAS spinlock &quot;</p>
    <p>Critical sections access 5 cache lines each&quot;</p>
  </div>
  <div class="page">
    <p>Objective: remove atomic instructions and reduce cache misses  Execute contended critical sections on a dedicated server core  Very fast transfer of control, no sync on global variable</p>
    <p>Faster than lock acquisitions when contention is high  Shared data remains on server core  fewer cache misses</p>
    <p>Remote Core Locking</p>
  </div>
  <div class="page">
    <p>Objective: remove atomic instructions and reduce cache misses  Execute contended critical sections on a dedicated server core  Very fast transfer of control, no sync on global variable</p>
    <p>Faster than lock acquisitions when contention is high  Shared data remains on server core  fewer cache misses</p>
    <p>Remote Core Locking</p>
  </div>
  <div class="page">
    <p>Objective: remove atomic instructions and reduce cache misses  Execute contended critical sections on a dedicated server core  Very fast transfer of control, no sync on global variable</p>
    <p>Faster than lock acquisitions when contention is high  Shared data remains on server core  fewer cache misses</p>
    <p>Remote Core Locking</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>Implementation: general idea</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4!</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;lock4!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>cache miss</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>Server executes critical section&quot;cache miss</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>NULL! Server executes critical section&quot;cache miss</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>NULL!cache miss</p>
    <p>cache miss</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>NULL!cache miss</p>
    <p>cache miss</p>
    <p>cache miss</p>
    <p>Client resumes execution&quot;</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>NULL!cache miss</p>
    <p>cache miss</p>
    <p>cache miss</p>
  </div>
  <div class="page">
    <p>Implementation based on cache line-sized mailboxes  Three fields: lock, context, function</p>
    <p>Client fills the field and waits for the function to be reset  Server loops across the fields</p>
    <p>&amp;foo!&amp;lock4! 0xa0dc5f3a!</p>
    <p>Implementation: general idea</p>
    <p>Client thread 2 wants to execute a critical section protected by lock4! Server continuously checks mailboxes and executes critical sections!</p>
    <p>NULL!cache miss</p>
    <p>cache miss</p>
    <p>cache miss</p>
    <p>Total = 3 cache misses only! No atomic instruction (CAS)!!</p>
  </div>
  <div class="page">
    <p>E xe</p>
    <p>cu tio</p>
    <p>n tim</p>
    <p>e (c</p>
    <p>yc le</p>
    <p>s)</p>
    <p>Delay (cycles)</p>
    <p>Performance</p>
    <p>CAS spinlock &quot;</p>
    <p>MCS &quot;</p>
    <p>RCL &quot;</p>
    <p>Higher contention&quot; Lower contention &quot;</p>
    <p>Lo w</p>
    <p>er is</p>
    <p>b et</p>
    <p>te r&quot;</p>
  </div>
  <div class="page">
    <p>E xe</p>
    <p>cu tio</p>
    <p>n tim</p>
    <p>e (c</p>
    <p>yc le</p>
    <p>s)</p>
    <p>Delay (cycles)</p>
    <p>Performance</p>
    <p>CAS spinlock &quot;</p>
    <p>MCS &quot;</p>
    <p>RCL &quot;</p>
    <p>Higher contention&quot; Lower contention &quot;</p>
    <p>Lo w</p>
    <p>er is</p>
    <p>b et</p>
    <p>te r&quot;</p>
    <p>Improvement!</p>
  </div>
  <div class="page">
    <p>E xe</p>
    <p>cu tio</p>
    <p>n tim</p>
    <p>e (c</p>
    <p>yc le</p>
    <p>s)</p>
    <p>Delay (cycles)</p>
    <p>CAS spinlock &quot;</p>
    <p>MCS &quot;</p>
    <p>RCL &quot;</p>
    <p>POSIX &quot; Flat Combining &quot;</p>
    <p>[Hendler10] &quot; &quot;</p>
    <p>Higher contention&quot; Lower contention &quot;</p>
    <p>Lo w</p>
    <p>er is</p>
    <p>b et</p>
    <p>te r&quot;</p>
    <p>Performance</p>
  </div>
  <div class="page">
    <p>Using RCL in legacy applications (1)</p>
    <p>RCL Runtime :  Handles blocking in critical sections (I/O, page faults)</p>
    <p>Pool of servicing threads on server  Able to service other (independent) critical sections when blocked</p>
    <p>Makes it possible to use condition variables (cond/wait)  Used by ~50% of applications that use POSIX locks in Debian 6.0.3</p>
  </div>
  <div class="page">
    <p>Reengineering:</p>
    <p>Critical sections must be encapsulated into functions  Local variables sent as parameters (context)</p>
    <p>Using RCL in legacy applications (2)</p>
  </div>
  <div class="page">
    <p>Reengineering:</p>
    <p>Critical sections must be encapsulated into functions  Local variables sent as parameters (context)</p>
    <p>Using RCL in legacy applications (2)</p>
    <p>void func(void) {! int a, b, x;! ! a = ;! ! pthread_mutex_lock();! a = f(a);! f(b);! pthread_mutex_unlock();! ! }!</p>
    <p>struct context { int a, b };! ! void func(void) {!</p>
    <p>!struct context c;! !int x;! !! !c.a = ;! !! !execute_rcl(__cs, &amp;c);! !!</p>
    <p>}! ! void __cs(struct context *c) {!</p>
    <p>!c-&gt;a = f(c-&gt;a)! !f(c-&gt;b)!</p>
    <p>}!</p>
  </div>
  <div class="page">
    <p>Reengineering:</p>
    <p>Critical sections must be encapsulated into functions  Local variables sent as parameters (context)</p>
    <p>Using RCL in legacy applications (2)</p>
    <p>void func(void) {! int a, b, x;! ! a = ;! ! pthread_mutex_lock();! a = f(a);! f(b);! pthread_mutex_unlock();! ! }!</p>
    <p>struct context { int a, b };! ! void func(void) {!</p>
    <p>!struct context c;! !int x;! !! !c.a = ;! !! !execute_rcl(__cs, &amp;c);! !!</p>
    <p>}! ! void __cs(struct context *c) {!</p>
    <p>!c-&gt;a = f(c-&gt;a)! !f(c-&gt;b)!</p>
    <p>}!</p>
  </div>
  <div class="page">
    <p>Reengineering:</p>
    <p>Critical sections must be encapsulated into functions  Local variables sent as parameters (context)</p>
    <p>Using RCL in legacy applications (2)</p>
    <p>void func(void) {! int a, b, x;! ! a = ;! ! pthread_mutex_lock();! a = f(a);! f(b);! pthread_mutex_unlock();! ! }!</p>
    <p>struct context { int a, b };! ! void func(void) {!</p>
    <p>!struct context c;! !int x;! !! !c.a = ;! !! !execute_rcl(__cs, &amp;c);! !!</p>
    <p>}! ! void __cs(struct context *c) {!</p>
    <p>!c-&gt;a = f(c-&gt;a)! !f(c-&gt;b)!</p>
    <p>}!</p>
  </div>
  <div class="page">
    <p>Reengineering:</p>
    <p>Critical sections must be encapsulated into functions  Local variables sent as parameters (context)</p>
    <p>Tool to reengineer applications automatically</p>
    <p>Possible to pick which locks use RCL  To avoid false serialization:</p>
    <p>Possible to pick which server(s) handle which lock(s).</p>
    <p>Using RCL in legacy applications (2)</p>
    <p>void func(void) {! int a, b, x;! ! a = ;! ! pthread_mutex_lock();! a = f(a);! f(b);! pthread_mutex_unlock();! ! }!</p>
    <p>struct context { int a, b };! ! void func(void) {!</p>
    <p>!struct context c;! !int x;! !! !c.a = ;! !! !execute_rcl(__cs, &amp;c);! !!</p>
    <p>}! ! void __cs(struct context *c) {!</p>
    <p>!c-&gt;a = f(c-&gt;a)! !f(c-&gt;b)!</p>
    <p>}!</p>
  </div>
  <div class="page">
    <p>Reengineering:</p>
    <p>Critical sections must be encapsulated into functions  Local variables sent as parameters (context)</p>
    <p>Tool to reengineer applications automatically</p>
    <p>Possible to pick which locks use RCL  To avoid false serialization:</p>
    <p>Possible to pick which server(s) handle which lock(s).</p>
    <p>Using RCL in legacy applications (2)</p>
  </div>
  <div class="page">
    <p>% o</p>
    <p>f t im</p>
    <p>e in</p>
    <p>C S</p>
    <p>Delay (cycles)</p>
    <p>All other locks have collapsed (60000 cycles): 70%</p>
    <p>Collapse of POSIX (120000 cycles): 20%</p>
    <p>% of time in CS</p>
    <p>Profiling:  Custom profiler to find good candidates  Metric: time spent in critical sections  Running the profiler on the microbenchmark shows that:</p>
    <p>If time spent in CS &gt; 20%, RCL is more efficient than POSIX locks  If time spent in CS &gt; 70%, RCL is more efficient than all other locks</p>
    <p>Using RCL in legacy applications (3)</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Benchmarks (highly contended  70% time spent in CS):</p>
    <p>SPLASH-2 benchmark suite  3 applications out of 10 are highly contended</p>
    <p>Phoenix2 benchmark suite  3 applications out of 7 are highly contended</p>
    <p>Memcached  Highly contended with the GET workload</p>
    <p>Berkeley DB / TPC-C  Highly contended with 2 workloads (Order Status, Stock Level)</p>
  </div>
  <div class="page">
    <p>Memcached Set</p>
    <p>Raytrace Balls4</p>
    <p>String Match</p>
    <p>Linear Regression</p>
    <p>Memcached Get</p>
    <p>Radiosity Raytrace Car</p>
    <p>Matrix Multiply</p>
    <p>B es</p>
    <p>t p er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>re la</p>
    <p>tiv e</p>
    <p>to b</p>
    <p>es t P</p>
    <p>O S</p>
    <p>IX p</p>
    <p>er fo</p>
    <p>rm an</p>
    <p>ce POSIX</p>
    <p>x 1 . 9 : 3</p>
    <p>x 2 5 . 8 : 3 5</p>
    <p>x 1 2 . 8 : 3 5</p>
    <p>x 4 . 7 : 2 2</p>
    <p>x 4 . 8 : 1 0</p>
    <p>x 1 0 . 2 : 1 5</p>
    <p>x 3 . 6 : 5</p>
    <p>x 3 . 6 : 2 1</p>
    <p>CAS spinlock</p>
    <p>x 2 . 6 : 7</p>
    <p>x 2 4 . 4 : 3 1</p>
    <p>x 1 0 . 0 : 2 6</p>
    <p>x 4 . 1 : 1 1</p>
    <p>x 8 . 8 : 1 0</p>
    <p>x 1 0 . 5 : 1 3</p>
    <p>x 4 . 5 : 6</p>
    <p>x 3 . 5 : 8</p>
    <p>Flat Combining</p>
    <p>x 3 0 . 2 : 4 8</p>
    <p>x 1 5 . 7 : 4 1</p>
    <p>x 6 . 6 : 2 1</p>
    <p>x 1 6 . 1 : 3 6</p>
    <p>x 5 . 4 : 1 6</p>
    <p>x 5 . 1 : 1 8</p>
    <p>MCS</p>
    <p>x 2 . 5 : 1 1</p>
    <p>x 3 4 . 1 : 4 8</p>
    <p>x 1 5 . 9 : 4 2</p>
    <p>x 6 . 0 : 2 2</p>
    <p>x 1 0 . 3 : 1 6</p>
    <p>x 1 5 . 9 : 3 2</p>
    <p>x 4 . 8 : 1 1</p>
    <p>x 5 . 0 : 2 9</p>
    <p>RCL</p>
    <p>x 5 . 0 : 2 2 / 4</p>
    <p>x 3 4 . 5 : 4 8 / 4 0</p>
    <p>x 1 5 . 4 : 3 5 / 2 3</p>
    <p>x 9 . 5 : 2 2 / 7</p>
    <p>x 1 0 . 6 : 1 5 / 1 2</p>
    <p>x 2 3 . 6 : 4 6 / 1 8</p>
    <p>x 1 2 . 2 : 3 1 / 7</p>
    <p>x 5 . 8 : 2 0 / 7</p>
    <p>Better performance and scalability when time in CS &gt; 70%  Performance improvement correlated with time in CS</p>
    <p>Only one or two locks replaced each time</p>
    <p>% in CS:! 55% (many DCMs)&quot;</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Evaluation results (1)</p>
  </div>
  <div class="page">
    <p>Memcached Set</p>
    <p>Raytrace Balls4</p>
    <p>String Match</p>
    <p>Linear Regression</p>
    <p>Memcached Get</p>
    <p>Radiosity Raytrace Car</p>
    <p>Matrix Multiply</p>
    <p>B es</p>
    <p>t p er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>re la</p>
    <p>tiv e</p>
    <p>to b</p>
    <p>es t P</p>
    <p>O S</p>
    <p>IX p</p>
    <p>er fo</p>
    <p>rm an</p>
    <p>ce POSIX</p>
    <p>x 1 . 9 : 3</p>
    <p>x 2 5 . 8 : 3 5</p>
    <p>x 1 2 . 8 : 3 5</p>
    <p>x 4 . 7 : 2 2</p>
    <p>x 4 . 8 : 1 0</p>
    <p>x 1 0 . 2 : 1 5</p>
    <p>x 3 . 6 : 5</p>
    <p>x 3 . 6 : 2 1</p>
    <p>CAS spinlock</p>
    <p>x 2 . 6 : 7</p>
    <p>x 2 4 . 4 : 3 1</p>
    <p>x 1 0 . 0 : 2 6</p>
    <p>x 4 . 1 : 1 1</p>
    <p>x 8 . 8 : 1 0</p>
    <p>x 1 0 . 5 : 1 3</p>
    <p>x 4 . 5 : 6</p>
    <p>x 3 . 5 : 8</p>
    <p>Flat Combining</p>
    <p>x 3 0 . 2 : 4 8</p>
    <p>x 1 5 . 7 : 4 1</p>
    <p>x 6 . 6 : 2 1</p>
    <p>x 1 6 . 1 : 3 6</p>
    <p>x 5 . 4 : 1 6</p>
    <p>x 5 . 1 : 1 8</p>
    <p>MCS</p>
    <p>x 2 . 5 : 1 1</p>
    <p>x 3 4 . 1 : 4 8</p>
    <p>x 1 5 . 9 : 4 2</p>
    <p>x 6 . 0 : 2 2</p>
    <p>x 1 0 . 3 : 1 6</p>
    <p>x 1 5 . 9 : 3 2</p>
    <p>x 4 . 8 : 1 1</p>
    <p>x 5 . 0 : 2 9</p>
    <p>RCL</p>
    <p>x 5 . 0 : 2 2 / 4</p>
    <p>x 3 4 . 5 : 4 8 / 4 0</p>
    <p>x 1 5 . 4 : 3 5 / 2 3</p>
    <p>x 9 . 5 : 2 2 / 7</p>
    <p>x 1 0 . 6 : 1 5 / 1 2</p>
    <p>x 2 3 . 6 : 4 6 / 1 8</p>
    <p>x 1 2 . 2 : 3 1 / 7</p>
    <p>x 5 . 8 : 2 0 / 7</p>
    <p>Better performance and scalability when time in CS &gt; 70%  Performance improvement correlated with time in CS</p>
    <p>Only one or two locks replaced each time</p>
    <p>% in CS:! 55% (many DCMs)&quot;</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Evaluation results (1)</p>
  </div>
  <div class="page">
    <p>Memcached Set</p>
    <p>Raytrace Balls4</p>
    <p>String Match</p>
    <p>Linear Regression</p>
    <p>Memcached Get</p>
    <p>Radiosity Raytrace Car</p>
    <p>Matrix Multiply</p>
    <p>B es</p>
    <p>t p er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>re la</p>
    <p>tiv e</p>
    <p>to b</p>
    <p>es t P</p>
    <p>O S</p>
    <p>IX p</p>
    <p>er fo</p>
    <p>rm an</p>
    <p>ce POSIX</p>
    <p>x 1 . 9 : 3</p>
    <p>x 2 5 . 8 : 3 5</p>
    <p>x 1 2 . 8 : 3 5</p>
    <p>x 4 . 7 : 2 2</p>
    <p>x 4 . 8 : 1 0</p>
    <p>x 1 0 . 2 : 1 5</p>
    <p>x 3 . 6 : 5</p>
    <p>x 3 . 6 : 2 1</p>
    <p>CAS spinlock</p>
    <p>x 2 . 6 : 7</p>
    <p>x 2 4 . 4 : 3 1</p>
    <p>x 1 0 . 0 : 2 6</p>
    <p>x 4 . 1 : 1 1</p>
    <p>x 8 . 8 : 1 0</p>
    <p>x 1 0 . 5 : 1 3</p>
    <p>x 4 . 5 : 6</p>
    <p>x 3 . 5 : 8</p>
    <p>Flat Combining</p>
    <p>x 3 0 . 2 : 4 8</p>
    <p>x 1 5 . 7 : 4 1</p>
    <p>x 6 . 6 : 2 1</p>
    <p>x 1 6 . 1 : 3 6</p>
    <p>x 5 . 4 : 1 6</p>
    <p>x 5 . 1 : 1 8</p>
    <p>MCS</p>
    <p>x 2 . 5 : 1 1</p>
    <p>x 3 4 . 1 : 4 8</p>
    <p>x 1 5 . 9 : 4 2</p>
    <p>x 6 . 0 : 2 2</p>
    <p>x 1 0 . 3 : 1 6</p>
    <p>x 1 5 . 9 : 3 2</p>
    <p>x 4 . 8 : 1 1</p>
    <p>x 5 . 0 : 2 9</p>
    <p>RCL</p>
    <p>x 5 . 0 : 2 2 / 4</p>
    <p>x 3 4 . 5 : 4 8 / 4 0</p>
    <p>x 1 5 . 4 : 3 5 / 2 3</p>
    <p>x 9 . 5 : 2 2 / 7</p>
    <p>x 1 0 . 6 : 1 5 / 1 2</p>
    <p>x 2 3 . 6 : 4 6 / 1 8</p>
    <p>x 1 2 . 2 : 3 1 / 7</p>
    <p>x 5 . 8 : 2 0 / 7</p>
    <p>Better performance and scalability when time in CS &gt; 70%  Performance improvement correlated with time in CS</p>
    <p>Only one or two locks replaced each time</p>
    <p>% in CS:! 55% (many DCMs)&quot;</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Evaluation results (1)</p>
  </div>
  <div class="page">
    <p>Order Status Stock Level 0</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>re la</p>
    <p>tiv e</p>
    <p>to b</p>
    <p>as e</p>
    <p>ap pl</p>
    <p>ic at</p>
    <p>io n</p>
    <p>(1 00</p>
    <p>c lie</p>
    <p>nt s)</p>
    <p>Base POSIX</p>
    <p>CAS spinlock</p>
    <p>Flat Combining MCS-TP</p>
    <p>RCL</p>
    <p>x 1 . 4</p>
    <p>x 1 . 4</p>
    <p>x 0 . 1</p>
    <p>x 0 . 2</p>
    <p>x 1 . 0</p>
    <p>x 1 . 6</p>
    <p>x 0 . 9</p>
    <p>x 1 . 4</p>
    <p>x 4 . 3 / 1 0</p>
    <p>x 7 . 7 / 1 0</p>
    <p>Berkeley DB with TPC-C (100 clients)  Large gains, % in CS underestimated</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Evaluation results (2)</p>
  </div>
  <div class="page">
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>POSIX CAS spinlock</p>
    <p>MCS RCL</p>
    <p>Memcached, SET requests:</p>
    <p>when benchmarking other types of locks. The remaining 47 cores each run a client, i.e., a thread that executes critical sections. Each client waits for a given delay between the end of one critical section and the beginning of the next one: the shorter the delay, the higher the contention. For each delay value, 1000 critical sections are executed. In each critical section, a client references and updates a given number of shared cache lines by incrementing the values in shared memory locations. These locations are scattered across memory in such a way that two such locations are never mapped to the same cache line: thus, to access n shared cache lines, the microbenchmark simply accesses n shared memory locations. In order to ensure that cache line accesses are not pipelined, we construct the address of the next memory location that is accessed using the value read from the current memory location [32].</p>
    <p>The results for critical section execution time are shown in Figure 7(a). Under high contention (the left side of the graph), RCL is always faster than all the other considered types of locks. Flat combining is the best after RCL, but is still 2.5 times slower. MCS is slower than flat combining. Due to the fact that each critical section is executed locally, its performance decreases significantly when 5 cache lines are accessed. The traditional spinlock is the slowest of all locks under high contention, due to the overhead of cache coherency messages when all threads spin on a compare-andswap instruction. Finally, POSIX locks are as efficient as MCS locks under very high contention, but their execution time increases as contention decreases.</p>
    <p>When contention is low (the right side of Figure 7(a)) and the critical section only accesses one cache line, spinlocks, MCS locks and RCL have similar performance. Spinlocks are best with a critical section execution time of 1350 cycles; RCL is the next best and is only 13% slower. However, when critical sections access 5 cache lines, the execution time of both traditional spinlocks and MCS locks increases significantly, whereas that of RCL remains stable. This is due to the fact that all critical sections are executed on the same core, thus improving cache locality. The execution time of flat combining also remains stable when the number of memory accesses increases, but it is more than ten times higher than that of RCL. POSIX locks perform better than flat combining but not as well as spinlocks, MCS and RCL.</p>
    <p>Figure 7(b) shows the number of L2 cache misses per critical section for each lock. The execution time of each lock is directly correlated with its number of cache misses, except for the POSIX locks, whose overhead is mainly due to the high cost of context switches. Even though the number of cache misses increases as the contention increases for both spinlocks and MCS locks, it remains stable for RCL, which shows how well RCL suited is for highlycontended locks. The number of cache misses when using flat combining increases as the contention decreases, which directly reflects the high execution time of flat combining</p>
    <p>under low contention. These cache misses are caused by the server when it scans the linked list of requests: accessing each element of this linked list typically incurs a cache miss. This scan also occurs at high contention, but several critical sections are also executed simultaneously, while at low contention, only one critical section is executed for the same number of cache misses.</p>
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>(a) Raytrace with the scene</p>
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>(b) Raytrace with the scene</p>
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>(c) Radiosity</p>
    <p>S pe</p>
    <p>ed up</p>
    <p>Number of cores</p>
    <p>posix spinlock mcs flat rcl</p>
    <p>Figure 8. SPLASH-2 results. Each data point is the average of 30 runs.</p>
    <p>As presented in Section 3.1, among all the locks used in SPLASH-2, we have identified only three as having high enough contention to be interesting candidates for RCL:</p>
    <p>RCL Scalability (1)</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
  </div>
  <div class="page">
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (t</p>
    <p>ra ns</p>
    <p>ac tio</p>
    <p>ns /s</p>
    <p>)</p>
    <p>Number of clients (1 client = 1 thread)</p>
    <p>Base POSIX</p>
    <p>CAS spinlock</p>
    <p>Flat Combining MCS</p>
    <p>MCS-TP</p>
    <p>RCL</p>
    <p>RCL Scalability (2)</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Berkeley DB / TPC-C, Stock Level requests:</p>
  </div>
  <div class="page">
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (t</p>
    <p>ra ns</p>
    <p>ac tio</p>
    <p>ns /s</p>
    <p>)</p>
    <p>Number of clients (1 client = 1 thread)</p>
    <p>MCS RCL</p>
    <p>RCL Scalability (2)</p>
    <p>Collapse!</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Berkeley DB / TPC-C, Stock Level requests:</p>
  </div>
  <div class="page">
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (t</p>
    <p>ra ns</p>
    <p>ac tio</p>
    <p>ns /s</p>
    <p>)</p>
    <p>Number of clients (1 client = 1 thread)</p>
    <p>MCS-TP MCS RCL</p>
    <p>RCL Scalability (2)</p>
    <p>Collapse!</p>
    <p>H ig</p>
    <p>he r</p>
    <p>is b</p>
    <p>et te</p>
    <p>r</p>
    <p>Berkeley DB / TPC-C, Stock Level requests:</p>
  </div>
  <div class="page">
    <p>RCL reduces lock acquisition time and improves data locality</p>
    <p>Profiler to detect when RCL can be useful</p>
    <p>Tool to ease the transformation of legacy code</p>
    <p>Future work: adaptive RCL runtime  Dynamically switch between locking strategies  Load balancing between servers</p>
    <p>Conclusion</p>
  </div>
</Presentation>
