<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Breaking NLI Systems with Sentences that Require Simple Lexical Inferences</p>
    <p>Max Glockner1, Vered Shwartz2 and Yoav Goldberg2</p>
    <p>July 18, 2018</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013])</p>
    <p>Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street  ENTAILMENTENTAILMENT</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street</p>
    <p>ENTAILMENTENTAILMENT</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street  ENTAILMENTENTAILMENT</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street  ENTAILMENTENTAILMENT</p>
    <p>NEUTRALNEUTRAL</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street  ENTAILMENTENTAILMENT</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street  ENTAILMENTENTAILMENT</p>
    <p>CONTRADICTIONCONTRADICTION</p>
    <p>Event co-reference assumption</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>SNLI [Bowman et al., 2015] A large scale dataset for NLI (Natural Language Inference; Recognizing Textual Entailment [Dagan et al., 2013]) Premises are image captions, hypotheses generated by crowdsourcing workers:</p>
    <p>Premise Street performer is doing his act for kids</p>
    <p>Hypotheses 1. A person performing for children on the street  ENTAILMENTENTAILMENT</p>
    <p>Event co-reference assumption Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 2 / 13</p>
  </div>
  <div class="page">
    <p>Neural NLI Models</p>
    <p>End-to-end, either sentence-encoding or attention-based</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Attention</p>
    <p>Lexical knowledge: only from pre-trained word embeddings As opposed to using resources like WordNet</p>
    <p>SOTA exceeds human performance... 1</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 3 / 13</p>
  </div>
  <div class="page">
    <p>Neural NLI Models</p>
    <p>End-to-end, either sentence-encoding or attention-based</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Attention</p>
    <p>Lexical knowledge: only from pre-trained word embeddings As opposed to using resources like WordNet</p>
    <p>SOTA exceeds human performance... 1</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 3 / 13</p>
  </div>
  <div class="page">
    <p>Neural NLI Models</p>
    <p>End-to-end, either sentence-encoding or attention-based</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Attention</p>
    <p>Lexical knowledge: only from pre-trained word embeddings As opposed to using resources like WordNet</p>
    <p>SOTA exceeds human performance... 1</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 3 / 13</p>
  </div>
  <div class="page">
    <p>Neural NLI Models</p>
    <p>End-to-end, either sentence-encoding or attention-based</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Attention</p>
    <p>Lexical knowledge: only from pre-trained word embeddings As opposed to using resources like WordNet</p>
    <p>SOTA exceeds human performance...</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 3 / 13</p>
  </div>
  <div class="page">
    <p>Neural NLI Models</p>
    <p>End-to-end, either sentence-encoding or attention-based</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Attention</p>
    <p>Lexical knowledge: only from pre-trained word embeddings As opposed to using resources like WordNet</p>
    <p>SOTA exceeds human performance... 1</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 3 / 13</p>
  </div>
  <div class="page">
    <p>Neural NLI Models</p>
    <p>End-to-end, either sentence-encoding or attention-based</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Label</p>
    <p>Classifier</p>
    <p>Extract Features</p>
    <p>Premise Encoder</p>
    <p>Hypothesis Encoder</p>
    <p>Premise Hypothesis</p>
    <p>Attention</p>
    <p>Lexical knowledge: only from pre-trained word embeddings As opposed to using resources like WordNet</p>
    <p>SOTA exceeds human performance... 1</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 3 / 13</p>
  </div>
  <div class="page">
    <p>Do neural NLI models implicitly learn lexical semantic relations?</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 4 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question</p>
    <p>Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set</p>
    <p>Hypothesis: Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings</p>
    <p>Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>New Test Set We constructed a new test set to answer this question Premise: sentences from the SNLI training set Hypothesis:</p>
    <p>Replacing a single term w in the premise with a related term w</p>
    <p>w is in the SNLI vocabulary and in pre-trained embeddings Crowdsourcing labels (mostly contradictions!)</p>
    <p>Contradiction The man is holding a saxophone  The man is holding an electric guitar</p>
    <p>Entailment A little girl is very sad  A little girl is very unhappy</p>
    <p>Neutral A couple drinking wine  A couple drinking champagne</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 5 / 13</p>
  </div>
  <div class="page">
    <p>Evaluation Setting</p>
    <p>Train on SNLI training set, test on the original &amp; new test set In the paper: enhancing with additional existing datasets</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 6 / 13</p>
  </div>
  <div class="page">
    <p>Evaluation Setting</p>
    <p>Train on SNLI training set, test on the original &amp; new test set In the paper: enhancing with additional existing datasets</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 6 / 13</p>
  </div>
  <div class="page">
    <p>Results Can neural NLI models recognize lexical inferences?</p>
    <p>Decomposable Attention ESIM Residual-Stacked-Encoder 0</p>
    <p>SNLI Test Set</p>
    <p>New Test Set</p>
    <p>Dramatic drop in performance across models. Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 7 / 13</p>
  </div>
  <div class="page">
    <p>Sanity Check Performance of WordNet-informed Models</p>
    <p>KIM [Chen et al., 2018]</p>
    <p>WordNet baseline</p>
    <p>The test set is solvable using WordNet.</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 8 / 13</p>
  </div>
  <div class="page">
    <p>What do neural NLI models learn with respect to lexical semantic relations?</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 9 / 13</p>
  </div>
  <div class="page">
    <p>Analysis 1: Word Similarity Models err on contradicting word-pairs with similar embeddings</p>
    <p>A man starts his day in India  A man starts his day in Malaysia</p>
    <p>Especially for fixed word embeddings</p>
    <p>Cosine Similarity of (word, replacement)</p>
    <p>D ec om po sa bl e A tt en ti on A cc ur ac y</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 10 / 13</p>
  </div>
  <div class="page">
    <p>Analysis 1: Word Similarity Models err on contradicting word-pairs with similar embeddings</p>
    <p>A man starts his day in India  A man starts his day in Malaysia Especially for fixed word embeddings</p>
    <p>Cosine Similarity of (word, replacement)</p>
    <p>D ec om po sa bl e A tt en ti on A cc ur ac y</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 10 / 13</p>
  </div>
  <div class="page">
    <p>Analysis 2: Frequency in Training</p>
    <p>Tuning embeddings may associate specific (word, replacement) pairs to a label, e.g. (man, woman)  contradiction</p>
    <p>Accuracy increases with frequency in training set</p>
    <p>Frequency of (word, replacement) pairs in contradiction training examples</p>
    <p>ES IM A cc ur ac y</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 11 / 13</p>
  </div>
  <div class="page">
    <p>Analysis 2: Frequency in Training</p>
    <p>Tuning embeddings may associate specific (word, replacement) pairs to a label, e.g. (man, woman)  contradiction Accuracy increases with frequency in training set</p>
    <p>Frequency of (word, replacement) pairs in contradiction training examples</p>
    <p>ES IM A cc ur ac y</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 11 / 13</p>
  </div>
  <div class="page">
    <p>Recap</p>
    <p>New NLI test set that evaluates systems ability to make inferences that require very simple lexical knowledge</p>
    <p>SOTA systems perform poorly on the test set</p>
    <p>Systems are limited in their generalization ability</p>
    <p>May be used as a complementary test set to assess the lexical inference abilities of NLI systems</p>
    <p>Thank you!</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 12 / 13</p>
  </div>
  <div class="page">
    <p>Recap</p>
    <p>New NLI test set that evaluates systems ability to make inferences that require very simple lexical knowledge</p>
    <p>SOTA systems perform poorly on the test set</p>
    <p>Systems are limited in their generalization ability</p>
    <p>May be used as a complementary test set to assess the lexical inference abilities of NLI systems</p>
    <p>Thank you!</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 12 / 13</p>
  </div>
  <div class="page">
    <p>Recap</p>
    <p>New NLI test set that evaluates systems ability to make inferences that require very simple lexical knowledge</p>
    <p>SOTA systems perform poorly on the test set</p>
    <p>Systems are limited in their generalization ability</p>
    <p>May be used as a complementary test set to assess the lexical inference abilities of NLI systems</p>
    <p>Thank you!</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 12 / 13</p>
  </div>
  <div class="page">
    <p>Recap</p>
    <p>New NLI test set that evaluates systems ability to make inferences that require very simple lexical knowledge</p>
    <p>SOTA systems perform poorly on the test set</p>
    <p>Systems are limited in their generalization ability</p>
    <p>May be used as a complementary test set to assess the lexical inference abilities of NLI systems</p>
    <p>Thank you!</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 12 / 13</p>
  </div>
  <div class="page">
    <p>Recap</p>
    <p>New NLI test set that evaluates systems ability to make inferences that require very simple lexical knowledge</p>
    <p>SOTA systems perform poorly on the test set</p>
    <p>Systems are limited in their generalization ability</p>
    <p>May be used as a complementary test set to assess the lexical inference abilities of NLI systems</p>
    <p>Thank you! Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 12 / 13</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[Bowman et al., 2015] Bowman, S. R., Angeli, G., Potts, C., and Manning, D. C. (2015). A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632642. Association for Computational Linguistics.</p>
    <p>[Chen et al., 2018] Chen, Q., Zhu, X., Ling, Z.-H., Inkpen, D., and Wei, S. (2018). Neural natural language inference models enhanced with external knowledge. In The 56th Annual Meeting of the Association for Computational Linguistics (ACL), Melbourne, Australia.</p>
    <p>[Chen et al., 2017] Chen, Q., Zhu, X., Ling, Z.-H., Wei, S., Jiang, H., and Inkpen, D. (2017). Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16571668, Vancouver, Canada. Association for Computational Linguistics.</p>
    <p>[Dagan et al., 2013] Dagan, I., Roth, D., Sammons, M., and Zanzotto, F. M. (2013). Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1220.</p>
    <p>[Gururangan et al., 2018] Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S. R., and Smith, N. A. (2018). Annotation artifacts in natural language inference data. In The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), New Orleans, Louisiana.</p>
    <p>[Nie and Bansal, 2017] Nie, Y. and Bansal, M. (2017). Shortcut-stacked sentence encoders for multi-domain inference. arXiv preprint arXiv:1708.02312.</p>
    <p>[Parikh et al., 2016] Parikh, A., Tckstrm, O., Das, D., and Uszkoreit, J. (2016). A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 22492255, Austin, Texas. Association for Computational Linguistics.</p>
    <p>[Poliak et al., 2018] Poliak, A., Naradowsky, J., Haldar, A., Rudinger, R., and Van Durme, B. (2018). Hypothesis Only Baselines in Natural Language Inference. In Joint Conference on Lexical and Computational Semantics (StarSem).</p>
    <p>Max Glockner, Vered Shwartz and Yoav Goldberg Breaking NLI Systems with Sentences that Require Simple Lexical Inferences 13 / 13</p>
  </div>
</Presentation>
