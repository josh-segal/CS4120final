<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Best of Both Worlds Combining Recent Advances in</p>
    <p>Neural Machine Translation</p>
    <p>Mia Xu Chen* Orhan Firat* Ankur Bapna* Melvin Johnson Wolfgang Macherey George Foster Llion Jones Mike Schuster</p>
    <p>Noam Shazeer Niki Parmar Ashish Vaswani Jakob Uszkoreit Lukasz Kaiser Zhifeng Chen Yonghui Wu Macduff Hughes</p>
    <p>July 16, 2018 ACL18 Mebourne *Equal Contribution</p>
  </div>
  <div class="page">
    <p>The Best of Both Worlds P 2</p>
    <p>This is NOT an architecture search paper!</p>
  </div>
  <div class="page">
    <p>A Brief History of NMT Models</p>
    <p>P 3The Best of Both Worlds</p>
    <p>Sutskever et al. Cho et al. (Seq2Seq)</p>
    <p>Bahdanau et al. (Attention)</p>
    <p>Wu et al. (Google-NMT)</p>
    <p>Gehring et al. (Conv-Seq2Seq)</p>
    <p>Vaswani et al. (Transformer)</p>
    <p>Chen et al. (RNMT+ and Hybrids)</p>
    <p>: Data : Model : Hyperparameters</p>
  </div>
  <div class="page">
    <p>The Best of Both Worlds P 4</p>
    <p>The Best of Both Worlds - I Each new approach is:  accompanied by a set of modeling and training techniques.</p>
    <p>Goal: 1. Tease apart architectures and their accompanying techniques. 2. Identify key modeling and training techniques. 3. Apply them on RNN based Seq2Seq  RNMT+</p>
    <p>Conclusion:  RNMT+ outperforms all previous three approaches.</p>
  </div>
  <div class="page">
    <p>The Best of Both Worlds P 5</p>
    <p>The Best of Both Worlds - II Also, each new approach has:  a fundamental architecture (signature wiring of neural network).</p>
    <p>Goal: 1. Analyse properties of each architecture. 2. Combine their strengths. 3. Devise new hybrid architectures  Hybrids</p>
    <p>Conclusion:  Hybrids obtain further improvements over all the others.</p>
  </div>
  <div class="page">
    <p>RNN Based NMT - RNMT  Convolutional NMT - ConvS2S  Conditional Transformation Based NMT</p>
    <p>Transformer</p>
    <p>Project name P 6</p>
    <p>Building Blocks</p>
  </div>
  <div class="page">
    <p>GNMT - Wu et al.</p>
    <p>The Best of Both Worlds P 7</p>
    <p>Core Components:  RNNs  Attention (Additive)  biLSTM + uniLSTM  Deep residuals  Async Training</p>
    <p>Pros:  De facto standard  Modelling state space</p>
    <p>Cons:  Temporal dependence  Not enough gradients</p>
    <p>*Figure from Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation Wu et al. 2016</p>
  </div>
  <div class="page">
    <p>ConvS2S - Gehring et al.</p>
    <p>P 8</p>
    <p>Core Components:  Convolution - GLUs  Multi-hop attention  Positional embeddings  Careful initialization  Careful normalization  Sync Training</p>
    <p>Pros:  No temporal dependence  More interpretable than RNN  Parallel decoder outputs during training</p>
    <p>Cons:  Need to stack more to increase</p>
    <p>the receptive field</p>
    <p>*Figure from Convolutional Sequence to Sequence Learning Gehring et al. 2017</p>
  </div>
  <div class="page">
    <p>Transformer - Vaswani et al.</p>
    <p>P 9</p>
    <p>Core Components:  Self-Attention  Multi-headed attention  Layout: N-&gt;f()-&gt;D-&gt;R  Careful normalization  Careful batching  Sync training  Label Smoothing  Per-token loss  Learning rate schedule  Checkpoint Averaging</p>
    <p>Pros:  Gradients everywhere - faster optimization  Parallel encoding both training/inference</p>
    <p>Cons:  Combines many advances at once  Fragile</p>
    <p>*Figure from Attention is All You Need Vaswani et al. 2017</p>
  </div>
  <div class="page">
    <p>P 10</p>
    <p>The Best of Both Worlds - I: RNMT+</p>
    <p>The Best of Both Worlds</p>
    <p>The Architecture:</p>
    <p>Bi-directional encoder 6 x LSTM  Uni-directional decoder 8 x LSTM  Layer normalized LSTM cell</p>
    <p>Per-gate normalization  Multi-head attention</p>
    <p>4 heads  Additive (Bahdanau)</p>
    <p>attention</p>
  </div>
  <div class="page">
    <p>Model Comparison - I : BLEU Scores</p>
    <p>P 11The Best of Both Worlds</p>
    <p>WMT14 En-Fr WMT14 En-De (35M sentence pairs) (4.5M sentence pairs)</p>
    <p>RNMT+/ConvS2S: 32 GPUs, 4096 sentence pairs/batch.</p>
    <p>Transformer Base/Big: 16 GPUs, 65536 tokens/batch.</p>
  </div>
  <div class="page">
    <p>Model Comparison - II : Speed and Size</p>
    <p>P 12The Best of Both Worlds</p>
    <p>WMT14 En-Fr WMT14 En-De (35M sentence pairs) (4.5M sentence pairs)</p>
    <p>RNMT+/ConvS2S: 32 GPUs, 4096 sentence pairs/batch.</p>
    <p>Transformer Base/Big: 16 GPUs, 65536 tokens/batch.</p>
  </div>
  <div class="page">
    <p>Stability: Ablations</p>
    <p>P 13</p>
    <p>WMT14 En-Fr</p>
    <p>The Best of Both Worlds</p>
    <p>Evaluate importance of four key techniques:</p>
    <p>(especially with multi-head attention)</p>
    <p>learning-rate schedule</p>
    <p>* Indicates an unstable training run</p>
  </div>
  <div class="page">
    <p>P 14</p>
    <p>The Best of Both Worlds - II: Hybrids</p>
    <p>The Best of Both Worlds</p>
    <p>Strengths of each architecture:</p>
    <p>RNMT+  Highly expressive - continuous state space representation.</p>
    <p>Transformer  Full receptive field - powerful feature extractor.</p>
    <p>Combining individual architecture strengths:  Capture complementary information - Best of Both Worlds.</p>
    <p>Trainability - important concern with hybrids  Connections between different types of layers need to be carefully designed.</p>
  </div>
  <div class="page">
    <p>Encoder - Decoder Hybrids</p>
    <p>P 15</p>
    <p>Separation of roles:</p>
    <p>Decoder - conditional LM  Encoder - build feature representations</p>
    <p>Designed to contrast the roles. (last two rows)</p>
    <p>The Best of Both Worlds</p>
  </div>
  <div class="page">
    <p>Encoder Layer Hybrids</p>
    <p>P 16</p>
    <p>Improved feature extraction:</p>
    <p>Enrich stateful representations with global self-attention</p>
    <p>Increased capacity</p>
    <p>Details:</p>
    <p>Pre-trained components to improve trainability  Layer normalization at layer boundaries</p>
    <p>Cascaded Hybrid - vertical combination Multi-Column Hybrid - horizontal combination</p>
    <p>The Best of Both Worlds</p>
  </div>
  <div class="page">
    <p>Encoder Layer Hybrids</p>
    <p>P 17The Best of Both Worlds</p>
  </div>
  <div class="page">
    <p>Lessons Learnt</p>
    <p>P 18The Best of Both Worlds</p>
    <p>Need to separate other improvements from the architecture itself:  Your good ol architecture may shine with new modelling and training techniques  Stronger baselines (Denkowski and Neubig, 2017)</p>
    <p>Dull Teachers - Smart Students  A model with a sufficiently advanced lr-schedule is indistinguishable from magic.</p>
    <p>Understanding and Criticism  Hybrids have the potential, more than duct taping.  Game is on for the next generation of NMT architectures</p>
  </div>
  <div class="page">
    <p>https://ai.google/research/join-us/</p>
    <p>https://ai.google/research/join-us/ai-residency/</p>
    <p>The Best of Both Worlds</p>
    <p>Thank You</p>
    <p>Open source implementation coming soon!</p>
  </div>
</Presentation>
