<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Non-Collective Communicator Creation in MPI</p>
    <p>James Dinan1, Sriram Krishnamoorthy2, Pavan Balaji1, Jeff Hammond1, Manojkumar Krishnan2, Vinod Tipparaju2, and Abhinav Vishnu2</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Believed impossible to support on MPI</p>
    <p>Sriram Krishnamoorthy (PNNL), Theresa Windus (ISU)</p>
  </div>
  <div class="page">
    <p>MPI Communicators</p>
    <p>Core concept in MPI:  All communication is encapsulated within a communicator  Enables libraries that doesnt interfere with application</p>
    <p>Two types of communicators  Intracommunicator  Communicate within one group  Intercommunicator  Communicate between two groups</p>
    <p>Communicator creation is collective  Believed that non-collective creation cant be supported by MPI</p>
  </div>
  <div class="page">
    <p>Non-Collective Communicator Creation</p>
    <p>Create a communicator collectively only on new members</p>
    <p>Global Arrays process groups  Past: collectives using MPI Send/Recv</p>
    <p>Overhead reduction  Multi-level parallelism  Small communicators when parent is large</p>
    <p>Recovery from failures  Not all ranks in parent can participate</p>
    <p>Load balancing</p>
  </div>
  <div class="page">
    <p>Intercommunicator Creation</p>
    <p>Intercommunicator creation parameters  Local comm  All ranks participate  Peer comm  Communicator used to identify remote leader  Local leader  Local rank that is in both local and peer</p>
    <p>comms  Remote leader  Parent communicator containing peer</p>
    <p>group leader  Safe tag  Communicate safely on peer comm</p>
    <p>Intracommunicator Intercommunicator</p>
    <p>Local Remote</p>
  </div>
  <div class="page">
    <p>Non-Collective Communicator Creation Algorithm</p>
    <p>MPI_COMM_SELF (intracommunicator)</p>
    <p>MPI_Intercomm_create (intercommunicator)MPI_Intercomm_merge</p>
  </div>
  <div class="page">
    <p>Non-Collective Algorithm in Detail</p>
    <p>R</p>
    <p>Translate group ranks to ordered list of ranks on parent communicator</p>
    <p>Calculate my group ID</p>
    <p>Left group</p>
    <p>Right group</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>IBM Blue Gene/P at Argonne  Node: 4 Core, 850 MHz PowerPC 450 processor, 2GB memory  Rack: 1024 Nodes  40 Racks = 163,840 cores  IBM MPI, derived from MPICH2</p>
    <p>Currently limited to two racks  Bug in MPI intercommunicators; still looking into it</p>
  </div>
  <div class="page">
    <p>Evaluation: Microbenchmark</p>
    <p>O(log2 g)</p>
    <p>O(log p)</p>
  </div>
  <div class="page">
    <p>Case Study: Markov Chain Monte Carlo</p>
    <p>Dynamical nucleation theory Monte Carlo (DNTMC)  Part of NWChem  Markov chain Monte Carlo</p>
    <p>Multiple levels of parallelism  Multi-node Walker groups  Walker: N random samples  Concurrent, sequential traversals</p>
    <p>Load imbalance  Sample computation (energy calculation) time is irregular  Sample can be rejected</p>
    <p>T L Windus et al 2008 J. Phys.: Conf. Ser. 125 012017</p>
  </div>
  <div class="page">
    <p>MCMC Load Balancing</p>
    <p>Inner parallelism is malleable  Group computation scales as nodes added to walker group</p>
    <p>Regroup idle processes with active groups</p>
    <p>Preliminary results:  30% decrease in total application execution time</p>
  </div>
  <div class="page">
    <p>MCMC Benchmark Kernel</p>
    <p>Simple MPI-only benchmark</p>
    <p>Walker groups  Initial group size: G = 4</p>
    <p>Simple work distribution  Samples: S = (leader % 32) * 10,000  Sample time: ts = 100 ms / |G|  Weak scaling benchmark</p>
    <p>Load balancing  Join right when idle  Asynchronous: Check for merge request after each work unit  Synchronous: Collectively regroup every i samples</p>
  </div>
  <div class="page">
    <p>Evaluation: Load Balancing Benchmark Results</p>
  </div>
  <div class="page">
    <p>Proposed MPI Extensions</p>
    <p>Eliminate O(log g) factor by direct implementation</p>
    <p>Local Remote</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Algorithm for non-collective communicator creation  Recursive intercommunicator creation and merging  Benefits:</p>
    <p>Support for GA groups  Lower overhead for small communicators  Recovering from failures  Load balancing (MCMC benchmark)</p>
    <p>Cost is high, proposed extension to MPI  Multicommunicators  MPI_Group_comm_create()</p>
    <p>Contact: Jim Dinan -- dinan@mcs.anl.gov</p>
  </div>
  <div class="page">
    <p>Additional Slides</p>
  </div>
  <div class="page">
    <p>Asynchronous Load Balancing</p>
  </div>
  <div class="page">
    <p>Collective Load Balancing (i = 2)</p>
  </div>
  <div class="page">
    <p>Number of Regrouping Operations on 8k Cores</p>
    <p>Load Balancing</p>
    <p>i Average Sandard Deviation</p>
    <p>Min Max</p>
    <p>None 0.00 0.00 0 0</p>
    <p>Asynch. 14.38 3.54 5 26</p>
    <p>Collective 1 5.38 2.12 2 8</p>
    <p>Collective 2 5.38 2.12 2 8</p>
    <p>Collective 4 5.38 2.12 2 8</p>
    <p>Collective 8 5.38 2.12 2 8</p>
    <p>Collective 16 5.38 2.12 2 8</p>
    <p>Collective 32 5.38 2.12 2 8</p>
    <p>Collective 64 3.75 1.20 2 5</p>
    <p>Collective 128 2.62 0.48 2 3</p>
  </div>
</Presentation>
