<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Efficient Multithreaded Context ID Allocation in MPI</p>
    <p>James Dinan, David Goodell, William Gropp, Rajeev Thakur, and Pavan Balaji</p>
  </div>
  <div class="page">
    <p>Multithreading and MPI Communicators</p>
    <p>M PI_Init_thread( , M PI_THREAD_M ULTIPLE,  )</p>
    <p>MPI-2 defined MPI+Threads semantics  One collective per comunicator at a time  Programmer must coordinate across threads  Multiple collectives concurrently on different communicators</p>
    <p>Communicator creation:  Collective operation  Multiple can occur concurrently on different parent communicators  Requires allocation of a context id</p>
    <p>Unique integer, uniform across processes  Matches messages to communicators</p>
  </div>
  <div class="page">
    <p>MPI-3: Non-Collective Communicator Creation</p>
    <p>Communicator creation is collective only on new members, useful for: 1. Reduce overhead</p>
    <p>Small communicators when parent is large</p>
    <p>Implementable on top of MPI, performance is poor  Recursive intercommunicator creation/merging algorithm [IMUDI 12]  O(log G) create/merge steps  total O(log2 G) cost</p>
  </div>
  <div class="page">
    <p>MPI-3: MPI_COMM_CREATE_GROUP</p>
    <p>MPI_COMM_CREATE_GROUP(comm, group, tag, newcomm) IN comm intracommunicator (handle) IN group group, which is a subset of the group of comm (handle) IN tag safe tag (integer) OUT newcomm new communicator (handle)</p>
    <p>Tagged collective  Multiple threads can call concurrently on</p>
    <p>same parent communicator  Calls are distinguished via the tag argument</p>
    <p>Requires efficient, thread-safe context ID allocation</p>
  </div>
  <div class="page">
    <p>High-Level Context ID Allocation Algorithm</p>
    <p>Extending to support MPI_Comm_create_group  Use a tagged, group-collective allreduce  Tag is shifted into a tagged collectives tag space by setting a high bit  Avoids conflicts with point-to-point messages</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>Rank 0, my_cid_avail Rank 1, my_cid_avail Allocation Result</p>
  </div>
  <div class="page">
    <p>Ensuring Successful Allocation</p>
    <p>Deadlock avoidance:  Reserve( ) must be non-blocking, if mask is unavailable get dummy value  Avoid blocking indefinitely in Allreduce, may require multiple attempts</p>
    <p>Livelock avoidance:  All threads in group must acquire mask to allocate  data race  MPI_CC: Prioritize based on parent communicator context id  MPI_CCG: Prioritize based on &lt; context id, tag &gt; pair</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>w hile (m y_cid = = 0) 1. m y_cid_avail = reserve( ctxid_m ask ) 2. cid_avail = Allreduce( m y_cid_avail, parent_com m ) 3. m y_cid = select( cid_avail )</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>w hile (m y_cid = = 0) 1. m y_cid_avail = reserve( ctxid_m ask ) 2. cid_avail = Allreduce( m y_cid_avail, parent_com m ) 3. m y_cid = select( cid_avail )</p>
    <p>Rank 0, my_cid_avail Rank 1, my_cid_avail Allocation Result</p>
  </div>
  <div class="page">
    <p>Full Context ID Allocation Algorithm (MPICH Var.)</p>
    <p>/* Input: my_comm, my_group, my_tag. Output: integer context ID */ /* Shared variables ( shared by threads at a each process ) */ mask[MAX_CTXID] = { 1 } /* Bit array, indicates if ctx ID is free */ mask_in_use = 0 /* Flag, indicates if mask is in use */ lowest_ctx_id = MAXINT, lowest_tag /* Indicates which thread has priority */</p>
    <p>/* Private variables ( not shared across threads ) */ local_mask[MAX_CTXID] /* Thread private copy of the mask */ i_own_the_mask = 0 /* Flag indicating if this thread holds mask */ context_id = 0 /* Output context ID */</p>
    <p>/* Allocation loop */ while ( context_id == 0 ) {</p>
    <p>reserve_mask( ) MPIR_Allreduce_group ( local_mask, MPI_BAND, my_comm, my_group, my_tag ) select_ctx_id( )</p>
    <p>}</p>
    <p>Rank 0 Rank 1 Allocation Result</p>
  </div>
  <div class="page">
    <p>Full Context ID Allocation Algorithm, reserve</p>
    <p>reserve_mask( ) { Mutex_lock( ) if ( have_higher_priority( ) ) { lowest_ctx_id = my_comm-&gt;context_id lowest_tag = my_tag } if ( !mask_in_use &amp;&amp; have_priority( ) ) { local_mask = mask, mask_in_use = 1, i_own_the_mask = 1 } else { local_mask = 0, i_own_the_mask = 0 } Mutex_unlock( ) }</p>
    <p>m ask[M AX_CTXID]</p>
    <p>w hile (m y_cid = = 0) 1. Reserve( ) 2. Allreduce( ) 3. Select( )</p>
    <p>m ask[M AX_CTXID]</p>
    <p>w hile (m y_cid = = 0) 1. Reserve( ) 2. Allreduce( ) 3. Select( )</p>
  </div>
  <div class="page">
    <p>Full Context ID Allocation Algorithm, Allreduce</p>
    <p>ctx_id = MPIR_Allreduce_group ( local_mask, MPI_BAND, my_comm, my_group, my_tag )</p>
    <p>m ask[M AX_CTXID]</p>
    <p>w hile (m y_cid = = 0) 1. reserve( ) 2. Allreduce( ) 3. select( )</p>
    <p>m ask[M AX_CTXID]</p>
    <p>w hile (m y_cid = = 0) 1. reserve( ) 2. Allreduce( ) 3. select( )</p>
    <p>Rank 0 Rank 1</p>
    <p>Allocation Result</p>
  </div>
  <div class="page">
    <p>Full Context ID Allocation Algorithm, Select</p>
    <p>select_ctx_id( ) { if ( i_own_the_mask ) { Mutex_lock () if ( local_mask != 0 ) { context_id = location of first set bit in local_mask mask[ context_id ] = 0 if ( have_priority( ) ) lowest_ctx_id = MAXINT } mask_in_use = 0 Mutex_unlock () } }</p>
    <p>m ask[M AX_CTXID]</p>
    <p>w hile (m y_cid = = 0) 1. reserve( ) 2. Allreduce( ) 3. select( )</p>
    <p>m ask[M AX_CTXID]</p>
    <p>w hile (m y_cid = = 0) 1. reserve( ) 2. Allreduce( ) 3. select( )</p>
    <p>ctx_id = select( )0 0 0 1 0</p>
  </div>
  <div class="page">
    <p>Deadlock Scenario</p>
    <p>if( thread_id == mpi_rank ) MPI_Comm_dup( MPI_COMM_SELF, &amp;self_dup ); MPI_Comm_dup( thread_comm, &amp;thread_comm_dup );</p>
    <p>Necessary and sufficient conditions  Hold: A thread acquires the mask at a particular process  Wait: Thread enters allreduce, waits for others to make matching calls</p>
    <p>Meanwhile, matching calls cant be made  A context ID allocation must succeed first, but mask is unavailable</p>
  </div>
  <div class="page">
    <p>Deadlock Avoidance</p>
    <p>Basic idea: Prevent threads from reserving context ID until all threads are ready to perform the operation.</p>
    <p>Simple approach, initial barrier  MPIR_Barrier_group( my_comm, my_group, my_tag )</p>
    <p>Eliminates wait condition and breaks deadlock  Threads cant enter Allreduce until all group members have arrived  Threads cant update priorities until all group members have arrived  Ensures that thread groups that are ready will be able to eventually</p>
    <p>acquire highest priority and succeed</p>
    <p>Cost: additional collective 12</p>
  </div>
  <div class="page">
    <p>Eager Context ID Allocation</p>
    <p>Basic idea: Do useful work during deadlock-avoiding synchronization.</p>
    <p>Split context ID space into Eager and Base parts  Eager: used on first attempt (threads may hold-and wait)  Base: used on remaining attempts (threads cant hold-and-wait)</p>
    <p>If eager mask is not available, allocate on base mask  Allocations using base mask are deadlock free  Threads synchronize in initial eager Allreduce</p>
    <p>All threads are present during base allocation  Eliminates wait condition</p>
    <p>Eager Mask Base Mask</p>
  </div>
  <div class="page">
    <p>Eager Context ID Allocation Algorithm</p>
    <p>No priority in eager mode  Threads holding the eager space, blocked in Allreduce dont</p>
    <p>prevent others from entering base allocation  Deadlock is avoided (detailed proof in the paper)</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>w hile (m y_cid = = 0) 1. m y_cid_avail = reserve( ctxid_m ask[SPLIT..] ) 2. cid_avail = Allreduce( m y_cid_avail, parent_com m ) 3. m y_cid = select( cid_avail )</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>w hile (m y_cid = = 0) 1. m y_cid_avail = reserve( ctxid_m ask[SPLIT..] ) 2. cid_avail = Allreduce( m y_cid_avail, parent_com m ) 3. m y_cid = select( cid_avail )</p>
  </div>
  <div class="page">
    <p>Is OpenMPI Affected?</p>
    <p>OpenMPI uses a similar algorithm  MPICH reserves full mask  OpenMPI reserves one context ID at a time  Requires a second allreduce to check for success</p>
    <p>Hold-and-wait can still occur  When number of threads at a process approaches number of free</p>
    <p>context ids  Less likely than in MPICH  Same deadlock avoidance technique can be applied</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>w hile (m y_cid = = 0) 1. m y_cid_avail = reserve_one( ctxid_m ask ) 2. cid_avail = Allreduce( m y_cid_avail, parent_com m , M PI_M AX ) 3. success = Allreduce( cid_avil = = m y_cid_avail, M PI_AND ) 4. If (success) m y_cid = cid_avail</p>
    <p>ctxid_m ask[M AX_CTXID] = { 1, 1,  }</p>
    <p>w hile (m y_cid = = 0) 1. m y_cid_avail = reserve_one( ctxid_m ask ) 2. cid_avail = Allreduce( m y_cid_avail, parent_com m , M PI_M AX ) 3. success = Allreduce( cid_avil = = m y_cid_avail, M PI_AND ) 4. If (success) m y_cid = cid_avail</p>
  </div>
  <div class="page">
    <p>Comparison: Base vs Eager, CC vs CCG</p>
    <p>Parent communicator is MPI_COMM_WORLD (size = 1024)  Eager improves over base by factor of two</p>
    <p>One Allreduce, versus Barrier + Allreduce  MPI_Comm_create_group( ) versus MPI_Comm_create( )</p>
    <p>Communication creation cost is proportional to output group size</p>
  </div>
  <div class="page">
    <p>Comparison With User-Level CCG</p>
    <p>User-level [IMUDI 11]: log(p) intercomm create/merge steps  Total communication cost is log2(p)</p>
    <p>Direct: One communicator creation step  Eliminates factor of log(p)</p>
    <p>P = 512, 1024 was more expensive that MPI_Comm_create</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Extended context ID allocation to support multithreaded allocation on the same parent communicator  Support MPI-3 MPI_Comm_create_group routine</p>
    <p>Identified subtle deadlock issue</p>
    <p>Deadlock avoidance  Break hold-and-wait through initial synchronization  Eager context ID allocation eliminates deadlock avoidance cost in</p>
    <p>common case</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
</Presentation>
