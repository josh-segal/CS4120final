<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Ming Liu</p>
    <p>Joint Work with: Wray Buntine and Reza Haffari Monash University, Australia</p>
    <p>{ming.m.liu, wray.buntine, gholamreza.haffari} @monash.edu</p>
    <p>Learning How to Actively Learn: A Deep Imitation Learning Approach</p>
  </div>
  <div class="page">
    <p>Roadmap  Introduction to active learning (AL)  Markov decision process (MDP) for agent-based AL  Deep imitation learning to train the AL policy  Experiments &amp; Analysis</p>
  </div>
  <div class="page">
    <p>Roadmap  Introduction to active learning (AL)  Markov decision process (MDP) for agent-based AL  Deep imitation learning to train the AL policy  Experiments &amp; Analysis</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Raw unlabeled data points x1, x2,</p>
    <p>Classifier Oracle/Expert: Provides labels for queries</p>
    <p>(x1,?)</p>
    <p>(x1,y1)</p>
    <p>(x2,?)</p>
    <p>(x2,y2)</p>
    <p>4</p>
  </div>
  <div class="page">
    <p>At any time during the AL process, we have a current guess for the classifier</p>
    <p>AL Strategy: Query the point closest to the decision boundary</p>
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Raw unlabeled data points x1, x2,</p>
    <p>Classifier Oracle/Expert: Provides labels for queries</p>
    <p>AL Heuristics (x1,?)</p>
    <p>(x1,y1)</p>
    <p>AL Heuristics (x2,?)</p>
    <p>(x2,y2)</p>
    <p>Warnings:  Not clear whether heuristics lead to</p>
    <p>optimal querying behavior  Not clear which hard coded heuristic is</p>
    <p>good for a task at hand</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Raw unlabeled data points x1, x2,</p>
    <p>Classifier Oracle/Expert: Provides labels for queries</p>
    <p>Can we learn the best active learning strategy ?</p>
    <p>AL Agent (x1,?)</p>
    <p>(x1,y1)</p>
    <p>AL Agent (x2,?)</p>
    <p>(x2,y2)</p>
  </div>
  <div class="page">
    <p>Roadmap  Introduction to active learning (AL)  Markov decision process (MDP) for agent-based AL  Deep imitation learning to train the AL policy  Experiments &amp; Analysis</p>
  </div>
  <div class="page">
    <p>Agent-based Active Learning</p>
    <p>Raw unlabeled data points x1, x2,</p>
    <p>Classifier Oracle/Expert: Provides labels for queries</p>
    <p>Need to train an AL agent to tell what data to select next, given  the previously selected data  the pool of unlabeled data available  the underlying classifier, learned so far</p>
    <p>AL Agent (x1,?)</p>
    <p>(x1,y1)</p>
    <p>AL Agent (x2,?)</p>
    <p>(x2,y2)</p>
  </div>
  <div class="page">
    <p>AL Query Strategy by an Agent</p>
    <p>Raw unlabeled data points x1, x2,</p>
    <p>Oracle/Expert: Provides labels for queries</p>
    <p>The Tutoring AL Agent &amp; Learning Student (Classifier)</p>
    <p>AL Agent (x1,?)</p>
    <p>(x1,y1)</p>
    <p>AL Agent (x2,?)</p>
    <p>(x2,y2)</p>
  </div>
  <div class="page">
    <p>Agent Operates in Markov Decision Process</p>
    <p>x1 y1</p>
    <p>x2 y2</p>
    <p>s1 a1 s2 a2 s3</p>
    <p>Reward: Accuracy ( , )Evaluation Set</p>
    <p>Learn the Optimal Query Policy 11</p>
  </div>
  <div class="page">
    <p>Roadmap  Introduction to active learning (AL)  Markov decision process (MDP) for agent-based AL  Deep imitation learning to train the AL policy  Experiments &amp; Analysis</p>
  </div>
  <div class="page">
    <p>IDEA: Lets train the agent based on AL simulation for a rich-data task and then transfer it to AL problem of interest</p>
    <p>This is Meta-Learning: Learning to Actively Learn</p>
    <p>Synthesize many AL problems</p>
    <p>Use Imitation/Reinforcement Learning algorithms</p>
    <p>Training Agents Policy</p>
  </div>
  <div class="page">
    <p>Synthesizing AL Problems</p>
    <p>Original Labeled Data</p>
    <p>Evaluation Set</p>
    <p>Training Set</p>
    <p>Pool of unlabeled data</p>
    <p>Evaluation Set</p>
    <p>Training Set</p>
    <p>Pool of unlabeled data</p>
    <p>Evaluation Set</p>
    <p>Training Set</p>
    <p>Pool of unlabeled data</p>
  </div>
  <div class="page">
    <p>Imitation Learning</p>
    <p>Train the agent (policy network) to prefer the correct action compared to incorrect ones (i.e. classification)</p>
    <p>The algorithmic oracle gives the correct action in each world state</p>
  </div>
  <div class="page">
    <p>Algorithmic Oracle  It computes the correct action in each world state</p>
    <p>Re-train the underlying model using all possible queries/actions</p>
    <p>Mark the one leading to the most accurate prediction on the evaluation set</p>
    <p>argmax(xi,yi) in Pool Accuracy ( Retrain( , xi,yi) , )</p>
    <p>Too slow for typical large pools of data  IDEA: Randomly sample a subset and maximize over it</p>
    <p>Leads to efficient training and effective learned policies</p>
    <p>Evaluation Set</p>
  </div>
  <div class="page">
    <p>Imitation Learning DAGGER</p>
    <p>The collected state-action pairs are not i.i.d. hence problematic for classifier learning</p>
    <p>Data Aggregation (DAGGER): Once in a while, use the predicted action by the policy network during training (Ross et al 2011)</p>
    <p>This is to make sure the policy sees bad states and the correct action to recover from them in the training time 17</p>
  </div>
  <div class="page">
    <p>Roadmap  Introduction to active learning (AL)  Markov decision process (MDP) for agent-based AL  Deep imitation learning to train the AL policy  Experiments &amp; Analysis</p>
  </div>
  <div class="page">
    <p>Sentiment Classification: Positive/Negative sentiment of a review  Train the AL policy on one product, and apply to the reviews of another</p>
    <p>Authorship Profiling: Gender of the author of a tweet  Train the AL policy on one language, and apply to another</p>
    <p>Experiments (Task 1: text classification)</p>
  </div>
  <div class="page">
    <p>Random sampling  Uncertainty-based sampling  Diversity-based sampling  PAL (Fang et al., 2017) : A deep reinforcement learning based approach, they</p>
    <p>designed a Q-network for stream-based AL</p>
    <p>Experiments (Baseline methods)</p>
  </div>
  <div class="page">
    <p>Experiments (Task 1: text classification)</p>
  </div>
  <div class="page">
    <p>Direct transfer: Initialize the classifier on the source data, without AL</p>
    <p>Cold-start: Start training the classifier from random initialization, continue training with AL agent</p>
    <p>Warm-start: Start training the classifier from the pre-trained model on the source data, continue training with AL agent</p>
    <p>ac cu</p>
    <p>ra cy</p>
    <p>Experiments (Task 1: text classification)</p>
  </div>
  <div class="page">
    <p>Experiments (Task 2: Named Entity Recognition)</p>
    <p>Data sets: CoNLL 2002/2003</p>
  </div>
  <div class="page">
    <p>Experiments (Task 2: Named Entity Recognition)</p>
  </div>
  <div class="page">
    <p>Analysis: Insight on the selected data</p>
    <p>acc=  #</p>
    <p>We use MRR(Mean reciprocal rank) and acc to show the agreement of queried data points returned by our AL agent and other strategies.</p>
    <p>MRR = 1 || =1</p>
    <p>|| 1</p>
  </div>
  <div class="page">
    <p>Analysis: Sensitivity to K (size of unlabeled subset)</p>
    <p>K: size of subset from the original unlabelled set</p>
  </div>
  <div class="page">
    <p>Analysis : (schedule parameter for the policy)</p>
    <p>Options for</p>
    <p>Fixed: =0.5  Linear:  = max(0.5, 1  0.01)  Exponential: =0.9</p>
    <p>Inverse sigmoid: = 5</p>
  </div>
  <div class="page">
    <p>Related work</p>
    <p>Meta learning eg learning to learn without gradient descent by gradient descent (Chen et al 2016)</p>
    <p>Stream-based AL as MDP; learning the policy with reinforcement learning (Fang et al, 2017) suffers from the credit assignment problem (Bechman et al 2017)</p>
    <p>Imitation Learning: Lerning from expert demonstrations eg (Schaal 2009, Abbeel &amp; Ng 2004, Silver et al 2008)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Use heuristics or learn an agent for the AL query strategy.  Agent-based AL as a Markov Decision Process.  Formulate learning AL strategies/policies as an imitation learning</p>
    <p>problem.  Our imitation learning approach performs better than previous</p>
    <p>heuristic-based and RL-based methods.</p>
  </div>
  <div class="page">
    <p>Thanks</p>
  </div>
</Presentation>
