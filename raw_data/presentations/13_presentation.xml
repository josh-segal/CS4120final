<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>A Latent Variable Model of Synchronous Parsing for</p>
    <p>Syntactic and Semantic Dependencies</p>
    <p>James Henderson 1 Paola Merlo 2 Gabriele Musillo 1 2</p>
    <p>Ivan Titov 3</p>
    <p>CoNLL 2008</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Motivation for synchronous parsing</p>
    <p>Syntax and semantics are separate structures, with different generalisations</p>
    <p>Sub Obj John broke the vase. A0 A1</p>
    <p>Sub The vase broke.</p>
    <p>A1 Syntax and semantics are highly correlated, and therefore should be learned jointly</p>
    <p>Synchronous parsing provides a single joint model of two separate structures</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Motivation for latent variables</p>
    <p>The correlations between syntax and semantics are partly lexical, and independence assumptions are hard to specify a priori The dataset is new, and there was little time for feature engineering</p>
    <p>Latent variables provide a powerful mechanism for discovering correlations both within and between the structures</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>The Probability Model</p>
    <p>A generative, history-based model of the joint probability of syntactic and semantic synchronous derivations synchronised at each word.</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Syntactic and semantic dependencies example</p>
    <p>ROOT Hope seems doomed to failure</p>
    <p>P(Td , Ts)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Syntactic and semantic derivations</p>
    <p>Define two separate derivations, one for the syntactic structure and one for the semantic structure.</p>
    <p>P(Td , Ts) = P(D 1 d , ..., D</p>
    <p>md d , D</p>
    <p>ms s )</p>
    <p>Actions of an incremental shift-reduce style parser similar to MALT [Nivre et al., 2006] Semantic derivations are less constrained, because their structures are less constrained Assumes each dependency structure is individually planar (projective)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Synchronisation granularity</p>
    <p>Use an intermediate synchronisation granularity, between full predications and individual actions.</p>
    <p>Ct = D btd d , ..., D</p>
    <p>etd d , shiftt , D</p>
    <p>bts s , ..., D</p>
    <p>ets s , shiftt</p>
    <p>P(D1d , ..., D md d , D</p>
    <p>ms s ) = P(C</p>
    <p>Synchronisation at each word prediction Results in one shared input queue Allows two separate stacks</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Synchronous parsing example</p>
    <p>ROOT Hope</p>
    <p>P(C1)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Synchronous parsing example</p>
    <p>ROOT Hope seems</p>
    <p>P(C1) P(C2|C1)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Synchronous parsing example</p>
    <p>ROOT Hope seems doomed</p>
    <p>P(C1) P(C2|C1) P(C3|C1, C2)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Synchronous parsing example</p>
    <p>ROOT Hope seems doomed to</p>
    <p>P(C1) P(C2|C1) P(C3|C1, C2) P(C4|C1, C2, C3)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Synchronous parsing example</p>
    <p>ROOT Hope seems doomed to failure</p>
    <p>P(C1) P(C2|C1) P(C3|C1, C2) P(C4|C1, C2, C3) P(C5|C1, C2, C3, C4)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed to</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed to</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed to</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed to failure</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed to failure</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Derivation example</p>
    <p>ROOT Hope seems doomed to failure</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Projectivisation</p>
    <p>Allows crossing links between syntax and semantics Use the HEAD method [Nivre et al., 2006] to projectivise syntax Use syntactic dependencies to projectivise semantic dependencies</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Projectivising semantic dependencies</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>A B</p>
    <p>C</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>C</p>
    <p>B</p>
    <p>A/C</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>The Machine Learning Method</p>
    <p>Synchronous derivations are modeled with an Incremental Sigmoid Belief Network (ISBN).</p>
    <p>ISBNs are Dynamic Bayesian Networks for modeling structures, with vectors of latent variables annotating derivation states that represent features of the derivation history. Use the neural network approximation of ISBNs [Titov and Henderson, ACL 2007] (Simple Synchrony Netowrks)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Statistical dependencies in the ISBN</p>
    <p>Connections between latent states reflect locality in the syntactic or semantic structure, thereby specifying the domain of locality for conditioning decisions Explicit conditioning features of the history are also specified</p>
    <p>D</p>
    <p>SS</p>
    <p>DD</p>
    <p>S</p>
    <p>t1tc</p>
    <p>tc t1 t</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Connections between latent states</p>
    <p>Distinguish between syntactic states and semantic states of the derivation Connections both within and between types of states</p>
    <p>Recent Current Syn-Syn Srl-Srl Syn-Srl Srl-Syn Next Next + + + (+) Top Top + + + (+) RgtDepTop Top + + LftDepTop Top + + HeadTop Top + + LftDepNext Top + + Next Top +</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Explicit conditioning features</p>
    <p>State Syntax LEX POS DEP</p>
    <p>Next + + SynTop + + SynTop - 1 + Head SynTop + RgtD SynTop + LftD SynTop + LftD Next +</p>
    <p>State Semantics LEX POS DEP SENSE</p>
    <p>Next + + + SemTop + + + SemTop - 1 + + Head SemTop + + RgtD SemTop + LftD SemTop + LftD Next + A0-A5 SemTop + A0-A5 Next +</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>The Evaluation</p>
    <p>Two models reported Submitted model:</p>
    <p>vocabulary of 1083 words latent vector of 60 features no semantics-to-syntax latent state connections a form of Minimum Bayes Risk (MBR) decoding for syntax</p>
    <p>Larger model: vocabulary of 4392 words latent vector of 80 features includes semantics-to-syntax latent state connections decoding optimises joint probability</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Results</p>
    <p>Syntactic Semantic Overall LAS P R F1 F1</p>
    <p>Submitted WSJ 87.8 79.6 66.2 72.3 80.2 Brn 80.0 66.6 55.3 60.4 70.3 WSJ+Brn 86.9 78.2 65.0 71.0 79.1</p>
    <p>Large WSJ 88.5 80.4 69.2 74.4 81.5 Brn 81.0 68.3 57.7 62.6 71.9 WSJ+Brn 87.6 79.1 67.9 73.1 80.5</p>
    <p>Larger model does better (1.5%) than smaller submitted model Large model would be fifth overall</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>MBR versus joint inference</p>
    <p>Syntactic LAS</p>
    <p>Submitted Dev 86.1</p>
    <p>Joint optimisation Dev 85.5 Large (joint optimisation) Dev 86.5</p>
    <p>MBR for syntax helps a bit (0.6%) but not as much as the large model (1.0%)</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Additional experiments</p>
    <p>Removing latent connections between syntax and semantics reduced semantic performance by 3.5%, indicating the importance of the latent variables for finding the correlations between these structures</p>
    <p>When evaluated only on syntactic dependencies, the submitted model performs slighly (0.2%) better than a model trained only on syntactic depedencies, indicating that training a joint model does not harm performance of the syntax component, and may help</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Conclusions</p>
    <p>Synchronous derivations are an effective way to build joint models of separate structures The latent features of ISBNs help find correlations between structures ISBNs extend well to more complex automata than push-down automata</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Current Work</p>
    <p>Derivations which projectivise on-line (81.8% overall F-measure, 1.3% improvement) Better feature engineering, particularly for semantic parse decisions</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Acknowledgements</p>
    <p>This work was partly funded by European Community FP7 project CLASSiC (www.classic-project.org), a Swiss NSF grant, two Swiss NSF fellowships.</p>
    <p>Part of this work was done when G. Musillo was visiting MIT/CSAIL.</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Projectivising semantic dependencies</p>
    <p>An arc is un-crossed by replacing its argument a with as syntactic head and noting this change in the arc label. This change is repeated as necessary using a heuristic greedy search.</p>
  </div>
  <div class="page">
    <p>university-logo</p>
    <p>Machine Learning Method Evaluation</p>
    <p>Decoding</p>
    <p>Beam search used to search for the most probable derivation For submitted model, chose syntactic structure by summing over beam of semantic structures</p>
  </div>
</Presentation>
