<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Enabling High-Performance</p>
    <p>Internet-Wide Measurements on</p>
    <p>Windows</p>
    <p>Matt Smith</p>
    <p>Joint work with Dmitri Loguinov</p>
    <p>Internet Research Lab</p>
    <p>Department of Computer Science and Engineering</p>
    <p>Texas A&amp;M University</p>
    <p>April 9, 2010</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Agenda</p>
    <p>Introduction</p>
    <p>Background and Motivations</p>
    <p>Windows / Linux Network Stacks: An Overview</p>
    <p>Our Approach: IRLstack</p>
    <p>Performance Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Introduction</p>
    <p>As the Internet continues to grow, capturing</p>
    <p>accurate large-scale measurements remains an</p>
    <p>important research problem</p>
    <p>How big is the web? Can we capture a snapshot of</p>
    <p>a P2P network? Etc.</p>
    <p>Distributed server clusters are often used in</p>
    <p>commercial applications</p>
    <p>May not be available to academic researchers</p>
    <p>Bottlenecks in measurement projects are often</p>
    <p>encountered at the client-side</p>
    <p>Rate at which requests can be issued</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Motivations</p>
    <p>A few of our representative projects which require</p>
    <p>high sustained rates of measurement traffic:</p>
    <p>P2P network analysis (Gnutella crawler)</p>
    <p>IRLbot web crawler</p>
    <p>DNS infrastructure traversal</p>
    <p>Service discovery using horizontal scanning</p>
    <p>All these projects measure networks which</p>
    <p>constantly evolve during the measurement period</p>
    <p>Other applications (e.g., IDS, monitoring tools)</p>
    <p>also benefit from a scalable network stack</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Motivations</p>
    <p>Our goal: wire-rate transmission and capture of</p>
    <p>packets of all sizes</p>
    <p>Similar work has been done on Linux platforms;</p>
    <p>however no serious efforts have used Windows</p>
    <p>thus far</p>
    <p>We show that Windows can be used as a</p>
    <p>platform for serious high-performance research</p>
    <p>as well</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Agenda</p>
    <p>Introduction</p>
    <p>Background and Motivations</p>
    <p>Windows / Linux Network Stacks: An Overview</p>
    <p>Our Approach: IRLstack</p>
    <p>Performance Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Windows Network Stack Overview</p>
    <p>Three classes of drivers which implement</p>
    <p>different layers of functionality</p>
    <p>NDIS: Network Driver Interface Specification</p>
    <p>Protocol drivers: accept requests from user</p>
    <p>space, construct link-layer frames as appropriate</p>
    <p>Filter drivers: receive and possibly process any</p>
    <p>frames sent to or from the NIC</p>
    <p>Note that WinPcap is implemented as a filter driver</p>
    <p>with a direct interface to user-space</p>
    <p>Miniport drivers: specific to each NIC, directly</p>
    <p>interface with hardware; DMA, interrupts, etc.</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Windows Network Stack Overview</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Linux Network Stack Overview</p>
    <p>Our primary Linux comparison focuses on the</p>
    <p>modified stack available from the ntop project,</p>
    <p>which makes two main contributions</p>
    <p>PF_RING: uses DMA and Intel I/OAT for zero</p>
    <p>copy availability of kernel memory buffers in</p>
    <p>user-space</p>
    <p>TNAPI: deserializes receive operations utilizing</p>
    <p>multiple RX queues and making them available</p>
    <p>concurrently</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Performance Evaluation: Winsock</p>
    <p>and WinPcap  As mentioned earlier, several of our projects</p>
    <p>would benefit from very high send/receive rates</p>
    <p>Hundreds of thousands of packets per second (pps);</p>
    <p>implies small packets and more overhead</p>
    <p>Ideally wire rate: 1,488,095 pps for Gigabit Ethernet</p>
    <p>Maximum single-NIC transmit rates on our test</p>
    <p>system left much to be desired, despite 100%</p>
    <p>CPU usage</p>
    <p>Winsock best case (raw IP socket, single destination):</p>
    <p>WinPcap: 50 kpps</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Performance Evaluation: Winsock</p>
    <p>and WinPcap  What bottlenecks prevent Winsock/WinPcap</p>
    <p>from achieving wire-rate performance?</p>
    <p>For Winsock, primarily IP table lookups</p>
    <p>At a common lower level (NDIS), synchronization</p>
    <p>spinlock overhead required for every transfer</p>
    <p>The amount of overhead work per packet</p>
    <p>(irrespective of length) is very high</p>
    <p>With these constraints we cannot achieve wire</p>
    <p>rate with minimum-sized packets  which</p>
    <p>measurement traffic tends to be</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Agenda</p>
    <p>Introduction</p>
    <p>Background and Motivations</p>
    <p>Windows / Linux Network Stacks: An Overview</p>
    <p>Our Approach: IRLstack</p>
    <p>Performance Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Our Approach: IRLstack</p>
    <p>Goal: build a very high-performance interface for</p>
    <p>sending/receiving packets directly from a NIC</p>
    <p>Applications can use this interface directly (i.e., a raw</p>
    <p>socket interface)</p>
    <p>Alternately, an intermediate layer (e.g., simplified TCP</p>
    <p>stack) can sit between IRLstack and the user-space</p>
    <p>application</p>
    <p>Any per-packet processing that isnt absolutely</p>
    <p>necessary is not included</p>
    <p>Batching of many packets (sending or receiving)</p>
    <p>maximizes useful work per request</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Our Approach: IRLstack</p>
    <p>IRLstack is implemented as an NDIS driver</p>
    <p>stack with direct access from user space</p>
    <p>E.g., ReadFile/WriteFile APIs</p>
    <p>Two components of the stack</p>
    <p>IRLstackP.sys protocol driver  user applications</p>
    <p>open a handle to this driver; processes send/receive</p>
    <p>requests</p>
    <p>IRLstackF.sys filter driver  intercepts all incoming</p>
    <p>packets, redirects to IRLstackP.sys as appropriate</p>
    <p>The filter driver uses a list of IP addresses assigned</p>
    <p>to IRLstack to rewrite link-layer frame headers and</p>
    <p>perform the redirection</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Our Approach: IRLstack</p>
    <p>To accommodate packet batching, IRLstack</p>
    <p>request buffers consist of a series of complete</p>
    <p>link-layer (usually Ethernet) frames</p>
    <p>A small header specific to IRLstack precedes each</p>
    <p>frame</p>
    <p>The same format is used on both TX and RX paths</p>
    <p>IRLstack header</p>
    <p>Link-layer frame</p>
    <p>First packet Second packet, etc.</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Our Approach: IRLstack</p>
    <p>Other design notes:</p>
    <p>Multiple requests can be outstanding at a time</p>
    <p>Checksums usually need not be calculated in</p>
    <p>software (if using a NIC with checksum offloading)</p>
    <p>An entire buffer (hundreds or thousands of packets) is</p>
    <p>processed as a single request at all levels in the</p>
    <p>kernel</p>
    <p>Zero-copy send path</p>
    <p>IRLstack coexists with the Windows network stack on</p>
    <p>a single adapter; default configuration requires an</p>
    <p>extra IP address to be assigned to the interface</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Agenda</p>
    <p>Introduction</p>
    <p>Background and Motivations</p>
    <p>Windows / Linux Network Stacks: An Overview</p>
    <p>Our Approach: IRLstack</p>
    <p>Performance Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Performance Evaluation</p>
    <p>Our testing focuses on minimum-size packets</p>
    <p>Reflects the properties of much of our measurement</p>
    <p>traffic</p>
    <p>This is the hardest scenario  most overhead</p>
    <p>Optimal transmission rate occurs around 512</p>
    <p>packets per call on our test setup (Intel Pro/1000</p>
    <p>PT network adapter)</p>
    <p>Batch sizes below 100 packets are unable to fully</p>
    <p>utilize a gigabit link; less work is done per trip down</p>
    <p>the network stack</p>
    <p>Large batches (thousands of packets) actually start to</p>
    <p>lose performance, which we attribute to the miniport</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Performance Evaluation</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Performance Evaluation</p>
    <p>Receive performance is somewhat (~20-50%)</p>
    <p>slower than send performance</p>
    <p>Not zero-copy at the moment</p>
    <p>Interrupt frequency is higher and batch size is lower</p>
    <p>(e.g., 64); this is out of our direct control as miniport</p>
    <p>drivers on Windows are typically not open-source</p>
    <p>For reference we look to the Linux numbers from</p>
    <p>the ntop project; IRLstacks performance</p>
    <p>compares favorably on similar hardware</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Agenda</p>
    <p>Introduction</p>
    <p>Background and Motivations</p>
    <p>Windows / Linux Network Stacks: An Overview</p>
    <p>Our Approach: IRLstack</p>
    <p>Performance Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>u te</p>
    <p>r S c</p>
    <p>ie n</p>
    <p>c e</p>
    <p>, Te</p>
    <p>x a</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U n</p>
    <p>iv e</p>
    <p>Conclusion</p>
    <p>More information about latency and TCP</p>
    <p>performance can be found in the full paper</p>
    <p>Windows has traditionally been avoided as a</p>
    <p>research platform; however with a well-designed</p>
    <p>driver suite such as IRLstack this need not be</p>
    <p>the case</p>
    <p>Areas of future work:</p>
    <p>Further optimization on receive path</p>
    <p>Evaluation on 10 Gigabit Ethernet NICs and with</p>
    <p>other new hardware features (e.g., DMA remapping)</p>
  </div>
</Presentation>
