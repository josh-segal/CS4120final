<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A La Carte Embedding: Cheap but Effective Induction of</p>
    <p>Semantic Feature Vectors Mikhail Khodak*,1, Nikunj Saunshi*,1, Yingyu Liang2,</p>
    <p>Tengyu Ma3, Brandon Stewart1, Sanjeev Arora1</p>
  </div>
  <div class="page">
    <p>Motivations</p>
    <p>Distributed representations for words / text have had lots of successes in NLP (language models, machine translation, text classification)</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Motivations</p>
    <p>Distributed representations for words / text have had lots of successes in NLP (language models, machine translation, text classification)</p>
    <p>Motivations for our work:</p>
    <p>Can we induce embeddings for all kinds of features, especially those with very few occurrences (e.g. ngrams, rare words)</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Motivations</p>
    <p>Distributed representations for words / text have had lots of successes in NLP (language models, machine translation, text classification)</p>
    <p>Motivations for our work:</p>
    <p>Can we induce embeddings for all kinds of features, especially those with very few occurrences (e.g. ngrams, rare words)</p>
    <p>Can we develop simple methods for unsupervised text embedding that compete well with state-of-the-art LSTM methods</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Motivations</p>
    <p>Distributed representations for words / text have had lots of successes in NLP (language models, machine translation, text classification)</p>
    <p>Motivations for our work:</p>
    <p>Can we induce embeddings for all kinds of features, especially those with very few occurrences (e.g. ngrams, rare words)</p>
    <p>Can we develop simple methods for unsupervised text embedding that compete well with state-of-the-art LSTM methods</p>
    <p>ACL 2018</p>
    <p>We make progress on both problems</p>
    <p>- Simple and efficient method for embedding features (ngrams, rare words, synsets)</p>
    <p>- Simple text embeddings using ngram embeddings which perform well on classification tasks</p>
  </div>
  <div class="page">
    <p>Word embeddings</p>
    <p>Core idea: Cooccurring words are trained to have high inner product  E.g. LSA, word2vec, GloVe and variants</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Word embeddings</p>
    <p>Core idea: Cooccurring words are trained to have high inner product  E.g. LSA, word2vec, GloVe and variants</p>
    <p>Require few passes over a very large text corpus and do non-convex optimization</p>
    <p>ACL 2018</p>
    <p>!&quot;  %Optimizing objectivecorpus</p>
    <p>word embeddings</p>
  </div>
  <div class="page">
    <p>Word embeddings</p>
    <p>Core idea: Cooccurring words are trained to have high inner product  E.g. LSA, word2vec, GloVe and variants</p>
    <p>Require few passes over a very large text corpus and do non-convex optimization</p>
    <p>Used for solving analogies, language models, machine translation, text classification</p>
    <p>ACL 2018</p>
    <p>!&quot;  %Optimizing objectivecorpus</p>
    <p>word embeddings</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Capturing meaning of other natural language features  E.g. ngrams, phrases, sentences, annotated words, synsets</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Capturing meaning of other natural language features  E.g. ngrams, phrases, sentences, annotated words, synsets</p>
    <p>Interesting setting: features with zero or few occurrences</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Capturing meaning of other natural language features  E.g. ngrams, phrases, sentences, annotated words, synsets</p>
    <p>Interesting setting: features with zero or few occurrences</p>
    <p>One approach (extension of word embeddings): Learn embeddings for all features in a text corpus</p>
    <p>ACL 2018</p>
    <p>!&quot;  %Optimizing objective</p>
    <p>corpus feature</p>
    <p>embeddings</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Issues</p>
    <p>Usually need to learn embeddings for all features together  Need to learn many parameters  Computation cost paid is prix fixe rather than  la carte</p>
    <p>Bad quality for rare features</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Firth revisited: Feature derives meaning from words around it</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Firth revisited: Feature derives meaning from words around it</p>
    <p>Given a feature ! and one (few) context(s) of words around it, can we find a reliable embedding for ! efficiently?</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Feature embeddings</p>
    <p>Firth revisited: Feature derives meaning from words around it</p>
    <p>Given a feature ! and one (few) context(s) of words around it, can we find a reliable embedding for ! efficiently?</p>
    <p>Petrichor: the earthy scent produce when rain falls on dry soil</p>
    <p>Scientists attending ACL work on cutting edge research in NLP</p>
    <p>Roger Federer won the first setNN of the match</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Problem setup</p>
    <p>f!&quot;!#  !$!&quot;%#</p>
    <p>+ &amp;'   *</p>
    <p>Algorithm &amp;+  *</p>
    <p>ACL 2018</p>
    <p>Given: Text corpus and high quality word embeddings trained on it</p>
    <p>Input: A feature in context(s) Output: Good quality embedding for the feature</p>
  </div>
  <div class="page">
    <p>Linear approach</p>
    <p>Given a feature f and words in a context c around it</p>
    <p>!&quot; #$% = 1|)| *</p>
    <p>+!+</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Linear approach</p>
    <p>Given a feature f and words in a context c around it</p>
    <p>!&quot; #$% = 1|)| *</p>
    <p>+!+</p>
    <p>Issues  stop words (is, the) are frequent but are less informative  Word vectors tend to share common components which will be amplified</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Potential fixes</p>
    <p>Ignore stop words</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Potential fixes</p>
    <p>Ignore stop words</p>
    <p>SIF weights1: Down-weight frequent words (similar to tf-idf)</p>
    <p>ACL 2018</p>
    <p>!&quot; = 1 |&amp;| '</p>
    <p>(* +( !( +( =</p>
    <p>, , + .(</p>
    <p>.( is frequency of w in corpus</p>
  </div>
  <div class="page">
    <p>Potential fixes</p>
    <p>Ignore stop words</p>
    <p>SIF weights1: Down-weight frequent words (similar to tf-idf)</p>
    <p>All-but-the-top2: Remove the component of top direction from word vectors</p>
    <p>ACL 2018</p>
    <p>!&quot; = 1 |&amp;| '</p>
    <p>(* +( !(</p>
    <p>, = -./_1234&amp;-2.5 !(</p>
    <p>!(6 = 347.!4_&amp;.7/.545-(!(,,) !&quot; =</p>
    <p>(* !(6 = ;  ,,= !(</p>
    <p>&gt;?@</p>
    <p>+( = A</p>
    <p>A + /( /( is frequency of w in corpus</p>
  </div>
  <div class="page">
    <p>Down-weighting and removing directions can be achieved by matrix multiplication</p>
    <p>Induced Embedding</p>
    <p>Induction Matrix</p>
    <p>!&quot;  $ 1 &amp; '</p>
    <p>(* !( = $!&quot;</p>
    <p>,-.</p>
    <p>Our more general approach ACL 2018</p>
  </div>
  <div class="page">
    <p>Down-weighting and removing directions can be achieved by matrix multiplication</p>
    <p>Learn ! by using words as features</p>
    <p>Learn ! by linear regression and is unsupervised</p>
    <p>Induced Embedding</p>
    <p>Induction Matrix</p>
    <p>&quot;#  ! 1 &amp; '</p>
    <p>(* &quot;( = !&quot;#</p>
    <p>,-.</p>
    <p>Our more general approach ACL 2018</p>
    <p>! = 0123456 ' ( |&quot;(  !&quot;(,-.|99</p>
  </div>
  <div class="page">
    <p>Theoretical justification</p>
    <p>[Arora et al. TACL 18] prove that under a generative model for text, there exists a matrix ! which satisfies</p>
    <p>&quot;#  !&quot;#%&amp;'</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Theoretical justification</p>
    <p>[Arora et al. TACL 18] prove that under a generative model for text, there exists a matrix ! which satisfies</p>
    <p>&quot;#  !&quot;#%&amp;'</p>
    <p>Empirically we find that the best ! recovers the original word vectors</p>
    <p>)*+,-. &quot;#,!&quot;#%&amp;'  0.9</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>A la carte embeddings</p>
    <p>! = $%&amp;'()* + , |.,  !.,</p>
    <p>ACL 2018</p>
    <p>+ .,</p>
    <p>Linear Regression</p>
  </div>
  <div class="page">
    <p>A la carte embeddings</p>
    <p>f</p>
    <p>!&quot;</p>
    <p>!# . . .</p>
    <p>!$</p>
    <p>!&quot;%# . . .</p>
    <p>&amp; = )*+,-./ 0 1 |31  &amp;31</p>
    <p>+ ? 395:;</p>
    <p>ACL 2018</p>
    <p>+ 31</p>
    <p>Linear Regression</p>
  </div>
  <div class="page">
    <p>A la carte embeddings</p>
    <p>f</p>
    <p>!&quot;</p>
    <p>!# . . .</p>
    <p>!$</p>
    <p>!&quot;%# . . .</p>
    <p>&amp; = )*+,-./ 0 1 |31  &amp;31</p>
    <p>+ ? 395:;</p>
    <p>ACL 2018</p>
    <p>+ 31</p>
    <p>Linear Regression</p>
    <p>Only once !!</p>
  </div>
  <div class="page">
    <p>Advantages</p>
    <p>la carte: Compute embedding only for given feature</p>
    <p>Simple optimization: Linear regression</p>
    <p>Computational efficiency: One pass over corpus and contexts</p>
    <p>Sample efficiency: Learn only !&quot;parameters for # (rather than %!)</p>
    <p>Versatility: Works for any feature which has at least 1 context</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Effect of induction matrix</p>
    <p>We plot the extent to which ! down-weights words against frequency of words compared to all-but-the-top</p>
  </div>
  <div class="page">
    <p>Effect of induction matrix</p>
    <p>We plot the extent to which ! down-weights words against frequency of words compared to all-but-the-top Change in Embedding Norm under Transform</p>
    <p>|!$%| |$%|</p>
    <p>log(*+,-.%)</p>
    <p>! mainly down-weights words with very high and very low frequency</p>
    <p>All-but-the-top mainly down-weights frequent words</p>
  </div>
  <div class="page">
    <p>Effect of number of contexts</p>
    <p>Contextual Rare Words (CRW) dataset1 providing contexts for rare words  Task: Predict human-rated similarity scores for pairs of words  Evaluation: Spearmans rank coefficient between inner product and score</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Effect of number of contexts</p>
    <p>Contextual Rare Words (CRW) dataset1 providing contexts for rare words  Task: Predict human-rated similarity scores for pairs of words  Evaluation: Spearmans rank coefficient between inner product and score</p>
    <p>Compare to the following methods:  Average of words in context  Average of non stop words  SIF weighted average  all-but-the-top</p>
    <p>ACL 2018</p>
    <p>SIF + all-but-the-top</p>
    <p>Average Average, all-but-the-top</p>
    <p>SIF Average, no stop words</p>
    <p>la carte</p>
  </div>
  <div class="page">
    <p>Task: Find embedding for unseen word/concept given its definition  Evaluation: Rank of word/concept based on cosine similarity with true</p>
    <p>embedding</p>
    <p>Nonce definitional task1</p>
    <p>iodine: is a chemical element with symbol I and atomic number 53</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Task: Find embedding for unseen word/concept given its definition  Evaluation: Rank of word/concept based on cosine similarity with true</p>
    <p>embedding</p>
    <p>Nonce definitional task1</p>
    <p>Method Mean Reciprocal Rank Median Rank word2vec 0.00007 111012</p>
    <p>average 0.00945 3381</p>
    <p>average, no stop words 0.03686 861</p>
    <p>nonce2vec1 0.04907 623</p>
    <p>la carte 0.07058 165.5</p>
    <p>iodine: is a chemical element with symbol I and atomic number 53</p>
    <p>ACL 2018</p>
    <p>modified version</p>
    <p>of word2vec</p>
  </div>
  <div class="page">
    <p>Ngram embeddings</p>
    <p>Induce embeddings for ngrams using contexts from a text corpus</p>
    <p>We evaluate the quality of embedding for a bigram ! = ($%,$') by looking at closest words to this embedding by cosine similarity.</p>
    <p>ACL 2018</p>
    <p>Method beef up cutting edge harry potter tight lipped</p>
    <p>)*+,, = )-. + )-0 meat, out cut, edges deathly, azkaban loose, fitting )* +12 but, however which, both which, but but, however</p>
    <p>ECO1 meats, meat weft, edges robards, keach scaly, bristly Sent2Vec2 add, reallocate science, multidisciplinary naruto, pokemon wintel, codebase</p>
    <p>la carte (3)* +12) need, improve innovative, technology deathly, hallows worried, very</p>
  </div>
  <div class="page">
    <p>Unsupervised text embeddings</p>
    <p>This movie is</p>
    <p>great!</p>
    <p>v&quot;</p>
    <p>v#</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>v  #</p>
    <p>Transducer</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Unsupervised text embeddings</p>
    <p>Sparse Bag-of-words, Bag-of-ngrams</p>
    <p>Good performance</p>
    <p>Linear Sum of word/ngram embeddings</p>
    <p>Compete with Bag-of-ngrams and LSTMs on some tasks</p>
    <p>LSTM Predict surrounding words / sentences</p>
    <p>SOTA on some tasks</p>
    <p>This movie is</p>
    <p>great!</p>
    <p>v&quot;</p>
    <p>v#</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>v  #</p>
    <p>Transducer</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>A la carte text embeddings ACL 2018</p>
    <p>Linear schemes are typically weighted sums of ngram embeddings</p>
  </div>
  <div class="page">
    <p>A la carte text embeddings</p>
    <p>DisC ECO Sent2Vec</p>
    <p>A La Carte</p>
    <p>ACL 2018</p>
    <p>Types of ngrams embeddings</p>
    <p>Linear schemes are typically weighted sums of ngram embeddings</p>
    <p>Compositional Learned</p>
    <p>Flexible High quality</p>
  </div>
  <div class="page">
    <p>A la carte text embeddings</p>
    <p>!&quot;#$%&amp;'()( = +!,#-&quot; ,+!/01-2&amp;23$ ,,+!(1-2&amp;23$DisC ECO Sent2Vec</p>
    <p>A La Carte</p>
    <p>ACL 2018</p>
    <p>Types of ngrams embeddings</p>
    <p>A La Carte text embeddings are as concatenations of sum of  la carte ngram embeddings (as in DisC)</p>
    <p>Linear schemes are typically weighted sums of ngram embeddings</p>
    <p>Compositional Learned</p>
    <p>Flexible High quality</p>
  </div>
  <div class="page">
    <p>A la carte text embeddings</p>
    <p>Method n dimension MR CR SUBJ MPQA TREC SST (1) SST IMDB</p>
    <p>Bag-of-ngrams 1-3 100K-1M 77.8 78.3 91.8 85.8 90.0 80.9 42.3 89.8</p>
    <p>Skip-thoughts1 4800 80.3 83.8 94.2 88.9 93.0 85.1 45.8</p>
    <p>SDAE2 2400 74.6 78.0 90.8 86.9 78.4</p>
    <p>CNN-LSTM3 4800 77.8 82.0 93.6 89.4 92.6</p>
    <p>MC-QT4 4800 82.4 86.0 94.8 90.2 92.4 87.6</p>
    <p>DisC5 2-3  4800 80.1 81.5 92.6 87.9 90.0 85.5 46.7 89.6</p>
    <p>Sent2Vec6 1-2 700 76.3 79.1 91.2 87.2 85.8 80.2 31.0 85.5</p>
    <p>la carte 2 2400 81.3 83.7 93.5 87.6 89.0 85.8 47.8 90.3</p>
    <p>ACL 2018</p>
    <p>&quot;#$%&amp;'()*+,% = .&quot;/$0# ,.&quot;2340+'+,% ,,.&quot;)40+'+,%</p>
    <p>LSTM</p>
    <p>Linear</p>
    <p>Sparse</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Simple and efficient method for inducing embeddings for many kinds of features, given at least one context of usage</p>
    <p>Embeddings produced are in same semantic space as word embeddings</p>
    <p>Good empirical performance for rare words, ngrams and synsets</p>
    <p>Text embeddings that compete with unsupervised LSTMs</p>
    <p>Code is on github: https://github.com/NLPrinceton/ALaCarte CRW dataset available: http://nlp.cs.princeton.edu/CRW/</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Zero shot learning of feature embeddings  Compositional approaches</p>
    <p>Harder to annotate features (synsets)</p>
    <p>Contexts based on other syntactic structures</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
    <p>{nsaunshi, mkhodak}@cs.princeton.edu</p>
    <p>ACL 2018</p>
  </div>
</Presentation>
