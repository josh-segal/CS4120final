<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Comprehensive Supersense Disambiguation of English</p>
    <p>Prepositions and Possessives</p>
    <p>Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Jakob Prange,</p>
    <p>Austin Blodgett, Sarah R. Moeller, Aviram Stern, Adi Bitan, Omri Abend</p>
  </div>
  <div class="page">
    <p>Adpositions are Pervasive</p>
    <p>Adpositions: prepositions or postpositions</p>
    <p>Order of Adposition and Noun Phrase WALS / Dryer and</p>
    <p>Haspelmath</p>
  </div>
  <div class="page">
    <p>Prepositions are some of the most frequent Words in English</p>
    <p>Based on the COCA list of 5000 most frequent words</p>
  </div>
  <div class="page">
    <p>We know Prepositions are challenging for Syntactic Parsing</p>
    <p>a talk at the conference on prepositions</p>
    <p>But what about the meaning beyond linking governor and object?</p>
  </div>
  <div class="page">
    <p>Prepositions are highly Polysemous</p>
    <p>in</p>
    <p>in the box</p>
    <p>in the afternoon</p>
    <p>in love, in trouble</p>
    <p>in fact</p>
    <p>for  leave for Paris  ate for hours  a gift for mother  raise money for the party</p>
  </div>
  <div class="page">
    <p>for</p>
    <p>pendant</p>
    <p>to</p>
    <p>pour</p>
    <p>ate for hours</p>
    <p>raise money to buy a house</p>
    <p>a gift for mother raise money for the church</p>
    <p>give the gift to mother</p>
    <p>go to Paris</p>
    <p>Translations are Many-to-Many</p>
  </div>
  <div class="page">
    <p>Potential Applications</p>
    <p>Machine Translation  MT into English: mistranslation of prepositions among most common errors</p>
    <p>(Hashemi and Hwa, 2014; Popovi, 2017)</p>
    <p>Grammatical Error Correction</p>
    <p>Semantic Parsing / SRL</p>
  </div>
  <div class="page">
    <p>Goal: Disambiguation</p>
    <p>Descriptive theory (annotation scheme)</p>
    <p>Lexical resource</p>
    <p>Annotated Dataset</p>
    <p>Disambiguation system (classifier)</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>In this paper: English</p>
  </div>
  <div class="page">
    <p>Senses vs. Supersenses</p>
    <p>Senses (e.g., Over-15-1) Supersenses (e.g., Frequency)</p>
  </div>
  <div class="page">
    <p>Challenges for Comprehensiveness</p>
    <p>What counts as a preposition/possessive marker?</p>
    <p>Prepositional multi-word expressions (of course)</p>
    <p>Phrasal verbs (give up)</p>
    <p>Rare senses (RateUnit, 40 miles per Gallon)</p>
    <p>Rare prepositions (in keeping with)</p>
    <p>Wicked polysemy</p>
  </div>
  <div class="page">
    <p>Supersense Inventory</p>
    <p>Semantic Network of Adposition and Case Supersenses (SNACS)</p>
    <p>50 supersenses, 4 levels of depth</p>
    <p>Simpler than its predecessor (Schneider et al., 2016)  Fewer categories, smaller hierarchy</p>
  </div>
  <div class="page">
    <p>Supersense Inventory</p>
    <p>Participant</p>
    <p>Usually core semantic roles</p>
    <p>Circumstance</p>
    <p>Usually non-core semantic roles</p>
    <p>Configuration</p>
    <p>Non-spatiotemporal information</p>
    <p>Static relations</p>
  </div>
  <div class="page">
    <p>Construal</p>
    <p>Challenge: the preposition itself and the verb may suggest different labels</p>
    <p>Similar meanings: the same label?</p>
    <p>at Grunnings: Locus or OrgRole ?</p>
    <p>for Grunning: Beneficiary or OrgRole ?</p>
    <p>Approach: distinguish scene role and preposition function</p>
  </div>
  <div class="page">
    <p>Construal</p>
    <p>Scene role and preposition function may diverge:</p>
    <p>Function  Scene Role in 1/3 of instances</p>
    <p>Beneficiary  OrgRole</p>
    <p>Locus  OrgRole</p>
  </div>
  <div class="page">
    <p>Documentation</p>
    <p>Large number of labels, prepositions, constructions and ultimately languages  careful documentation is imperative</p>
    <p>Extensive guidelines  450 examples</p>
    <p>80 pages</p>
    <p>Xposition: (under development)  A web-app and repository of prepositions/supersenses</p>
    <p>Standardized format and querying tools to retrieve relevant examples/guidelines</p>
  </div>
  <div class="page">
    <p>Re-annotated Dataset</p>
    <p>STREUSLE is a corpus annotated with (preposition) supersenses  Text: review section of the English Web Treebank</p>
    <p>Complete revision of STREUSLE: version 4.0  https://github.com/nert-gu/streusle/</p>
    <p>5,455 target prepositions, including 1,104 possessives  80:10:10% train:dev:test split See Blodgett and</p>
    <p>Schneider, LREC 2018 for details</p>
  </div>
  <div class="page">
    <p>Preposition Distribution</p>
    <p>249 prepositions</p>
    <p>10 account for 2/3 of the mass</p>
    <p>to</p>
    <p>o u</p>
    <p>r</p>
    <p>th a</p>
    <p>n</p>
    <p>w it</p>
    <p>h o</p>
    <p>u t</p>
    <p>h o</p>
    <p>m e</p>
    <p>b e</p>
    <p>tw e</p>
    <p>e n</p>
    <p>a ll</p>
    <p>o v e</p>
    <p>r</p>
    <p>b e</p>
    <p>lo w</p>
    <p>ju st</p>
    <p>a b</p>
    <p>o u</p>
    <p>t</p>
    <p>in t</p>
    <p>im e</p>
    <p>o f</p>
    <p>n e</p>
    <p>e d</p>
    <p>o v</p>
    <p>e r</p>
    <p>th e</p>
    <p>y e</p>
    <p>a rs</p>
    <p>a cr</p>
    <p>o ss</p>
    <p>a h</p>
    <p>e a</p>
    <p>d o</p>
    <p>f ti</p>
    <p>m e</p>
    <p>o n</p>
    <p>t h</p>
    <p>e c</p>
    <p>h e</p>
    <p>a p</p>
    <p>o u</p>
    <p>t o</p>
    <p>f d</p>
    <p>a te</p>
    <p>a l</p>
    <p>e a</p>
    <p>st</p>
    <p>a cc</p>
    <p>o rd</p>
    <p>in g</p>
    <p>t o</p>
    <p>u n</p>
    <p>d e</p>
    <p>r ci</p>
    <p>rc u</p>
    <p>m st</p>
    <p>a n</p>
    <p>ce s</p>
    <p>fo tit</p>
    <p>in t</p>
    <p>h e</p>
    <p>p ro</p>
    <p>ce ss</p>
    <p>o f</p>
    <p>in t</p>
    <p>im e</p>
    <p>a b</p>
    <p>o u</p>
    <p>re g</p>
    <p>a rd</p>
    <p>le ss</p>
    <p>o f</p>
    <p>o u</p>
    <p>t fr</p>
    <p>o n</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Supersense Distribution</p>
    <p>Lo cu</p>
    <p>s</p>
    <p>G e</p>
    <p>st a</p>
    <p>lt</p>
    <p>T im</p>
    <p>e</p>
    <p>To p</p>
    <p>ic</p>
    <p>C o</p>
    <p>m p</p>
    <p>a ri</p>
    <p>so n</p>
    <p>R e</p>
    <p>f</p>
    <p>D ir</p>
    <p>e ct</p>
    <p>io n</p>
    <p>S o</p>
    <p>u rc</p>
    <p>e</p>
    <p>E xp</p>
    <p>la n</p>
    <p>a ti</p>
    <p>o n</p>
    <p>A g</p>
    <p>e n</p>
    <p>t</p>
    <p>D u</p>
    <p>ra ti</p>
    <p>o n</p>
    <p>A p</p>
    <p>p ro</p>
    <p>xi m</p>
    <p>a to</p>
    <p>r</p>
    <p>C ir</p>
    <p>cu m</p>
    <p>st a</p>
    <p>n ce</p>
    <p>S ti</p>
    <p>m u</p>
    <p>lu s</p>
    <p>E xp</p>
    <p>e ri</p>
    <p>e n</p>
    <p>ce r</p>
    <p>C o</p>
    <p>-A g</p>
    <p>e n</p>
    <p>t</p>
    <p>E xt</p>
    <p>e n</p>
    <p>t</p>
    <p>C o</p>
    <p>st</p>
    <p>P a</p>
    <p>th</p>
    <p>S ta</p>
    <p>rt T</p>
    <p>im e</p>
    <p>In st</p>
    <p>ru m</p>
    <p>e n</p>
    <p>t</p>
    <p>M e</p>
    <p>a n</p>
    <p>s</p>
    <p>C o</p>
    <p>-T h</p>
    <p>e m</p>
    <p>e</p>
    <p>In st</p>
    <p>e a</p>
    <p>d O</p>
    <p>f</p>
    <p>R a</p>
    <p>te U</p>
    <p>n it</p>
    <p>47 attested supersenses</p>
    <p>Frequencies:  25% are spatial</p>
    <p>10% are temporal</p>
    <p>8% involve possession</p>
  </div>
  <div class="page">
    <p>Inter-Annotator Agreement</p>
    <p>Annotated a small sample of The Little Prince  216 preposition tokens</p>
    <p>5 annotators, varied familiarity with scheme</p>
    <p>Exact agreement (pairwise avg.): 74.4% on scene roles, 81.3% on functions</p>
  </div>
  <div class="page">
    <p>Disambiguation Models</p>
    <p>Use Universal</p>
    <p>Dependencies</p>
    <p>Syntax to detect</p>
    <p>governor and</p>
    <p>object</p>
  </div>
  <div class="page">
    <p>Target Identification</p>
    <p>Main challenges:  Multi-word prepositions, especially rare ones (e.g., after the fashion of)</p>
    <p>Idiomatic PPs (e.g., in action, by far)</p>
    <p>Approach: rule-based</p>
    <p>Results:</p>
    <p>F1</p>
    <p>Gold Syntax 89.2</p>
    <p>Auto Syntax 85.9</p>
  </div>
  <div class="page">
    <p>Disambiguation Results</p>
    <p>With gold standard syntax &amp; target identification:</p>
    <p>Role Acc Fxn Acc Full Acc</p>
    <p>Most Frequent Neural Feature-rich linear</p>
  </div>
  <div class="page">
    <p>Predicting function label is more difficult than role label  ~8% gap in F1 score in both settings</p>
    <p>This mirrors a similar effect in IAA, and is probably due to:  Less ambiguity in function labels (given a preposition)</p>
    <p>The more literal nature of function labels</p>
    <p>Syntax plays an important role  4-7% difference in performance</p>
    <p>Results: Summary</p>
  </div>
  <div class="page">
    <p>Neural and feature-rich approach are not far off in terms of performance  Feature-rich is marginally better</p>
    <p>They agree on about 2/3 of cases; agreement area is 5% more accurate</p>
    <p>Results: Summary</p>
  </div>
  <div class="page">
    <p>Multi-Lingual Perspective</p>
    <p>Work is underway in Chinese, Korean, Hebrew and German</p>
    <p>Parallel Text: The Little Prince</p>
    <p>Challenges:</p>
    <p>Complex interaction with morphology (e.g., via case)</p>
    <p>How do prepositions change in translation?</p>
    <p>How do role/function labels change in translation?</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>A new approach to comprehensive analysis of the semantics of prepositions and possessives in English  Simpler and more concise than previous version</p>
    <p>Good inter-annotator agreement</p>
    <p>Extensive documentation</p>
    <p>Encouraging initial disambiguation results</p>
  </div>
  <div class="page">
    <p>Ongoing Work</p>
    <p>Focus on:  Multi-lingual extensions to four languages</p>
    <p>Streamlining the documentation and annotation processes</p>
    <p>Semi-supervised and multi-lingual disambiguation systems</p>
    <p>Integrating the scheme with a structural scheme (UCCA)</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>Discussion and Support</p>
    <p>Oliver Richardson Na-Rae Han Archna Bhatia Tim OGorman Ken Litkowski Bill Croft Martha Palmer</p>
    <p>CU annotators</p>
    <p>Evan Coles-Harris Audrey Farber Nicole Gordiyenko Megan Hutto Celeste Smitz Tim Watervoort</p>
    <p>CMU pilot annotators</p>
    <p>Archna Bhatia Carlos Ramirez Yulia Tsvetkov Michael Mordowanec Matt Gardner Spencer Onuffer Nora Kazour</p>
    <p>Special Thanks</p>
    <p>Noah Smith Mark Steedman Claire Bonial Tim Baldwin Miriam Butt Chris Dyer Ed Hovy Lingpeng Kong Lori Levin Ken Litkowski Orin Hargraves Michael Ellsworth Dipanjan Das &amp; Google</p>
  </div>
</Presentation>
