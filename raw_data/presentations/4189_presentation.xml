<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A D I T Y A B H A R G A V A A N D G R Z E G O R Z K O N D R A K</p>
    <p>U N I V E R S I T Y O F A L B E R T A</p>
    <p>N A A C L - H L T 2 0 1 0</p>
    <p>J U N E 3 , 2 0 1 0</p>
    <p>Language identification of names with SVMs</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction: task definition &amp; motivation</p>
    <p>Previous work: character language models</p>
    <p>Using SVMs</p>
    <p>Intrinsic evaluation</p>
    <p>SVMs outperform language models</p>
    <p>Applying language identification to machine transliteration</p>
    <p>Training separate models</p>
    <p>Conclusion &amp; future work</p>
  </div>
  <div class="page">
    <p>Task definition</p>
    <p>Given a name, what is its language?</p>
    <p>Same script (no diacritics)</p>
    <p>Beckham English</p>
    <p>Brillault French</p>
    <p>Velazquez Spanish</p>
    <p>Friesenbichler German</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Improving letter-to-phoneme performance (Font Llitjs and Black, 2001)</p>
    <p>Improving machine transliteration performance (Huang, 2005)</p>
    <p>Adjusting for different semantic transliteration rules between languages (Li et al., 2007)</p>
  </div>
  <div class="page">
    <p>Previous approaches</p>
    <p>Character language models (Cavnar and Trenkle, 1994)  Construct models for each language, then choose the language with</p>
    <p>the most similar model to the test data</p>
    <p>99.5% accuracy given &gt;300 characters &amp; 14 languages</p>
    <p>Given 50 bytes (and 17 languages), language models give only 90.2% (Kruengkrai et al., 2005)</p>
    <p>Between 13 languages, average F1 on last names is 50%; full names gives 60% (Konstantopoulos, 2007)</p>
    <p>Easier with more dissimilar languages: English vs. Chinese vs. Japanese (same script) gives 94.8% (Li et al., 2007)</p>
  </div>
  <div class="page">
    <p>Using SVMs</p>
    <p>Features</p>
    <p>Substrings (n-grams) of length n for n=1 to 5</p>
    <p>Include special characters at the beginning and the end to account for prefixes and suffixes</p>
    <p>Length of string</p>
    <p>Kernels</p>
    <p>Linear, sigmoid, RBF</p>
    <p>Other kernels (polynomial, string kernels) did not work well</p>
  </div>
  <div class="page">
    <p>Evaluation: Transfermarkt corpus</p>
    <p>European national soccer player names (Konstantopoulos, 2007) from 13 national languages</p>
    <p>~15k full names (average length 14.8 characters)</p>
    <p>~12k last names (average length 7.8 characters)</p>
    <p>Noisy data</p>
    <p>e.g. Dario Dakovic born in Bosnia but plays for Austria, so annotated as German</p>
  </div>
  <div class="page">
    <p>Evaluation: Transfermarkt corpus</p>
    <p>Last names Full names</p>
    <p>A c</p>
    <p>c u</p>
    <p>r a</p>
    <p>c y</p>
    <p>Language models</p>
    <p>Linear SVM</p>
    <p>RBF SVM</p>
    <p>Sigmoid SVM</p>
  </div>
  <div class="page">
    <p>Evaluation: Transfermarkt corpus</p>
    <p>cs da de en es fr it nl no pl pt se yu Recall cs 19 0 15 4 1 3 1 0 0 4 2 1 7 0.33 da 0 27 15 2 0 3 1 1 9 0 0 1 0 0.46 de 4 2 183 12 2 11 2 12 5 10 2 2 9 0.72 en 0 1 20 69 1 12 2 2 1 2 1 0 0 0.62 es 2 0 9 4 25 7 23 0 0 1 9 0 2 0.31 fr 0 0 17 10 5 41 13 1 1 1 4 0 2 0.43 it 1 0 6 2 10 5 84 0 0 2 2 0 1 0.74 nl 1 3 19 9 3 9 1 36 1 2 1 0 0 0.42 no 1 7 9 1 1 3 1 3 17 1 0 2 1 0.36 pl 2 0 13 2 3 3 1 2 1 63 0 0 3 0.68 pt 1 0 4 4 8 7 8 1 0 1 8 0 1 0.19 se 2 0 14 0 1 2 1 2 2 1 1 23 4 0.43 yu 3 0 11 1 2 0 4 1 0 2 0 2 84 0.76 Precision 0.53 0.68 0.55 0.58 0.40 0.39 0.59 0.59 0.46 0.70 0.27 0.74 0.74</p>
  </div>
  <div class="page">
    <p>Evaluation: CEJ corpus</p>
    <p>Chinese, English, and Japanese names (Li et al., 2007)</p>
    <p>~97k total names, average length 7.6 characters</p>
    <p>Demonstrates a higher baseline with dissimilar languages</p>
    <p>Linear SVM only (RBF and sigmoid were slow)</p>
    <p>A c</p>
    <p>c u</p>
    <p>r a</p>
    <p>c y Language</p>
    <p>models</p>
    <p>Linear SVM</p>
  </div>
  <div class="page">
    <p>Application to machine transliteration</p>
    <p>Language origin knowledge may help machine transliteration systems pick appropriate rules</p>
    <p>To test, we manually annotated data</p>
    <p>English-Hindi transliteration data set from the NEWS 2009 shared task (Li et al., 2009; MSRI, 2009)</p>
    <p>454 Indian names, 546 non-Indian names</p>
    <p>Average length 7 characters</p>
    <p>SVM gives 84% language identification accuracy</p>
  </div>
  <div class="page">
    <p>Application to machine transliteration</p>
    <p>Basic idea: use language identification to split data into two language-specific sets</p>
    <p>Train two separate transliteration models (with less data per model), then combine</p>
    <p>We use DirecTL (Jiampojamarn et al., 2009)</p>
    <p>Baseline comparison: random split</p>
    <p>Three tests:</p>
    <p>DirecTL (Standard)</p>
    <p>DirecTL with random split (Random)</p>
    <p>DirecTL with language identificationinformed split (LangID)</p>
  </div>
  <div class="page">
    <p>Application to machine transliteration</p>
    <p>T o</p>
    <p>p -1</p>
    <p>a c</p>
    <p>c u</p>
    <p>r a</p>
    <p>c y</p>
    <p>Standard</p>
    <p>Random</p>
    <p>LangID</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Language identification of names is difficult</p>
    <p>SVMs with n-grams as features work better than language models</p>
    <p>No significant effect on machine transliteration</p>
    <p>But there does seem to be some useful information</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Web data</p>
    <p>Other ways of incorporating language information for machine transliteration</p>
    <p>Direct use as a feature</p>
    <p>Overlapping (non-disjoint) splits</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
