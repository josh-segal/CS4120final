<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Knowledge-Gradient Stopping Rule for Ranking and Selection</p>
    <p>Peter Frazier, Warren Powell</p>
    <p>Department of Operations Research and Financial Engineering Princeton University</p>
    <p>December 8, 2008 Winter Simulation Conefernce</p>
  </div>
  <div class="page">
    <p>Ranking and Selection</p>
    <p>We have a collection of M alternatives, e.g. hospital staffing policies.</p>
    <p>In each simulation run, we select one alternative and use simulation to obtain a noisy measurement of its value.</p>
    <p>At a time of our choosing, we stop simulating and choose one alternative as the best.</p>
    <p>Question: Which alternatives should we sample? When should we stop sampling?</p>
  </div>
  <div class="page">
    <p>Objective Function</p>
    <p>If we take  samples and, after stopping, choose alternative x as the one we believe to be the best, we earn a reward x C ( ).</p>
    <p>The cost of sampling is given by C ( ). Possible choices for C :</p>
    <p>C (t) = ct, (linear).  C (t) = c1t1{t&lt;T} + c2t1{tT}, (delay is more costly after T ).  C (t) = 1{tT}, (a fixed measurement budget of T ).  Anything convex and non-decreasing.</p>
    <p>Goal: Use measurements as efficiently as possible to discover a good alternative.</p>
  </div>
  <div class="page">
    <p>Model: Prior Distribution</p>
    <p>Observations of alternative x are independent and normally distributed with mean x and precision x ,</p>
    <p>samples from alt. x  N (x , 1/x ).</p>
    <p>We begin with a prior belief on these unknown values given by,</p>
    <p>x  Gamma(a0x , b 0 x )</p>
    <p>x | x  Normal( 0x , 1/ 0 x x ),</p>
    <p>with the pairs (x , x ), x = 1, . . . , M, independent. One common choice is the noninformative prior:</p>
    <p>a0x = 1/2; b 0 x = 0;</p>
  </div>
  <div class="page">
    <p>Model: Posterior Distribution</p>
    <p>After n observations, the posterior belief on the sampling distribution for alternative x is</p>
    <p>x  Gamma(anx , b n x )</p>
    <p>x | x  Normal( nx , 1/ n x x ),</p>
    <p>where the anx ,b n x ,</p>
    <p>n x and</p>
    <p>n x are computed recursively from the</p>
    <p>observations and the prior.</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean</p>
    <p>sa m</p>
    <p>p lin</p>
    <p>g p</p>
    <p>re ci</p>
    <p>si o</p>
    <p>n</p>
    <p>Belief an=3.0 bn=3.3 n=7.0 n=1.6</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Bayesian Updating Example</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean sa</p>
    <p>m p</p>
    <p>lin g</p>
    <p>p re</p>
    <p>ci si</p>
    <p>o n</p>
    <p>Belief an=1.0 bn=0.7 n=3.0 n=0.9</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Bayesian Updating Example</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean sa</p>
    <p>m p</p>
    <p>lin g</p>
    <p>p re</p>
    <p>ci si</p>
    <p>o n</p>
    <p>Belief an=1.5 bn=1.0 n=4.0 n=1.1</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Bayesian Updating Example</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean sa</p>
    <p>m p</p>
    <p>lin g</p>
    <p>p re</p>
    <p>ci si</p>
    <p>o n</p>
    <p>Belief an=2.0 bn=1.4 n=5.0 n=1.3</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Bayesian Updating Example</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean sa</p>
    <p>m p</p>
    <p>lin g</p>
    <p>p re</p>
    <p>ci si</p>
    <p>o n</p>
    <p>Belief an=2.5 bn=3.2 n=6.0 n=1.7</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Bayesian Updating Example</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean sa</p>
    <p>m p</p>
    <p>lin g</p>
    <p>p re</p>
    <p>ci si</p>
    <p>o n</p>
    <p>Belief an=3.0 bn=3.3 n=7.0 n=1.6</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Bayesian Updating Example</p>
    <p>5 5 0</p>
    <p>=1.7</p>
    <p>1/2=(0.7)1/2</p>
    <p>observations</p>
    <p>d e</p>
    <p>n si</p>
    <p>ty</p>
    <p>Sampling Density</p>
    <p>2 0 2 0</p>
    <p>sampling mean sa</p>
    <p>m p</p>
    <p>lin g</p>
    <p>p re</p>
    <p>ci si</p>
    <p>o n</p>
    <p>Belief an=3.5 bn=3.6 n=8.0 n=1.7</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Sequential R&amp;S as a Stochastic Optimization Problem</p>
    <p>Formally, the problem is</p>
    <p>sup ,</p>
    <p>E [ max</p>
    <p>x</p>
    <p>x C ( )</p>
    <p>] ,</p>
    <p>where  ranges over the set of adapted sampling policies and  over the set of stopping times.</p>
    <p>This is a stochastic optimization problem whose state space has 2M continuous dimensions (bx and</p>
    <p>n x ) and M + 1</p>
    <p>discrete dimensions (x and ax , which move together, and n).</p>
    <p>This state space is too large to solve this problem with existing dynamic programming techniques.</p>
    <p>Instead, we search for heuristic techniques that perform well and have good intuition behind them.</p>
  </div>
  <div class="page">
    <p>Knowledge-Gradient Policy</p>
    <p>Define the knowledge-gradient (KG) factor,  nx , for measuring a given alternative x to be the marginal value of that measurement:</p>
    <p>n x = En</p>
    <p>[ max</p>
    <p>x</p>
    <p>n+1 x | x</p>
    <p>n = x</p>
    <p>] max</p>
    <p>x</p>
    <p>n x.</p>
    <p>The KG policy is:</p>
    <p>The KG policy is optimal for a version of the problem in which  is restricted to   n + 1.</p>
  </div>
  <div class="page">
    <p>Calculating the Knowledge-Gradient Factor</p>
    <p>The knowledge-gradient factor can be calculated as</p>
    <p>n x =</p>
    <p>1/2 {x}  nx</p>
    <p>(</p>
    <p>n x max</p>
    <p>x 6=x</p>
    <p>n x| )</p>
    <p>,</p>
    <p>where {x} and d are defined by</p>
    <p>{x} :=  n x (</p>
    <p>n x + 1)a</p>
    <p>n x /b</p>
    <p>n x ,</p>
    <p>d (s) :=</p>
    <p>u=s d (u) du =</p>
    <p>d + s2</p>
    <p>d 1 d (s)sd (s),</p>
    <p>and d and d are respectively the cdf and pdf of the student-t distribution with d degrees of freedom. (Chick,Branke &amp; Schmidt 2007).</p>
  </div>
  <div class="page">
    <p>Other Sampling and Stopping Rules</p>
    <p>LL1 fully sequential sampling rule (Chick,Branke &amp; Schmidt 2007): Sample arg maxx</p>
    <p>n x . This is the sampling portion of</p>
    <p>the KG stopping/sampling rule.</p>
    <p>EOCBonf stopping rule (Branke, Chick, &amp; Schmidt 2005): At each time, use the Bonferonni inequality to estimate the expected opportunity cost incurred by stopping now, and stop when this estimate drops below a threshold.</p>
    <p>LL(S) batch sequential sampling rule (Chick &amp; Inoue 2001): Approximate the benefit of a batch of measurements using the Bonferonni inequality, and choose the allocation of the next batch that optimizes this approximation.</p>
  </div>
  <div class="page">
    <p>Numerical Results: Standard Configurations</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>Monotone Decreasing Means mu=[0, -.5, -1, ..., -4.5]</p>
    <p>beta=[1, ..., 1]</p>
    <p>KG LL1, Fixed N</p>
    <p>LL1, EOC Bonf</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>Slippage Configuration mu=[0, -.5, -.5, -.5, -.5]</p>
    <p>beta=[1, 1, 1, 1, 1]</p>
    <p>KG LL1, Fixed N</p>
    <p>LL1, EOC Bonf</p>
  </div>
  <div class="page">
    <p>Numerical Results: Standard Configurations</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>Monotone Decreasing Means mu=[0, -.5, -1, ..., -4.5]</p>
    <p>beta=[1, ..., 1]</p>
    <p>KG LL, KG stop</p>
    <p>LL, EOC Bonf</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>Slippage Configuration mu=[0, -.5, -.5, -.5, -.5]</p>
    <p>beta=[1, 1, 1, 1, 1]</p>
    <p>KG LL, KG stop</p>
    <p>LL, EOC Bonf</p>
  </div>
  <div class="page">
    <p>Numerical Results: Random Configurations</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL1, Fixed N</p>
    <p>LL1, EOC Bonf</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL1, Fixed N</p>
    <p>LL1, EOC Bonf</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL1, Fixed N</p>
    <p>LL1, EOC Bonf</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL1, Fixed N</p>
    <p>LL1, EOC Bonf</p>
    <p>Random configurations with 5 alternatives. Sampling means and precisions were drawn randomly from a normal-gamma distribution.</p>
  </div>
  <div class="page">
    <p>Numerical Results: Random Configurations</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL, KG stopping</p>
    <p>LL, EOC Bonf</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL, KG stopping</p>
    <p>LL, EOC Bonf</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL, KG stopping</p>
    <p>LL, EOC Bonf</p>
    <p>lo g 1</p>
    <p>[O C</p>
    <p>])</p>
    <p>E[]</p>
    <p>KG LL, KG stopping</p>
    <p>LL, EOC Bonf</p>
    <p>Random configurations with 5 alternatives. Sampling means and precisions were drawn randomly from a normal-gamma distribution.</p>
  </div>
  <div class="page">
    <p>Summary of Numerical Experiments</p>
    <p>When using LL1 sampling, KG stopping is best.</p>
    <p>When using LL sampling, EOCBonf stopping is best.</p>
    <p>LL1/KG and LL/EOCBonf perform similarly.</p>
  </div>
  <div class="page">
    <p>Proposition</p>
    <p>Proposition</p>
    <p>Let  KG be the KG stopping rule and let  be the optimal stopping rule for the problem</p>
    <p>sup</p>
    <p>E=LL1 [ max</p>
    <p>x</p>
    <p>x C ( )</p>
    <p>] .</p>
    <p>Then    KG almost surely. In other words, the KG stopping rule is a lower bound on the optimal stopping rule.</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Example</p>
    <p>4</p>
    <p>2</p>
    <p>alternatives</p>
    <p>va lu</p>
    <p>e</p>
    <p>8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>num measurements</p>
    <p>lo g</p>
    <p>(K G</p>
    <p>f a</p>
    <p>ct o</p>
    <p>r)</p>
    <p>num measurements</p>
    <p>o p</p>
    <p>p o</p>
    <p>rt u</p>
    <p>n ity</p>
    <p>c o</p>
    <p>st</p>
  </div>
  <div class="page">
    <p>Correlated Knowledge Gradients: Average Performance</p>
    <p>1</p>
    <p>0.5</p>
    <p>E[N]</p>
    <p>lo g</p>
    <p>[O C</p>
    <p>])</p>
    <p>Fixed stopping KG stopping</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>The KG policy offers a flexible decision-theoretic framework that can account for a variety of cost criteria and statistical models.</p>
    <p>The choice of stopping rule should be made together with the choice of sampling rule. (EOCBonf for LL, KG for LL1).</p>
  </div>
</Presentation>
