<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Akshitha Sriraman, Thomas F. Wenisch University of Michigan</p>
    <p>OSDI 2018 10/08/2018</p>
  </div>
  <div class="page">
    <p>OLDI: From Monoliths to Microservices</p>
    <p>&gt; 100 ms SLO</p>
    <p>Monolith Microservices</p>
    <p>Sub-ms SLO</p>
    <p>Sub-ms-scale system overheads must be characterized for microservices</p>
    <p>Monolithic)service</p>
    <p>Scaling Scaling</p>
    <p>RPC</p>
    <p>Microservice</p>
    <p>Monolith</p>
  </div>
  <div class="page">
    <p>Impact of Threading on Microservices  Our focus: Sub-ms overheads due to threading design</p>
    <p>Threading model that exhibits best tail latency is a funcNon of load</p>
    <p>Lock contenNon Thread wakeups Spurious context switch</p>
    <p>Polling</p>
    <p>Low load</p>
    <p>Blocking Polling</p>
    <p>Polling</p>
    <p>High load</p>
  </div>
  <div class="page">
    <p>ContribuNons  A taxonomy of threading models</p>
    <p>Structured understanding of threading implicaNons  Reveals tail inflecNon points across load  Peak load-sustaining model is worse at low load</p>
    <p>Tune:  Uses tail inflecNon insights to opNmize tail latency  Tunes model &amp; thread pool size across load  Simple interface: Abstracts threading model from RPC code</p>
    <p>Latency</p>
    <p>Up to 1.9x tail improvement over staNc throughput-opNmized model</p>
  </div>
  <div class="page">
    <p>Outline  MoNvaNon  A taxonomy of threading models  Tune:</p>
    <p>Simple interface design  AutomaNc load adaptaNon system</p>
    <p>EvaluaNon</p>
  </div>
  <div class="page">
    <p>Mid-Ner Faces More Threading Overheads</p>
    <p>Front-End Microserver Mid-Tier Microserver</p>
    <p>Leaf Microserver 1</p>
    <p>Leaf Microserver 2  Mid-Ner  subject to more threading overheads</p>
    <p>Manages RPC fan-out to many leaves  RPC layer interacNons dominate computaNon</p>
    <p>Threading overheads must be characterized for mid-%er microservices</p>
  </div>
  <div class="page">
    <p>A Taxonomy of Threading Models</p>
    <p>Front-End Mid-Tier Leaf</p>
    <p>NW socket</p>
    <p>(1) Poll vs. Block Request</p>
    <p>(2) In-line vs. Dispatch</p>
    <p>(3) Synchronous vs.</p>
    <p>Asynchronous</p>
    <p>Synchronous</p>
    <p>Asynchronous</p>
    <p>Block Poll</p>
    <p>In-line SIB SIP</p>
    <p>Dispatch SDB SDP</p>
    <p>Block Poll</p>
    <p>In-line AIB AIP</p>
    <p>Dispatch ADB ADP</p>
    <p>vs. ...</p>
    <p>vs. ...</p>
    <p>vs. ...</p>
    <p>Network poller</p>
    <p>Worker</p>
    <p>Response</p>
  </div>
  <div class="page">
    <p>Latency Tradeoffs Across Threading Models</p>
    <p>Load (Queries Per Second)</p>
    <p>saturaNon</p>
    <p>HDSearch: Sync.</p>
    <p>In-line Block</p>
    <p>In-line Poll</p>
    <p>Dispatch Block</p>
    <p>Dispatch Poll</p>
    <p>In-line Poll has lowest low-load latency: Avoids thread wakeup delays</p>
  </div>
  <div class="page">
    <p>Latency Tradeoffs Across Threading Models</p>
    <p>Load (Queries Per Second)</p>
    <p>saturaNon</p>
    <p>HDSearch: Sync.</p>
    <p>In-line Block</p>
    <p>In-line Poll</p>
    <p>Dispatch Block</p>
    <p>Dispatch Poll</p>
    <p>In-Line Poll faces contenNon; Dispatch Poll with one network poller is best</p>
  </div>
  <div class="page">
    <p>Latency Tradeoffs Across Threading Models</p>
    <p>Load (Queries Per Second)</p>
    <p>saturaNon</p>
    <p>HDSearch: Sync.</p>
    <p>In-line Block</p>
    <p>In-line Poll</p>
    <p>Dispatch Block</p>
    <p>Dispatch Poll</p>
    <p>Dispatch Block is best at high load as it does not waste CPU</p>
  </div>
  <div class="page">
    <p>Latency Tradeoffs Across Threading Models</p>
    <p>Load (Queries Per Second)</p>
    <p>saturaNon</p>
    <p>HDSearch: Sync.</p>
    <p>In-line Block</p>
    <p>In-line Poll</p>
    <p>Dispatch Block</p>
    <p>Dispatch Poll</p>
    <p>No single threading model works best at all loads</p>
  </div>
  <div class="page">
    <p>Threading choice can significantly affect tail latency  Threading latency trade-offs are not obvious  Most sohware face latency penalNes due to staNc threading</p>
    <p>Need for AutomaNc Load AdaptaNon: Tune</p>
    <p>Opportunity: Exploit trade-offs among threading models at run-Nme</p>
  </div>
  <div class="page">
    <p>Load adaptaNon: Vary threading model &amp; pool size at run-Nme  Abstract threading model boiler-plate code from RPC code</p>
    <p>Microservice funcNonality: ProcessReq(), InvokeLeaf(), FinalizeResp()</p>
    <p>Tune automaNc load adaptaNon system</p>
    <p>RPC layer</p>
    <p>App layer</p>
    <p>Tune</p>
    <p>Network layer</p>
    <p>Tune</p>
    <p>Simple interface: Developer defines only three funcNons</p>
  </div>
  <div class="page">
    <p>Tune: Goals &amp; Challenges</p>
    <p>Service code</p>
    <p>Tunes threading framework</p>
    <p>Simple interface</p>
    <p>Quick load change detecNon</p>
    <p>Fast threading model switches</p>
    <p>Scale thread pools</p>
  </div>
  <div class="page">
    <p>Tune System Design: Auto-Tuner  Dynamically picks threading model &amp; pool sizes based on load</p>
    <p>Request rate Best TM Ideal no. of threads</p>
    <p>. . .</p>
    <p>Offline training</p>
    <p>Create piecewise linear model</p>
    <p>Mid-tier Leaf</p>
    <p>Front end</p>
    <p>Request</p>
    <p>Compute</p>
    <p>In-line thread</p>
    <p>In-line thread</p>
    <p>Response</p>
    <p>NW socket</p>
    <p>In-line thread: &lt;block&gt;</p>
    <p>Synchronous</p>
    <p>Mid-tier Leaf</p>
    <p>Front end</p>
    <p>Request</p>
    <p>Compute</p>
    <p>In-line thread</p>
    <p>Response</p>
    <p>NW socket</p>
    <p>Synchronous</p>
    <p>(a) (b)</p>
    <p>Mid-tier Frontend</p>
    <p>Request</p>
    <p>Worker</p>
    <p>Response</p>
    <p>NW socket</p>
    <p>Network thread: &lt;block&gt;</p>
    <p>Task queue</p>
    <p>Worker notified</p>
    <p>Worker awaits notification</p>
    <p>Synchronous</p>
    <p>Dispatch</p>
    <p>Leaf</p>
    <p>Compute</p>
    <p>Mid-tier Frontend</p>
    <p>Request</p>
    <p>Worker</p>
    <p>Response</p>
    <p>NW socket</p>
    <p>Network thread: &lt;poll&gt;</p>
    <p>Task queue</p>
    <p>Worker notified</p>
    <p>Worker awaits notification</p>
    <p>Synchronous</p>
    <p>Dispatch</p>
    <p>Leaf</p>
    <p>Compute</p>
    <p>(a) (b)</p>
    <p>Mid-tier Frontend</p>
    <p>Request</p>
    <p>Compute</p>
    <p>In-line thread</p>
    <p>Resp. thread: &lt;block&gt;</p>
    <p>Response</p>
    <p>NW (server) socket</p>
    <p>In-line thread: &lt;block&gt;</p>
    <p>Asynchronous</p>
    <p>Leaf</p>
    <p>NW (client) socket</p>
    <p>Mid-tier Frontend</p>
    <p>Request</p>
    <p>Compute</p>
    <p>In-line thread</p>
    <p>Resp. thread: &lt;poll&gt;</p>
    <p>Response</p>
    <p>NW (server) socket</p>
    <p>In-line thread: &lt;poll&gt;</p>
    <p>Asynchronous</p>
    <p>Leaf</p>
    <p>NW (client) socket</p>
    <p>(a) (b)</p>
    <p>Frontend</p>
    <p>Request</p>
    <p>Network thread: &lt;block&gt;</p>
    <p>Task queue</p>
    <p>Worker notified</p>
    <p>Dispatch</p>
    <p>Worker awaits notification</p>
    <p>Asynchronous</p>
    <p>Mid-tier Leaf</p>
    <p>Compute Response</p>
    <p>NW (client) socket</p>
    <p>NW (server) socket</p>
    <p>Resp. thread: &lt;block&gt;</p>
    <p>Request</p>
    <p>Network thread: &lt;poll&gt;</p>
    <p>Task queue</p>
    <p>Worker notified</p>
    <p>Dispatch</p>
    <p>Worker awaits notification</p>
    <p>Asynchronous</p>
    <p>Compute Response</p>
    <p>NW (client) socket</p>
    <p>NW (server) socket</p>
    <p>Resp. thread: &lt;poll&gt;</p>
    <p>Frontend</p>
    <p>Mid-tier Leaf</p>
    <p>(a) (b)</p>
    <p>SIB SIP SDB SDP AIB AIP ADB ADP</p>
    <p>(a) Tune framework</p>
    <p>Offline training</p>
    <p>(b) Async. Tunes automatic load adaptation system</p>
    <p>Create piecewise linear model</p>
    <p>Request'rate' Best'TM' Ideal'no.'of'threads'</p>
    <p>.'</p>
    <p>.'</p>
    <p>ADB' NW'poller:'one'' Workers:'few'(eg.'4),' Resp.'threads:'few'</p>
    <p>Online: Request from front-end</p>
    <p>gRPC</p>
    <p>Circular event buffer 1</p>
    <p>Request' rate'</p>
    <p>compute'</p>
    <p>Send to switching logic</p>
    <p>Switch'to'best' TM'and'</p>
    <p>thread'poll' sizes'if'needed'</p>
    <p>ProcessRequest()</p>
    <p>InvokeLeafAsync()</p>
    <p>Request to leaf</p>
    <p>Mid-tier Frontend</p>
    <p>Request</p>
    <p>Compute</p>
    <p>In-line thread</p>
    <p>Resp. thread: &lt;block&gt;</p>
    <p>Response</p>
    <p>NW (server) socket</p>
    <p>In-line thread: &lt;block&gt;</p>
    <p>Asynchronous</p>
    <p>Leaf</p>
    <p>NW (client) socket</p>
    <p>Mid-tier Frontend</p>
    <p>Request</p>
    <p>Compute</p>
    <p>In-line thread</p>
    <p>Resp. thread: &lt;poll&gt;</p>
    <p>Response</p>
    <p>NW (server) socket</p>
    <p>In-line thread: &lt;poll&gt;</p>
    <p>Asynchronous</p>
    <p>Leaf</p>
    <p>NW (client) socket</p>
    <p>(a) (b)</p>
    <p>Frontend</p>
    <p>Request</p>
    <p>Network thread: &lt;block&gt;</p>
    <p>Task queue</p>
    <p>Worker notified</p>
    <p>Dispatch</p>
    <p>Worker awaits notification</p>
    <p>Asynchronous</p>
    <p>Mid-tier Leaf</p>
    <p>Compute Response</p>
    <p>NW (client) socket</p>
    <p>NW (server) socket</p>
    <p>Resp. thread: &lt;block&gt;</p>
    <p>Request</p>
    <p>Network thread: &lt;poll&gt;</p>
    <p>Task queue</p>
    <p>Worker notified</p>
    <p>Dispatch</p>
    <p>Worker awaits notification</p>
    <p>Asynchronous</p>
    <p>Compute Response</p>
    <p>NW (client) socket</p>
    <p>NW (server) socket</p>
    <p>Resp. thread: &lt;poll&gt;</p>
    <p>Frontend</p>
    <p>Mid-tier Leaf</p>
    <p>(a) (b)</p>
    <p>FinalizeResponse() Response from leaf Response to</p>
    <p>front-end 3</p>
    <p>Circular event buffer Online: Request from</p>
    <p>front-end gRPC</p>
    <p>Request rate compute</p>
    <p>Send to switching logic</p>
    <p>Switch to best TM &amp; thread pool</p>
    <p>sizes</p>
    <p>Request to leaf</p>
  </div>
  <div class="page">
    <p>Experimental Setup  Suite [Sriraman 18] benchmark suite:</p>
    <p>Load generator, a mid-Ner, 4 or 16 leaf microservers  Study Tunes adaptaNon in two load scenarios:</p>
    <p>Steady-state  Transient</p>
  </div>
  <div class="page">
    <p>EvaluaNon: Tunes Load AdaptaNon</p>
    <p>Converges to best threading model &amp; pool sizes to improve tails by up to 1.9x</p>
    <p>Load (Queries Per Second)</p>
    <p>6.17  HDSearch: Async.</p>
    <p>In-Line Poll Dispatch Poll Dispatch Block Haque 15 Abdelzaher 99 Tune</p>
    <p>saturaNon</p>
    <p>&lt;5% mean overhead</p>
  </div>
  <div class="page">
    <p>Conclusion  Taxonomy of threading models</p>
    <p>No single threading model has best tail latency across all load  Tune  threading model framework + load adaptaNon system  Achieved up to 1.9x tail speedup over best staNc model</p>
  </div>
  <div class="page">
    <p>Tune: Auto-Tuned Threading for OLDI Microservices Akshitha Sriraman, Thomas F. Wenisch</p>
    <p>University of Michigan</p>
    <p>hwps://github.com/wenischlab/MicroTune</p>
    <p>Poster number: 29</p>
  </div>
</Presentation>
