<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Lets do it again: A First Computational Approach to Detecting Adverbial Presupposition Triggers</p>
    <p>( * E Q U A L C O N T R I B U T I O N )</p>
    <p>A N D R E C I A N F L O N E * , Y U L A N F E N G * , JA D K A B B A R A * &amp; J A C K I E C K C H E U N G</p>
  </div>
  <div class="page">
    <p>Again</p>
    <p>Make the middle class mean something again, with rising incomes and broader horizons.</p>
    <p>Heard on the campaign trail:</p>
    <p>Make America great again.</p>
    <p>Hillary Clinton</p>
    <p>Donald Trump</p>
  </div>
  <div class="page">
    <p>What is presupposition?</p>
    <p>Presuppositions: assumptions shared by discourse participants in an utterance (Frege 1892, Strawson 1950, Stalnaker 1973, Stalnaker1998).  Presupposition triggers: expressions that indicate the presence of presuppositions.  Example: Oops! I did it again</p>
    <p>Presupposes Britney did it before Trigger</p>
  </div>
  <div class="page">
    <p>Linguistic Analysis</p>
    <p>Presuppositions are preconditions for statements to be true or false (Kaplan 1970; Strawson, 1950).  Classes of construction that can trigger presupposition (Zare et al., 2012):</p>
    <p>Definite descriptions (Kabbara et al., 2016), e.g.: The queen of the United Kingdom.</p>
    <p>Stressed constituents (Krifka, 1998), e.g.: Yes, Peter did eat pasta.  Factive verbs, e.g.: Michael regrets eating his mothers cookies.  Implicative verbs, e.g.: She managed to make it to the airport on time.  Relations between verbs (Tremper and Frank, 2013; Bos, 2003), e.g.:</p>
    <p>won &gt;&gt; played.</p>
  </div>
  <div class="page">
    <p>Motivation &amp; Applications</p>
    <p>Interesting testbed for pragmatic reasoning: investigating presupposition triggers requires understanding preceding context.  Presupposition triggers influencing political discourse:</p>
    <p>- The abundant use of presupposition triggers helps to better communicate political messages and consequently persuade the audience (Liang and Liu, 2016).</p>
    <p>To improve the readability and coherence in language generation applications (e.g., summarization, dialogue systems).</p>
  </div>
  <div class="page">
    <p>Adverbial Presupposition Triggers</p>
    <p>Adverbial presupposition triggers such as again, also, and still.  Indicate the recurrence, continuation, or termination of an event in the discourse context, or the presence of a similar event.  The most commonly occurring presupposition triggers (after existential triggers) (Khaleel, 2010).  Little work has been done on these triggers in the computational literature from a statistical, corpus-driven perspective.</p>
    <p>Existential All others (lexical and structural) Adverbial clauses</p>
  </div>
  <div class="page">
    <p>This Work</p>
    <p>Computational approach to detecting presupposition triggers.  Create new datasets for the task of detecting adverbial presupposition triggers.  Control for potential confounding factors such as class balance and syntactic governor of the triggering adverb.  Present a new weighted pooling attention mechanism for the task.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Task Definition Learning Model Experiments &amp; Results</p>
  </div>
  <div class="page">
    <p>Task</p>
    <p>Detect contexts in which adverbial presupposition triggers can be used.  Requires detecting recurring or similar events in the discourse context.  Five triggers of interest: too, again, also, still, yet.  Frame the learning problem as a binary classification for predicting the presence of an adverbial presupposition (as opposed to the identity of the adverb).</p>
  </div>
  <div class="page">
    <p>Sample Configuration</p>
    <p>3-tuple: label, list of tokens, list of POS tags.  Back to our example:</p>
    <p>Make America great again.</p>
  </div>
  <div class="page">
    <p>Sample Configuration</p>
    <p>3-tuple: label, list of tokens, list of POS tags.  Back to our example:</p>
    <p>Make America great again. Trigger</p>
  </div>
  <div class="page">
    <p>Sample Configuration</p>
    <p>3-tuple: label, list of tokens, list of POS tags.  Back to our example:</p>
    <p>Make America great again. Trigger</p>
    <p>Headword (aka governor of again)</p>
  </div>
  <div class="page">
    <p>Sample Configuration</p>
    <p>3-tuple: label, list of tokens, list of POS tags.  Back to our example:</p>
    <p>@@@@ Make America great again.</p>
    <p>Special token: to identify the candidate context in the passage to the model.</p>
    <p>Trigger</p>
    <p>Headword (aka governor of again)</p>
  </div>
  <div class="page">
    <p>Sample Configuration</p>
    <p>3-tuple: label, list of tokens, list of POS tags.  Back to our example:</p>
    <p>@@@@ Make America great again. Trigger</p>
    <p>Headword (aka governor of again)</p>
    <p>REMOVE ADVERBS</p>
  </div>
  <div class="page">
    <p>Sample Configuration</p>
    <p>3-tuple: label, list of tokens, list of POS tags.  Back to our example:</p>
    <p>( again, [@@@@, Make, America, great], [@@@@, VB, NNP, JJ ] )</p>
    <p>Trigger Tokens POS tags</p>
  </div>
  <div class="page">
    <p>Positive vs Negative Samples</p>
    <p>Negative samples - Same governors as in the positive cases but without triggering</p>
    <p>presupposition.</p>
    <p>Example of positive sample: - Juan is coming to the event too.</p>
    <p>Example of negative sample: - Whitney is coming tomorrow.</p>
  </div>
  <div class="page">
    <p>Extracting Positive Samples</p>
    <p>Scan through all the documents to search for target adverbs.  For each occurrence of a target adverb:</p>
    <p>- Store the location and the governor of the adverb. - Extract 50 unlemmatized tokens preceding the governor, together with the</p>
    <p>tokens right after it up to the end of the sentence (where the adverb is). - Remove adverb.</p>
  </div>
  <div class="page">
    <p>Extracting Negative Samples</p>
    <p>Extract sentences containing the same governors (as in the positive cases) but not any of the target adverbs. - Number of samples in the positive and negative classes roughly balanced.</p>
    <p>Negative samples are extracted/constructed in the same manner as the positive examples.</p>
  </div>
  <div class="page">
    <p>Position-Related Confounding Factors</p>
    <p>We try to control position-related confounding factors by two randomization approaches:</p>
    <p>document.</p>
  </div>
  <div class="page">
    <p>Learning Model</p>
    <p>Presupposition involves reasoning over multiple spans of text.  At a high level, our model extends a bidirectional LSTM model by:</p>
    <p>No new parameters compared to standard bidirectional LSTM.</p>
  </div>
  <div class="page">
    <p>Learning Model: Overview</p>
  </div>
  <div class="page">
    <p>Learning Model: Input</p>
    <p>Embedding + POS</p>
    <p>Embed input.  Optionally concatenate with POS tags.</p>
  </div>
  <div class="page">
    <p>Learning Model: RNN</p>
    <p>Bidirectional LSTM: Matrix ! = $||&amp;||||( concatenates all hidden states.  E.g.: We continue to feel that the stock market is the @@@@ place to be for long-term appreciation.</p>
    <p>biLSTM</p>
  </div>
  <div class="page">
    <p>Learning Model: Matching Matrix</p>
    <p>Pair-wise matching matrix M</p>
    <p>M = HTH</p>
  </div>
  <div class="page">
    <p>Learning Model: Softmax</p>
    <p>Column-wise softmax: Learn how to aggregate. softmax</p>
  </div>
  <div class="page">
    <p>Learning Model: Softmax</p>
    <p>Column-wise softmax: Learn how to aggregate.  Row-wise softmax: Attention distribution over words.</p>
    <p>softmax</p>
  </div>
  <div class="page">
    <p>Learning Model: Attention Score</p>
    <p>! The columns of &quot;# are then averaged, forming vector !.</p>
  </div>
  <div class="page">
    <p>Learning Model: Attention Score</p>
    <p>!</p>
    <p>The columns of &quot;# are then averaged, forming vector $.  Final attention vector !:</p>
    <p>! = &quot;&amp; $ based on (Cui et al., 2017).</p>
  </div>
  <div class="page">
    <p>Learning Model: Attend</p>
    <p>!</p>
    <p>Attend: ! = $%&amp;' ($$ .</p>
    <p>A form of self-attention (Paulus 2017, Vaswani 2017).</p>
  </div>
  <div class="page">
    <p>Learning Model: Predict</p>
    <p>Predict: - Dense layer: ! = # $%&amp; + (% .</p>
    <p>- Softmax: ) = *($,! + (,).</p>
  </div>
  <div class="page">
    <p>Datasets</p>
    <p>New datasets extracted from:  The English Gigaword corpus:</p>
    <p>- Individual sub-datasets (i.e., presence of each adverb vs. absence). - ALL (i.e., presence of one of the 5 adverbs vs. absence).  The Penn Tree Bank (PTB) corpus:</p>
    <p>- ALL. Corpus Training Test PTB 5,175 482 Gigaword yet 63,843 15840 Gigaword too 85,745 21501 Gigaword again 85,944 21762 Gigaword still 194,661 48741 Gigaword also 537,626 132928</p>
  </div>
  <div class="page">
    <p>Results Overview</p>
    <p>Our model outperforms all other models in 10 out of 14 scenarios (combinations of datasets and whether or not POS tags are used).  WP outperforms regular LSTM without introducing additional parameters.  For all models, we find that including POS tags benefits the detection of adverbial presupposition triggers in Gigaword and PTB datasets.</p>
  </div>
  <div class="page">
    <p>Results  WSJ</p>
    <p>WP best on WSJ.  RNNs outperform baselines by large margin.</p>
    <p>WSJ - Accuracy</p>
    <p>Models Variants All adverbs</p>
    <p>MFC - 51.66</p>
    <p>LogReg + POS 52.81</p>
    <p>- POS 54.47</p>
    <p>CNN + POS 58.84</p>
    <p>- POS 62.16</p>
    <p>LSTM + POS 74.23</p>
    <p>- POS 73.18</p>
    <p>WP + POS 76.09 - POS 74.84</p>
    <p>MFC: Most Frequent Class LogReg: Logistic Regression</p>
    <p>LSTM: bidirectional LSTM CNN: Convolutional Network based on (Kim 2014)</p>
  </div>
  <div class="page">
    <p>Results  Gigaword</p>
    <p>Baselines Gigaword - Accuracy</p>
    <p>Models Variants All adverbs Again Still Too Yet Also MFC - 50.24 50.25 50.29 65.06 50.19 50.32</p>
    <p>LogReg + POS 53.65 59.49 56.36 69.77 61.05 52.00 - POS 52.86 58.60 55.29 67.60 58.60 56.07</p>
    <p>CNN + POS 59.12 60.26 59.54 67.53 59.69 61.53 - POS 57.21 57.28 56.95 67.84 56.53 59.76</p>
    <p>LSTM + POS 60.58 61.81 60.72 69.70 59.13 81.48 - POS 58.86 59.93 58.97 68.32 55.71 81.16</p>
    <p>WP + POS 60.62 61.59 61.00 69.38 57.68 82.42 - POS 58.87 58.49 59.03 68.37 56.68 81.64</p>
  </div>
  <div class="page">
    <p>Results  Gigaword</p>
    <p>LSTM and LSTM with Attention (WP)</p>
    <p>Gigaword - Accuracy</p>
    <p>Models Variants All adverbs Again Still Too Yet Also MFC - 50.24 50.25 50.29 65.06 50.19 50.32</p>
    <p>LogReg + POS 53.65 59.49 56.36 69.77 61.05 52.00 - POS 52.86 58.60 55.29 67.60 58.60 56.07</p>
    <p>CNN + POS 59.12 60.26 59.54 67.53 59.69 61.53 - POS 57.21 57.28 56.95 67.84 56.53 59.76</p>
    <p>LSTM + POS 60.58 61.81 60.72 69.70 59.13 81.48 - POS 58.86 59.93 58.97 68.32 55.71 81.16</p>
    <p>WP + POS 60.62 61.59 61.00 69.38 57.68 82.42 - POS 58.87 58.49 59.03 68.37 56.68 81.64</p>
  </div>
  <div class="page">
    <p>Results  Gigaword</p>
    <p>WP outperforms in 10 out of 14 cases.  Better performance with POS.</p>
    <p>Gigaword - Accuracy</p>
    <p>Models Variants All adverbs Again Still Too Yet Also MFC - 50.24 50.25 50.29 65.06 50.19 50.32</p>
    <p>LogReg + POS 53.65 59.49 56.36 69.77 61.05 52.00 - POS 52.86 58.60 55.29 67.60 58.60 56.07</p>
    <p>CNN + POS 59.12 60.26 59.54 67.53 59.69 61.53 - POS 57.21 57.28 56.95 67.84 56.53 59.76</p>
    <p>LSTM + POS 60.58 61.81 60.72 69.70 59.13 81.48 - POS 58.86 59.93 58.97 68.32 55.71 81.16</p>
    <p>WP + POS 60.62 61.59 61.00 69.38 57.68 82.42 - POS 58.87 58.49 59.03 68.37 56.68 81.64</p>
  </div>
  <div class="page">
    <p>Qualitative Analysis</p>
    <p>Positive sample: ... We continue to feel that the stock market is the @@@@ place to be for long-term appreciation.</p>
    <p>Negative sample: ... Careers count most for the well-to-do. Many affluent people @@@@ place personal success and money above family.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>New task, detection of adverbial presupposition triggers  New datasets for the task.  New attention model tailored for the task.  Our model outperforms other strong baselines without additional parameters over the standard LSTM model.</p>
  </div>
  <div class="page">
    <p>Future Directions</p>
    <p>Incorporate such a system in an NLG pipeline (e.g., dialogue or summarization with text rewriting).  Discourse analysis with presupposition (e.g., political speech).  Investigate other types of presupposition.</p>
  </div>
  <div class="page">
    <p>Thank you! J</p>
    <p>Thank you to our co-authors:</p>
    <p>Yulan Feng Prof. Jackie CK Cheung</p>
    <p>Thank you to our sponsors:</p>
  </div>
</Presentation>
