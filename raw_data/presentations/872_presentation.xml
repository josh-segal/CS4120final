<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Diagnosing Performance Problems in Wide Area Providers</p>
    <p>Partha Kanuparthy, Constantine Dovrolis</p>
    <p>USENIX ATC 2014</p>
  </div>
  <div class="page">
    <p>Wide Area Providers  Internet providers: residential, enterprise, transit  Content and cloud providers</p>
  </div>
  <div class="page">
    <p>Wide Area Providers</p>
  </div>
  <div class="page">
    <p>Contain inter-domain paths</p>
    <p>Wide Area Providers</p>
  </div>
  <div class="page">
    <p>Contain inter-domain paths  Unknown changes: network upgrades, traffic matrix</p>
    <p>Wide Area Providers</p>
  </div>
  <div class="page">
    <p>Contain inter-domain paths  Unknown changes: network upgrades, traffic matrix  Unknown performance problems: new, short-lived,</p>
    <p>Wide Area Providers</p>
  </div>
  <div class="page">
    <p>Use monitoring deployments  troubleshooting long-term problems (offline)  e.g., perfSONAR: 1,000+ monitors  ping, traceroute: delay, loss, reordering, paths</p>
    <p>Wide Area Providers</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
  </div>
  <div class="page">
    <p>One system, three goals:  Detection: is there a performance problem?  Diagnosis: root cause of the problem?  Localization: where is the problem?</p>
    <p>What is Pythia?</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
  </div>
  <div class="page">
    <p>Pythia and Wide Area Providers</p>
    <p>Impact: Enable quick troubleshooting</p>
  </div>
  <div class="page">
    <p>Pythia and Wide Area Providers</p>
    <p>Near real time detection, diagnosis, localization</p>
    <p>Impact: Enable quick troubleshooting</p>
  </div>
  <div class="page">
    <p>Pythia and Wide Area Providers</p>
    <p>Near real time detection, diagnosis, localization  Works with data from existing monitoring deployments</p>
    <p>Impact: Enable quick troubleshooting</p>
  </div>
  <div class="page">
    <p>Pythia and Wide Area Providers</p>
    <p>Near real time detection, diagnosis, localization  Works with data from existing monitoring deployments  Diagnose short-lived performance problems</p>
    <p>Impact: Enable quick troubleshooting</p>
  </div>
  <div class="page">
    <p>Pythia and Wide Area Providers</p>
    <p>Near real time detection, diagnosis, localization  Works with data from existing monitoring deployments  Diagnose short-lived performance problems  Tackle unknown problems (e.g., inter-domain)</p>
    <p>Impact: Enable quick troubleshooting</p>
  </div>
  <div class="page">
    <p>Lightweight agents at ISP monitors  read measurement data in real time  detect and diagnose problems</p>
    <p>Typical Deployment</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>Agent</p>
    <p>Agent</p>
    <p>Agent</p>
    <p>Detection Diagnosis</p>
    <p>One-way delays, loss,</p>
    <p>reordering</p>
  </div>
  <div class="page">
    <p>Lightweight agents at ISP monitors  read measurement data in real time  detect and diagnose problems</p>
    <p>Typical Deployment</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>Agent</p>
    <p>Agent</p>
    <p>Agent</p>
    <p>Detection Diagnosis</p>
    <p>One-way delays, loss,</p>
    <p>reordering</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
  </div>
  <div class="page">
    <p>Handling Inter-domain: Pathology Specifications</p>
    <p>PATHOLOGY ContextSwitch DEF delay-exist AND large-triangle</p>
    <p>PATHOLOGY EndHostNoise DEF delay-exist AND unipoint-peaks</p>
    <p>PATHOLOGY RandomLoss DEF loss-exist AND NOT delay-loss-corr</p>
    <p>PATHOLOGY DelayCorrelatedLoss DEF loss-exist AND delay-loss-corr</p>
    <p>PATHOLOGY ShortOutage DEF loss-exist AND lossevent-small-dur</p>
    <p>PATHOLOGY LargeBuffer DEF delay-exist AND high-delayIQR AND NOT largetriangle AND NOT unipoint-peaks AND NOT delay-levelshift AND NOT loss-exist OR delay-loss-corr</p>
    <p>PATHOLOGY RouteNTPChange DEF delay-exist AND NOT large-triangle AND NOT unipoint-peaks AND delay-levelshift</p>
    <p>PATHOLOGY CongestionOverload DEF delay-exist AND high-util AND NOT burstydelays AND NOT high-delayIQR AND NOT large-triangle AND NOT unipoint-peaks AND NOT delay-levelshift</p>
    <p>PATHOLOGY ReorderPersistent DEF reorder-exist AND NOT reorder-shift</p>
    <p>SymptomsPathologies</p>
    <p>Specification language: operators can input problem definitions via domain/operational knowledge</p>
  </div>
  <div class="page">
    <p>Handling Inter-domain: Pathology Specifications</p>
    <p>PATHOLOGY ContextSwitch DEF delay-exist AND large-triangle</p>
    <p>PATHOLOGY EndHostNoise DEF delay-exist AND unipoint-peaks</p>
    <p>PATHOLOGY RandomLoss DEF loss-exist AND NOT delay-loss-corr</p>
    <p>PATHOLOGY DelayCorrelatedLoss DEF loss-exist AND delay-loss-corr</p>
    <p>PATHOLOGY ShortOutage DEF loss-exist AND lossevent-small-dur</p>
    <p>PATHOLOGY LargeBuffer DEF delay-exist AND high-delayIQR AND NOT largetriangle AND NOT unipoint-peaks AND NOT delay-levelshift AND NOT loss-exist OR delay-loss-corr</p>
    <p>PATHOLOGY RouteNTPChange DEF delay-exist AND NOT large-triangle AND NOT unipoint-peaks AND delay-levelshift</p>
    <p>PATHOLOGY CongestionOverload DEF delay-exist AND high-util AND NOT burstydelays AND NOT high-delayIQR AND NOT large-triangle AND NOT unipoint-peaks AND NOT delay-levelshift</p>
    <p>PATHOLOGY ReorderPersistent DEF reorder-exist AND NOT reorder-shift</p>
    <p>SymptomsPathologies</p>
    <p>Specification language: operators can input problem definitions via domain/operational knowledge</p>
    <p>Allows incremental diagnosis</p>
    <p>deployment</p>
  </div>
  <div class="page">
    <p>Processing Pathology Specifications</p>
    <p>Create a forest of decision trees  different trees contain disjoint symptoms</p>
    <p>Prune unused symptoms  Generate diagnosis code</p>
    <p>PATHOLOGY ContextSwitch DEF delay-exist AND large-triangle</p>
    <p>PATHOLOGY EndHostNoise DEF delay-exist AND unipoint-peaks</p>
    <p>PATHOLOGY RandomLoss DEF loss-exist AND NOT delay-loss-corr</p>
    <p>Symptoms</p>
    <p>Pathologies</p>
    <p>Efficient diagnosis code</p>
  </div>
  <div class="page">
    <p>Processing Pathology Specifications</p>
    <p>Create a forest of decision trees  different trees contain disjoint symptoms</p>
    <p>Prune unused symptoms  Generate diagnosis code</p>
    <p>PATHOLOGY ContextSwitch DEF delay-exist AND large-triangle</p>
    <p>PATHOLOGY EndHostNoise DEF delay-exist AND unipoint-peaks</p>
    <p>PATHOLOGY RandomLoss DEF loss-exist AND NOT delay-loss-corr</p>
    <p>Symptoms</p>
    <p>Pathologies</p>
    <p>Efficient diagnosis code</p>
  </div>
  <div class="page">
    <p>Live Deployment</p>
    <p>3 domains, 260+ inter-domain paths  11 pathologies diagnosed  100,000+ events/day (at highest sensitivity)</p>
    <p>http://pythia.cc.gatech.edu</p>
  </div>
  <div class="page">
    <p>Live Deployment</p>
    <p>3 domains, 260+ inter-domain paths  11 pathologies diagnosed  100,000+ events/day (at highest sensitivity)</p>
    <p>http://pythia.cc.gatech.edu</p>
  </div>
  <div class="page">
    <p>Detecting a Problem  Input: one-way delays, loss,</p>
    <p>reordering</p>
    <p>Output: Is there a problem?  Solution: find significant deviations</p>
    <p>from baseline.</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>One-way meas.</p>
    <p>Type Baseline</p>
    <p>Delay Propagation + TX delay + noise</p>
    <p>Loss No loss</p>
    <p>Reordering No reordering</p>
  </div>
  <div class="page">
    <p>Default Pathology Specifications</p>
    <p>Monitor effects  Congestion &amp; buffering  Loss events  Routing changes  Reordering events</p>
  </div>
  <div class="page">
    <p>Default Pathology Specifications</p>
    <p>Monitor effects  Congestion &amp; buffering  Loss events  Routing changes  Reordering events</p>
    <p>Monitor effects Internet2 4% PlanetLab 72%</p>
    <p>Residential 63% ESnet 95%</p>
    <p>Matters for short-lived problems!</p>
    <p>Depends on monitor activity</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
    <p>large triangles inter-packet gap &lt; vacation</p>
  </div>
  <div class="page">
    <p>Monitor Effects Busy OS environments =&gt; vacation periods</p>
    <p>large triangles inter-packet gap &lt; vacation</p>
    <p>single point peaks inter-packet gap &gt; vacation</p>
    <p>&lt; 100ms peaks</p>
  </div>
  <div class="page">
    <p>Congestion and Buffers</p>
    <p>Congestion: significant backlog for long periods (&gt; seconds)</p>
    <p>overload?  bursty?</p>
    <p>Bottleneck buffer:  over-provisioned?  under-provisioned?</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 2 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 0</p>
    <p>(a) Link overload: the congestion does not return to baseline.</p>
    <p>-20</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 3 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 1</p>
    <p>(b) Delay-correlated losses: misdiagnosed as random and level-shift losses due to sawtooth pattern.</p>
    <p>-50</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 2 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 1 cong-Bursty: 0</p>
    <p>(c) Congestion overload with random losses: all losses are diagnosed level shift due to sawtooth pattern.</p>
    <p>-10</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 3 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 0</p>
    <p>(d) Random + level shift/coordinated losses.</p>
    <p>-10</p>
    <p>-30 -25 -20 -15 -10 -5 0 5</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 3 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 0</p>
    <p>(e) Losses diagnosed as level shift + random.</p>
    <p>-50 0</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: -1 Short-out: -1 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 1</p>
    <p>(f) Congestion burstiness diagnosed.</p>
    <p>-20 0</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 2 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 1</p>
    <p>(g) Correlated losses correctly diagnosed as level shift losses; also note bursty congestion diagnosis.</p>
    <p>-50</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: -1 Short-out: -1 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 1</p>
    <p>(h) Bursty congestion diagnosed.</p>
    <p>-50</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: -1 Short-out: -1 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 0</p>
    <p>(i) Overload for 6s undiagnosed: is this congestion?</p>
    <p>-50 0</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: -1 Short-out: -1 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 1</p>
    <p>(j) Bursts diagnosed.</p>
    <p>-50</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 3 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 0</p>
    <p>(k) Random losses diagnosed as random + level shift losses.</p>
    <p>-10</p>
    <p>-60 -50 -40 -30 -20 -10 0</p>
    <p>O W</p>
    <p>D (m</p>
    <p>s)</p>
    <p>time (s)</p>
    <p>C-S: 0 Host-Noise: 0 Rnd-loss: 2 Short-out: 0 incorr-Buf: 0 0 cong-Overload: 0 cong-Bursty: 1</p>
    <p>(l) Correlated losses diagnosed as level shift losses (note: sawtooth peaks are large in magnitude).</p>
    <p>Figure 9: ShaperProbe case studies: losses are almost always misdiagnosed since the congestion is persistent and probing induces a sawtooth in local neighborhoods.</p>
    <p>Overload</p>
    <p>Bursty</p>
  </div>
  <div class="page">
    <p>Loss events</p>
    <p>Delay-correlated losses Random losses</p>
  </div>
  <div class="page">
    <p>Unknowns</p>
    <p>Events that do not match any known signatures</p>
    <p>Rule out unknown end-host events by correlating paths to/from monitor</p>
    <p>Typically &lt;10%</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>ifest in different network types.</p>
    <p>The remainder of the paper is organized as follows. Section 3 describes the system. Section 4 describes detection. Section 5 talks about diagnosing performance and diagnosis accuracy. Section 6 describes our live deployment. Section 8 is a study of performance problems in networks. Section 9 lists ongoing and future work.</p>
    <p>There has been significant prior work on detection and diagnosis of performance problems. The work can be categorized into two classes; our goal is to present representative work in each class. First, diagnosis methods that use significant amount of data sources usually available in enterprises and single administrative domains, but not in a wide-area inter-domain setting. In such settings, a detailed and fine-grained diagnosis is feasible. There is a tradeoff, however, between how detailed the diagnosis can be and the wide-area applicability (generality) of a diagnosis method. G-RCA [31] works on data sources within a single network such as SNMP traps, syslogs, alarms, router configurations, topology and end-to-end measurements. It mines dependency graphs from the data and constructs diagnosis rules from the graphs. Giza [20] uses multiresolution analysis and mines dependencies on similar data. SyslogDigest [24] mines faults from router syslogs. NetMedic [12] and Sherlock [3] diagnose faults in an enterprise by profiling end-host variables and by mining dependencies from historic data. NICE [22] enables troubleshooting by analyzing correlations across logs, router data and loss measurements. META [29] looks at both spatial and temporal features of network data to learn fault signatures; and a follow-up work [30] matches these signatures using network-wide data. NetPrints [2] learns decision trees of configuration data. A recent study [28] used email logs and network data to analyze routing-based failures. Learning approaches may require frequent training. Feather et al. look at diagnosing soft failures in a LAN</p>
    <p>using domain knowledge on a pre-defined set of features [7]. We take a similar approach to diagnosis using domain knowledge. Lakhina et al. used unsupervised clustering methods on packet-level features in packet traces to classify performance anomalies [17]; this method requires packet traces of traffic, however, it can complement Pythia by classifying problems that cannot be diagnosed using domain knowledge. Huang et al. use structural properties of network packet traces to detect performance problems in a LAN [9]. It has also been shown [10] that inter-domain routing anomalies can be identified using BGP updates. Mercury [21] uses a statistical change point detection method to detect changes in the network. There has been extensive work on structural methods to detect anomalies in volume data in an ISP;</p>
    <p>Monitoring</p>
    <p>nodes +</p>
    <p>Pythia agents</p>
    <p>Internet</p>
    <p>Database</p>
    <p>cluster Front end /</p>
    <p>web cluster</p>
    <p>Measurement paths</p>
    <p>FE - DB comm.</p>
    <p>Pythia commits to DB</p>
    <p>Figure 1: System architecture. DB and FE refer to the</p>
    <p>data repository and front end respectively.</p>
    <p>for example, the influential work by Lakhina et al. used dimensionality reduction [16].</p>
    <p>Second, methods that rely on active probing. These methods can reveal detailed and accurate diagnosis, since they provide the choice of carefully crafting probing structures based on domain knowledge. It is, however, hard to widely deploy a new active probing tool in a large monitoring network, especially if some of the monitors are in other domains; moreover, active probing methods are tailored to diagnose specific pathologies and not the overall network health. Netalyzr [15] probes to help a user with troubleshooting information. Tulip [19] diagnoses and localizes reordering, loss and congestion using a single end-point. PlanetSeer [33] uses a combination of active and passive methods to monitor path failures. Probing tools have been designed to detect Ethernet duplex mismatch [26], measure bottleneck buffer size [6], detect buffering problems [23] and reordering [18] using TCP. In the context of 802.11 wireless LANs, WLANProbe [14] and AirShark [25] diagnose root causes of performance problems, while Jigsaw [5] diagnoses root causes of TCP performance problems using distributed 802.11 monitors in an enterprise.</p>
    <p>Pythia works on end-to-end active measurements, and can easily plug into an existing monitoring infrastructure. It is applicable to the inter-domain setting, since it does not need network-specific data such as syslogs, SNMP data, routing tables or trouble tickets/alerts.</p>
    <p>The Pythia infrastructure is designed to operate in conjunction with monitoring systems that are based on the following model. A set of N monitors are deployed at vantage points on the Internet so that the paths between them cover a reasonably large set of links in the network between them. The monitors run active probes between each other, potentially covering N  (N 1) endto-end paths (in case of a full mesh measurement). A measurement 3-tuple</p>
    <p>!</p>
    <p>Ts,p,Ss,p,Tr,p &quot;</p>
    <p>of sender and receiver timestamps Ts,p and Tr,p and a sequence number Ss,p is recorded for each probe p. The sequence number is incremented by one at the sender for each probe in</p>
    <p>many end-host events?</p>
  </div>
  <div class="page">
    <p>Perf. Problems in Networks</p>
    <p>Academic networks are similar  Residential: more loss and congestion events</p>
    <p>Figure 6: The Pythia frontend.</p>
    <p>Dataset # events E.H.N. C.S. Unk.</p>
    <p>ESnet 465,135 52% 43% 3%</p>
    <p>Internet2 18,774 1% 3% 13%</p>
    <p>PlanetLab 718,459 56% 16% 1%</p>
    <p>ShaperProbe 8,790 54% 9% 9%</p>
    <p>Table 2: Non-network events in different datasets. Abbrevia</p>
    <p>tions: E.H.N. - EndHostNoise, C.S. - ContextSwitch,</p>
    <p>Unk. - Unknown.</p>
    <p>different aspects of the data below.</p>
    <p>End-host-related events: Table 2 shows events that are not necessarily related to the network; specifically, it describes the fraction of detected events that were diagnosed as either EndHostNoise, ContextSwitch or Unknown. We see that the fraction of events that Pythia tags as Unknown are typically lower than 10%. The Internet2 and ESnet data come from perfSONAR deployments. The Internet2 data shows a relatively small fraction of end-host-related events compared to ESnet; this is because the ESnet monitors are also used to host other processes such as MySQL data stores (and hence, the OS environment is more likely to be busy), while the Internet2 monitors are resources dedicated to active probing. PlanetLab slices are virtualized instances, and hence are also likely to be busy environments. ShaperProbe data comes from a userspace tool running on commodity OSes in homes. We note that ContextSwitch events in ShaperProbe may include events arising from vacation periods in either end-hosts or 802.11 access links in homes; we do not differentiate between the two types in this analysis.</p>
    <p>In the rest of this section, we focus on events that are neither end-host-related nor Unknown events.</p>
    <p>C ongestionB</p>
    <p>ursty</p>
    <p>C ongestionO</p>
    <p>verload</p>
    <p>D elayC</p>
    <p>orrelatedLoss</p>
    <p>LargeB uffer</p>
    <p>R andom</p>
    <p>Loss</p>
    <p>R outeN</p>
    <p>TP C hange</p>
    <p>S hortO</p>
    <p>utage</p>
    <p>S m</p>
    <p>allB uffer</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f e</p>
    <p>v e</p>
    <p>n ts</p>
    <p>( %</p>
    <p>)</p>
    <p>ESnet I2</p>
    <p>ShaperProbe PlanetLab</p>
    <p>Figure 7: Composition of different network event types among</p>
    <p>the four data sources (omitting Unknown, EndHostNoise</p>
    <p>and ContextSwitch events).</p>
    <p>Network-related events: Figure 7 shows the fraction of each network-related event type across the datasets. We see that events in ESnet and Inernet2 networks have a similar composition. ShaperProbe and PlanetLab have a relatively higher incidence of loss-based events. In particular, PlanetLab has a comparatively high fraction of random loss and short outage events; while ShaperProbe data also includes a significant fraction of congestion events. Note that we do not see a large fraction of LargeBuffer events in ShaperProbe data, though the presence of large buffer configurations in home networks is well known [15] - this is because we can sample a LargeBuffer event only if cross traffic fills the large buffer to a significant extent during the 10s probing duration.</p>
    <p>Residential broadband performance: We look at four large residential broadband providers in our dataset: cable Internet providers Comcast, Road Runner and Cox;</p>
    <p>Data: 10Hz 40-byte probing:</p>
    <p>ESnet  Internet2  ShaperProbe</p>
    <p>(residential)</p>
    <p>PlanetLab</p>
  </div>
  <div class="page">
    <p>Residential Networks</p>
    <p>Cable: more upstream events</p>
    <p>DSL: more downstream events</p>
    <p>C om</p>
    <p>cast</p>
    <p>R oadR</p>
    <p>unner</p>
    <p>C ox</p>
    <p>A T&amp;</p>
    <p>T</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f ru</p>
    <p>n s d</p>
    <p>e te</p>
    <p>c te</p>
    <p>d a</p>
    <p>s p</p>
    <p>ro b</p>
    <p>le m</p>
    <p>( %</p>
    <p>)</p>
    <p>Upstream Downstream</p>
    <p>Figure 8: Fraction of runs detected as problem.</p>
    <p>C ongestionB</p>
    <p>ursty</p>
    <p>C ongestionO</p>
    <p>verload</p>
    <p>D elayC</p>
    <p>orrelatedLoss</p>
    <p>LargeB uffer</p>
    <p>R andom</p>
    <p>Loss</p>
    <p>S hortO</p>
    <p>utage</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f e</p>
    <p>v e</p>
    <p>n ts</p>
    <p>( %</p>
    <p>)</p>
    <p>Comcast Road Runner</p>
    <p>Cox AT&amp;T</p>
    <p>Figure 9: Composition of different network event types among</p>
    <p>residential ISPs.</p>
    <p>and DSL provider AT&amp;T. Figure 8 shows fraction of user runs detected as a performance problem. We select those runs for which we have recorded over 50% of expected number of samples. We see a difference in uplink and downlink detections between cable and DSL providers. A plausible explanation is that the DOCSIS uplink is a non-FIFO scheduler, while the downlink is multiplexed with neighborhood homes; this is not the case for DSL. Note that the ShaperProbe data has an inherent bias the tool is more likely run by a home user when the user perceives a problem. To cross-check the detection numbers, we looked at the delay range - difference in 95th and 5th percentiles of the delay distribution. We found about 59% upstream and 25% downstream Comcast runs had a delay range exceeding 5ms (the default detection threshold for minimum delay range); while, for AT&amp;T, we the figures were 45% and 63% respectively.</p>
    <p>Figure 9 shows the composition of performance problems across the ISPs. A notable aspect of the plot is that DSL provider AT&amp;T shows a higher incidence of lossbased events than the cable providers.</p>
    <p>We next look at whether cable links show different</p>
    <p>C ongestionB</p>
    <p>ursty</p>
    <p>C ongestionO</p>
    <p>verload</p>
    <p>D elayC</p>
    <p>orrelatedLoss</p>
    <p>LargeB uffer</p>
    <p>R andom</p>
    <p>Loss</p>
    <p>S hortO</p>
    <p>utage</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f e</p>
    <p>v e n</p>
    <p>ts (</p>
    <p>% )</p>
    <p>Comcast Up Comcast Down</p>
    <p>Figure 10: Composition of different network event types with</p>
    <p>direction in Comcast.</p>
    <p>pathology distributions in the two directions. Figure 10 looks at composition of diagnosis types across all Comcast runs. We do not see a significant difference in the compositions, although there are more problem detections in the cable uplink.</p>
    <p>We have designed and deployed Pythia, a wide-area, inter-domain, performance problem detection and diagnosis system that works in conjunction with a monitoring infrastructure. Pythia currently works in an inter-domain setting with 43 wide-area inter-domain paths and diagnoses thousands of performance pathologies a day. We have studied the nature of performance problems in three wide-area networks: research, commercial and residential broadband. The live system is at: http://pythia.cc.gatech.edu</p>
    <p>There are open questions in Pythia, some of which we are currently working on, and others we leave for future work. First, we are working on adding localization functionality to diagnosed events, based on our prior work [32]. Second, we plan to augment Unknown diagnoses with a similarity measure over a pre-specified space of features. Similarity compares the event against a representative set of diagnoses to find the most similar diagnosis with respect to the known pathologies. Third, we are scaling Pythia to a large number of monitors; this may require tuning the the database tier to scale vertically.</p>
    <p>References</p>
    <p>[1] Internet performance monitoring, Keynote Systems. http://www.keynote.com.</p>
    <p>[2] B. Aggarwal, R. Bhagwan, T. Das, S. Eswaran, V. Padmanabhan, and G. Voelker. NetPrints: Diagnosing home network misconfigurations using shared knowledge. In USENIX NSDI, 2009.</p>
    <p>C om</p>
    <p>cast</p>
    <p>R oadR</p>
    <p>unner</p>
    <p>C ox</p>
    <p>A T&amp;</p>
    <p>T</p>
    <p>F ra</p>
    <p>c ti o</p>
    <p>n o</p>
    <p>f ru</p>
    <p>n s d</p>
    <p>e te</p>
    <p>c te</p>
    <p>d a</p>
    <p>s p</p>
    <p>ro b</p>
    <p>le m</p>
    <p>( %</p>
    <p>)</p>
    <p>Upstream Downstream</p>
    <p>Figure 8: Fraction of runs detected as problem.</p>
    <p>C ongestionB</p>
    <p>ursty</p>
    <p>C ongestionO</p>
    <p>verload</p>
    <p>D elayC</p>
    <p>orrelatedLoss</p>
    <p>LargeB uffer</p>
    <p>R andom</p>
    <p>Loss</p>
    <p>S hortO</p>
    <p>utage F</p>
    <p>ra c ti o</p>
    <p>n o</p>
    <p>f e</p>
    <p>v e</p>
    <p>n ts</p>
    <p>( %</p>
    <p>)</p>
    <p>Comcast Road Runner</p>
    <p>Cox AT&amp;T</p>
    <p>Figure 9: Composition of different network event types among</p>
    <p>residential ISPs.</p>
    <p>and DSL provider AT&amp;T. Figure 8 shows fraction of user runs detected as a performance problem. We select those runs for which we have recorded over 50% of expected number of samples. We see a difference in uplink and downlink detections between cable and DSL providers. A plausible explanation is that the DOCSIS uplink is a non-FIFO scheduler, while the downlink is multiplexed with neighborhood homes; this is not the case for DSL. Note that the ShaperProbe data has an inherent bias the tool is more likely run by a home user when the user perceives a problem. To cross-check the detection numbers, we looked at the delay range - difference in 95th and 5th percentiles of the delay distribution. We found about 59% upstream and 25% downstream Comcast runs had a delay range exceeding 5ms (the default detection threshold for minimum delay range); while, for AT&amp;T, we the figures were 45% and 63% respectively.</p>
    <p>Figure 9 shows the composition of performance problems across the ISPs. A notable aspect of the plot is that DSL provider AT&amp;T shows a higher incidence of lossbased events than the cable providers.</p>
    <p>We next look at whether cable links show different</p>
    <p>C ongestionB</p>
    <p>ursty</p>
    <p>C ongestionO</p>
    <p>verload</p>
    <p>D elayC</p>
    <p>orrelatedLoss</p>
    <p>LargeB uffer</p>
    <p>R andom</p>
    <p>Loss</p>
    <p>S hortO</p>
    <p>utage</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f e v e n ts</p>
    <p>( %</p>
    <p>)</p>
    <p>Comcast Up Comcast Down</p>
    <p>Figure 10: Composition of different network event types with</p>
    <p>direction in Comcast.</p>
    <p>pathology distributions in the two directions. Figure 10 looks at composition of diagnosis types across all Comcast runs. We do not see a significant difference in the compositions, although there are more problem detections in the cable uplink.</p>
    <p>We have designed and deployed Pythia, a wide-area, inter-domain, performance problem detection and diagnosis system that works in conjunction with a monitoring infrastructure. Pythia currently works in an inter-domain setting with 43 wide-area inter-domain paths and diagnoses thousands of performance pathologies a day. We have studied the nature of performance problems in three wide-area networks: research, commercial and residential broadband. The live system is at: http://pythia.cc.gatech.edu</p>
    <p>There are open questions in Pythia, some of which we are currently working on, and others we leave for future work. First, we are working on adding localization functionality to diagnosed events, based on our prior work [32]. Second, we plan to augment Unknown diagnoses with a similarity measure over a pre-specified space of features. Similarity compares the event against a representative set of diagnoses to find the most similar diagnosis with respect to the known pathologies. Third, we are scaling Pythia to a large number of monitors; this may require tuning the the database tier to scale vertically.</p>
    <p>References</p>
    <p>[1] Internet performance monitoring, Keynote Systems. http://www.keynote.com.</p>
    <p>[2] B. Aggarwal, R. Bhagwan, T. Das, S. Eswaran, V. Padmanabhan, and G. Voelker. NetPrints: Diagnosing home network misconfigurations using shared knowledge. In USENIX NSDI, 2009.</p>
    <p>DSL has more loss-based events than cable</p>
  </div>
  <div class="page">
    <p>Takeaways from Study</p>
    <p>Domain and operational knowledge-based models give detailed diagnosis in wide area</p>
    <p>Nature of pathologies varies with the wide area network</p>
    <p>Configurable, real time, diagnosis systems can help!</p>
  </div>
  <div class="page">
    <p>Diagnosing Performance Problems in Wide Area Providers</p>
    <p>Partha Kanuparthy, Constantine Dovrolis</p>
    <p>USENIX ATC 2014</p>
  </div>
</Presentation>
