<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Efficient Intranode Communication in GPU-Accelerated Systems</p>
    <p>Feng Ji, Ashwin M. Aji, James Dinan, Darius Buntinas, Pavan Balaji, Wu-chun Feng, Xiaosong Ma</p>
    <p>Presented by: James Dinan James Wallace Givens Postdoctoral Fellow Argonne National Laboratory</p>
  </div>
  <div class="page">
    <p>Fields using GPU Accelerators at Argonne</p>
    <p>Computed Tomography Micro-tomography</p>
    <p>Cosmology Bioengineering</p>
    <p>Acknowledgement: Venkatram Vishwanath @ ANL</p>
  </div>
  <div class="page">
    <p>GPU-Based Supercomputers</p>
  </div>
  <div class="page">
    <p>GPU-Accelerated High Performance Computing</p>
    <p>GPUs are general purpose, highly parallel processors  High FLOPs/Watt and FLOPs/$  Unit of execution Kernel  Separate memory subsystem  Prog. Models: CUDA, OpenCL,</p>
    <p>Clusters with GPUs are becoming common  Multiple GPUs per node  Nonuniform node architecture  Node topology plays role in performance</p>
    <p>New programmability and performance challenges for programming models and runtime systems</p>
    <p>Keeneland Node Architecture</p>
  </div>
  <div class="page">
    <p>Heterogeneity and Intra-node GPU-GPU Xfers</p>
    <p>CUDA provides GPU-GPU DMA using CUDA IPC  Same I/O hub  DMA best  Different I/O hubs  Shared memory best</p>
    <p>Mismatch between PCIe and QPI ordering semantics</p>
    <p>GPU-GPU, Same I/O Hub GPU-GPU, Diff. I/O Hub</p>
  </div>
  <div class="page">
    <p>MPI-ACC: Programmability and Performance</p>
    <p>GPU Global memory  Separate address space  Manually managed</p>
    <p>Message Passing Interface  Most popular parallel</p>
    <p>programming model  Host memory only</p>
    <p>Integrate accelerator awareness with MPI (ANL, NCSU, VT)  Productivity and</p>
    <p>performance benefits</p>
    <p>MPI rank 0</p>
    <p>MPI rank 1</p>
    <p>MPI rank 2</p>
    <p>MPI rank 3</p>
    <p>NIC</p>
    <p>Main memory</p>
    <p>CPU</p>
    <p>Global memory</p>
    <p>Shared memory</p>
    <p>Multiprocessor GPU</p>
    <p>PCIe, HT/QPI</p>
  </div>
  <div class="page">
    <p>Current MPI+GPU Programming</p>
    <p>MPI operates on data in host memory only  Manual copy between host and GPU memory serializes PCIe, Interconnect</p>
    <p>Can do better than this, but will incur protocol overheads multiple times  Productivity: Manual data movement  Performance: Inefficient, unless large, non-portable investment in tuning</p>
  </div>
  <div class="page">
    <p>MPI-ACC Interface: Passing GPU Buffers to MPI</p>
    <p>Unified Virtual Address (UVA) space  Allow device pointer in MPI routines directly  Currently supported only by CUDA and newer NVIDIA GPUs  Query cost is high and added to every operation (CPU-CPU)</p>
    <p>Explicit Interface  e.g. MPI_CUDA_Send(), overloading  MPI Datatypes  Compatible with MPI and many accelerator models</p>
    <p>MPI Datatype</p>
    <p>MPI Datatype</p>
    <p>MPI-ACC Interface Cost (CPU-CPU)</p>
    <p>CL_ContextCL_Context</p>
    <p>CL_MemCL_Mem</p>
    <p>CL_Device_IDCL_Device_ID</p>
    <p>CL_Cmd_queueCL_Cmd_queue</p>
    <p>Attributes</p>
    <p>BUFTYPE=OCLBUFTYPE=OCL</p>
  </div>
  <div class="page">
    <p>MPI-ACC: Integrated, Optimized Data Movement  Use MPI for all data movement</p>
    <p>Support multiple accelerators and prog. models (CUDA, OpenCL, )  Allow application to portably leverage system-specific optimizations</p>
    <p>Inter-node data movement [Aji HPCC12]  Pipelining: Fully utilize PCIe and network links  GPU direct (CUDA): Multi-device pinning eliminates data copying  Handle caching (OpenCL): Avoid expensive command queue creation</p>
    <p>Intra-node data movement  Shared memory protocol [Ji ASHES12]</p>
    <p>Sender and receiver drive independent DMA transfers  Direct DMA protocol [Ji HPCC12]</p>
    <p>GPU-GPU DMA transfer (CUDA IPC)  Both protocols needed, PCIe limitations serialize DMA across I/O hubs</p>
  </div>
  <div class="page">
    <p>Integrated Support for User-Defined Datatypes</p>
    <p>MPI_Send(buffer, datatype, count, to,  ) MPI_Recv(buffer, datatype, count, from,  )</p>
    <p>What if the datatype is noncontiguous?  CUDA doesnt support arbitrary noncontiguous transfers  Pack data on the GPU [Jenkins 12]</p>
    <p>Flatten datatype tree representation  Packing kernel that can saturate memory bus/banks</p>
  </div>
  <div class="page">
    <p>Intranode Communication in MPICH2 (Nemesis)</p>
    <p>Short Message Transport  Shared message queue  Large, persistent queue  Single buffer transport</p>
    <p>Large Message Transport  Point-to-point shared buffer  Ring buffer allocated at first</p>
    <p>connection  Multi-buffered transport</p>
    <p>Rank 0</p>
    <p>Rank 1</p>
    <p>Rank 2</p>
    <p>Rank 3</p>
    <p>Sh ar</p>
    <p>ed M</p>
    <p>em or</p>
    <p>y Rank</p>
    <p>Sh ar</p>
    <p>ed M</p>
    <p>em or</p>
    <p>y</p>
    <p>Rank 2</p>
    <p>Rank 3</p>
  </div>
  <div class="page">
    <p>Non-integrated Intranode Communication</p>
    <p>Communication without accelerator integration  2 PCIe data copies + 2 main memory copies  Transfers are serialized</p>
    <p>GPU</p>
    <p>Host</p>
    <p>Process 0 Process 1</p>
    <p>Nemesis Shr. Memory</p>
  </div>
  <div class="page">
    <p>Performance Potential: Intranode Bandwidth</p>
    <p>Bandwidth measurement when using manual data movement  Theoretical node bandwidth: 6 GB/sec</p>
    <p>Achieved for host-host transfers  Observed bandwidth: 1.6  2.8 GB/sec</p>
    <p>With one/two GPU buffers  one/two copies 13</p>
  </div>
  <div class="page">
    <p>Eliminating Extra Copies</p>
    <p>Integration allows direct transfer into shared memory buffer  LMT: sender and receiver drive transfer concurrently</p>
    <p>Pipeline data transfer  Full utilization of PCIe links</p>
    <p>GPU</p>
    <p>Host</p>
    <p>Process 0 Process 1</p>
    <p>Nemesis Shr. Memory</p>
  </div>
  <div class="page">
    <p>Copying Data Between Host and Device</p>
    <p>Three choices for selecting the right copy operation: 1. UVA-Default: Use cudaMemcpy(, cudaMemcpyDefault) 2. Query-and-copy: UVA query buffer type</p>
    <p>Dispatch memcpy or cudaMemcpy</p>
    <p>Host to Device Host to Host</p>
  </div>
  <div class="page">
    <p>Large Message Transport Protocol</p>
    <p>Shared buffer mapped between pairs of communicating processes  Enables pipelined transfer  Sender and receiver drive DMA</p>
    <p>concurrently</p>
    <p>Fixed-size ring Buffer  Set of fixed-size partitions  R - receivers pointer  S - senders pointer</p>
    <p>Partition size  Set to data length by Sender  Set to zero by Receiver</p>
    <p>R</p>
    <p>S</p>
  </div>
  <div class="page">
    <p>Extending LMT Protocol to Accelerators</p>
    <p>Sender and receiver issue asynchronous PCIe data transfers  Add Ra and Sa pointers  Mark section of R/S segment in</p>
    <p>use by PCIe transfers</p>
    <p>Proactively generate PCIe data transfers  Move to the next partition  Start new PCIe data copy  Repeat until full or RB is empty  Update R/S when checking PCIe</p>
    <p>operations later</p>
    <p>R</p>
    <p>S</p>
    <p>Ra</p>
    <p>Sa</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>Keeneland node architecture  2x Intel Xeon X5660 CPUs, 24 GB Memory, 3x Nvidia M2070 GPUs</p>
  </div>
  <div class="page">
    <p>Eager Parameters: Message Queue Element Size and Eager/LMT Threshold</p>
    <p>GPU-GPU transfer of varying size  Shared buf is manual data transfer</p>
    <p>Large message queue can support more requests  Eager up to 64 kB, LMT for +64 kB</p>
    <p>The same value as host-only 19</p>
  </div>
  <div class="page">
    <p>LMT Parameter: Shared Ring Buffer Unit Size</p>
    <p>Host-to-host  Default buffer unit size 32 KB</p>
    <p>GPU involved  Use 256 KB  PCIe bandwidth favors larger</p>
    <p>messages</p>
    <p>Parameter choice requires knowledge of buffer locations on sender and receiver  Exchange information during</p>
    <p>handshaking phase of LMT protocol</p>
    <p>Intranode bandwidth  two processes</p>
  </div>
  <div class="page">
    <p>Latency &amp; Bandwidth Improvement</p>
    <p>Less impact on D2D case  PCIe latency dominant</p>
    <p>Improvement: 6.7% (D2D), 15.7% (H2D), 10.9% (D2H)</p>
    <p>Bandwidth discrepancy in different PCIe bus directions</p>
    <p>Improvement: 56.5% (D2D), 48.7% (H2D), 27.9% (D2H)</p>
    <p>Nearly saturates peak (6 GB/sec) in D2H case</p>
  </div>
  <div class="page">
    <p>Application Performance: Stencil2D Kernel</p>
    <p>Nine-point stencil computation, SHOC benchmark suite  Halo exchange with neighboring processes  Benefit from latency improvement  Relative fraction of communication time decreases with problem size  Average execution time improvement of 4.3%</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Accelerators are ubiquitous, moving target  Exciting new opportunities for systems researchers  Requires evolution of HPC software stack</p>
    <p>Integrate accelerator-awareness with MPI  Support multiple accelerators and programming models  Goals are productivity and performance</p>
    <p>Optimized Intranode communication  Eliminate extra main memory copies  Pipeline data flow in ring buffer  Optimize data copying</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Host-side Memcpy Bandwidth</p>
  </div>
</Presentation>
