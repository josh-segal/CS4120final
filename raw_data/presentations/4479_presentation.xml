<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Study of Poisson Query Generation Model for Information Retrieval</p>
    <p>Qiaozhu Mei, Hui Fang, and ChengXiang Zhai</p>
    <p>University of Illinois at Urbana-Champaign</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background of query generation in IR</p>
    <p>Query generation with Poisson language model  Smoothing in Poisson query generation model</p>
    <p>Poisson v.s. multinomial in query generation IR  Analytical comparison</p>
    <p>Empirical experiments</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Query Generation IR Model [Ponte &amp; Croft 98]</p>
    <p>)|(),( dqpqdScore</p>
    <p>Nd</p>
    <p>Scoring documents with query likelihood  Known as the language modeling (LM) approach to IR  Different from document generation</p>
    <p>d1</p>
    <p>d2</p>
    <p>Document Language Model</p>
    <p>Query Likelihood</p>
    <p>dN</p>
    <p>q)|( 1d</p>
    <p>qp</p>
    <p>)|( 2d</p>
    <p>qp</p>
    <p>)|( Nd</p>
    <p>qp</p>
  </div>
  <div class="page">
    <p>Interpretation of LM d</p>
    <p>d : a model for queries posed by users who like document d [Lafferty and Zhai 01]  Estimate d using document d  use d to approximate</p>
    <p>the queries used by users who like d</p>
    <p>Existing methods differ mainly in the choice of d and how d is estimated (smoothing)  Multi-Bernoulli: e.g, [Ponte and Croft 98, Metzler et al 04]</p>
    <p>Multinomial: (most popular) e.g., [Hiemstra et al. 99, Miller et al. 99, Zhai and Lafferty 01]</p>
  </div>
  <div class="page">
    <p>Multi-Bernoulli vs. Multinomial</p>
    <p>qwqw</p>
    <p>dwpdwpdqp )|0()|1()|(</p>
    <p>text mining model</p>
    <p>clustering text</p>
    <p>model text</p>
    <p>Doc: d text mining  model</p>
    <p>Multi-Bernoulli: Flip a coin for each word</p>
    <p>Multinomial: Toss a die to choose a word</p>
    <p>text</p>
    <p>mining</p>
    <p>m o</p>
    <p>d e l</p>
    <p>H H T</p>
    <p>Query q: text mining</p>
    <p>text</p>
    <p>mining</p>
    <p>Query q: text mining</p>
    <p>||</p>
    <p>),( )|()|(</p>
    <p>V</p>
    <p>j</p>
    <p>qwc j</p>
    <p>jdwpdqp</p>
  </div>
  <div class="page">
    <p>Problems of Multinomial</p>
    <p>Reality is harder than expected:  Empirical estimates:</p>
    <p>mean (tf) &lt; variance (tf) (Church &amp; Gale 95)</p>
    <p>Estimates on AP88-89:  All terms: : 0.0013;</p>
    <p>2: 0.0044  Query terms: : 0.1289;</p>
    <p>2: 0.3918</p>
    <p>Multinomial/Bernoulli: mean &gt; variance</p>
    <p>Does not model term absence</p>
    <p>Sum-to-one over all terms</p>
  </div>
  <div class="page">
    <p>Poisson?</p>
    <p>Poisson models frequency directly (including zero freq.)</p>
    <p>No sum-to-one constraint on different w</p>
    <p>Mean = Variance</p>
    <p>Poisson is explored in document generation models, but not in query generation models</p>
    <p>!</p>
    <p>)( ))((</p>
    <p>k</p>
    <p>e kwcp</p>
    <p>k</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Poisson has been explored in document generation models, e.g.,  2-Poisson  Okapi/BM25 (Robertson and Walker 94)</p>
    <p>Parallel derivation of probabilistic models (Roelleke and Wang 06)</p>
    <p>Our work add to this body of exploration of Poisson  With query generation framework</p>
    <p>Explore specific features Poisson brings in LM</p>
  </div>
  <div class="page">
    <p>Research Questions</p>
    <p>How can we model query generation with Poisson language model?</p>
    <p>How can we smooth such a Poisson query generation model?</p>
    <p>How is a Poisson model different from a multinomial model in the context of query generation retrieval?</p>
  </div>
  <div class="page">
    <p>Query Generation with Poisson</p>
    <p>!1</p>
    <p>) 7 3 ( 17/3 e</p>
    <p>!2</p>
    <p>) 7 2 ( 27/2 e</p>
    <p>!0</p>
    <p>) 7 1 ( 07/1 e</p>
    <p>text mining model</p>
    <p>clustering text</p>
    <p>model text</p>
    <p>Query: mining text mining systems</p>
    <p>/</p>
    <p>/</p>
    <p>Rates of arrival of w:</p>
    <p>text</p>
    <p>mining</p>
    <p>model</p>
    <p>clustering</p>
    <p>[ ]</p>
    <p>[ ]</p>
    <p>[ ]</p>
    <p>[ ]</p>
    <p>[ ]</p>
    <p>: |q|</p>
    <p>Poisson: Each term as an emitter</p>
    <p>Query: receiver</p>
    <p>!0</p>
    <p>) 7 1 ( 07/1 e</p>
    <p>!1</p>
    <p>)( 1e )|( dqp</p>
  </div>
  <div class="page">
    <p>Query Generation with Poisson (II)</p>
    <p>text mining model</p>
    <p>clustering text</p>
    <p>model text</p>
    <p>text</p>
    <p>mining</p>
    <p>model</p>
    <p>clustering</p>
    <p>[ c(w1, q) ]</p>
    <p>|q|</p>
    <p>n</p>
    <p>i i</p>
    <p>qwc i</p>
    <p>qn</p>
    <p>i i</p>
    <p>qwc</p>
    <p>qe dqwcpdqp</p>
    <p>ii</p>
    <p>),(||</p>
    <p>|)|( )|),(()|(</p>
    <p>},...,{ 1 nd</p>
    <p>[ c(w2, q) ]</p>
    <p>[ c(w3, q) ]</p>
    <p>[ c(w4, q) ]</p>
    <p>[ c(wN, q) ]</p>
    <p>1</p>
    <p>2</p>
    <p>3</p>
    <p>4</p>
    <p>N</p>
    <p>w1</p>
    <p>w2</p>
    <p>w3</p>
    <p>w4</p>
    <p>wN</p>
    <p>q = c(w1, q), c(w2 , q), , c(wn , q)</p>
    <p>Dd Vw</p>
    <p>Dd i</p>
    <p>i dwc</p>
    <p>dwc</p>
    <p>'</p>
    <p>),'(</p>
    <p>),(</p>
    <p>MLE</p>
  </div>
  <div class="page">
    <p>Smoothing Poisson LM</p>
    <p>Vw</p>
    <p>dqwcpqdScore ) |),((log),(</p>
    <p>text mining model</p>
    <p>clustering text</p>
    <p>model</p>
    <p>d</p>
    <p>MLEd</p>
    <p>text 0.02 mining 0.01 model 0.02  system 0?</p>
    <p>C Background Collection</p>
    <p>text 0.0001 mining 0.0002 model 0.0001  system 0.0001</p>
    <p>+</p>
    <p>Different smoothing methods lead to different retrieval formulae</p>
    <p>Query: text mining systems</p>
    <p>e.g., text:  * 0.02 + (1-  )* 0.0001 system:  * 0 + (1-  )* 0.0001</p>
  </div>
  <div class="page">
    <p>Interpolation (JM):</p>
    <p>Bayesian smoothing with Gamma prior:</p>
    <p>Two stage smoothing:</p>
    <p>Smoothing Poisson LM</p>
    <p>C</p>
    <p>MLEd C</p>
    <p>)|),(()|),(()1( CMLEd qwcpqwcp   1</p>
    <p>wdwdwdwd Cdp wd</p>
    <p>,,,, d),|(</p>
    <p>,</p>
    <p>2</p>
    <p>+</p>
    <p>MLEd d Gamma prior</p>
    <p>C</p>
    <p>MLEd d'</p>
    <p>d</p>
    <p>U</p>
  </div>
  <div class="page">
    <p>Smoothing Poisson LM (II)</p>
    <p>Two-stage smoothing:  Similar to multinomial 2-stage (Zhai and Lafferty 02)</p>
    <p>Verbose queries need to be smoothed more</p>
    <p>)|),(()|),(()1(),|),(( UqwcpqwcpUqwcp dd</p>
    <p>A smoothed version of document model (from and ) CMLEd</p>
    <p>A background model of user query preference Use when no user prior is known</p>
    <p>||</p>
    <p>),( , ,</p>
    <p>d</p>
    <p>dwc wC wd</p>
    <p>e.g.,</p>
    <p>C</p>
  </div>
  <div class="page">
    <p>Analytical Comparison: Basic Distributions</p>
    <p>multi-Bernoulli multinomial Poisson</p>
    <p>Event space Appearance /absence</p>
    <p>V frequency</p>
    <p>Model absence? Yes No Yes</p>
    <p>Model frequency?</p>
    <p>No Yes Yes</p>
    <p>Model length? (document/query)</p>
    <p>No No Yes</p>
    <p>Sum-to-one constraint?</p>
    <p>No Yes No</p>
  </div>
  <div class="page">
    <p>Analytical: Equivalency of basic models</p>
    <p>Equivalent with basic model and MLE:</p>
    <p>Poisson + Gamma Smoothing = multinomial + Dirichlet Smoothing</p>
    <p>Basic model + JM smoothing behaves similarly</p>
    <p>(with a variant component of document length normalization )</p>
    <p>||</p>
    <p>log||)</p>
    <p>|| ),(</p>
    <p>),( 1log(),(),(</p>
    <p>d q</p>
    <p>C Cwc</p>
    <p>dwc qwcqdScore</p>
    <p>dqw</p>
    <p>0),( ),(</p>
    <p>),( log),(),(</p>
    <p>qwc Vw</p>
    <p>dwc</p>
    <p>dwc qwcqdScore</p>
  </div>
  <div class="page">
    <p>Benefits: Per-term Smoothing</p>
    <p>Poisson doesnt require sum-to-one over different terms (different event space)</p>
    <p>Thus  in JM smoothing and 2-stage smoothing can be made term dependent (per-term)  multinomial cannot achieve per-term smoothing</p>
    <p>Can use EM algorithm to estimate ws.</p>
    <p>)|),(()|),(()1( CMLEd qwcpqwcp   w w</p>
  </div>
  <div class="page">
    <p>Benefits: Modeling Background</p>
    <p>Traditional: as a single model  Not matching the reality</p>
    <p>as a mixture model: increase variance  multinomial mixture (e.g., clusters, PLSA, LDA)</p>
    <p>Inefficient (no close form, iterative estimation)</p>
    <p>Poisson mixture (e.g., Katzs K-Mixture, 2Poisson, Negative Binomial) (Church &amp; Gale 95)</p>
    <p>Have close forms, efficient computation</p>
    <p>C</p>
    <p>C</p>
  </div>
  <div class="page">
    <p>Hypotheses</p>
    <p>H1: With basic query generation retrieval models (JM smoothing and Gamma smoothing): Poisson behaves similarly to multinomial</p>
    <p>H2: Per-term smoothing with Poisson may outperform term independent smoothing  More help on verbose queries</p>
    <p>H3: Background efficiently modeled as Poisson mixtures may perform better than single Poisson</p>
  </div>
  <div class="page">
    <p>Experiment Setup</p>
    <p>Data: TREC collections and Topics  AP88-89, Trec7, Trec8, Wt2g</p>
    <p>Query type:  Short keyword (keyword title);</p>
    <p>Short verbose (one sentence);</p>
    <p>Long verbose (multiple sentences);</p>
    <p>Measurement:  Mean average precision (MAP)</p>
  </div>
  <div class="page">
    <p>JM + Poisson JM + Multinomial</p>
    <p>Gamma/Dirichlet &gt; JM (Poisson/ Multinomial)</p>
    <p>H1: Basic models behave similarly</p>
    <p>JM+Poisson JM+Multi nomial</p>
    <p>Gamma/Dirichlet &gt; J M (Poisson/ Multinomi al)</p>
    <p>MAP</p>
  </div>
  <div class="page">
    <p>H2: Per-term outperforms termindependent smoothing</p>
    <p>Data Q Gamma/ Dirichlet</p>
    <p>Per-term 2-stage</p>
    <p>SK 0.224 0.226</p>
    <p>AP SV 0.204 0.217*</p>
    <p>LV 0.291 0.304*</p>
    <p>SK 0.186 0.185</p>
    <p>Trec-7 SV 0.182 0.196*</p>
    <p>LV 0.224 0.236*</p>
    <p>SK 0.257 0.256</p>
    <p>Trec-8 SV 0.228 0.246*</p>
    <p>LV 0.260 0.274*</p>
    <p>SK 0.302 0.307</p>
    <p>Web SV 0.273 0.292*</p>
    <p>LV 0.283 0.311*</p>
    <p>Per-term &gt; Non-per-term</p>
  </div>
  <div class="page">
    <p>Improvement Comes from Per-term</p>
    <p>Data Q JM JM+per -term</p>
    <p>AP SK 0.203 0.206 0.223 0.226*</p>
    <p>SV 0.183 0.214* 0.204 0.217*</p>
    <p>Trec-7 SK 0.168 0.174 0.186 0.185</p>
    <p>SV 0.176 0.198* 0.194 0.196</p>
    <p>Trec-8 SK 0.239 0.227 0.257 0.256</p>
    <p>SV 0.234 0.249* 0.242 0.246*</p>
    <p>Web SK 0.250 0.220* 0.291 0.307*</p>
    <p>SV 0.217 0.261* 0.273 0.292*</p>
    <p>JM + Per-term &gt; JM</p>
    <p>Significant improvement on verbose query</p>
  </div>
  <div class="page">
    <p>H3: Poisson Mixture Background Improves Performance</p>
    <p>Data Query c = single Poisson</p>
    <p>c = Katz K-Mixture</p>
    <p>AP SK 0.203 0.204</p>
    <p>SV 0.183 0.188*</p>
    <p>Trec-7 SK 0.168 0.169</p>
    <p>SV 0.176 0.178*</p>
    <p>Trec-8 SK 0.239 0.239</p>
    <p>SV 0.234 0.238*</p>
    <p>Web SK 0.250 0.250</p>
    <p>SV 0.217 0.223*</p>
    <p>Katz K-Mixture &gt; Single Poisson</p>
  </div>
  <div class="page">
    <p>Poisson Opens Other Potential Flexibilities</p>
    <p>Document length penalization?  JM introduced a variant component of document length</p>
    <p>normalization</p>
    <p>Require more expensive computation</p>
    <p>Pseudo-feedback?  in the 2-stage smoothing</p>
    <p>Use feedback documents to estimate term dependent ws.</p>
    <p>Lead to future research directions</p>
    <p>)|),(( Uqwcp</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Poisson: Another family of retrieval models based on query generation</p>
    <p>Basic models behave similarly to multinomial  Benefits: per-term smoothing and efficient</p>
    <p>mixture background model  Many other potential flexibilities  Future work:</p>
    <p>explore document length normalization and pseudofeedback</p>
    <p>better estimation of per-term smoothing coefficients</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
</Presentation>
