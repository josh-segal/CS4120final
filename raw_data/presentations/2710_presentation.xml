<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Gaia: Geo-Distributed Machine Learning Approaching LAN Speeds</p>
    <p>Kevin Hsieh Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis, Gregory R. Ganger, Phillip B. Gibbons, Onur Mutlu</p>
  </div>
  <div class="page">
    <p>Machine Learning and Big Data</p>
    <p>Machine learning is widely used to derive useful information from large-scale data</p>
    <p>Image Classification</p>
    <p>VideosPictures</p>
    <p>Video Analytics</p>
    <p>User Activities</p>
    <p>Preference Prediction</p>
  </div>
  <div class="page">
    <p>Big Data is Geo-Distributed  A large amount of data is generated rapidly, all over the world</p>
  </div>
  <div class="page">
    <p>Centralizing Data is Infeasible [1, 2, 3]  Moving data over wide-area networks (WANs) can be</p>
    <p>extremely slow  It is also subject to data sovereignty laws</p>
  </div>
  <div class="page">
    <p>Geo-distributed ML is Challenging  No ML system is designed to run across data centers</p>
    <p>(up to 53X slowdown in our study)</p>
  </div>
  <div class="page">
    <p>Our Goal</p>
    <p>Develop a geo-distributed ML system  Minimize communication over wide-area networks</p>
    <p>Retain the accuracy and correctness of ML algorithms</p>
    <p>Without requiring changes to the algorithms</p>
    <p>Key Result: 1.8-53.5X speedup over state-of-the-art ML systems on WANs</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem &amp; Goal Background &amp; Motivation Gaia System Overview Approximate Synchronous Parallel System Implementation Evaluation Conclusion</p>
  </div>
  <div class="page">
    <p>Worker Machine N</p>
    <p>Parameter Server</p>
    <p>Background: Parameter Server Architecture</p>
    <p>The parameter server architecture has been widely adopted in many ML systems</p>
    <p>Worker Machine 1</p>
    <p>Data 1</p>
    <p>Parameter Server</p>
    <p>ML ModelUpdateUpdate</p>
    <p>ReadRead</p>
    <p>Training Data</p>
    <p>Data N</p>
    <p>UpdateUpdate</p>
    <p>ReadRead</p>
  </div>
  <div class="page">
    <p>Worker Machine N</p>
    <p>Parameter Server</p>
    <p>Background: Parameter Server Architecture</p>
    <p>The parameter server architecture has been widely adopted in many ML systems</p>
    <p>Worker Machine 1</p>
    <p>Data 1</p>
    <p>Parameter Server</p>
    <p>ML ModelUpdateUpdate</p>
    <p>ReadRead</p>
    <p>Training Data</p>
    <p>Data N</p>
    <p>UpdateUpdate</p>
    <p>ReadRead</p>
    <p>Synchronization is critical to the accuracy and correctness of ML algorithms</p>
  </div>
  <div class="page">
    <p>Deploy Parameter Servers on WANs  Deploying parameter servers across data centers</p>
    <p>requires a lot of communication over WANs</p>
    <p>Worker Machine N</p>
    <p>Parameter Server</p>
    <p>Worker Machine 1</p>
    <p>Parameter Server</p>
    <p>ML Model</p>
    <p>Data Center 1 Data Center 2</p>
  </div>
  <div class="page">
    <p>WAN: Low Bandwidth and High Cost  WAN bandwidth is 15X smaller than LAN bandwidth on average,</p>
    <p>and up to 60X smaller  In Amazon EC2, the monetary cost of WAN communication is</p>
    <p>up to 38X the cost of renting machines</p>
    <p>VirginiaCalifornia OregonIreland</p>
    <p>FrankfurtTokyo SeoulSingapore</p>
    <p>SydneyMumbai So Paulo</p>
    <p>N et</p>
    <p>w or</p>
    <p>k B</p>
    <p>an dw</p>
    <p>id th</p>
    <p>(M b/</p>
    <p>s)</p>
  </div>
  <div class="page">
    <p>LAN EC2-ALL V/C WAN S/S WANN or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E xe</p>
    <p>cu tio</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>un til</p>
    <p>C on</p>
    <p>ve rg</p>
    <p>en ce IterStore Bsen</p>
    <p>ML System Performance on WANs</p>
    <p>Matrix Factorization</p>
  </div>
  <div class="page">
    <p>LAN EC2-ALL V/C WAN S/S WANN or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E xe</p>
    <p>cu tio</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>un til</p>
    <p>C on</p>
    <p>ve rg</p>
    <p>en ce IterStore Bsen</p>
    <p>ML System Performance on WANs</p>
    <p>Running ML systems on WANs can seriously slow down ML applications</p>
    <p>Virginia / California Singapore / So Paulo</p>
    <p>Matrix Factorization</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem &amp; Goal  Background &amp; Motivation  Gaia System Overview  Approximate Synchronous Parallel  System Implementation  Evaluation  Conclusion</p>
  </div>
  <div class="page">
    <p>Gaia System Overview  Key idea: Decouple the synchronization model within</p>
    <p>the data center from the synchronization model between data centers</p>
    <p>Parameter Server</p>
    <p>Data Center 1</p>
    <p>Parameter Server</p>
    <p>Worker Machine</p>
    <p>Local Sync</p>
    <p>Parameter Server</p>
    <p>Parameter Server</p>
    <p>Data Center 2</p>
    <p>Worker Machine Worker Machine</p>
    <p>Approximately Correct Model Copy</p>
    <p>Approximately Correct Model Copy</p>
    <p>Remote Sync</p>
  </div>
  <div class="page">
    <p>Gaia System Overview  Key idea: Decouple the synchronization model within</p>
    <p>the data center from the synchronization model between data centers</p>
    <p>Parameter Server</p>
    <p>Data Center 1</p>
    <p>Parameter Server</p>
    <p>Worker Machine</p>
    <p>Local Sync</p>
    <p>Parameter Server</p>
    <p>Parameter Server</p>
    <p>Data Center 2</p>
    <p>Worker Machine Worker Machine</p>
    <p>Approximately Correct Model Copy</p>
    <p>Approximately Correct Model Copy</p>
    <p>Remote Sync</p>
    <p>Communicate over WANs only significant updates</p>
  </div>
  <div class="page">
    <p>gn ifi</p>
    <p>ca nt</p>
    <p>U pd</p>
    <p>at es</p>
    <p>%</p>
    <p>Threshold of Significant Updates</p>
    <p>Matrix Factorization Topic Modeling Image Classification</p>
    <p>Key Finding: Study of Update Significance</p>
    <p>The vast majority of updates are insignificant</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem &amp; Goal  Background &amp; Motivation  Gaia System Overview  Approximate Synchronous Parallel  System Implementation  Evaluation  Conclusion</p>
  </div>
  <div class="page">
    <p>Approximate Synchronous Parallel</p>
    <p>The significance filter  Filter updates based on their significance</p>
    <p>ASP selective barrier  Ensure significant updates are read in time</p>
    <p>Mirror clock  Safe guard for pathological cases</p>
  </div>
  <div class="page">
    <p>The Significance Filter</p>
    <p>Worker Machine</p>
    <p>Parameter Server</p>
    <p>Parameter X Value Aggregated Update</p>
    <p>Update (1) on X</p>
    <p>Significance Function</p>
    <p>Other Parameters</p>
    <p>Significance Threshold&gt;?</p>
    <p>.</p>
    <p>Update (2) on X</p>
    <p>11+20</p>
  </div>
  <div class="page">
    <p>Approximate Synchronous Parallel</p>
    <p>The significance filter  Filter updates based on their significance</p>
    <p>ASP selective barrier  Ensure significant updates are read in time</p>
    <p>Mirror clock  Safeguard for pathological cases</p>
  </div>
  <div class="page">
    <p>ASP Selective Barrier</p>
    <p>Data Center 1 Data Center 2</p>
    <p>Parameter Server Parameter Server</p>
    <p>Significant Update</p>
    <p>Significant Update</p>
    <p>Significant Update</p>
    <p>Significant Update</p>
    <p>Arrive too late!</p>
    <p>Data Center 1 Data Center 2</p>
    <p>Parameter Server Parameter Server</p>
    <p>Significant Update</p>
    <p>Significant Update</p>
    <p>Significant Update</p>
    <p>Significant Update</p>
    <p>Selective Barrier</p>
    <p>Only workers that depend on these parameters are blocked</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem &amp; Goal  Background &amp; Motivation  Gaia System Overview  Approximate Synchronous Parallel  System Implementation  Evaluation  Conclusion</p>
  </div>
  <div class="page">
    <p>Put it All Together: The Gaia System</p>
    <p>Local Server</p>
    <p>Gaia Parameter Server</p>
    <p>Worker Machine</p>
    <p>Significance Filter</p>
    <p>Parameter Store</p>
    <p>Worker Machine</p>
    <p>Worker Machine</p>
    <p>Mirror Server</p>
    <p>Mirror Client</p>
    <p>Data Center Boundary</p>
    <p>Control Queue</p>
    <p>Data Queue</p>
    <p>Gaia Parameter Server</p>
    <p>Update Aggregated Update</p>
    <p>Selective Barrier</p>
  </div>
  <div class="page">
    <p>Put it All Together: The Gaia System</p>
    <p>Local Server</p>
    <p>Gaia Parameter Server</p>
    <p>Worker Machine</p>
    <p>Significance Filter</p>
    <p>Parameter Store</p>
    <p>Worker Machine</p>
    <p>Worker Machine</p>
    <p>Mirror Server</p>
    <p>Mirror Client</p>
    <p>Data Center Boundary</p>
    <p>Control Queue</p>
    <p>Data Queue</p>
    <p>Gaia Parameter Server</p>
    <p>Update Aggregated Update</p>
    <p>Selective Barrier</p>
    <p>Control messages (barriers, etc.) are always prioritized</p>
    <p>No change is required for ML algorithms and ML programs</p>
  </div>
  <div class="page">
    <p>Problem: Broadcast Significant Updates</p>
    <p>Communication overhead is proportional to the number of data centers</p>
  </div>
  <div class="page">
    <p>Mitigation: Overlay Networks and Hubs</p>
    <p>Save communication on WANs by aggregating the updates at hubs</p>
    <p>Data Center Group Data Center Group Data Center Group</p>
    <p>Data Center Group</p>
    <p>Hub</p>
    <p>Hub Hub</p>
    <p>Hub</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem &amp; Goal  Background &amp; Motivation  Gaia System Overview  Approximate Synchronous Parallel  System Implementation  Evaluation  Conclusion</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Applications  Matrix Factorization with the Netflix dataset  Topic Modeling with the Nytimes dataset  Image Classification with the ILSVRC12 dataset</p>
    <p>Hardware platform  22 machines with emulated EC2 WAN bandwidth  We validated the performance with a real EC2 deployment</p>
    <p>Baseline  IterStore (Cui et al., SoCC14) and GeePS (Cui et al., EuroSys16) on WAN</p>
    <p>Performance metrics  Execution time until algorithm convergence  Monetary cost of algorithm convergence</p>
  </div>
  <div class="page">
    <p>Matrix Factorization Topic Modeling Image Classification</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E xe</p>
    <p>c. T</p>
    <p>im e</p>
    <p>Baseline Gaia LAN</p>
    <p>Performance  11 EC2 Data Centers</p>
    <p>Gaia achieves 3.7-6.0X speedup over Baseline Gaia is at most 1.40X of LAN speeds</p>
  </div>
  <div class="page">
    <p>Matrix Factorization</p>
    <p>Topic Modeling Image Classification</p>
    <p>Baseline Gaia LAN</p>
    <p>Matrix Factorization</p>
    <p>Topic Modeling Image Classification</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E xe</p>
    <p>c. T</p>
    <p>im e</p>
    <p>Baseline Gaia LAN</p>
    <p>Performance and WAN Bandwidth</p>
    <p>V/C WAN (Virginia/California)</p>
    <p>S/S WAN (Singapore/So Paulo)</p>
    <p>Gaia achieves 3.7-53.5X speedup over Baseline Gaia is at most 1.23X of LAN speeds</p>
  </div>
  <div class="page">
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>EC2-ALL V/C WAN S/S WAN</p>
    <p>Results  EC2 Monetary Cost</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>EC2-ALL V/C WAN S/S WAN</p>
    <p>N or</p>
    <p>m lia</p>
    <p>ed C</p>
    <p>os t</p>
    <p>Communication Cost Machine Cost (Network) Machine Cost (Compute)</p>
    <p>Matrix Factorization Topic Modeling</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>B as</p>
    <p>el in</p>
    <p>e</p>
    <p>G ai</p>
    <p>a</p>
    <p>EC2-ALL V/C WAN S/S WAN</p>
    <p>Image Classification</p>
    <p>Gaia is 2.6-59.0X cheaper than Baseline</p>
  </div>
  <div class="page">
    <p>More in the Paper</p>
    <p>Convergence proof of Approximate Synchronous Parallel (ASP)</p>
    <p>ASP vs. fully asynchronous</p>
    <p>Gaia vs. centralizing data approach</p>
  </div>
  <div class="page">
    <p>Key Takeaways  The Problem: How to perform ML on geo-distributed data?</p>
    <p>Centralizing data is infeasible. Geo-distributed ML is very slow</p>
    <p>Our Gaia Approach  Decouple the synchronization model within the data center from</p>
    <p>that across data centers  Eliminate insignificant updates across data centers</p>
    <p>A new synchronization model: Approximate Synchronous Parallel  Retain the correctness and accuracy of ML algorithms</p>
    <p>Key Results:  1.8-53.5X speedup over state-of-the-art ML systems on WANs  at most 1.40X of LAN speeds  without requiring changes to algorithms 34</p>
  </div>
  <div class="page">
    <p>Gaia: Geo-Distributed Machine Learning Approaching LAN Speeds</p>
    <p>Kevin Hsieh Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis, Gregory R. Ganger, Phillip B. Gibbons, Onur Mutlu</p>
  </div>
  <div class="page">
    <p>Executive Summary  The Problem: How to perform ML on geo-distributed data?</p>
    <p>Centralizing data is infeasible. Geo-distributed ML is very slow</p>
    <p>Our Goal  Minimize communication over WANs  Retain the correctness and accuracy of ML algorithms  Without requiring changes to ML algorithms</p>
    <p>Our Gaia Approach  Decouple the synchronization model within the data center from</p>
    <p>that across data centers: Eliminate insignificant updates on WANs  A new synchronization model: Approximate Synchronous Parallel</p>
    <p>Key Results:  1.8-53.5X speedup over state-of-the-art ML systems on WANs  within 1.40X of LAN speeds 36</p>
  </div>
  <div class="page">
    <p>Approximate Synchronous Parallel</p>
    <p>The significance filter  Filter updates based their significance</p>
    <p>ASP selective barrier  Ensure significant updates are read in time</p>
    <p>Mirror clock  Safeguard for pathological cases</p>
  </div>
  <div class="page">
    <p>Mirror Clock</p>
    <p>Data Center 1 Data Center 2</p>
    <p>Parameter Server Parameter Server</p>
    <p>d</p>
    <p>Data Center 2 Data Center 1</p>
    <p>Parameter Server Parameter Server</p>
    <p>Clock N</p>
    <p>Clock N Clock N + DS Guarantees all significant updates</p>
    <p>are seen after DS clocks</p>
    <p>BarrierNo guarantee under extreme network conditions</p>
  </div>
  <div class="page">
    <p>Effect of Synchronization Mechanisms</p>
    <p>Ob je ct</p>
    <p>iv e va lu</p>
    <p>e</p>
    <p>Time (Seconds)</p>
    <p>Gaia Gaia_Async</p>
    <p>Convergence value</p>
    <p>Ob je ct</p>
    <p>iv e va lu</p>
    <p>e</p>
    <p>Time (Seconds)</p>
    <p>Gaia Gaia_Async</p>
    <p>Convergence value</p>
    <p>Matrix Factorization Topic Modeling</p>
  </div>
  <div class="page">
    <p>Methodology Details  Hardware</p>
    <p>A 22-node cluster. Each has a 16-core Intel Xeon CPU (E5-2698), a NVIDIA Titan X GPU, 64GB RAM, and a 40GbE NIC</p>
    <p>Application details  Matrix Factorization: SGD algorithm, 500 ranks  Topic Modeling: Gibbs sampling, 500 topics</p>
    <p>Convergence criteria  The value of the objective function changes less than 2% over the</p>
    <p>course of 10 iterations  Significance Threshold</p>
    <p>1% and shrinks over time 1% 2</p>
  </div>
  <div class="page">
    <p>ML System Performance Comparison</p>
    <p>IterStore [Cui et al. SoCC15] shows 10X performance improvement over PowerGraph [Gonzalez et al., OSDI12] for Matrix Factorization</p>
    <p>PowerGraph matches the performance of GraphX [Gonzalez et al., OSDI14], a Spark-based system</p>
  </div>
  <div class="page">
    <p>Matrix Factorization (1/3)  Matrix factorization (also known as collaborative filtering) is a</p>
    <p>technique commonly used in recommender systems</p>
  </div>
  <div class="page">
    <p>Matrix Factorization (2/3)</p>
    <p>Movie</p>
    <p>User 4</p>
    <p>Rank (User Preference Parameters) ()</p>
    <p>Rank (Movie Parameters) (x)</p>
  </div>
  <div class="page">
    <p>Matrix Factorization (3/3)</p>
    <p>Objective function (L2 regularization)</p>
    <p>Solve with stochastic gradient decent (SGD)</p>
  </div>
  <div class="page">
    <p>Background  BSP  BSP (Bulk Synchronous Parallel)</p>
    <p>All machines need to receive all updates before proceeding to the next iteration</p>
    <p>Worker 1</p>
    <p>Worker 2</p>
    <p>Worker 3 Clock</p>
  </div>
  <div class="page">
    <p>Background  SSP  SSP (Stale Synchronous Parallel)</p>
    <p>Allows the fastest worker ahead of the slowest worker by a bounded number of iterations</p>
    <p>Worker 1</p>
    <p>Worker 2</p>
    <p>Worker 3 Clock</p>
    <p>Staleness = 1</p>
  </div>
  <div class="page">
    <p>Compare Against Centralizing Approach</p>
    <p>Gaia Speedup over Centralize</p>
    <p>Gaia to Centralize Cost Ratio</p>
    <p>Matrix Factorization EC2-ALL 1.11 3.54 V/C WAN 1.22 1.00 S/S WAN 2.13 1.17</p>
    <p>Topic Modeling EC2-ALL 0.80 6.14 V/C WAN 1.02 1.26 S/S WAN 1.25 1.92</p>
    <p>Image Classification EC2-ALL 0.76 3.33 V/C WAN 1.12 1.07 S/S WAN 1.86 1.08</p>
  </div>
  <div class="page">
    <p>SSP Performance  11 Data Centers</p>
    <p>Baseline Gaia LAN Baseline Gaia LAN</p>
    <p>BSP SSP</p>
    <p>N ro</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E xe</p>
    <p>cu tio</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>Amazon-EC2 Emulation-EC2 Emulation-Full-Speed</p>
    <p>Matrix Factorization</p>
  </div>
  <div class="page">
    <p>SSP Performance  11 Data Centers</p>
    <p>Topic Modeling</p>
    <p>Baseline Gaia LAN Baseline Gaia LAN</p>
    <p>BSP SSP</p>
    <p>N ro</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E xe</p>
    <p>cu tio</p>
    <p>n Ti</p>
    <p>m e Emulation-EC2</p>
    <p>Emulation-Full-Speed</p>
  </div>
  <div class="page">
    <p>SSP Performance  V/C WAN</p>
    <p>Matrix Factorization</p>
    <p>BSP SSP</p>
    <p>Baseline Gaia LAN</p>
    <p>Topic Modeling</p>
    <p>BSP SSP</p>
    <p>Baseline Gaia LAN</p>
  </div>
  <div class="page">
    <p>SSP Performance  S/S WAN</p>
    <p>Matrix Factorization Topic Modeling</p>
    <p>BSP SSP</p>
    <p>Baseline Gaia LAN</p>
    <p>BSP SSP</p>
    <p>Baseline Gaia LAN</p>
  </div>
</Presentation>
