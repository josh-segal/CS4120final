<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MegaPipe: A New Programming Interface for Scalable Network I/O</p>
    <p>Sangjin Han</p>
    <p>in collabora=on with</p>
    <p>Sco? Marshall Byung-Gon Chun Sylvia Ratnasamy</p>
    <p>University of California, Berkeley Yahoo! Research</p>
  </div>
  <div class="page">
    <p>tl;dr?</p>
    <p>MegaPipe is a new network programming API</p>
    <p>for message-oriented workloads</p>
    <p>to avoid the performance issues of BSD Socket API</p>
    <p>OSDI 2012 2</p>
  </div>
  <div class="page">
    <p>Two Types of Network Workloads</p>
    <p>Ex: video streaming, HDFS  Very cheap</p>
    <p>A half CPU core is enough to saturate a 10G link</p>
  </div>
  <div class="page">
    <p>Two Types of Network Workloads</p>
    <p>Ex: video streaming, HDFS  Very cheap</p>
    <p>A half CPU core is enough to saturate a 10G link</p>
    <p>CPU-intensive!</p>
  </div>
  <div class="page">
    <p>BSD Socket API Performance Issues</p>
    <p>n_events = epoll_wait(); // wait for I/O readiness for () {  new_fd = accept(listen_fd); // new connec=on  bytes = recv(fd2, buf, 4096); // new data for fd2</p>
    <p>OSDI 2012 5</p>
    <p>Issues with message-oriented workloads  System call overhead</p>
  </div>
  <div class="page">
    <p>BSD Socket API Performance Issues</p>
    <p>n_events = epoll_wait(); // wait for I/O readiness for () {  new_fd = accept(listen_fd); // new connec=on  bytes = recv(fd2, buf, 4096); // new data for fd2</p>
    <p>OSDI 2012 6</p>
    <p>Issues with message-oriented workloads  System call overhead  Shared listening socket</p>
  </div>
  <div class="page">
    <p>BSD Socket API Performance Issues</p>
    <p>n_events = epoll_wait(); // wait for I/O readiness for () {  new_fd = accept(listen_fd); // new connec=on  bytes = recv(fd2, buf, 4096); // new data for fd2</p>
    <p>OSDI 2012 7</p>
    <p>Issues with message-oriented workloads  System call overhead  Shared listening socket  File abstrac=on overhead</p>
  </div>
  <div class="page">
    <p>Microbenchmark: How Bad?</p>
    <p>RPC-like test on an 8-core Linux server (with epoll)</p>
    <p>new TCP connec=on</p>
    <p>request (64B)</p>
    <p>response (64B)</p>
    <p>Teardown</p>
  </div>
  <div class="page">
    <p>C P</p>
    <p>U U</p>
    <p>sa ge</p>
    <p>(% )</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>Message Size (B)</p>
    <p>Throughput CPU Usage</p>
    <p>Low throughput High overhead 9 OSDI 2012</p>
  </div>
  <div class="page">
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>an sa</p>
    <p>ct io</p>
    <p>ns /s</p>
    <p>)</p>
    <p>Number of Transactions per Connection</p>
  </div>
  <div class="page">
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>an sa</p>
    <p>ct io</p>
    <p>ns /s</p>
    <p>)</p>
    <p># of CPU Cores</p>
    <p>Ideal scaling</p>
    <p>Actual scaling</p>
  </div>
  <div class="page">
    <p>MEGAPIPE DESIGN</p>
  </div>
  <div class="page">
    <p>Design Goals</p>
    <p>OSDI 2012 13</p>
    <p>Concurrency as a first-class ci=zen</p>
    <p>Unified interface for various I/O types  Network connec=ons, disk files, pipes, signals, etc.</p>
    <p>Low overhead &amp; mul=-core scalability  Main focus of this presenta=on</p>
  </div>
  <div class="page">
    <p>OSDI 2012 14</p>
    <p>Overview</p>
    <p>Low per-core</p>
    <p>performance</p>
    <p>Poor mul=-core scalability</p>
    <p>System call overhead</p>
    <p>Shared listening socket</p>
    <p>VFS overhead</p>
    <p>System call batching</p>
    <p>Listening socket</p>
    <p>par==oning</p>
    <p>Lightweight socket</p>
    <p>Problem Cause Solution</p>
  </div>
  <div class="page">
    <p>Key Primi=ves  Handle</p>
    <p>Similar to file descriptor  But only valid within a channel</p>
    <p>TCP connec=on, pipe, disk file,</p>
    <p>Channel  Per-core, bidirec=onal pipe between user and kernel  Mul=plexes I/O opera=ons of its handles</p>
    <p>OSDI 2012 15</p>
  </div>
  <div class="page">
    <p>Sketch: How Channels Help (1/3)</p>
    <p>OSDI 2012 16</p>
    <p>User</p>
    <p>Kernel</p>
    <p>Handles</p>
    <p>Channel</p>
    <p>I/O Batching</p>
  </div>
  <div class="page">
    <p>Sketch: How Channels Help (2/3)</p>
    <p>OSDI 2012 17</p>
    <p>Core 1 Core 2 Core 3</p>
    <p>Shared accept queue</p>
    <p>accept()</p>
    <p>New connec=ons</p>
  </div>
  <div class="page">
    <p>Sketch: How Channels Help (2/3)</p>
    <p>OSDI 2012 18</p>
    <p>Core 1 Core 2 Core 3</p>
    <p>Listening socket par==oning</p>
    <p>New connec=ons</p>
  </div>
  <div class="page">
    <p>Sketch: How Channels Help (3/3)</p>
    <p>OSDI 2012 19</p>
    <p>Core 1 Core 2 Core 3</p>
    <p>VFS</p>
  </div>
  <div class="page">
    <p>Sketch: How Channels Help (3/3)</p>
    <p>OSDI 2012 20</p>
    <p>Core 1 Core 2 Core 3</p>
    <p>VFS</p>
    <p>Lightweight socket</p>
  </div>
  <div class="page">
    <p>MegaPipe API Func=ons</p>
    <p>mp_create() / mp_destroy()  Create/close a channel</p>
    <p>mp_register() / mp_unregister()  Register a handle (regular FD or lwsocket) into a channel</p>
    <p>mp_accept() /mp_read() / mp_write() /   Issue an asynchronous I/O command for a given handle</p>
    <p>mp_dispatch()  Dispatch an I/O comple=on event from a channel</p>
    <p>OSDI 2012 21</p>
  </div>
  <div class="page">
    <p>Comple=on No=fica=on Model  MegaPipe</p>
    <p>Go-and-Wait (Comple=on no=fica=on)</p>
    <p>OSDI 2012 22</p>
    <p>BSD Socket API  Wait-and-Go (Readiness model)</p>
    <p>mp_read(handle1, ); mp_read(handle2, );  ev = mp_dispatch(channel);  ev = mp_dispatch(channel);</p>
    <p>epoll_ctl(fd1, EPOLLIN); epoll_ctl(fd2, EPOLLIN); epoll_wait();  ret1 = recv(fd1, );  ret2 = recv(fd2, );</p>
    <p>L</p>
    <p>J</p>
    <p>J Batching J Easy and intui=ve J Compa=ble with disk files</p>
  </div>
  <div class="page">
    <p>OSDI 2012 23</p>
    <p>Transparent batching  Exploits parallelism of independent handles</p>
    <p>MegaPipe User-Level Library</p>
    <p>MegaPipe Kernel Module</p>
    <p>Read data from handle 6 Accept a new connection Read data from handle 3 Write data to handle 5</p>
    <p>New connection arrived Write done to handle 5</p>
    <p>Read done from handle 6</p>
    <p>Applica=on</p>
    <p>MegaPipe API (non-batched)</p>
    <p>Batched system calls</p>
  </div>
  <div class="page">
    <p>Instead of the globally shared accept queue</p>
    <p>OSDI 2012 24</p>
    <p>User</p>
    <p>Kernel</p>
    <p>mp_register()</p>
    <p>Accept queue</p>
    <p>Listening socket</p>
  </div>
  <div class="page">
    <p>Instead of the globally shared accept queue</p>
    <p>OSDI 2012 25</p>
    <p>User</p>
    <p>Kernel</p>
    <p>mp_register()</p>
    <p>Accept queue</p>
    <p>Accept queue</p>
    <p>Accept queue</p>
    <p>Listening socket</p>
  </div>
  <div class="page">
    <p>Instead of the globally shared accept queue</p>
    <p>OSDI 2012 26</p>
    <p>User</p>
    <p>Kernel</p>
    <p>Accept queue</p>
    <p>Accept queue</p>
    <p>Accept queue</p>
    <p>Listening socket</p>
    <p>mp_accept()</p>
    <p>New connec=ons</p>
  </div>
  <div class="page">
    <p>Sockets are ephemeral and rarely shared  Bypass the VFS layer  Convert into a regular file descriptor only when necessary</p>
    <p>OSDI 2012 27</p>
    <p>File instance (states)</p>
    <p>inode dentry</p>
    <p>TCP socket</p>
    <p>File descriptor</p>
    <p>File name?</p>
  </div>
  <div class="page">
    <p>Sockets are ephemeral and rarely shared  Bypass the VFS layer  Convert into a regular file descriptor only when necessary</p>
    <p>OSDI 2012 28</p>
    <p>File instance (states)</p>
    <p>inode dentry</p>
    <p>TCP socket</p>
    <p>File descriptor File descriptor</p>
    <p>dup() or fork()</p>
  </div>
  <div class="page">
    <p>Sockets are ephemeral and rarely shared  Bypass the VFS layer  Convert into a regular file descriptor only when necessary</p>
    <p>OSDI 2012 29</p>
    <p>File instance (states)</p>
    <p>inode dentry</p>
    <p>TCP socket</p>
    <p>File descriptor File descriptor</p>
    <p>File instance (states)</p>
    <p>open()</p>
  </div>
  <div class="page">
    <p>Sockets are ephemeral and rarely shared  Bypass the VFS layer  Convert into a regular file descriptor only when necessary</p>
    <p>OSDI 2012 30</p>
    <p>File instance (states)</p>
    <p>inode dentry</p>
    <p>TCP socket</p>
    <p>File descriptor</p>
    <p>TCP socket</p>
    <p>lwsocket</p>
  </div>
  <div class="page">
    <p>EVALUATION</p>
  </div>
  <div class="page">
    <p>Microbenchmark 1/2  Throughput improvement with various message sizes</p>
    <p>OSDI 2012 32</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t I</p>
    <p>m pr</p>
    <p>ov em</p>
    <p>en t (</p>
    <p>% )</p>
    <p># of CPU Cores</p>
  </div>
  <div class="page">
    <p>Microbenchmark 1/2  Throughput improvement with various message sizes</p>
    <p>OSDI 2012 33</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t I</p>
    <p>m pr</p>
    <p>ov em</p>
    <p>en t (</p>
    <p>% )</p>
    <p># of CPU Cores</p>
  </div>
  <div class="page">
    <p>Microbenchmark 1/2  Throughput improvement with various message sizes</p>
    <p>OSDI 2012 34</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t I</p>
    <p>m pr</p>
    <p>ov em</p>
    <p>en t (</p>
    <p>% )</p>
    <p># of CPU Cores</p>
  </div>
  <div class="page">
    <p>Microbenchmark 2/2  Mul=-core scalability</p>
    <p>with various connec=on lengths (# of transac=ons)</p>
    <p>OSDI 2012 35</p>
    <p>P ar</p>
    <p>al le</p>
    <p>l S pe</p>
    <p>ed up</p>
    <p># of CPU Cores</p>
    <p>Baseline MegaPipe</p>
  </div>
  <div class="page">
    <p>Macrobenchmark</p>
    <p>memcached  In-memory key-value store  Limited scalability</p>
    <p>Object store is shared by all cores with a global lock</p>
    <p>nginx  Web server  Highly scalable</p>
    <p>Nothing is shared by cores, except for the listening socket</p>
    <p>OSDI 2012 36</p>
  </div>
  <div class="page">
    <p>memcached</p>
    <p>MegaPipe Baseline</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>eq ue</p>
    <p>st s/</p>
    <p>s)</p>
    <p>Number of Requests per Connection 37 OSDI 2012</p>
    <p>memaslap with 90% GET, 10% SET, 64B keys, 1KB values</p>
    <p>Global lock bo?leneck</p>
  </div>
  <div class="page">
    <p>memcached</p>
    <p>MegaPipe</p>
    <p>MegaPipe-FL</p>
    <p>Baseline</p>
    <p>Baseline-FL</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>eq ue</p>
    <p>st s/</p>
    <p>s)</p>
    <p>Number of Requests per Connection 38 OSDI 2012</p>
    <p>memaslap with 90% GET, 10% SET, 64B keys, 1KB values</p>
  </div>
  <div class="page">
    <p>nginx</p>
    <p>Im pr</p>
    <p>ov em</p>
    <p>en t (</p>
    <p>% )</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p># of CPU Cores</p>
    <p>Improvement MegaPipe Baseline</p>
    <p>Based on Yahoo! HTTP traces: 6.3KiB, 2.3 trans/conn on avg.</p>
  </div>
  <div class="page">
    <p>CONCLUSION</p>
  </div>
  <div class="page">
    <p>Related Work  Batching [FlexSC, OSDI10] [libflexsc, ATC11]</p>
    <p>Excep=on-less system call  MegaPipe solves the scalability issues</p>
    <p>Par==oning [Affinity-Accept, EuroSys12]  Per-core accept queue  MegaPipe provides explicit control over par==oning</p>
    <p>VFS scalability [Mosbench, OSDI10]  MegaPipe bypasses the issues rather than mi=ga=ng</p>
    <p>OSDI 2012 41</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Short connec=ons or small messages:  High CPU overhead  Poorly scaling with mul=-core CPUs</p>
    <p>MegaPipe  Key abstrac=on: per-core channel  Enabling three op=miza=on opportuni=es:</p>
    <p>Batching, par==oning, lwsocket  15+% improvement for memcached, 75% for nginx</p>
  </div>
  <div class="page">
    <p>BACKUP SLIDES</p>
  </div>
  <div class="page">
    <p>Small Messages with MegaPipe</p>
    <p>OSDI 2012 44</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>an s/</p>
    <p>s)</p>
    <p># of Transactions per Connection</p>
    <p>Baseline Throughput MegaPipe</p>
  </div>
  <div class="page">
    <p># of messages ma?ers, not the volume of traffic  Per-message cost &gt;&gt;&gt; per-byte cost</p>
    <p>1KB msg is only 2% more expensive than 64B msg  10G link with 1KB messages  1M IOPS!</p>
    <p>Thus 1M+ system calls</p>
    <p>System calls are expensive [FlexSC, 2010]  Mode switching between kernel and user  CPU cache pollu=on</p>
  </div>
  <div class="page">
    <p>Short Connec=ons with MegaPipe</p>
    <p>OSDI 2012 46</p>
    <p>C P</p>
    <p>U U</p>
    <p>sa ge</p>
    <p>(% )</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>Message Size (B)</p>
    <p>Baseline CPU Usage MegaPipe</p>
  </div>
  <div class="page">
    <p>Connec=on establishment is expensive  Three-way handshaking / four-way teardown</p>
    <p>More packets  More system calls: accept(), epoll_ctl(), close(),</p>
    <p>Socket is represented as a file in UNIX  File overhead  VFS overhead</p>
  </div>
  <div class="page">
    <p>Mul=-Core Scalability with MegaPipe</p>
    <p>OSDI 2012 48</p>
    <p>E ff</p>
    <p>ic ie</p>
    <p>nc y</p>
    <p>(% )</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>an s/</p>
    <p>s)</p>
    <p># of CPU Cores</p>
    <p>Baseline Per-Core Efficiency MegaPipe</p>
  </div>
  <div class="page">
    <p>User</p>
    <p>Kernel</p>
    <p>Listening socket</p>
    <p>Shared queue issues [Affinity-Accept, 2012]  Conten=on on the listening socket  Poor connec=on affinity</p>
    <p>accept()</p>
    <p>RX</p>
    <p>TX</p>
  </div>
  <div class="page">
    <p>File instance</p>
    <p>inode dentry</p>
    <p>TCP socket</p>
    <p>Applica=on</p>
    <p>Process  file descriptor table</p>
    <p>VFS</p>
    <p>TCP/IP</p>
    <p>File/VFS mul=-core scalability issues</p>
    <p>Shared by threads</p>
    <p>Globally visible in the system</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Core 1</p>
    <p>Core N Channel instance</p>
    <p>File handles</p>
    <p>lwsocket handles</p>
    <p>Pending completion</p>
    <p>events</p>
    <p>VFS</p>
    <p>TCP/IP</p>
    <p>MegaPipe user-level library</p>
    <p>User application thread</p>
    <p>Batched async I/O commands</p>
    <p>Batched completion</p>
    <p>events</p>
    <p>Linux kernel</p>
    <p>Core 2</p>
    <p>MegaPipe API</p>
    <p>Handles</p>
    <p>Channel</p>
  </div>
  <div class="page">
    <p>Ping-Pong Server Example ch = mp_create() handle = mp_register(ch, listen_sd, mask=my_cpu_id) mp_accept(handle)</p>
    <p>while true: ev = mp_dispatch(ch) conn = ev.cookie if ev.cmd == ACCEPT: mp_accept(conn.handle) conn = new Connection() conn.handle = mp_register(ch, ev.fd, cookie=conn) mp_read(conn.handle, conn.buf, READSIZE) elif ev.cmd == READ: mp_write(conn.handle, conn.buf, ev.size) elif ev.cmd == WRITE: mp_read(conn.handle, conn.buf, READSIZE) elif ev.cmd == DISCONNECT: mp_unregister(ch, conn.handle) delete conn OSDI 2012 52</p>
  </div>
  <div class="page">
    <p>Contribu=on Breakdown</p>
    <p>OSDI 2012 53</p>
  </div>
  <div class="page">
    <p>memcached latency</p>
    <p>L at</p>
    <p>en cy</p>
    <p>( s)</p>
    <p># of Concurrent Client Connections</p>
    <p>Baseline-FL 99%</p>
    <p>MegaPipe-FL 99%</p>
    <p>Baseline-FL 50%</p>
    <p>MegaPipe-FL 50%</p>
  </div>
  <div class="page">
    <p>Clean-Slate vs. Dirty-Slate</p>
    <p>MegaPipe: a clean-slate approach with new APIs  Quick prototyping for various op=miza=ons  Performance improvement: worthwhile!</p>
    <p>Can we apply the same techniques back to the BSD Socket API?  Each technique has its own challenges</p>
    <p>Embracing all could be even harder  Future WorkTM</p>
    <p>OSDI 2012 55</p>
  </div>
</Presentation>
