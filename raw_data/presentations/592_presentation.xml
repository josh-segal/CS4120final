<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Recursive Random Fields</p>
    <p>Daniel Lowd University of Washington</p>
    <p>(Joint work with Pedro Domingos)</p>
  </div>
  <div class="page">
    <p>One-Slide Summary</p>
    <p>Question: How to represent uncertainty in relational domains?  State-of-the-Art: Markov logic [Richardson &amp; Domingos, 2004]</p>
    <p>Markov logic network (MLN) = First-order KB with weights:</p>
    <p>Problem: Only top-level conjunction and universal quantifiers are probabilistic</p>
    <p>Solution: Recursive random fields (RRFs)  RRF = MLN whose features are MLNs  Inference: Gibbs sampling, iterated conditional modes  Learning: Back-propagation</p>
    <p>i iinwxX Z exp)Pr( 1</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Example: Friends and Smokers  Recursive random fields</p>
    <p>Representation  Inference  Learning</p>
    <p>Experiments: Databases with probabilistic integrity constraints</p>
    <p>Future work and conclusion</p>
  </div>
  <div class="page">
    <p>Example: Friends and Smokers</p>
    <p>Predicates:</p>
    <p>Smokes(x); Cancer(x); Friends(x,y)</p>
    <p>We wish to represent beliefs such as:  Smoking causes cancer  Friends of friends are friends (transitivity)  Everyone has a friend who smokes</p>
    <p>[Richardson and Domingos, 2004]</p>
  </div>
  <div class="page">
    <p>First-Order Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
    <p>w1 w2</p>
    <p>w3</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
    <p>w1 w2</p>
    <p>w3</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
    <p>w1 w2</p>
    <p>w3</p>
    <p>This becomes a disjunction of n conjunctions.</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
    <p>w1 w2</p>
    <p>w3</p>
    <p>In CNF, each grounding explodes into 2n clauses!</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
    <p>w1 w2</p>
    <p>w3</p>
  </div>
  <div class="page">
    <p>Markov Logic</p>
    <p>Sm(x)</p>
    <p>Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>f0</p>
    <p>x  x,y,z  x</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic</p>
    <p>L o</p>
    <p>g ic</p>
    <p>a l</p>
    <p>w1 w2</p>
    <p>w3</p>
    <p>Where: fi (x) = 1/Zi exp()</p>
  </div>
  <div class="page">
    <p>Recursive Random Fields</p>
    <p>Sm(x) Ca(x) Fr(x,y) Fr(y,z) Fr(x,z)</p>
    <p>f0</p>
    <p>x f1(x)</p>
    <p>Fr(x,y) Sm(y)</p>
    <p>y f4(x,y)</p>
    <p>P ro</p>
    <p>b a</p>
    <p>b ili</p>
    <p>st ic w1</p>
    <p>w2 w3</p>
    <p>x,y,z f2(x,y,z) x f3(x)</p>
    <p>w4 w6 w5</p>
    <p>w7 w8</p>
    <p>w9</p>
    <p>w10 w11</p>
    <p>Where: fi (x) = 1/Zi exp()</p>
  </div>
  <div class="page">
    <p>RRF features are parameterized and are grounded using objects in the domain.</p>
    <p>Leaves = Predicates:</p>
    <p>Recursive features are built up from other RRF</p>
    <p>features:</p>
    <p>The RRF Model</p>
    <p>)()(exp 1</p>
    <p>),( 22113 yfwxfw Z</p>
    <p>yxf</p>
    <p>)(Smokes)(1 xxf</p>
    <p>y</p>
    <p>yfwxfw Z</p>
    <p>xf )()(exp 1</p>
    <p>)( 213</p>
  </div>
  <div class="page">
    <p>Representing Logic: AND</p>
    <p>(x1    xn)</p>
    <p>(W o</p>
    <p>rl d</p>
    <p>) # true literals</p>
  </div>
  <div class="page">
    <p>Representing Logic: OR</p>
    <p>(x1    xn)</p>
    <p>(x1   xn)</p>
    <p>(x1    xn)  1/Z exp(w1 x1 + + wnxn)</p>
    <p>De Morgan:</p>
    <p>(x  y)  (x  y)</p>
    <p>(W o</p>
    <p>rl d</p>
    <p>) # true literals</p>
  </div>
  <div class="page">
    <p>Representing Logic: FORALL</p>
    <p>(x1    xn)</p>
    <p>(x1   xn)</p>
    <p>(x1    xn)  1/Z exp(w1 x1 + + wnxn)</p>
    <p>a: f(a)</p>
    <p>P (W</p>
    <p>o rl</p>
    <p>d )</p>
    <p># true literals</p>
  </div>
  <div class="page">
    <p>Representing Logic: EXIST</p>
    <p>(x1    xn)</p>
    <p>(x1   xn)</p>
    <p>(x1    xn)  1/Z exp(w1 x1 + + wnxn)</p>
    <p>a: f(a)</p>
    <p>a: f(a)  ( a: f(a)) 1/Z exp(w x1 + w x2 + )</p>
    <p>P (W</p>
    <p>o rl</p>
    <p>d )</p>
    <p># true literals</p>
  </div>
  <div class="page">
    <p>Distributions MLNs and RRFs can compactly represent</p>
    <p>Distribution MLNs RRFs</p>
    <p>Propositional MRF Yes Yes</p>
    <p>Deterministic KB Yes Yes</p>
    <p>Soft conjunction Yes Yes</p>
    <p>Soft universal quantification Yes Yes</p>
    <p>Soft disjunction No Yes</p>
    <p>Soft existential quantification No Yes</p>
    <p>Soft nested formulas No Yes</p>
  </div>
  <div class="page">
    <p>Inference and Learning</p>
    <p>Inference  MAP: Iterated conditional modes (ICM)  Conditional probabilities: Gibbs sampling</p>
    <p>Learning  Back-propagation  Pseudo-likelihood  RRF weight learning is more powerful than</p>
    <p>MLN structure learning (cf. KBANN)  More flexible theory revision</p>
  </div>
  <div class="page">
    <p>Experiments: Databases with Probabilistic Integrity Constraints</p>
    <p>Integrity constraints: First-order logic  Inclusion:</p>
    <p>If x is in table R, it must also be in table S  Functional dependency:</p>
    <p>In table R, each x determines a unique y</p>
    <p>Need to make them probabilistic Perfect application of MLNs/RRFs</p>
  </div>
  <div class="page">
    <p>Experiment 1: Inclusion Constraints</p>
    <p>Task: Clean a corrupt database  Relations</p>
    <p>ProjectLead(x,y)  x is in charge of project y  ManagerOf(x,z)  x manages employee z  Corrupt versions: ProjectLead(x,y); ManagerOf(x,z)</p>
    <p>Constraints  Every project leader manages at least one employee.</p>
    <p>i.e., x.(y.ProjectLead(x,y))  (z.Manages(x,z))  Corrupt database is related to original database</p>
    <p>i.e., ProjectLead(x,y)  ProjectLead(x,y)</p>
  </div>
  <div class="page">
    <p>Experiment 1: Inclusion Constraints</p>
    <p>Data  100 people, 100 projects  25% are managers of ~10 projects each, and</p>
    <p>manage ~5 employees per project  Added extra ManagerOf(x,y) relations  Predicate truth values flipped with probability p</p>
    <p>Models  Converted FOL to MLN and RRF  Maximized pseudo-likelihood</p>
  </div>
  <div class="page">
    <p>Experiment 1: Results</p>
  </div>
  <div class="page">
    <p>Experiment 2: Functional Dependencies</p>
    <p>Task: Determine which names are pseudonyms  Relation:</p>
    <p>Supplier(TaxID,CompanyName,PartType)  Describes a company that supplies parts</p>
    <p>Constraint  Company names with same TaxID are equivalent</p>
    <p>i.e., x,y1,y2.( z1,z2.Supplier(x,y1,z1)  Supplier(x,y2,z2) )  y1 = y2</p>
  </div>
  <div class="page">
    <p>Experiment 2: Functional Dependencies</p>
    <p>Data  30 tax IDs, 30 company names, 30 part types  Each company supplies 25% of all part types  Each company has k names  Company names are changed with probability p</p>
    <p>Models  Converted FOL to MLN and RRF  Maximized pseudo-likelihood</p>
  </div>
  <div class="page">
    <p>Experiment 2: Results</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Scaling up  Pruning, caching  Alternatives to Gibbs, ICM, gradient descent</p>
    <p>Experiments with real-world databases  Probabilistic integrity constraints  Information extraction, etc.</p>
    <p>Extract information a la TREPAN (Craven and Shavlik, 1995)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Recursive random fields:</p>
    <p>Less intuitive than Markov logic</p>
    <p>More computationally costly</p>
    <p>+ Compactly represent many distributions MLNs cannot</p>
    <p>+ Make conjunctions, existentials, and nested formulas probabilistic</p>
    <p>+ Offer new methods for structure learning and theory revision</p>
    <p>Questions: lowd@cs.washington.edu</p>
  </div>
</Presentation>
