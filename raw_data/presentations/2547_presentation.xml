<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Keepin It Real: Semi-Supervised Learning with</p>
    <p>Realistic Tuning</p>
    <p>Computer Sciences Department University of Wisconsin-Madison</p>
    <p>Andrew B. Goldberg goldberg@cs.wisc.edu</p>
    <p>Xiaojin Zhu jerryzhu@cs.wisc.edu</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>Gap between Semi-Supervised Learning (SSL) research and practical applications</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>Gap between Semi-Supervised Learning (SSL) research and practical applications</p>
    <p>Semi-Supervised Learning: Using unlabeled data to build better classifiers</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>Gap between Semi-Supervised Learning (SSL) research and practical applications</p>
    <p>Semi-Supervised Learning: Using unlabeled data to build better classifiers</p>
    <p>Real World  natural language</p>
    <p>processing  computer vision  web search &amp; IR  bioinformatics  etc</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>Gap between Semi-Supervised Learning (SSL) research and practical applications</p>
    <p>Semi-Supervised Learning: Using unlabeled data to build better classifiers</p>
    <p>Real World  natural language</p>
    <p>processing  computer vision  web search &amp; IR  bioinformatics  etc</p>
    <p>Assumptions  manifold? clusters?  low-density gap?  multiple views?</p>
    <p>Parameters regularization? graph weights? kernel parameters?</p>
    <p>Model Selection Little labeled data Many parameters Computational costs</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>Gap between Semi-Supervised Learning (SSL) research and practical applications</p>
    <p>Semi-Supervised Learning: Using unlabeled data to build better classifiers</p>
    <p>Real World  natural language</p>
    <p>processing  computer vision  web search &amp; IR  bioinformatics  etc</p>
    <p>Assumptions  manifold? clusters?  low-density gap?  multiple views?</p>
    <p>Parameters regularization? graph weights? kernel parameters?</p>
    <p>Model Selection Little labeled data Many parameters Computational costs</p>
    <p>Wrong choices could hurt performance!</p>
    <p>How can we ensure that SSL is never worse than supervised learning?</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR FOCUS</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR FOCUS</p>
    <p>Two critical issues  Parameter tuning  Choosing which (if any) SSL algorithm to use</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR FOCUS</p>
    <p>Two critical issues  Parameter tuning  Choosing which (if any) SSL algorithm to use</p>
    <p>Interested in realistic settings:  Practitioner is given some new labeled and unlabeled data  Must produce the best classifier possible</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
    <p>Eight less-familiar NLP tasks, three evaluation metrics</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
    <p>Eight less-familiar NLP tasks, three evaluation metrics</p>
    <p>Experimental protocol explores several real-world settings</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
    <p>Eight less-familiar NLP tasks, three evaluation metrics</p>
    <p>Experimental protocol explores several real-world settings</p>
    <p>All parameters are tuned realistically via cross validation</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
    <p>Eight less-familiar NLP tasks, three evaluation metrics</p>
    <p>Experimental protocol explores several real-world settings</p>
    <p>All parameters are tuned realistically via cross validation</p>
    <p>Findings under these conditions:</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
    <p>Eight less-familiar NLP tasks, three evaluation metrics</p>
    <p>Experimental protocol explores several real-world settings</p>
    <p>All parameters are tuned realistically via cross validation</p>
    <p>Findings under these conditions:</p>
    <p>Each SSL can be worse than SL on some data sets</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUR CONTRIBUTIONS  Medium-scale empirical study</p>
    <p>Compares one supervised learning (SL) and two SSL methods</p>
    <p>Eight less-familiar NLP tasks, three evaluation metrics</p>
    <p>Experimental protocol explores several real-world settings</p>
    <p>All parameters are tuned realistically via cross validation</p>
    <p>Findings under these conditions:</p>
    <p>Each SSL can be worse than SL on some data sets</p>
    <p>Can achieve agnostic SSL by using cross validation accuracy to select among SL and SSL algorithms</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OUTLINE  Introduce realistic tuning for SSL  Empirical study protocol</p>
    <p>Data sets  Algorithms  Meta algorithm for SSL model selection  Performance metrics</p>
    <p>Results  Conclusions</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>No, this is cheating</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>No, this is cheating</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>k-fold cross validation?</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>k-fold cross validation?</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>Little labeled data, but best available option</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>k-fold cross validation?</p>
    <p>Cross validation choices:</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>Little labeled data, but best available option</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>k-fold cross validation?</p>
    <p>Cross validation choices:</p>
    <p>number of folds</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>Little labeled data, but best available option</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>k-fold cross validation?</p>
    <p>Cross validation choices:</p>
    <p>number of folds</p>
    <p>how labeled and unlabeled data is divided into folds</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>Little labeled data, but best available option</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SSL WITH REALISTIC TUNING  Given labeled and unlabeled data,</p>
    <p>how should you set parameters for some algorithm?</p>
    <p>Tune based on test set performance?</p>
    <p>Use default values based on heuristics/experience?</p>
    <p>k-fold cross validation?</p>
    <p>Cross validation choices:</p>
    <p>number of folds</p>
    <p>how labeled and unlabeled data is divided into folds</p>
    <p>parameter grid</p>
    <p>No, this is cheating</p>
    <p>May fail on new data</p>
    <p>Little labeled data, but best available option</p>
    <p>{(x1, y1), . . . , (xl, yl), xl+1, ..., xl+u}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE Input:</p>
    <p>a single data set of labeled and unlabeled data (one real-world scenario)</p>
    <p>an algorithm (SSL or SL) and data-independent parameter grid</p>
    <p>performance metric M</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE Input:</p>
    <p>a single data set of labeled and unlabeled data (one real-world scenario)</p>
    <p>an algorithm (SSL or SL) and data-independent parameter grid</p>
    <p>performance metric M</p>
    <p>Procedure:</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE Input:</p>
    <p>a single data set of labeled and unlabeled data (one real-world scenario)</p>
    <p>an algorithm (SSL or SL) and data-independent parameter grid</p>
    <p>performance metric M</p>
    <p>Procedure:</p>
    <p>Compute 5-fold average performance Mparams=p</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE Input:</p>
    <p>a single data set of labeled and unlabeled data (one real-world scenario)</p>
    <p>an algorithm (SSL or SL) and data-independent parameter grid</p>
    <p>performance metric M</p>
    <p>Procedure:</p>
    <p>Compute 5-fold average performance Mparams=p</p>
    <p>Output:</p>
    <p>Model trained using the best parameters p = argmax Mparams Best average tuning performance (max Mparams)</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Designed to simulate different settings a real-world practitioner might face for a new task and a set of algorithms to choose from</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Designed to simulate different settings a real-world practitioner might face for a new task and a set of algorithms to choose from</p>
    <p>Labeled sizes = 10 or 100</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Designed to simulate different settings a real-world practitioner might face for a new task and a set of algorithms to choose from</p>
    <p>Labeled sizes = 10 or 100</p>
    <p>Unlabeled sizes = 100 or 1000</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Designed to simulate different settings a real-world practitioner might face for a new task and a set of algorithms to choose from</p>
    <p>Labeled sizes = 10 or 100</p>
    <p>Unlabeled sizes = 100 or 1000</p>
    <p>For each combination, run 10 trials with different random labeled and unlabeled data (same samples across algorithms)</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Designed to simulate different settings a real-world practitioner might face for a new task and a set of algorithms to choose from</p>
    <p>Labeled sizes = 10 or 100</p>
    <p>Unlabeled sizes = 100 or 1000</p>
    <p>For each combination, run 10 trials with different random labeled and unlabeled data (same samples across algorithms)</p>
    <p>Same grid of algorithm-specific parameters used for all data sets</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL Input:</p>
    <p>Fully labeled data set Algorithm, Performance metric Labeled sizes = {10, 100}, Unlabeled sizes = {100, 1000}</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL Input:</p>
    <p>Fully labeled data set Algorithm, Performance metric Labeled sizes = {10, 100}, Unlabeled sizes = {100, 1000}</p>
    <p>Procedure:</p>
    <p>Divide data into training data pool and a single test set For each l and u value: Randomly select labeled &amp; unlabeled data from training pool Use RealSSL for parameter tuning and model building Compute transductive and test performance</p>
    <p>{Repeat10 times</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL Input:</p>
    <p>Fully labeled data set Algorithm, Performance metric Labeled sizes = {10, 100}, Unlabeled sizes = {100, 1000}</p>
    <p>Procedure:</p>
    <p>Divide data into training data pool and a single test set For each l and u value: Randomly select labeled &amp; unlabeled data from training pool Use RealSSL for parameter tuning and model building Compute transductive and test performance</p>
    <p>Output: Tuning, transductive, and test performance for all l/u settings in 10 trials</p>
    <p>{Repeat10 times</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>DATA SETS  Binary classification tasks</p>
    <p>Name d P(y=+) |Dtest| Description</p>
    <p>MacWin 7511 0.51 846 Mac vs. Windows newsgroups</p>
    <p>Interest 2687 0.53 1268 WSD: monetary sense vs. others</p>
    <p>aut-avn 20707 0.65 70075 Auto vs. Aviation, SRAA corpus</p>
    <p>real-sim 20958 0.31 71209 Real vs. Simulated, SRAA corpus</p>
    <p>ccat 47236 0.47 22019 Corporate vs. rest, RCV1 corpus</p>
    <p>gcat 47236 0.30 22019 Government vs. rest, RCV1 corpus</p>
    <p>Wish-politics 13610 0.34 4999 Wish detection in political discussion</p>
    <p>Wish-products 4823 0.12 129 Wish detection in product reviews</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>ALGORITHMS</p>
    <p>Linear classifiers only:</p>
    <p>Supervised SVM:</p>
    <p>ignores the unlabeled data</p>
    <p>Semi-Supervised SVM (S3VM):</p>
    <p>assumes low density gap between classes</p>
    <p>Manifold Regularization (MR):</p>
    <p>assumes smoothness w.r.t. graph</p>
    <p>f (x) = w!x + b</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SUPERVISED SVM Maximizes margin between decision boundary and labeled data</p>
    <p>min f</p>
    <p>l!</p>
    <p>i=1</p>
    <p>max(0, 1 &quot; yif (xi))</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SUPERVISED SVM Maximizes margin between decision boundary and labeled data</p>
    <p>min f</p>
    <p>l!</p>
    <p>i=1</p>
    <p>max(0, 1 &quot; yif (xi))</p>
    <p>yf (x)</p>
    <p>Hinge loss</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SUPERVISED SVM Maximizes margin between decision boundary and labeled data</p>
    <p>min f</p>
    <p>l!</p>
    <p>i=1</p>
    <p>max(0, 1 &quot; yif (xi))</p>
    <p>yf (x)</p>
    <p>Hinge loss</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SEMI-SUPERVISED SVM (S3VM) Places decision boundary in low density region</p>
    <p>min f</p>
    <p>!</p>
    <p>l!</p>
    <p>i=1</p>
    <p>max(0, 1 &quot; yif (xi)) + !!</p>
    <p>u</p>
    <p>l+u!</p>
    <p>j=l+1</p>
    <p>max(0, 1 &quot; |f (xj )|)</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SEMI-SUPERVISED SVM (S3VM) Places decision boundary in low density region</p>
    <p>min f</p>
    <p>!</p>
    <p>l!</p>
    <p>i=1</p>
    <p>max(0, 1 &quot; yif (xi)) + !!</p>
    <p>u</p>
    <p>l+u!</p>
    <p>j=l+1</p>
    <p>max(0, 1 &quot; |f (xj )|)</p>
    <p>Hat loss</p>
    <p>-1 f (x)</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>SEMI-SUPERVISED SVM (S3VM) Places decision boundary in low density region</p>
    <p>min f</p>
    <p>!</p>
    <p>l!</p>
    <p>i=1</p>
    <p>max(0, 1 &quot; yif (xi)) + !!</p>
    <p>u</p>
    <p>l+u!</p>
    <p>j=l+1</p>
    <p>max(0, 1 &quot; |f (xj )|)</p>
    <p>Hat loss</p>
    <p>-1 f (x)Parameters: !, !!</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MANIFOLD REGULARIZATION (MR) Assumes smoothness w.r.t. graph over labeled/unlabeled data</p>
    <p>(similar examples should get similar labels)</p>
    <p>kNN graph, where wij = exp</p>
    <p>! ! &quot;xi ! xj&quot;2</p>
    <p>min f</p>
    <p>!A!f!22 + 1 l</p>
    <p>l!</p>
    <p>i=1</p>
    <p>V (yif (xi)) + !I l+u!</p>
    <p>i=1</p>
    <p>l+u!</p>
    <p>j=1</p>
    <p>wij (f (xi) &quot; f (xj ))2</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MANIFOLD REGULARIZATION (MR) Assumes smoothness w.r.t. graph over labeled/unlabeled data</p>
    <p>(similar examples should get similar labels)</p>
    <p>Unsmoothness penalty: if is large,</p>
    <p>should be small.</p>
    <p>wij (f (xi) ! f (xj ))2</p>
    <p>kNN graph, where wij = exp</p>
    <p>! ! &quot;xi ! xj&quot;2</p>
    <p>min f</p>
    <p>!A!f!22 + 1 l</p>
    <p>l!</p>
    <p>i=1</p>
    <p>V (yif (xi)) + !I l+u!</p>
    <p>i=1</p>
    <p>l+u!</p>
    <p>j=1</p>
    <p>wij (f (xi) &quot; f (xj ))2</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MANIFOLD REGULARIZATION (MR) Assumes smoothness w.r.t. graph over labeled/unlabeled data</p>
    <p>(similar examples should get similar labels)</p>
    <p>Unsmoothness penalty: if is large,</p>
    <p>should be small.</p>
    <p>wij (f (xi) ! f (xj ))2</p>
    <p>Parameters: !A, !I</p>
    <p>k in kNN !</p>
    <p>kNN graph, where wij = exp</p>
    <p>! ! &quot;xi ! xj&quot;2</p>
    <p>min f</p>
    <p>!A!f!22 + 1 l</p>
    <p>l!</p>
    <p>i=1</p>
    <p>V (yif (xi)) + !I l+u!</p>
    <p>i=1</p>
    <p>l+u!</p>
    <p>j=1</p>
    <p>wij (f (xi) &quot; f (xj ))2</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TOWARD AGNOSTIC SSL</p>
    <p>Important question: How can we automatically choose between SL={SVM}, SSL={S3VM, MR}?</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TOWARD AGNOSTIC SSL</p>
    <p>Recall our goal of ensuring that unlabeled data doesnt hurt us</p>
    <p>Important question: How can we automatically choose between SL={SVM}, SSL={S3VM, MR}?</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TOWARD AGNOSTIC SSL</p>
    <p>Recall our goal of ensuring that unlabeled data doesnt hurt us</p>
    <p>Common view is that model selection with CV is unreliable with little labeled data</p>
    <p>Important question: How can we automatically choose between SL={SVM}, SSL={S3VM, MR}?</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TOWARD AGNOSTIC SSL</p>
    <p>Recall our goal of ensuring that unlabeled data doesnt hurt us</p>
    <p>Common view is that model selection with CV is unreliable with little labeled data</p>
    <p>We explicitly tested this hypothesis</p>
    <p>Important question: How can we automatically choose between SL={SVM}, SSL={S3VM, MR}?</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TOWARD AGNOSTIC SSL</p>
    <p>Recall our goal of ensuring that unlabeled data doesnt hurt us</p>
    <p>Common view is that model selection with CV is unreliable with little labeled data</p>
    <p>We explicitly tested this hypothesis</p>
    <p>Also use meta-level model selection procedure  Select model family as well as member within the family</p>
    <p>Important question: How can we automatically choose between SL={SVM}, SSL={S3VM, MR}?</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MODEL SELECTION</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MODEL SELECTION Given several algorithms (e.g., SL={SVM}, SSL={S3VM, MR})</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MODEL SELECTION Given several algorithms (e.g., SL={SVM}, SSL={S3VM, MR})</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MODEL SELECTION Given several algorithms (e.g., SL={SVM}, SSL={S3VM, MR})</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MODEL SELECTION Given several algorithms (e.g., SL={SVM}, SSL={S3VM, MR})</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>MODEL SELECTION Given several algorithms (e.g., SL={SVM}, SSL={S3VM, MR})</p>
    <p>Note: On a per-trial basis to simulate single real-world training set</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>PERFORMANCE METRICS Three commonly used metrics in NLP</p>
    <p>Accuracy:</p>
    <p>Maximum F1 value achieved over entire precision-recall curve</p>
    <p>AUROC: area under the ROC curve</p>
    <p>Each is used for both parameter tuning and evaluation</p>
    <p>n!</p>
    <p>i=1</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OVERALL RESULTS accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OVERALL RESULTS accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
    <p>Just kidding...</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OBSERVATIONS</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OBSERVATIONS  No algorithm is universally superior</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OBSERVATIONS  No algorithm is universally superior</p>
    <p>Each of the SSL algorithms can be significantly worse than SL</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OBSERVATIONS  No algorithm is universally superior</p>
    <p>Each of the SSL algorithms can be significantly worse than SL</p>
    <p>Tuning with accuracy as the metric is valid for SSL model selection</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OBSERVATIONS  No algorithm is universally superior</p>
    <p>Each of the SSL algorithms can be significantly worse than SL</p>
    <p>Tuning with accuracy as the metric is valid for SSL model selection  Out of 32 settings (8 data sets x 4 labeled/unlabeled sizes):</p>
    <p>Significantly Better Same Worse</p>
    <p>Best Tuning vs. SVM</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>OBSERVATIONS  No algorithm is universally superior</p>
    <p>Each of the SSL algorithms can be significantly worse than SL</p>
    <p>Tuning with accuracy as the metric is valid for SSL model selection  Out of 32 settings (8 data sets x 4 labeled/unlabeled sizes):</p>
    <p>Significantly Better Same Worse</p>
    <p>Best Tuning vs. SVM</p>
    <p>Tuning with maxF1 or AUROC as the metric is less reliable</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS Compared relative performance across all data sets in terms of:</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
    <p>CV using accuracy and maxF1 mitigates some risk in applying SSL: worse than SVM in fewer trials</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
    <p>CV using accuracy and maxF1 mitigates some risk in applying SSL: worse than SVM in fewer trials</p>
    <p>Even with only 10 labeled points!</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
    <p>CV using accuracy and maxF1 mitigates some risk in applying SSL: worse than SVM in fewer trials</p>
    <p>But...due to conservative tie-breaking strategy, outperforms SVM in fewer trials as well</p>
    <p>Even with only 10 labeled points!</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS (#trials worse than SVM, #trials equal to SVM, #trials better than SVM) out of 80 trials (10 trials x 8 data sets) per l/u setting</p>
    <p>accuracy maxF1 AUROC</p>
    <p>u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000 Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR</p>
    <p>[MacWin]</p>
    <p>[Interest]</p>
    <p>[aut-avn]</p>
    <p>[real-sim]</p>
    <p>[ccat]</p>
    <p>[gcat]</p>
    <p>[WISH-politics]</p>
    <p>[WISH-products]</p>
    <p>Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,</p>
    <p>theboldface indicates themaximumvalue ineach row, aswell asothers in the rowthat arenot statistically significantly</p>
    <p>different based on a paired t-test.</p>
    <p>u = 100 u = 1000 Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>accuracy 10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14) Test</p>
    <p>Metric l S3VM MR Best Tuning S3VM MR Best Tuning</p>
    <p>AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13) Test</p>
    <p>Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of</p>
    <p>the form (#trials worse than SVM, #trials equal to SVM, #trials better than SVM).</p>
    <p>AUROC as the performance metric is less reliable</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS Average test performance over the 80 runs in each setting:</p>
    <p>u = 100 u = 1000 Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>accuracy 10 0.61 0.62 0.67 0.68 0.61 0.63 0.64 0.67 Test</p>
    <p>Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>maxF1 10 0.59 0.61 0.64 0.59 0.59 0.61 0.61 0.59 Test</p>
    <p>Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>AUROC 10 0.63 0.64 0.72 0.61 0.63 0.64 0.67 0.61 Test</p>
    <p>Table 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.</p>
    <p>outperformsSVMinfewer trials than theotheralgo</p>
    <p>rithms in some settings for these two metrics. This</p>
    <p>is because Best Tuning conservatively selects SVM</p>
    <p>inmany trials. The takehomemessage is that tuning</p>
    <p>using CV based on accuracy (and to a lesser extent</p>
    <p>maxF1) appears to mitigate some risk involved in</p>
    <p>applying SSL. AUROC, on the other hand, does not</p>
    <p>appearaseffective for thispurpose. Table3(bottom)</p>
    <p>shows that, for u = 1000, Best Tuning is worse than SVM fewer times, but for u = 100, MR achieves better performance overall.</p>
    <p>Wealsocompareoverall average testperformance</p>
    <p>(across datasets) for each metric and l,u combination. Table 4 reports these results for accuracy,</p>
    <p>maxF1, and AUROC. In terms of accuracy, we see</p>
    <p>that the Best Tuning approach leads to better per</p>
    <p>formance than SVM, S3VM, or MR in all settings</p>
    <p>when averaged over datasets. We appear to achieve</p>
    <p>some synergy in dynamically choosing a different</p>
    <p>algorithm in each trial. In terms of maxF1, Best</p>
    <p>Tuning, S3VM, and MR are all at least as good as</p>
    <p>SL in three of the four l,u settings, and nearly as good in the fourth. Based on AUROC, though, the</p>
    <p>results are mixed depending on the specific setting.</p>
    <p>Notably, though, Best Tuning consistently leads to</p>
    <p>worse performance than SL when using this metric.</p>
    <p>The experiments were carried out using the Condor</p>
    <p>High-Throughput Computing platform (Thain et al.,</p>
    <p>ferent datasets, l, u, and metrics). Each trial involved training hundreds of models using different</p>
    <p>parameter configurations repeated across five folds,</p>
    <p>and then training once more using the selected pa</p>
    <p>rameters. In the end, we trained a grand total of</p>
    <p>Table 2. Through distributed computing on approxi</p>
    <p>mately 50 machines in parallel, we were able to run</p>
    <p>all these experiments in less than a week, while us</p>
    <p>ing roughly three months worth of CPU time.</p>
    <p>WehaveexploredrealisticSSL,whereall parame</p>
    <p>ters are tuned via 5-fold cross validation, to simulate</p>
    <p>a real-world experience of trying to use unlabeled</p>
    <p>data in a novel NLP task. Our medium-scale empir</p>
    <p>ical study of SVM, S3VM, and MR revealed that no</p>
    <p>algorithm is always superior, and furthermore that</p>
    <p>there are cases in which each SSL algorithm we ex</p>
    <p>aminedcanperformworse thanSVM(insomecases</p>
    <p>significantly worse across 10 trials). To mitigate</p>
    <p>such risks, we proposed a simple meta-level proce</p>
    <p>dure that selects one of the three models based on</p>
    <p>tuning performance. While cross validation is often</p>
    <p>dismissed for model selection in SSL due to a lack</p>
    <p>of labeleddata, thisBestTuningapproachprovesef</p>
    <p>fective in helping to ensure that incorporating unla</p>
    <p>beled data does not hurt performance. Interestingly,</p>
    <p>this works well only when optimizing accuracy dur</p>
    <p>ing tuning. For future work, we plan to extend this</p>
    <p>study to include additional datasets, algorithms, and</p>
    <p>tuning criteria. We also plan to develop more so</p>
    <p>phisticated techniques for choosing which SL/SSL</p>
    <p>algorithm to use in practice.</p>
    <p>Acknowledgments</p>
    <p>A. Goldberg is supported in part by a Yahoo! Key</p>
    <p>Technical Challenges Grant.</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS Average test performance over the 80 runs in each setting:</p>
    <p>u = 100 u = 1000 Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>accuracy 10 0.61 0.62 0.67 0.68 0.61 0.63 0.64 0.67 Test</p>
    <p>Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>maxF1 10 0.59 0.61 0.64 0.59 0.59 0.61 0.61 0.59 Test</p>
    <p>Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>AUROC 10 0.63 0.64 0.72 0.61 0.63 0.64 0.67 0.61 Test</p>
    <p>Table 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.</p>
    <p>outperformsSVMinfewer trials than theotheralgo</p>
    <p>rithms in some settings for these two metrics. This</p>
    <p>is because Best Tuning conservatively selects SVM</p>
    <p>inmany trials. The takehomemessage is that tuning</p>
    <p>using CV based on accuracy (and to a lesser extent</p>
    <p>maxF1) appears to mitigate some risk involved in</p>
    <p>applying SSL. AUROC, on the other hand, does not</p>
    <p>appearaseffective for thispurpose. Table3(bottom)</p>
    <p>shows that, for u = 1000, Best Tuning is worse than SVM fewer times, but for u = 100, MR achieves better performance overall.</p>
    <p>Wealsocompareoverall average testperformance</p>
    <p>(across datasets) for each metric and l,u combination. Table 4 reports these results for accuracy,</p>
    <p>maxF1, and AUROC. In terms of accuracy, we see</p>
    <p>that the Best Tuning approach leads to better per</p>
    <p>formance than SVM, S3VM, or MR in all settings</p>
    <p>when averaged over datasets. We appear to achieve</p>
    <p>some synergy in dynamically choosing a different</p>
    <p>algorithm in each trial. In terms of maxF1, Best</p>
    <p>Tuning, S3VM, and MR are all at least as good as</p>
    <p>SL in three of the four l,u settings, and nearly as good in the fourth. Based on AUROC, though, the</p>
    <p>results are mixed depending on the specific setting.</p>
    <p>Notably, though, Best Tuning consistently leads to</p>
    <p>worse performance than SL when using this metric.</p>
    <p>The experiments were carried out using the Condor</p>
    <p>High-Throughput Computing platform (Thain et al.,</p>
    <p>ferent datasets, l, u, and metrics). Each trial involved training hundreds of models using different</p>
    <p>parameter configurations repeated across five folds,</p>
    <p>and then training once more using the selected pa</p>
    <p>rameters. In the end, we trained a grand total of</p>
    <p>Table 2. Through distributed computing on approxi</p>
    <p>mately 50 machines in parallel, we were able to run</p>
    <p>all these experiments in less than a week, while us</p>
    <p>ing roughly three months worth of CPU time.</p>
    <p>WehaveexploredrealisticSSL,whereall parame</p>
    <p>ters are tuned via 5-fold cross validation, to simulate</p>
    <p>a real-world experience of trying to use unlabeled</p>
    <p>data in a novel NLP task. Our medium-scale empir</p>
    <p>ical study of SVM, S3VM, and MR revealed that no</p>
    <p>algorithm is always superior, and furthermore that</p>
    <p>there are cases in which each SSL algorithm we ex</p>
    <p>aminedcanperformworse thanSVM(insomecases</p>
    <p>significantly worse across 10 trials). To mitigate</p>
    <p>such risks, we proposed a simple meta-level proce</p>
    <p>dure that selects one of the three models based on</p>
    <p>tuning performance. While cross validation is often</p>
    <p>dismissed for model selection in SSL due to a lack</p>
    <p>of labeleddata, thisBestTuningapproachprovesef</p>
    <p>fective in helping to ensure that incorporating unla</p>
    <p>beled data does not hurt performance. Interestingly,</p>
    <p>this works well only when optimizing accuracy dur</p>
    <p>ing tuning. For future work, we plan to extend this</p>
    <p>study to include additional datasets, algorithms, and</p>
    <p>tuning criteria. We also plan to develop more so</p>
    <p>phisticated techniques for choosing which SL/SSL</p>
    <p>algorithm to use in practice.</p>
    <p>Acknowledgments</p>
    <p>A. Goldberg is supported in part by a Yahoo! Key</p>
    <p>Technical Challenges Grant.</p>
    <p>CV with accuracy metric: better than any single model due to per-trial selection strategy</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>AGGREGATE RESULTS Average test performance over the 80 runs in each setting:</p>
    <p>u = 100 u = 1000 Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>accuracy 10 0.61 0.62 0.67 0.68 0.61 0.63 0.64 0.67 Test</p>
    <p>Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>maxF1 10 0.59 0.61 0.64 0.59 0.59 0.61 0.61 0.59 Test</p>
    <p>Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning</p>
    <p>AUROC 10 0.63 0.64 0.72 0.61 0.63 0.64 0.67 0.61 Test</p>
    <p>Table 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.</p>
    <p>outperformsSVMinfewer trials than theotheralgo</p>
    <p>rithms in some settings for these two metrics. This</p>
    <p>is because Best Tuning conservatively selects SVM</p>
    <p>inmany trials. The takehomemessage is that tuning</p>
    <p>using CV based on accuracy (and to a lesser extent</p>
    <p>maxF1) appears to mitigate some risk involved in</p>
    <p>applying SSL. AUROC, on the other hand, does not</p>
    <p>appearaseffective for thispurpose. Table3(bottom)</p>
    <p>shows that, for u = 1000, Best Tuning is worse than SVM fewer times, but for u = 100, MR achieves better performance overall.</p>
    <p>Wealsocompareoverall average testperformance</p>
    <p>(across datasets) for each metric and l,u combination. Table 4 reports these results for accuracy,</p>
    <p>maxF1, and AUROC. In terms of accuracy, we see</p>
    <p>that the Best Tuning approach leads to better per</p>
    <p>formance than SVM, S3VM, or MR in all settings</p>
    <p>when averaged over datasets. We appear to achieve</p>
    <p>some synergy in dynamically choosing a different</p>
    <p>algorithm in each trial. In terms of maxF1, Best</p>
    <p>Tuning, S3VM, and MR are all at least as good as</p>
    <p>SL in three of the four l,u settings, and nearly as good in the fourth. Based on AUROC, though, the</p>
    <p>results are mixed depending on the specific setting.</p>
    <p>Notably, though, Best Tuning consistently leads to</p>
    <p>worse performance than SL when using this metric.</p>
    <p>The experiments were carried out using the Condor</p>
    <p>High-Throughput Computing platform (Thain et al.,</p>
    <p>ferent datasets, l, u, and metrics). Each trial involved training hundreds of models using different</p>
    <p>parameter configurations repeated across five folds,</p>
    <p>and then training once more using the selected pa</p>
    <p>rameters. In the end, we trained a grand total of</p>
    <p>Table 2. Through distributed computing on approxi</p>
    <p>mately 50 machines in parallel, we were able to run</p>
    <p>all these experiments in less than a week, while us</p>
    <p>ing roughly three months worth of CPU time.</p>
    <p>WehaveexploredrealisticSSL,whereall parame</p>
    <p>ters are tuned via 5-fold cross validation, to simulate</p>
    <p>a real-world experience of trying to use unlabeled</p>
    <p>data in a novel NLP task. Our medium-scale empir</p>
    <p>ical study of SVM, S3VM, and MR revealed that no</p>
    <p>algorithm is always superior, and furthermore that</p>
    <p>there are cases in which each SSL algorithm we ex</p>
    <p>aminedcanperformworse thanSVM(insomecases</p>
    <p>significantly worse across 10 trials). To mitigate</p>
    <p>such risks, we proposed a simple meta-level proce</p>
    <p>dure that selects one of the three models based on</p>
    <p>tuning performance. While cross validation is often</p>
    <p>dismissed for model selection in SSL due to a lack</p>
    <p>of labeleddata, thisBestTuningapproachprovesef</p>
    <p>fective in helping to ensure that incorporating unla</p>
    <p>beled data does not hurt performance. Interestingly,</p>
    <p>this works well only when optimizing accuracy dur</p>
    <p>ing tuning. For future work, we plan to extend this</p>
    <p>study to include additional datasets, algorithms, and</p>
    <p>tuning criteria. We also plan to develop more so</p>
    <p>phisticated techniques for choosing which SL/SSL</p>
    <p>algorithm to use in practice.</p>
    <p>Acknowledgments</p>
    <p>A. Goldberg is supported in part by a Yahoo! Key</p>
    <p>Technical Challenges Grant.</p>
    <p>CV with accuracy metric: better than any single model due to per-trial selection strategy</p>
    <p>Mixed results based on maxF1 Poor results based on AUROC</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TAKE-HOME MESSAGE</p>
    <p>Model selection + cross validation + accuracy metric = agnostic SSL with as few as 10 labeled points!</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TAKE-HOME MESSAGE</p>
    <p>Model selection + cross validation + accuracy metric = agnostic SSL with as few as 10 labeled points!</p>
    <p>Future Work:  Expand empirical study to more data sets and algorithms  Extend beyond binary classification tasks  More sophisticated model selection techniques</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>TAKE-HOME MESSAGE</p>
    <p>Model selection + cross validation + accuracy metric = agnostic SSL with as few as 10 labeled points!</p>
    <p>Future Work:  Expand empirical study to more data sets and algorithms  Extend beyond binary classification tasks  More sophisticated model selection techniques</p>
    <p>Thank you! Questions?</p>
  </div>
  <div class="page">
    <p>EXTRA SLIDES</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}</p>
    <p>u j=1, algorithm, performance metric</p>
    <p>Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}. Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}. Combine partitions: Let Df old k = Dlk ! Duk for all k = 1, . . . , 5. foreach parameter configuration in grid do</p>
    <p>foreach fold k do Train model using algorithm on !i!=kDf old i. Evaluate metric on Df old k.</p>
    <p>end</p>
    <p>Compute the average metric value across the 5 folds. end</p>
    <p>Choose parameter configuration that optimizes average metric. Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.</p>
    <p>Output: Optimal model; Average metric value achieved by optimal parameters during tuning.</p>
    <p>Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a</p>
    <p>specific labeled and unlabeled dataset using cross-validation to tune parameters.</p>
    <p>Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U, trials T Randomly divide D into Dpool (of size max(L) + max(U )) and Dtest (the rest). foreach l in L do foreach u in U do foreach trial 1 up to T do</p>
    <p>Randomly select Dlabeled = {xj , yj}lj=l and Dunlabeled = {xk} u k=1 from Dpool.</p>
    <p>Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuning performance value (see Algorithm 1).</p>
    <p>Use model to classify Dunlabeled and record transductive metric value. Use model to classify Dtest and record test metric value.</p>
    <p>end</p>
    <p>end</p>
    <p>end</p>
    <p>Output: Tuning, transductive, and test performance for T runs of algorithm using all l and u combinations.</p>
    <p>Algorithm 2: Experimental procedure used for all comparisons.</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>REALSSL PROCEDURE Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}</p>
    <p>u j=1, algorithm, performance metric</p>
    <p>Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}. Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}. Combine partitions: Let Df old k = Dlk ! Duk for all k = 1, . . . , 5. foreach parameter configuration in grid do</p>
    <p>foreach fold k do Train model using algorithm on !i!=kDf old i. Evaluate metric on Df old k.</p>
    <p>end</p>
    <p>Compute the average metric value across the 5 folds. end</p>
    <p>Choose parameter configuration that optimizes average metric. Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.</p>
    <p>Output: Optimal model; Average metric value achieved by optimal parameters during tuning.</p>
    <p>Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a</p>
    <p>specific labeled and unlabeled dataset using cross-validation to tune parameters.</p>
    <p>Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U, trials T Randomly divide D into Dpool (of size max(L) + max(U )) and Dtest (the rest). foreach l in L do foreach u in U do foreach trial 1 up to T do</p>
    <p>Randomly select Dlabeled = {xj , yj}lj=l and Dunlabeled = {xk} u k=1 from Dpool.</p>
    <p>Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuning performance value (see Algorithm 1).</p>
    <p>Use model to classify Dunlabeled and record transductive metric value. Use model to classify Dtest and record test metric value.</p>
    <p>end</p>
    <p>end</p>
    <p>end</p>
    <p>Output: Tuning, transductive, and test performance for T runs of algorithm using all l and u combinations.</p>
    <p>Algorithm 2: Experimental procedure used for all comparisons.</p>
    <p>Folds maintain labeled/ unlabeled proportion</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj} u j=1, algorithm, performance metric</p>
    <p>Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}. Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}. Combine partitions: Let Df old k = Dlk ! Duk for all k = 1, . . . , 5. foreach parameter configuration in grid do</p>
    <p>foreach fold k do Train model using algorithm on !i!=kDf old i. Evaluate metric on Df old k.</p>
    <p>end</p>
    <p>Compute the average metric value across the 5 folds. end</p>
    <p>Choose parameter configuration that optimizes average metric. Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.</p>
    <p>Output: Optimal model; Average metric value achieved by optimal parameters during tuning.</p>
    <p>Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a</p>
    <p>specific labeled and unlabeled dataset using cross-validation to tune parameters.</p>
    <p>Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U, trials T Randomly divide D into Dpool (of size max(L) + max(U )) and Dtest (the rest). foreach l in L do foreach u in U do foreach trial 1 up to T do</p>
    <p>Randomly select Dlabeled = {xj , yj}lj=l and Dunlabeled = {xk} u k=1 from Dpool.</p>
    <p>Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuning performance value (see Algorithm 1).</p>
    <p>Use model to classify Dunlabeled and record transductive metric value. Use model to classify Dtest and record test metric value.</p>
    <p>end</p>
    <p>end</p>
    <p>end</p>
    <p>Output: Tuning, transductive, and test performance for T runs of algorithm using all l and u combinations.</p>
    <p>Algorithm 2: Experimental procedure used for all comparisons.</p>
  </div>
  <div class="page">
    <p>Andrew B. Goldberg (UW-Madison), SSL with Realistic Tuning</p>
    <p>EMPIRICAL STUDY PROTOCOL</p>
    <p>Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj} u j=1, algorithm, performance metric</p>
    <p>Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}. Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}. Combine partitions: Let Df old k = Dlk ! Duk for all k = 1, . . . , 5. foreach parameter configuration in grid do</p>
    <p>foreach fold k do Train model using algorithm on !i!=kDf old i. Evaluate metric on Df old k.</p>
    <p>end</p>
    <p>Compute the average metric value across the 5 folds. end</p>
    <p>Choose parameter configuration that optimizes average metric. Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.</p>
    <p>Output: Optimal model; Average metric value achieved by optimal parameters during tuning.</p>
    <p>Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a</p>
    <p>specific labeled and unlabeled dataset using cross-validation to tune parameters.</p>
    <p>Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U, trials T Randomly divide D into Dpool (of size max(L) + max(U )) and Dtest (the rest). foreach l in L do foreach u in U do foreach trial 1 up to T do</p>
    <p>Randomly select Dlabeled = {xj , yj}lj=l and Dunlabeled = {xk} u k=1 from Dpool.</p>
    <p>Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuning performance value (see Algorithm 1).</p>
    <p>Use model to classify Dunlabeled and record transductive metric value. Use model to classify Dtest and record test metric value.</p>
    <p>end</p>
    <p>end</p>
    <p>end</p>
    <p>Output: Tuning, transductive, and test performance for T runs of algorithm using all l and u combinations.</p>
    <p>Algorithm 2: Experimental procedure used for all comparisons.</p>
    <p>Repeat each labeled and unlabeled size for 10 trials; Tune parameters and build model using RealSSL</p>
  </div>
</Presentation>
