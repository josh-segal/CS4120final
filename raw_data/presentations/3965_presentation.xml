<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Scalability of the Microsoft Cluster Service</p>
    <p>Werner Vogels, Dan Dumitriu, Ashutosh Agrawal,</p>
    <p>Teck Chia, Katherine Guo</p>
    <p>Reliable Distributed Systems Group</p>
    <p>Dept. of Computer Science Cornell University</p>
  </div>
  <div class="page">
    <p>Agenda  Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Disclaimer</p>
    <p>The tests have taken MSCS far beyond the goals set in its design.</p>
    <p>Any limitations are due to to pushing the technology to extremes, and are not present in the commercial systems.</p>
  </div>
  <div class="page">
    <p>Agenda  Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Research Goals</p>
    <p>General: Reliable Distributed Systems</p>
    <p>Specific Cluster Research:  Efficient Distributed Management</p>
    <p>Low Overhead Scalability</p>
    <p>Cluster Collections</p>
    <p>Cluster Aware Programming Tools (Quintet)</p>
  </div>
  <div class="page">
    <p>Research into Scalable Clusters</p>
    <p>Todays practice  Parallel Computing on 512++ nodes</p>
    <p>High-Availability up to 16 nodes</p>
    <p>Distribution and Fault Management are very scale sensitive.  Failure Management</p>
    <p>Node Membership</p>
    <p>Cluster-Wide Consistency</p>
  </div>
  <div class="page">
    <p>SMP Processors</p>
    <p>Cluster Nodes</p>
    <p>Clusters of</p>
    <p>SMP Systems</p>
    <p>For example 16 Nodes of 16 Proc SMP Systems = 256 CPUs</p>
    <p>The Reality of Scalable Clusters</p>
  </div>
  <div class="page">
    <p>Microsoft.com</p>
  </div>
  <div class="page">
    <p>Mandatory Reading</p>
    <p>In Search of Clusters the ongoing battle in lowly parallel computing</p>
    <p>Gregory Pfister second edition Prentice Hall</p>
  </div>
  <div class="page">
    <p>Agenda  Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Windows NT Clusters What is clustering to Microsoft?</p>
    <p>Group of independent systems that appear as a single system</p>
    <p>Managed as a single system</p>
    <p>Common namespace</p>
    <p>Services are cluster-wide</p>
    <p>Ability to tolerate component failures</p>
    <p>Components can be added transparently to users</p>
    <p>Existing client connectivity is not effected by clustered applications</p>
  </div>
  <div class="page">
    <p>Windows NT Clusters Development goals</p>
    <p>Extend Windows NT to seamlessly include cluster</p>
    <p>features</p>
    <p>Ship high-availability features for Windows NT first</p>
    <p>Support key applications without modification</p>
    <p>Failover support for base Windows NT hardware,</p>
    <p>services, and applications</p>
    <p>Available API for ISV products</p>
    <p>Develop scalability product later</p>
  </div>
  <div class="page">
    <p>MSCS Features</p>
    <p>Shared nothing  Simplified hardware configuration</p>
    <p>Remoteable tools</p>
    <p>Windows NT manageability enhancements  Never take a cluster down: rolling upgrade</p>
    <p>Microsoft BackOffice product support</p>
    <p>3rd Party Support: SAP, Oracle</p>
  </div>
  <div class="page">
    <p>Non-Features Of MSCS</p>
    <p>Not lock-step/fault-tolerant</p>
    <p>Not able to move running applications  MSCS restarts applications that are failed over to other</p>
    <p>cluster members</p>
    <p>Not able to recover shared state between client and server (i.e., file position)  All client/server transactions should</p>
    <p>be atomic</p>
    <p>Standard client/server development rules still apply</p>
  </div>
  <div class="page">
    <p>MSCS Cluster</p>
    <p>Client PCs</p>
    <p>Server A Server B</p>
    <p>Disk cabinet A</p>
    <p>Disk cabinet B</p>
    <p>Heartbeat</p>
    <p>Cluster management</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Agenda</p>
    <p>Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Scaling Distributed Systems 101</p>
    <p>Reduce algorithmic dependency on the number of nodes.</p>
    <p>Traditional Solutions:  Reduce Synchronous Behavior</p>
    <p>Reduce System Complexity</p>
    <p>Radical Solutions:  Epidemic (gossip, probabilistic) techniques</p>
  </div>
  <div class="page">
    <p>Scaling MSCS?</p>
    <p>Why do we care? (Tools, Tools, Tools)</p>
    <p>Do the Distributed Algorithms scale?</p>
    <p>Are there bottlenecks in the implementation?</p>
    <p>Is it a good basis for Cluster Aware Support?</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Cornell Test Cluster</p>
    <p>32 node MSCS Cluster</p>
    <p>Modified MSCS code</p>
    <p>300 MHz PII - 200 P6 (128 Mb memory)</p>
    <p>100 Mbit/sec Switched Ethernet</p>
    <p>Test environment  Unloaded systems</p>
    <p>Loaded system with IO intensive Apps</p>
  </div>
  <div class="page">
    <p>Cluster.Exe Cluster API DLL</p>
    <p>Cluster API stub</p>
    <p>Cluster administrator</p>
    <p>Database Manager Membership</p>
    <p>Manager</p>
    <p>Global Update</p>
    <p>Manager</p>
    <p>Failover Manager</p>
    <p>Event Processor</p>
    <p>Node Manager</p>
    <p>Resource Manager</p>
    <p>Physical resource DLL</p>
    <p>Logical resource DLL</p>
    <p>Application resource DLL</p>
    <p>Resource API</p>
    <p>Reliable Cluster Transport + Heartbeat</p>
    <p>Application resource DLL</p>
    <p>Resource monitors</p>
    <p>Object Manager</p>
    <p>MSCLUS.DLL</p>
    <p>Log Manager</p>
    <p>Checkpoint Manager</p>
    <p>Cluster API DLL</p>
    <p>Cluster API DLL</p>
    <p>Network</p>
    <p>MSCS 1.X Architecture</p>
    <p>Res COM Res API</p>
  </div>
  <div class="page">
    <p>Components under Investigation</p>
    <p>Failure Detection</p>
    <p>Node Membership</p>
    <p>Join operation</p>
    <p>Reconfiguration after failure</p>
    <p>Consistent Distributed State Management</p>
  </div>
  <div class="page">
    <p>Failure Detection</p>
    <p>Heartbeat broadcast</p>
    <p>over all interfaces</p>
    <p>period 1.2 second</p>
    <p>Interface suspicion after 3 misses</p>
    <p>Node Suspicion after 6 misses (7.2 seconds)</p>
  </div>
  <div class="page">
    <p>Membership Join</p>
    <p>6 phase operation  discovery</p>
    <p>lock</p>
    <p>enable network</p>
    <p>petition</p>
    <p>database sync</p>
    <p>unlock</p>
  </div>
  <div class="page">
    <p>Membership Regroup</p>
    <p>5 Phase fully distributed  Activate</p>
    <p>Closing</p>
    <p>Pruning</p>
    <p>Cleanup phase one</p>
    <p>Cleanup phase two</p>
  </div>
  <div class="page">
    <p>Global Update I</p>
    <p>Atomic / Total Order  Organize nodes in a</p>
    <p>ring</p>
    <p>Acquire lock</p>
    <p>Transmit to each node in order</p>
    <p>Release lock</p>
    <p>Handles a number of failure scenarios</p>
  </div>
  <div class="page">
    <p>Global Update</p>
    <p>Developed for sparse updates of OS structures</p>
    <p>Implemented in MSCS using repeated RPC</p>
    <p>Collapses under load</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Conclusions  Can the current Algorithms scale?</p>
    <p>FD &amp; Regroup: Yes</p>
    <p>GUP: 10-16 nodes</p>
    <p>Are there bottlenecks in the implementation?  FD &amp; Regroup: Repeated p2p in</p>
    <p>Join &amp; GUP: RPC Trains</p>
    <p>Is it a good basis for cluster aware support  NO</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Research Goals</p>
    <p>Intro into MS Cluster Service</p>
    <p>Practical Scalability</p>
    <p>Evaluation of MSCS components</p>
    <p>Conclusions</p>
    <p>Whats Cookin?</p>
  </div>
  <div class="page">
    <p>Rat Pack Clusters</p>
  </div>
  <div class="page">
    <p>A Quick Glance in the Kitchen</p>
    <p>Tested on 200++ nodes</p>
    <p>Mixed Nuts: NT &amp; Unix</p>
    <p>Provides Cluster Events</p>
    <p>Epidemic FD &amp; Membership</p>
    <p>Probabilistic Communication Tools</p>
    <p>Sub-Clusters for Limited Scalability operations</p>
    <p>Rat Pack Clusters</p>
  </div>
  <div class="page">
    <p>Be Courageous, Do A Demo</p>
  </div>
  <div class="page">
    <p>Any Questions?</p>
  </div>
</Presentation>
