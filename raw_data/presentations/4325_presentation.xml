<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Taylors law for Human Linguistic Sequences</p>
    <p>Tatsuru Kobayashi Kumiko Tanaka-Ishii</p>
    <p>Research Center for Advanced Science Technology The University of Tokyo</p>
  </div>
  <div class="page">
    <p>For Moby Dick</p>
    <p>Power laws of natural language</p>
    <p>These can be analyzed through power laws</p>
    <p>Todays talk is about quantifying the degree of fluctuation. How these could be useful will be presented at the end.</p>
  </div>
  <div class="page">
    <p>Any words (any word, any set of words) occur in clusters Occurrences of rare words in Moby Dick (below 3162th)</p>
    <p>Two ways of analysis  Fluctuation analysis  Long range correlation  weaknesses</p>
    <p>Fluctuation underlying text</p>
  </div>
  <div class="page">
    <p>Any words (any word, any set of words) occur in clusters Occurrences of rare words in Moby Dick (below 3162th)</p>
    <p>Two ways of analysis  Fluctuation analysis  Long range correlation</p>
    <p>Fluctuation Analysis (Ebeling 1994) variance w.r.t.</p>
    <p>Taylors analysis variance w.r.t. mean</p>
    <p>Our achievements</p>
    <p>Fluctuation underlying text  Look at variance in</p>
    <p>Variance is larger when events are clustered vs. random</p>
  </div>
  <div class="page">
    <p>Power law between standard deviation and mean of event occurrences within (space or) time</p>
    <p>' Empirically 0.5    1.0 (but  &lt; 0.5 is of course possible, too)</p>
    <p>Empirically known to hold in vast fields (Eisler, 2007) ecology, life science, physics, finance, human dynamics</p>
    <p>The only application to language is Gerlach &amp; Altmann (2014)  not really Taylor analysis</p>
    <p>We devised a new method based on the original concept of Taylors law</p>
    <p>Taylors law (Smith, 1938; Taylor, 1961)</p>
  </div>
  <div class="page">
    <p>0</p>
    <p>Word sequence (text)</p>
    <p>0</p>
    <p>1 1</p>
    <p>,5 = argmin=,' , ,</p>
    <p>, = 1</p>
    <p>@ log C  log C ' 1</p>
    <p>E</p>
    <p>CF0</p>
    <p>.</p>
    <p>Our method</p>
  </div>
  <div class="page">
    <p>- Here,   5000. - Every point is a word kind - Estimated Taylor exponent  = 0.57.</p>
    <p>- Taylor exponent  corresponds to gradient of log-log plot.</p>
    <p>Frequent</p>
    <p>Fl uc tu at ed</p>
    <p>Taylors law of natural language</p>
    <p>Moby Dick English, 250k words, vocabulary size 20k words</p>
    <p>Taylors law in log scale</p>
  </div>
  <div class="page">
    <p>Frequent</p>
    <p>Fl uc tu at ed</p>
    <p>Taylors law of natural language</p>
    <p>Keywords</p>
    <p>Functional words</p>
    <p>Moby Dick (English)s Taylors law in log scale</p>
  </div>
  <div class="page">
    <p>Theoretical analysis of the exponent Empirically 0.5    1.0</p>
    <p>= 0.5 if all words are independent and identically distributed (i.i.d.).</p>
    <p>Taylor Exponent  = 0.5 because shuffled text is equivalent to i.i.d. process.</p>
    <p>Shuffled Moby Dick   5000.</p>
  </div>
  <div class="page">
    <p>Theoretical analysis of the exponent  = 1.0</p>
    <p>if words always co-occur with the same proportion. ex) Suppose that  = {0,1}, and 1 occurs always twice as 0</p>
    <p>1 = 20,1 = 20</p>
    <p>0:3,1:6</p>
    <p>0 1</p>
    <p>gradient  = 1</p>
    <p>log 2</p>
    <p>log 2</p>
    <p>0:17,1:34</p>
  </div>
  <div class="page">
    <p>Child directed speech Thomas, English, CHILDES 450k words (8.2k diff. words)</p>
    <p>Programming source code Lisp, crawled and parsed 3.7m words (160k diff. words)</p>
    <p>up things hand</p>
    <p>thisdear truck</p>
    <p>xload</p>
    <p>insert platform</p>
    <p>letunless</p>
    <p>and</p>
    <p>Taylors law for other data</p>
  </div>
  <div class="page">
    <p>Kind Languages Number of texts</p>
    <p>Average size Example</p>
    <p>Gutenberg &amp; Aozora (Long, single author)</p>
    <p>Newspapers 3 (En,Zh,Ja) 4 580,488,956 WSJ</p>
    <p>Tagged Wiki 1 (En+tag) 1 14,637,848 enwiki8</p>
    <p>CHILDES 10(En, Fr, ) 10 193,434 Thomas (English) Music - 12 135,993 Matthus (Bach)</p>
    <p>Program Codes 4 4 34,161,018 C++, Lisp, Haskell, Python</p>
    <p>Datasets</p>
  </div>
  <div class="page">
    <p>Taylor exponents of various data kind</p>
    <p>Single author texts</p>
    <p>Written Texts mean  = 0.58</p>
    <p>Random Texts</p>
    <p>= 0.50 Other data   0.63</p>
    <p>None of the real texts showed the exponent 0.5</p>
  </div>
  <div class="page">
    <p>Summary thus far  Taylors law holds in vast fields including natural/social science  Taylors law also holds in languages and other linguistic related sequential data  Taylor exponent shows the degree of co-occurrence among words  Taylor exponent  differs among text categories</p>
    <p>(No such quality for Zipfs law, Heaps law) How can our results be useful?  Do machine generated texts produce  &gt; 0.5?</p>
  </div>
  <div class="page">
    <p>bigrams of Moby Dick</p>
    <p>Machine generated text by n-grams</p>
  </div>
  <div class="page">
    <p>Machine generated texts by character-based LSTM language model</p>
    <p>Learning: Shakespeare by naive setting Generation: Probabilistic generation of succeeding characters (2 million characters)</p>
    <p>LSTM 256 nodes</p>
    <p>Stacked LSTM (3 LSTM layers) Distribution of following character</p>
    <p>State-of the art models present different results (in another paper) 16</p>
  </div>
  <div class="page">
    <p>Les Miserables translated by Google translator (in English)</p>
    <p>Texts generated by machine translation</p>
    <p>Les Miserables (original, French)</p>
    <p>Fluctuation that derives from the context is provided by the source text 17</p>
  </div>
  <div class="page">
    <p>Conclusion  Taylors law holds in vast fields including natural/social science  Taylors law also holds in languages and other linguistic related sequential data  Taylor exponent shows the degree of co-occurrence among words  Taylor exponent  differs among text categories</p>
    <p>(No such quality for Zipfs law, Heaps law)</p>
    <p>The nature of  &gt; 0.5 : context and long memory  one limitation of CL  Taylor analysis would possibly evaluate machine outputs  Knowing mathematical characteristic of texts serve for language engineering</p>
    <p>How can our results be useful?  Do machine generated texts produce  &gt; 0.5?</p>
  </div>
  <div class="page">
    <p>Thank you</p>
  </div>
</Presentation>
