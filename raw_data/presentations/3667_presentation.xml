<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Latent Structure Models for NLP</p>
    <p>Andr Mar ns Ins tuto de Telecomunicaes &amp; IST &amp; Unbabel Tsvetomila Mihaylova Ins tuto de Telecomunicaes</p>
    <p>Nikita Nangia NYU Vlad Niculae Ins tuto de Telecomunicaes</p>
    <p>deep-spin.github.io/tutorial</p>
  </div>
  <div class="page">
    <p>I. Introduc on</p>
  </div>
  <div class="page">
    <p>Structured predic on and NLP</p>
    <p>Structured predic on: a machine learning framework for predic ng structured, constrained, and interdependent outputs  NLP deals with structured and ambiguous textual data:  machine transla on  speech recogni on  syntac c parsing  seman c parsing  informa on extrac on  ...</p>
    <p>deep-spin.github.io/tutorial 2</p>
  </div>
  <div class="page">
    <p>Examples of structure in NLP</p>
    <p>POS tagging VERB PREP NOUN dog on wheels</p>
    <p>NOUN PREP NOUN dog on wheels</p>
    <p>NOUN DET NOUN dog on wheels</p>
    <p>Dependency parsing</p>
    <p>dog on wheels</p>
    <p>dog on wheels</p>
    <p>dog on wheels</p>
    <p>Word alignments</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>Exponen ally many parse trees! Cannot enumerate.</p>
    <p>deep-spin.github.io/tutorial 3</p>
  </div>
  <div class="page">
    <p>Examples of structure in NLP</p>
    <p>POS tagging VERB PREP NOUN dog on wheels</p>
    <p>NOUN PREP NOUN dog on wheels</p>
    <p>NOUN DET NOUN dog on wheels</p>
    <p>Dependency parsing</p>
    <p>dog on wheels</p>
    <p>dog on wheels</p>
    <p>dog on wheels</p>
    <p>Word alignments</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>Exponen ally many parse trees! Cannot enumerate.</p>
    <p>deep-spin.github.io/tutorial 3</p>
  </div>
  <div class="page">
    <p>Examples of structure in NLP POS tagging</p>
    <p>VERB PREP NOUN dog on wheels</p>
    <p>NOUN PREP NOUN dog on wheels</p>
    <p>NOUN DET NOUN dog on wheels</p>
    <p>Dependency parsing</p>
    <p>dog on wheels</p>
    <p>dog on wheels</p>
    <p>dog on wheels</p>
    <p>Word alignments</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>dog on</p>
    <p>wheels</p>
    <p>hond op wielen</p>
    <p>Exponen ally many parse trees! Cannot enumerate.</p>
    <p>deep-spin.github.io/tutorial 3</p>
  </div>
  <div class="page">
    <p>NLP 5 years ago: Structured predic on and pipelines</p>
  </div>
  <div class="page">
    <p>NLP 5 years ago: Structured predic on and pipelines</p>
    <p>Big pipeline systems, connec ng di erent structured predictors, trained separately  Advantages: fast and simple to train, can rearrange pieces</p>
    <p>Disadvantage: linguis c annota ons required for each component  Bigger disadvantage: error propagates through the pipeline</p>
    <p>deep-spin.github.io/tutorial 5</p>
  </div>
  <div class="page">
    <p>NLP 5 years ago: Structured predic on and pipelines</p>
    <p>Big pipeline systems, connec ng di erent structured predictors, trained separately  Advantages: fast and simple to train, can rearrange pieces  Disadvantage: linguis c annota ons required for each component</p>
    <p>Bigger disadvantage: error propagates through the pipeline</p>
    <p>deep-spin.github.io/tutorial 5</p>
  </div>
  <div class="page">
    <p>NLP 5 years ago: Structured predic on and pipelines</p>
    <p>Big pipeline systems, connec ng di erent structured predictors, trained separately  Advantages: fast and simple to train, can rearrange pieces  Disadvantage: linguis c annota ons required for each component  Bigger disadvantage: error propagates through the pipeline</p>
    <p>deep-spin.github.io/tutorial 5</p>
  </div>
  <div class="page">
    <p>NLP today: End-to-end training</p>
  </div>
  <div class="page">
    <p>NLP today: End-to-end training</p>
    <p>Forget pipelinestrain everything from scratch!  No more error propaga on or linguis c annota ons!</p>
    <p>Treat everything as latent!</p>
    <p>deep-spin.github.io/tutorial 7</p>
  </div>
  <div class="page">
    <p>NLP today: End-to-end training</p>
    <p>Forget pipelinestrain everything from scratch!  No more error propaga on or linguis c annota ons!  Treat everything as latent!</p>
    <p>deep-spin.github.io/tutorial 7</p>
  </div>
  <div class="page">
    <p>Representa on learning</p>
    <p>Uncover hidden representa ons useful for the downstream task.  Neural networks are well-suited for this: deep computa on graphs.</p>
    <p>Neural representa ons are unstructured, inscrutable. Language data has underlying structure!</p>
    <p>posi ve neutral nega ve</p>
    <p>input</p>
    <p>deep-spin.github.io/tutorial 8</p>
  </div>
  <div class="page">
    <p>Representa on learning</p>
    <p>Uncover hidden representa ons useful for the downstream task.  Neural networks are well-suited for this: deep computa on graphs.  Neural representa ons are unstructured, inscrutable. Language data has underlying structure!</p>
    <p>posi ve neutral nega ve</p>
    <p>input</p>
    <p>deep-spin.github.io/tutorial 8</p>
  </div>
  <div class="page">
    <p>Latent structure models</p>
    <p>Seek structured hidden representa ons instead! posi ve</p>
    <p>neutral nega ve</p>
    <p>input</p>
    <p>deep-spin.github.io/tutorial 9</p>
  </div>
  <div class="page">
    <p>Latent structure models</p>
    <p>Seek structured hidden representa ons instead! posi ve</p>
    <p>neutral nega ve</p>
    <p>input</p>
    <p>deep-spin.github.io/tutorial 9</p>
  </div>
  <div class="page">
    <p>Latent structure models arent so new!</p>
    <p>They have a very long history in NLP:  IBM Models for SMT (latent word alignments) [Brown et al., 1993]  HMMs [Rabiner, 1989]  CRFs with hidden variables [Qua oni et al., 2007]  Latent PCFGs [Petrov and Klein, 2008, Cohen et al., 2012]  Trained with EM, spectral learning, method of moments, ...  O en, very strict assump ons (e.g. strong factoriza ons)  Today, neural networks opened up some new possibili es!</p>
    <p>deep-spin.github.io/tutorial 10</p>
  </div>
  <div class="page">
    <p>Why do we love latent structure models?</p>
    <p>The inferred latent variables can bring us some interpretability  They o er a way of injec ng prior knowledge as a structured bias  Hopefully: Higher predic ve power with fewer model parameters</p>
    <p>smaller carbon footprint!</p>
    <p>deep-spin.github.io/tutorial 11</p>
  </div>
  <div class="page">
    <p>Why do we love latent structure models?</p>
    <p>The inferred latent variables can bring us some interpretability  They o er a way of injec ng prior knowledge as a structured bias  Hopefully: Higher predic ve power with fewer model parameters  smaller carbon footprint!</p>
    <p>deep-spin.github.io/tutorial 11</p>
  </div>
  <div class="page">
    <p>What this tutorial is about:</p>
    <p>Discrete, combinatorial latent structures  O en the structure is inspired by some linguis c intui on  Well cover both:  RL methods (structure built incrementally, reward coming from downstream task)  ... vs end-to-end di eren able approaches (global op miza on, marginaliza on)  stochas c computa on graphs  ... vs determinis c graphs.  All plugged in discrimina ve neural models.</p>
    <p>deep-spin.github.io/tutorial 12</p>
  </div>
  <div class="page">
    <p>This tutorial is not about:</p>
    <p>Its not about con nuous latent variables  Its not about deep genera ve learning  We wont cover GANs, VAEs, etc.  There are (very good) recent tutorials on deep varia onal models for NLP:  Varia onal Inference and Deep Genera ve Models (Schulz and Aziz, ACL 2018)  Deep Latent-Variable Models for Natural Language (Kim, Wiseman, Rush, EMNLP 2018)</p>
    <p>deep-spin.github.io/tutorial 13</p>
  </div>
  <div class="page">
    <p>Background</p>
  </div>
  <div class="page">
    <p>Unstructured vs structured</p>
    <p>To be er explain the math, well o en backtrack to unstructured models (where the latent variable is a categorical) before jumping to the structured ones</p>
    <p>deep-spin.github.io/tutorial 14</p>
  </div>
  <div class="page">
    <p>The unstructured case: Probability simplex</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>Each vertex is an indicator vector, represen ng one class:</p>
    <p>zc = [0, . . . ,0, 1 cth posi on</p>
    <p>,0, . . . ,0].</p>
    <p>Points inside are probability vectors, a convex combina on of classes:</p>
    <p>p  0,  c pc =1.</p>
    <p>deep-spin.github.io/tutorial 15</p>
  </div>
  <div class="page">
    <p>The unstructured case: Probability simplex</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>Each vertex is an indicator vector, represen ng one class:</p>
    <p>zc = [0, . . . ,0, 1 cth posi on</p>
    <p>,0, . . . ,0].</p>
    <p>Points inside are probability vectors, a convex combina on of classes:</p>
    <p>p  0,  c pc =1.</p>
    <p>deep-spin.github.io/tutorial 15</p>
  </div>
  <div class="page">
    <p>The unstructured case: Probability simplex</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>Each vertex is an indicator vector, represen ng one class:</p>
    <p>zc = [0, . . . ,0, 1 cth posi on</p>
    <p>,0, . . . ,0].</p>
    <p>Points inside are probability vectors, a convex combina on of classes:</p>
    <p>p  0,  c pc =1.</p>
    <p>deep-spin.github.io/tutorial 15</p>
  </div>
  <div class="page">
    <p>Whats the analogous of  for a structure?  A structured object z can be represented as a bit vector.</p>
    <p>Example:</p>
    <p>a dependency tree can be represented a O(L2) vector indexed by arcs  each entry is 1 i the arc belongs to the tree  structural constraints: not all bit vectors represent valid trees!</p>
    <p>z1 = [1,0,0,0,1,0,0,0,1]</p>
    <p>dog on wheels</p>
    <p>z2 = [0,0,1,0,0,1,1,0,0]</p>
    <p>dog on wheels</p>
    <p>z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>dog on wheels</p>
    <p>deep-spin.github.io/tutorial 16</p>
  </div>
  <div class="page">
    <p>Whats the analogous of  for a structure?  A structured object z can be represented as a bit vector.  Example:  a dependency tree can be represented a O(L2) vector indexed by arcs  each entry is 1 i the arc belongs to the tree  structural constraints: not all bit vectors represent valid trees!</p>
    <p>z1 = [1,0,0,0,1,0,0,0,1]</p>
    <p>dog on wheels</p>
    <p>z2 = [0,0,1,0,0,1,1,0,0]</p>
    <p>dog on wheels</p>
    <p>z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>dog on wheels</p>
    <p>deep-spin.github.io/tutorial 16</p>
  </div>
  <div class="page">
    <p>Whats the analogous of  for a structure?  A structured object z can be represented as a bit vector.  Example:  a dependency tree can be represented a O(L2) vector indexed by arcs  each entry is 1 i the arc belongs to the tree  structural constraints: not all bit vectors represent valid trees!</p>
    <p>z1 = [1,0,0,0,1,0,0,0,1]</p>
    <p>dog on wheels</p>
    <p>z2 = [0,0,1,0,0,1,1,0,0]</p>
    <p>dog on wheels</p>
    <p>z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>dog on wheels</p>
    <p>deep-spin.github.io/tutorial 16</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z  Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 17</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z</p>
    <p>Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 17</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z  Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 17</p>
  </div>
  <div class="page">
    <p>Unstructured vs Structured</p>
    <p>Unstructured case: simplex</p>
    <p>Structured case: marginal polytopeM</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 18</p>
  </div>
  <div class="page">
    <p>Unstructured vs Structured</p>
    <p>Unstructured case: simplex</p>
    <p>Structured case: marginal polytopeM</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 18</p>
  </div>
  <div class="page">
    <p>Unstructured vs Structured</p>
    <p>Unstructured case: simplex</p>
    <p>Structured case: marginal polytopeM</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 18</p>
  </div>
  <div class="page">
    <p>Compu ng the most likely structure is a very high-dimensional argmax</p>
    <p>s zinput x</p>
    <p>outputby</p>
    <p>There are exponen ally many structures (s cannot t in memory;</p>
    <p>we cannot loop over s nor z)</p>
    <p>deep-spin.github.io/tutorial 19</p>
  </div>
  <div class="page">
    <p>Compu ng the most likely structure is a very high-dimensional argmax</p>
    <p>s zinput x</p>
    <p>outputby There are exponen ally</p>
    <p>many structures (s cannot t in memory;</p>
    <p>we cannot loop over s nor z)</p>
    <p>deep-spin.github.io/tutorial 19</p>
  </div>
  <div class="page">
    <p>Dealing with the combinatorial explosion</p>
    <p>1. Incremental structures  Build structure greedily, as sequence of discrete choices (e.g., shi -reduce).  Scores (par al structure, ac on) tuples.  Advantages: exible, rich histories.  Disadvantages: greedy, local decisions are subop mal, error propaga on.</p>
    <p>max     , , , ,</p>
    <p>deep-spin.github.io/tutorial 20</p>
  </div>
  <div class="page">
    <p>The challenge of discrete choices.</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zinput x</p>
    <p>outputby s= f(x) by=g(z,x)</p>
    <p>L(by,y) w =? or, essen ally,</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 21</p>
  </div>
  <div class="page">
    <p>The challenge of discrete choices.</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s</p>
    <p>zinput x</p>
    <p>outputby s= f(x) by=g(z,x)</p>
    <p>L(by,y) w =? or, essen ally,</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 21</p>
  </div>
  <div class="page">
    <p>The challenge of discrete choices.</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s z</p>
    <p>input x</p>
    <p>outputby s= f(x) by=g(z,x)</p>
    <p>L(by,y) w =? or, essen ally,</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 21</p>
  </div>
  <div class="page">
    <p>The challenge of discrete choices.</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zinput x</p>
    <p>outputby s= f(x) by=g(z,x)</p>
    <p>L(by,y) w =? or, essen ally,</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 21</p>
  </div>
  <div class="page">
    <p>The challenge of discrete choices.</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zinput x</p>
    <p>outputby s= f(x) by=g(z,x)</p>
    <p>L(by,y) w =?</p>
    <p>or, essen ally, zs=?</p>
    <p>deep-spin.github.io/tutorial 21</p>
  </div>
  <div class="page">
    <p>The challenge of discrete choices.</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zinput x</p>
    <p>outputby s= f(x) by=g(z,x)</p>
    <p>L(by,y) w =? or, essen ally,</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 21</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Discrete mappings are  at s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s zz</p>
    <p>z s=?</p>
    <p>deep-spin.github.io/tutorial 22</p>
  </div>
  <div class="page">
    <p>Argmax</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s z</p>
    <p>z s =0</p>
    <p>z1</p>
    <p>s10</p>
    <p>s2 1 s2 s2 +1</p>
    <p>deep-spin.github.io/tutorial 23</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s</p>
    <p>zp</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2  log exp(sc)Z</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2  log exp(sc)Z</p>
    <p>predict topic c (z=ec)</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>predict topic c (z=ec)</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>L Ws</p>
    <p>= Ly y v</p>
    <p>v z</p>
    <p>0 z s</p>
    <p>s Ws</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Op on 1. Pretrain latent classi er Ws</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2  log exp(sc)Z</p>
    <p>Op on 2. Mul -task learning</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L=Ez(yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Op on 3. Stochas city! Ez(y(z)y) 2</p>
    <p>Ws =/ 0</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,z]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Op on 4. Gradient surrogates (e.g. straight-through, zs  I)</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Example: Regression with latent categoriza on</p>
    <p>(c)</p>
    <p>s</p>
    <p>z</p>
    <p>p</p>
    <p>input x</p>
    <p>u</p>
    <p>embeddings E Ws</p>
    <p>u= 1|x|</p>
    <p>j Exj s=Wsu</p>
    <p>output y</p>
    <p>v</p>
    <p>Wv Wy</p>
    <p>v= tanh (Wv[u,p]) y=Wyv L= (yy)2</p>
    <p>log exp(sc)Z</p>
    <p>Op on 5. Con nuous relaxa on (e.g. so max)</p>
    <p>Workarounds: circumven ng the issue, bypassing discrete variables</p>
    <p>Tackling discreteness end-to-end</p>
    <p>deep-spin.github.io/tutorial 24</p>
  </div>
  <div class="page">
    <p>Dealing with discrete latent variables</p>
    <p>(Part 2)</p>
    <p>(Part 3)</p>
    <p>(Part 4)</p>
    <p>deep-spin.github.io/tutorial 25</p>
  </div>
  <div class="page">
    <p>Dealing with discrete latent variables</p>
    <p>deep-spin.github.io/tutorial 25</p>
  </div>
  <div class="page">
    <p>Roadmap of the tutorial</p>
    <p>Part 1: Introduc on  Part 2: Reinforcement learning  Part 3: Gradient surrogates</p>
    <p>Co ee Break</p>
    <p>Part 4: End-to-end di eren able models  Part 5: Conclusions</p>
    <p>deep-spin.github.io/tutorial 26</p>
  </div>
  <div class="page">
    <p>II. Reinforcement Learning Methods</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x) and incur a loss,</p>
    <p>L(y(z;x),y)</p>
    <p>or simply L(z)</p>
    <p>But we dont know z!  In this sec on:</p>
    <p>we jointly learn a structured predic on model (z | x)</p>
    <p>by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x)</p>
    <p>and incur a loss, L(y(z;x),y)</p>
    <p>or simply L(z)</p>
    <p>But we dont know z!  In this sec on:</p>
    <p>we jointly learn a structured predic on model (z | x)</p>
    <p>by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x) and incur a loss,</p>
    <p>L(y(z;x),y)</p>
    <p>or simply L(z)</p>
    <p>But we dont know z!  In this sec on:</p>
    <p>we jointly learn a structured predic on model (z | x)</p>
    <p>by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x) and incur a loss,</p>
    <p>L(y(z;x),y) or simply L(z)</p>
    <p>But we dont know z!  In this sec on:</p>
    <p>we jointly learn a structured predic on model (z | x)</p>
    <p>by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x) and incur a loss,</p>
    <p>L(y(z;x),y) or simply L(z)</p>
    <p>But we dont know z!</p>
    <p>In this sec on: we jointly learn a structured predic on model (z | x)</p>
    <p>by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x) and incur a loss,</p>
    <p>L(y(z;x),y) or simply L(z)</p>
    <p>But we dont know z!  In this sec on:</p>
    <p>we jointly learn a structured predic on model (z | x)</p>
    <p>by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>Latent structure via marginaliza on  Given a sentence-label pair (x,y) and its known parse tree z,</p>
    <p>we can make a predic on y(z;x) and incur a loss,</p>
    <p>L(y(z;x),y) or simply L(z)</p>
    <p>But we dont know z!  In this sec on:</p>
    <p>we jointly learn a structured predic on model (z | x) by op mizing the expected loss,</p>
    <p>E(z|x)  L(z)</p>
    <p>deep-spin.github.io/tutorial 27</p>
  </div>
  <div class="page">
    <p>But rst, supervised SPINN</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 28</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>Joint learning: Combines a cons tuency parser and a sentence representa on model.</p>
    <p>The parser, f(x) is a transi on-based shi -reduce parser. It looks at top two elements of stack and top element of the bu er.  TreeLSTM combines top two elements of the stack when the parser choses the</p>
    <p>reduce ac on.</p>
    <p>deep-spin.github.io/tutorial 29</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>Joint learning: Combines a cons tuency parser and a sentence representa on model.  The parser, f(x) is a transi on-based shi -reduce parser. It looks at top two elements of stack and top element of the bu er.</p>
    <p>TreeLSTM combines top two elements of the stack when the parser choses the reduce ac on.</p>
    <p>deep-spin.github.io/tutorial 29</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>Joint learning: Combines a cons tuency parser and a sentence representa on model.  The parser, f(x) is a transi on-based shi -reduce parser. It looks at top two elements of stack and top element of the bu er.  TreeLSTM combines top two elements of the stack when the parser choses the</p>
    <p>reduce ac on.</p>
    <p>deep-spin.github.io/tutorial 29</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 30</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 30</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 30</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 30</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 30</p>
  </div>
  <div class="page">
    <p>Stack-augmented Parser-Interpreter Neural-Network</p>
    <p>[Bowman et al., 2016]</p>
    <p>deep-spin.github.io/tutorial 30</p>
  </div>
  <div class="page">
    <p>Shi -Reduce parsing</p>
    <p>We can write a shi -reduce style parse as a sequence of Bernoulli random variables,</p>
    <p>z={z1, . . . ,z2L1}</p>
    <p>where, zj  {0,1} j  [1,2L1]</p>
    <p>deep-spin.github.io/tutorial 31</p>
  </div>
  <div class="page">
    <p>Shi -Reduce parsing</p>
    <p>A sequence of Bernoulli trials but with condi onal dependence,</p>
    <p>p(z1,z2, . . . ,z2L1)= 2L1 j=1</p>
    <p>p(zj | z&lt;j)</p>
    <p>deep-spin.github.io/tutorial 32</p>
  </div>
  <div class="page">
    <p>Latent structure learning with SPINN</p>
    <p>But now, remove syntac c supervision from SPINN.</p>
    <p>We model the parse, z, as a latent variable with our parser as the score func on es mator, f(x).  With shi -reduce parsing, were making discrete decisionsREINFORCE as a natural solu on.</p>
    <p>deep-spin.github.io/tutorial 33</p>
  </div>
  <div class="page">
    <p>Latent structure learning with SPINN  But now, remove syntac c supervision from SPINN.</p>
    <p>We model the parse, z, as a latent variable with our parser as the score func on es mator, f(x).  With shi -reduce parsing, were making discrete decisionsREINFORCE as a natural solu on.</p>
    <p>deep-spin.github.io/tutorial 33</p>
  </div>
  <div class="page">
    <p>Latent structure learning with SPINN  But now, remove syntac c supervision from SPINN.</p>
    <p>We model the parse, z, as a latent variable with our parser as the score func on es mator, f(x).</p>
    <p>With shi -reduce parsing, were making discrete decisionsREINFORCE as a natural solu on.</p>
    <p>deep-spin.github.io/tutorial 33</p>
  </div>
  <div class="page">
    <p>Latent structure learning with SPINN  But now, remove syntac c supervision from SPINN.</p>
    <p>We model the parse, z, as a latent variable with our parser as the score func on es mator, f(x).  With shi -reduce parsing, were making discrete decisionsREINFORCE as a natural solu on.</p>
    <p>deep-spin.github.io/tutorial 33</p>
  </div>
  <div class="page">
    <p>Unsupervised SPINN</p>
  </div>
  <div class="page">
    <p>Unsupervised SPINN</p>
    <p>No syntac c supervision. Only reward is from the downstream task. We only get this reward a er parsing the full sentence.</p>
    <p>deep-spin.github.io/tutorial 34</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE [Williams, 1992]</p>
    <p>Some basic terminology,  The ac on space is zj  {shift,reduce}, and z is a sequence of ac ons.</p>
    <p>Training parser network parameters,  with REINFORCE  The state, h, is the top two elements of the stack and the top element of the bu er.  Learning a policy network (z | h;)  Maximize the reward, whereR is performance on the downstream task like sentence classi ca on.</p>
    <p>NOTE: Only a single reward at the end of parsing.</p>
    <p>deep-spin.github.io/tutorial 35</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE [Williams, 1992]</p>
    <p>Some basic terminology,  The ac on space is zj  {shift,reduce}, and z is a sequence of ac ons.  Training parser network parameters,  with REINFORCE</p>
    <p>The state, h, is the top two elements of the stack and the top element of the bu er.  Learning a policy network (z | h;)  Maximize the reward, whereR is performance on the downstream task like sentence classi ca on.</p>
    <p>NOTE: Only a single reward at the end of parsing.</p>
    <p>deep-spin.github.io/tutorial 35</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE [Williams, 1992]</p>
    <p>Some basic terminology,  The ac on space is zj  {shift,reduce}, and z is a sequence of ac ons.  Training parser network parameters,  with REINFORCE  The state, h, is the top two elements of the stack and the top element of the bu er.</p>
    <p>Learning a policy network (z | h;)  Maximize the reward, whereR is performance on the downstream task like sentence classi ca on.</p>
    <p>NOTE: Only a single reward at the end of parsing.</p>
    <p>deep-spin.github.io/tutorial 35</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE [Williams, 1992]</p>
    <p>Some basic terminology,  The ac on space is zj  {shift,reduce}, and z is a sequence of ac ons.  Training parser network parameters,  with REINFORCE  The state, h, is the top two elements of the stack and the top element of the bu er.  Learning a policy network (z | h;)</p>
    <p>Maximize the reward, whereR is performance on the downstream task like sentence classi ca on.</p>
    <p>NOTE: Only a single reward at the end of parsing.</p>
    <p>deep-spin.github.io/tutorial 35</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE [Williams, 1992]</p>
    <p>Some basic terminology,  The ac on space is zj  {shift,reduce}, and z is a sequence of ac ons.  Training parser network parameters,  with REINFORCE  The state, h, is the top two elements of the stack and the top element of the bu er.  Learning a policy network (z | h;)  Maximize the reward, whereR is performance on the downstream task like sentence classi ca on.</p>
    <p>NOTE: Only a single reward at the end of parsing.</p>
    <p>deep-spin.github.io/tutorial 35</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE [Williams, 1992]</p>
    <p>Some basic terminology,  The ac on space is zj  {shift,reduce}, and z is a sequence of ac ons.  Training parser network parameters,  with REINFORCE  The state, h, is the top two elements of the stack and the top element of the bu er.  Learning a policy network (z | h;)  Maximize the reward, whereR is performance on the downstream task like sentence classi ca on.</p>
    <p>NOTE: Only a single reward at the end of parsing.</p>
    <p>deep-spin.github.io/tutorial 35</p>
  </div>
  <div class="page">
    <p>Through the looking glass of REINFORCE</p>
    <p>Ez(z|x)[L(z)]</p>
    <p>=</p>
    <p>z L(z)(z | x)</p>
    <p>(By de ni on of expecta on. How to evaluate?)</p>
    <p>=  z L(z)(z | x)</p>
    <p>=  z L(z)(z | x) log (z | x)</p>
    <p>(By Leibniz integral rule for log )</p>
    <p>=Ez(z|x)[L(z) log (z | x)]</p>
    <p>deep-spin.github.io/tutorial 36</p>
  </div>
  <div class="page">
    <p>Through the looking glass of REINFORCE</p>
    <p>Ez(z|x)[L(z)]=</p>
    <p>z L(z)(z | x)</p>
    <p>(By de ni on of expecta on. How to evaluate?)</p>
    <p>=  z L(z)(z | x)</p>
    <p>=  z L(z)(z | x) log (z | x)</p>
    <p>(By Leibniz integral rule for log )</p>
    <p>=Ez(z|x)[L(z) log (z | x)]</p>
    <p>deep-spin.github.io/tutorial 36</p>
  </div>
  <div class="page">
    <p>Through the looking glass of REINFORCE</p>
    <p>Ez(z|x)[L(z)]=</p>
    <p>z L(z)(z | x)</p>
    <p>(By de ni on of expecta on. How to evaluate?)</p>
    <p>=  z L(z)(z | x)</p>
    <p>=  z L(z)(z | x) log (z | x)</p>
    <p>(By Leibniz integral rule for log )</p>
    <p>=Ez(z|x)[L(z) log (z | x)]</p>
    <p>deep-spin.github.io/tutorial 36</p>
  </div>
  <div class="page">
    <p>Through the looking glass of REINFORCE</p>
    <p>Ez(z|x)[L(z)]=</p>
    <p>z L(z)(z | x)</p>
    <p>(By de ni on of expecta on. How to evaluate?)</p>
    <p>=  z L(z)(z | x)</p>
    <p>=  z L(z)(z | x) log (z | x)</p>
    <p>(By Leibniz integral rule for log )</p>
    <p>=Ez(z|x)[L(z) log (z | x)]</p>
    <p>deep-spin.github.io/tutorial 36</p>
  </div>
  <div class="page">
    <p>Through the looking glass of REINFORCE</p>
    <p>Ez(z|x)[L(z)]=</p>
    <p>z L(z)(z | x)</p>
    <p>(By de ni on of expecta on. How to evaluate?)</p>
    <p>=  z L(z)(z | x)</p>
    <p>=  z L(z)(z | x) log (z | x)</p>
    <p>(By Leibniz integral rule for log )</p>
    <p>=Ez(z|x)[L(z) log (z | x)] deep-spin.github.io/tutorial 36</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE, aka RL-SPINN</p>
    <p>Yogatama et al. [2017] uses REINFORCE to train SPINN!</p>
    <p>However, this vanilla implementa on isnt very e ec ve at learning syntax. This model fails to solve a simple toy problem.</p>
    <p>deep-spin.github.io/tutorial 37</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE, aka RL-SPINN</p>
    <p>Yogatama et al. [2017] uses REINFORCE to train SPINN! However, this vanilla implementa on isnt very e ec ve at learning syntax.</p>
    <p>This model fails to solve a simple toy problem.</p>
    <p>deep-spin.github.io/tutorial 37</p>
  </div>
  <div class="page">
    <p>SPINN with REINFORCE, aka RL-SPINN</p>
    <p>Yogatama et al. [2017] uses REINFORCE to train SPINN! However, this vanilla implementa on isnt very e ec ve at learning syntax. This model fails to solve a simple toy problem.</p>
    <p>deep-spin.github.io/tutorial 37</p>
  </div>
  <div class="page">
    <p>Toy problem: ListOps [Nangia and Bowman, 2018]</p>
    <p>[max29 [min47 ]0 ]</p>
    <p>deep-spin.github.io/tutorial 38</p>
  </div>
  <div class="page">
    <p>Toy problem: ListOps [Nangia and Bowman, 2018]</p>
    <p>Accuracy Self Model ()()() max F1 LSTM 71.5 (1.5) 74.4 RL-SPINN 60.7 (2.6) 64.8 30.8</p>
    <p>Random Trees - - 30.1 F1 wrt. Avg.</p>
    <p>Model LB RB GT Depth 48D RL-SPINN 64.5 16.0 32.1 14.6 128D RL-SPINN 43.5 13.0 71.1 10.4 GT Trees 41.6 8.8 100.0 9.6 Random Trees 24.0 24.0 24.2 5.2</p>
    <p>But why?</p>
    <p>deep-spin.github.io/tutorial 39</p>
  </div>
  <div class="page">
    <p>Toy problem: ListOps [Nangia and Bowman, 2018]</p>
    <p>Accuracy Self Model ()()() max F1 LSTM 71.5 (1.5) 74.4 RL-SPINN 60.7 (2.6) 64.8 30.8</p>
    <p>Random Trees - - 30.1 F1 wrt. Avg.</p>
    <p>Model LB RB GT Depth 48D RL-SPINN 64.5 16.0 32.1 14.6 128D RL-SPINN 43.5 13.0 71.1 10.4 GT Trees 41.6 8.8 100.0 9.6 Random Trees 24.0 24.0 24.2 5.2</p>
    <p>But why?</p>
    <p>deep-spin.github.io/tutorial 39</p>
  </div>
  <div class="page">
    <p>RL-SPINNs Troubles</p>
    <p>This system faces at least two big problems,</p>
    <p>deep-spin.github.io/tutorial 40</p>
  </div>
  <div class="page">
    <p>RL-SPINNs Troubles</p>
    <p>This system faces at least two big problems, 1. High variance of gradients 2. Coadapta on</p>
    <p>deep-spin.github.io/tutorial 40</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>We have a single reward at the end of parsing.</p>
    <p>We are sampling parses from very large search space! Catalan number of binary trees.  And the policy is stochas c.</p>
    <p>deep-spin.github.io/tutorial 41</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>We have a single reward at the end of parsing.  We are sampling parses from very large search space! Catalan number of binary trees.</p>
    <p>And the policy is stochas c.</p>
    <p>deep-spin.github.io/tutorial 41</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>We have a single reward at the end of parsing.  We are sampling parses from very large search space! Catalan number of binary trees.</p>
    <p>And the policy is stochas c.</p>
    <p>deep-spin.github.io/tutorial 41</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>We have a single reward at the end of parsing.  We are sampling parses from very large search space! Catalan number of binary trees.  And the policy is stochas c.</p>
    <p>deep-spin.github.io/tutorial 41</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>So, some mes the policy lands in a rewarding state:</p>
    <p>[sm[sm[sm[max56 ]2 ]0 ]5086 ] Figure: Truth: 7; Pred: 7</p>
    <p>Some mes it doesnt:</p>
    <p>deep-spin.github.io/tutorial 42</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>So, some mes the policy lands in a rewarding state:</p>
    <p>Some mes it doesnt:</p>
    <p>[max [med [med1 [sm313 ]9 ]6 ]5 ] Figure: Truth: 6; Pred: 5</p>
    <p>deep-spin.github.io/tutorial 42</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>Catalan number of parses means we need many many samples to lower variance!</p>
    <p>Possible solu ons, 1. Gradient normaliza on 2. Control variates, aka baselines</p>
    <p>deep-spin.github.io/tutorial 43</p>
  </div>
  <div class="page">
    <p>High variance</p>
    <p>Catalan number of parses means we need many many samples to lower variance! Possible solu ons, 1. Gradient normaliza on 2. Control variates, aka baselines</p>
    <p>deep-spin.github.io/tutorial 43</p>
  </div>
  <div class="page">
    <p>Control variates</p>
    <p>A simple control variate: moving average of recent rewards</p>
    <p>Parameters are updated using the advantagewhich is the di erence between the reward,R, and the baseline predic on. So,</p>
    <p>Ez(z) =Ez(z)[(L(z)b(x))(z)]</p>
    <p>Which we can do because, z b(x)(z)=b(x)</p>
    <p>z (z)=b(x)1=0</p>
    <p>deep-spin.github.io/tutorial 44</p>
  </div>
  <div class="page">
    <p>Control variates</p>
    <p>A simple control variate: moving average of recent rewards  Parameters are updated using the advantagewhich is the di erence between the reward,R, and the baseline predic on.</p>
    <p>So,</p>
    <p>Ez(z) =Ez(z)[(L(z)b(x))(z)]</p>
    <p>Which we can do because, z b(x)(z)=b(x)</p>
    <p>z (z)=b(x)1=0</p>
    <p>deep-spin.github.io/tutorial 44</p>
  </div>
  <div class="page">
    <p>Control variates</p>
    <p>A simple control variate: moving average of recent rewards  Parameters are updated using the advantagewhich is the di erence between the reward,R, and the baseline predic on. So,</p>
    <p>Ez(z) =Ez(z)[(L(z)b(x))(z)]</p>
    <p>Which we can do because, z b(x)(z)=b(x)</p>
    <p>z (z)=b(x)1=0</p>
    <p>deep-spin.github.io/tutorial 44</p>
  </div>
  <div class="page">
    <p>Control variates</p>
    <p>A simple control variate: moving average of recent rewards  Parameters are updated using the advantagewhich is the di erence between the reward,R, and the baseline predic on. So,</p>
    <p>Ez(z) =Ez(z)[(L(z)b(x))(z)]</p>
    <p>Which we can do because, z b(x)(z)=b(x)</p>
    <p>z (z)=b(x)1=0</p>
    <p>deep-spin.github.io/tutorial 44</p>
  </div>
  <div class="page">
    <p>Issues with SPINN with REINFORCE</p>
    <p>This system faces two big problems, 1. High variance of gradients 2. Coadapta on</p>
    <p>deep-spin.github.io/tutorial 45</p>
  </div>
  <div class="page">
    <p>Coadapta on problem</p>
    <p>Learning composi on func on parameters  with backpropaga on, and parser parameters  with REINFORCE.</p>
    <p>Generally,  will be learned more quickly than , making it harder to explore the parsing search space and op mize for .</p>
    <p>Di erence in variance of two gradient es mates.</p>
    <p>Possible solu on: Proximal Policy Op miza on (Schulman et al., 2017)</p>
    <p>deep-spin.github.io/tutorial 46</p>
  </div>
  <div class="page">
    <p>Coadapta on problem</p>
    <p>Learning composi on func on parameters  with backpropaga on, and parser parameters  with REINFORCE.</p>
    <p>Generally,  will be learned more quickly than , making it harder to explore the parsing search space and op mize for .</p>
    <p>Di erence in variance of two gradient es mates.</p>
    <p>Possible solu on: Proximal Policy Op miza on (Schulman et al., 2017)</p>
    <p>deep-spin.github.io/tutorial 46</p>
  </div>
  <div class="page">
    <p>Coadapta on problem</p>
    <p>Learning composi on func on parameters  with backpropaga on, and parser parameters  with REINFORCE.</p>
    <p>Generally,  will be learned more quickly than , making it harder to explore the parsing search space and op mize for .</p>
    <p>Di erence in variance of two gradient es mates.</p>
    <p>Possible solu on: Proximal Policy Op miza on (Schulman et al., 2017)</p>
    <p>deep-spin.github.io/tutorial 46</p>
  </div>
  <div class="page">
    <p>Coadapta on problem</p>
    <p>Learning composi on func on parameters  with backpropaga on, and parser parameters  with REINFORCE.</p>
    <p>Generally,  will be learned more quickly than , making it harder to explore the parsing search space and op mize for .</p>
    <p>Di erence in variance of two gradient es mates.</p>
    <p>Possible solu on: Proximal Policy Op miza on (Schulman et al., 2017)</p>
    <p>deep-spin.github.io/tutorial 46</p>
  </div>
  <div class="page">
    <p>Making REINFORCE+SPINN work</p>
    <p>Havrylov et al. [2019] use, 1. Input dependent control variate 2. Gradient normaliza on 3. Proximal Policy Op miza on</p>
    <p>They solve ListOps! However, does not learn English grammars.</p>
    <p>deep-spin.github.io/tutorial 47</p>
  </div>
  <div class="page">
    <p>Making REINFORCE+SPINN work</p>
    <p>Havrylov et al. [2019] use, 1. Input dependent control variate 2. Gradient normaliza on 3. Proximal Policy Op miza on</p>
    <p>They solve ListOps!</p>
    <p>However, does not learn English grammars.</p>
    <p>deep-spin.github.io/tutorial 47</p>
  </div>
  <div class="page">
    <p>Making REINFORCE+SPINN work</p>
    <p>Havrylov et al. [2019] use, 1. Input dependent control variate 2. Gradient normaliza on 3. Proximal Policy Op miza on</p>
    <p>They solve ListOps! However, does not learn English grammars.</p>
    <p>deep-spin.github.io/tutorial 47</p>
  </div>
  <div class="page">
    <p>Should I? Shouldnt I?</p>
    <p>Unbiased!</p>
    <p>In a simple se ng, with enough tricks, it can work!</p>
    <p>High variance  Has not yet been very e ec ve at learning English syntax.</p>
    <p>deep-spin.github.io/tutorial 48</p>
  </div>
  <div class="page">
    <p>Should I? Shouldnt I?</p>
    <p>Unbiased!</p>
    <p>In a simple se ng, with enough tricks, it can work!</p>
    <p>High variance</p>
    <p>Has not yet been very e ec ve at learning English syntax.</p>
    <p>deep-spin.github.io/tutorial 48</p>
  </div>
  <div class="page">
    <p>Should I? Shouldnt I?</p>
    <p>Unbiased!  In a simple se ng, with enough tricks, it can work!</p>
    <p>High variance</p>
    <p>Has not yet been very e ec ve at learning English syntax.</p>
    <p>deep-spin.github.io/tutorial 48</p>
  </div>
  <div class="page">
    <p>Should I? Shouldnt I?</p>
    <p>Unbiased!  In a simple se ng, with enough tricks, it can work!</p>
    <p>High variance  Has not yet been very e ec ve at learning English syntax.</p>
    <p>deep-spin.github.io/tutorial 48</p>
  </div>
  <div class="page">
    <p>III. Gradient Surrogates</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.</p>
    <p>Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x)</p>
    <p>incur loss L  z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly</p>
    <p>3B: use this strategy to reduce variance in the stochas c model.</p>
    <p>deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>So far:  Tackled expected loss in a stochas c computa on graph</p>
    <p>E(z|x)  L(z)</p>
    <p>Op mized with the REINFORCE es mator.  Struggled with variance &amp; sampling.</p>
    <p>In this sec on:  Consider the determinis c alterna ve:</p>
    <p>pick best structure z(x):= arg maxzM (z | x) incur loss L</p>
    <p>z(x)</p>
    <p>3A: try to op mize the determinis c loss directly  3B: use this strategy to reduce variance in the stochas c model. deep-spin.github.io/tutorial 49</p>
  </div>
  <div class="page">
    <p>Recap: The argmax problem</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s z</p>
    <p>z s =0</p>
    <p>z1</p>
    <p>s10</p>
    <p>s2 1 s2 s2 +1</p>
    <p>z=arg max(s)</p>
    <p>deep-spin.github.io/tutorial 50</p>
  </div>
  <div class="page">
    <p>So max</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s p</p>
    <p>p s =diag(p)pp</p>
    <p>p1</p>
    <p>s10</p>
    <p>s2 1 s2 s2 +1</p>
    <p>pj =exp(sj)/Z</p>
    <p>deep-spin.github.io/tutorial 51</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0</p>
    <p>simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp</p>
    <p>More explana on in a while</p>
    <p>s</p>
    <p>z</p>
    <p>pWhat about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)</p>
    <p>Backward: pretend z was some con nuous p; ps =/ 0</p>
    <p>simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp</p>
    <p>More explana on in a while</p>
    <p>s z</p>
    <p>pWhat about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)</p>
    <p>Backward: pretend z was some con nuous p; ps =/ 0</p>
    <p>simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp</p>
    <p>More explana on in a while</p>
    <p>s z</p>
    <p>pWhat about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0</p>
    <p>simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp  More explana on in a while</p>
    <p>s z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0</p>
    <p>simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp  More explana on in a while</p>
    <p>s z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0  simplest: iden ty, p(s) =s, ps = I</p>
    <p>others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp  More explana on in a while</p>
    <p>s z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0  simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp</p>
    <p>More explana on in a while</p>
    <p>s z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0  simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp  More explana on in a while</p>
    <p>s z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator [Hinton, 2012, Bengio et al., 2013]</p>
    <p>Forward: z=arg max(s)  Backward: pretend z was some con nuous p; ps =/ 0  simplest: iden ty, p(s) =s, ps = I  others, e.g. so max p(s) = softmax(s), ps = diag(p)  pp  More explana on in a while</p>
    <p>s z</p>
    <p>pWhat about the structured case?</p>
    <p>deep-spin.github.io/tutorial 52</p>
  </div>
  <div class="page">
    <p>Dealing with the combinatorial explosion</p>
    <p>1. Incremental structures  Build structure greedily, as sequence of discrete choices (e.g., shi -reduce).  Scores (par al structure, ac on) tuples.  Advantages: exible, rich histories.  Disadvantages: greedy, local decisions are subop mal, error propaga on.</p>
    <p>max     , , , ,</p>
    <p>deep-spin.github.io/tutorial 53</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  In this case, we just apply the straight-through es mator for each step.  Forward: the highest scoring ac on for each step  Backward: pretend that we had used a di eren able surrogate func on Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)</p>
    <p>Assigns a score to any (par al structure, ac on) tuple.  In this case, we just apply the straight-through es mator for each step.  Forward: the highest scoring ac on for each step  Backward: pretend that we had used a di eren able surrogate func on Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.</p>
    <p>In this case, we just apply the straight-through es mator for each step.  Forward: the highest scoring ac on for each step  Backward: pretend that we had used a di eren able surrogate func on Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  In this case, we just apply the straight-through es mator for each step.</p>
    <p>Forward: the highest scoring ac on for each step  Backward: pretend that we had used a di eren able surrogate func on Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  In this case, we just apply the straight-through es mator for each step.  Forward: the highest scoring ac on for each step</p>
    <p>Backward: pretend that we had used a di eren able surrogate func on Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  In this case, we just apply the straight-through es mator for each step.  Forward: the highest scoring ac on for each step  Backward: pretend that we had used a di eren able surrogate func on</p>
    <p>Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  In this case, we just apply the straight-through es mator for each step.  Forward: the highest scoring ac on for each step  Backward: pretend that we had used a di eren able surrogate func on Example: Latent Tree Learning with Di eren able Parsers: Shi -Reduce Parsing and Chart Parsing [Maillard and Clark, 2018] (STE through beam search).</p>
    <p>deep-spin.github.io/tutorial 54</p>
  </div>
  <div class="page">
    <p>STE for the factorized approach</p>
    <p>Requires a bit more work:  Recap: marginal polytope  Predic ng structures globally: Maximum A Posteriori (MAP)  Deriving Straight-Through and SPIGOT</p>
    <p>deep-spin.github.io/tutorial 55</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z  Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 56</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z</p>
    <p>Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 56</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z  Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 56</p>
  </div>
  <div class="page">
    <p>Predic ng structures from scores of parts</p>
    <p>(i  j): score of arc i  j  z(i  j): is arc i  j selected?</p>
    <p>Task-speci c algorithm for the highest-scoring structure.</p>
    <p>z outputey</p>
    <p>deep-spin.github.io/tutorial 57</p>
  </div>
  <div class="page">
    <p>Predic ng structures from scores of parts</p>
    <p>(i  j): score of arc i  j  z(i  j): is arc i  j selected?  Task-speci c algorithm for the highest-scoring structure.</p>
    <p>z outputey</p>
    <p>deep-spin.github.io/tutorial 57</p>
  </div>
  <div class="page">
    <p>Algorithms for speci c structures Best structure (MAP)</p>
    <p>Marginals</p>
    <p>Sequence tagging Viterbi[Rabiner, 1989]</p>
    <p>Forward-Backward [Rabiner, 1989]</p>
    <p>Cons tuent trees CKY</p>
    <p>[Kasami, 1966, Younger, 1967] [Cocke and Schwartz, 1970]</p>
    <p>Inside-Outside [Baker, 1979]</p>
    <p>Temporal alignments DTW[Sakoe and Chiba, 1978]</p>
    <p>So -DTW [Cuturi and Blondel, 2017]</p>
    <p>Dependency trees Max. Spanning Arborescence[Chu and Liu, 1965, Edmonds, 1967]</p>
    <p>Matrix-Tree [Kirchho , 1847]</p>
    <p>Assignments Kuhn-Munkres[Kuhn, 1955, Jonker and Volgenant, 1987]</p>
    <p>#P-complete [Valiant, 1979, Taskar, 2004]</p>
    <p>deep-spin.github.io/tutorial 58</p>
  </div>
  <div class="page">
    <p>Structured Straight-Through</p>
    <p>Forward pass: Find highest-scoring structure: z=arg max</p>
    <p>zZ z</p>
    <p>Backward pass: pretend we used =.</p>
    <p>z</p>
    <p>deep-spin.github.io/tutorial 59</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).</p>
    <p>if we had labels (mul -task learning), LMTL =L  y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue.</p>
    <p>We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz</p>
    <p>LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through Es mator Revisited</p>
    <p>In the forward pass, z=arg max(s).  if we had labels (mul -task learning), LMTL =L</p>
    <p>y(z),y  +Lhid(s,ztrue)</p>
    <p>One choice: perceptron loss Lhid(s,ztrue)=szsztrue; Lhids =zztrue. We dont have labels! Induce labels by pulling back the downstream target: the best (unconstrained) latent value would be: arg minzRD L</p>
    <p>y(z),y</p>
    <p>One gradient descent step star ng from z: ztrue  z Lz LMTL s</p>
    <p>= L</p>
    <p>s =0</p>
    <p>+ Lhid s</p>
    <p>=z  z</p>
    <p>L</p>
    <p>z</p>
    <p>= L</p>
    <p>z</p>
    <p>[Mar ns and Niculae, 2019]</p>
    <p>deep-spin.github.io/tutorial 60</p>
  </div>
  <div class="page">
    <p>Straight-Through in the structured case [Peng et al., 2018, Mar ns and Niculae, 2019]</p>
    <p>Structured STE: perceptron update with induced annota on arg min RD</p>
    <p>L(y(),y)  zzL(z)  ztrue</p>
    <p>(one step of gradient descent)</p>
    <p>SPIGOT takes into account the constraints; uses the induced annota on</p>
    <p>arg min M</p>
    <p>L(y(),y)  ProjM  zzL(z)  ztrue</p>
    <p>(one step of projected gradient descent!)  We discuss a generic way to compute the projec on in part 4.</p>
    <p>z</p>
    <p>ztrue = z - zL(z)</p>
    <p>z</p>
    <p>z - zL(z)</p>
    <p>ztrue</p>
    <p>deep-spin.github.io/tutorial 61</p>
  </div>
  <div class="page">
    <p>Straight-Through in the structured case [Peng et al., 2018, Mar ns and Niculae, 2019]</p>
    <p>Structured STE: perceptron update with induced annota on arg min RD</p>
    <p>L(y(),y)  zzL(z)  ztrue</p>
    <p>(one step of gradient descent)  SPIGOT takes into account the constraints; uses the induced annota on</p>
    <p>arg min M</p>
    <p>L(y(),y)  ProjM  zzL(z)  ztrue</p>
    <p>(one step of projected gradient descent!)</p>
    <p>We discuss a generic way to compute the projec on in part 4.</p>
    <p>z</p>
    <p>ztrue = z - zL(z)</p>
    <p>z</p>
    <p>z - zL(z)</p>
    <p>ztrue</p>
    <p>deep-spin.github.io/tutorial 61</p>
  </div>
  <div class="page">
    <p>Straight-Through in the structured case [Peng et al., 2018, Mar ns and Niculae, 2019]</p>
    <p>Structured STE: perceptron update with induced annota on arg min RD</p>
    <p>L(y(),y)  zzL(z)  ztrue</p>
    <p>(one step of gradient descent)  SPIGOT takes into account the constraints; uses the induced annota on</p>
    <p>arg min M</p>
    <p>L(y(),y)  ProjM  zzL(z)  ztrue</p>
    <p>(one step of projected gradient descent!)  We discuss a generic way to compute the projec on in part 4.</p>
    <p>z</p>
    <p>ztrue = z - zL(z)</p>
    <p>z</p>
    <p>z - zL(z)</p>
    <p>ztrue</p>
    <p>deep-spin.github.io/tutorial 61</p>
  </div>
  <div class="page">
    <p>Summary: Straight-Through Es mator</p>
    <p>We saw how to use the Straight-Through Es mator to allow learning models with argmax in the middle of the computa on graph. We were op mizing L</p>
    <p>z(x)</p>
    <p>Now we will see how to apply STE for stochas c graphs, as an alterna ve approach of REINFORCE.</p>
    <p>deep-spin.github.io/tutorial 62</p>
  </div>
  <div class="page">
    <p>Summary: Straight-Through Es mator</p>
    <p>We saw how to use the Straight-Through Es mator to allow learning models with argmax in the middle of the computa on graph.</p>
    <p>We were op mizing L  z(x)</p>
    <p>Now we will see how to apply STE for stochas c graphs, as an alterna ve approach of REINFORCE.</p>
    <p>deep-spin.github.io/tutorial 62</p>
  </div>
  <div class="page">
    <p>Summary: Straight-Through Es mator</p>
    <p>We saw how to use the Straight-Through Es mator to allow learning models with argmax in the middle of the computa on graph. We were op mizing L</p>
    <p>z(x)</p>
    <p>Now we will see how to apply STE for stochas c graphs, as an alterna ve approach of REINFORCE.</p>
    <p>deep-spin.github.io/tutorial 62</p>
  </div>
  <div class="page">
    <p>Summary: Straight-Through Es mator</p>
    <p>We saw how to use the Straight-Through Es mator to allow learning models with argmax in the middle of the computa on graph. We were op mizing L</p>
    <p>z(x)</p>
    <p>Now we will see how to apply STE for stochas c graphs, as an alterna ve approach of REINFORCE.</p>
    <p>deep-spin.github.io/tutorial 62</p>
  </div>
  <div class="page">
    <p>Stochas c node in the computa on graph</p>
    <p>Recall the stochas c objec ve:</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE (previous sec on).</p>
    <p>High variance.</p>
    <p>An alterna ve is using the reparameteriza on trick [Kingma and Welling, 2014].</p>
    <p>deep-spin.github.io/tutorial 63</p>
  </div>
  <div class="page">
    <p>Stochas c node in the computa on graph</p>
    <p>Recall the stochas c objec ve:</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE (previous sec on).</p>
    <p>High variance.</p>
    <p>An alterna ve is using the reparameteriza on trick [Kingma and Welling, 2014].</p>
    <p>deep-spin.github.io/tutorial 63</p>
  </div>
  <div class="page">
    <p>Stochas c node in the computa on graph</p>
    <p>Recall the stochas c objec ve:</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE (previous sec on).</p>
    <p>High variance.  An alterna ve is using the reparameteriza on trick [Kingma and Welling, 2014].</p>
    <p>deep-spin.github.io/tutorial 63</p>
  </div>
  <div class="page">
    <p>Stochas c node in the computa on graph</p>
    <p>Recall the stochas c objec ve:</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE (previous sec on). High variance.</p>
    <p>An alterna ve is using the reparameteriza on trick [Kingma and Welling, 2014].</p>
    <p>deep-spin.github.io/tutorial 63</p>
  </div>
  <div class="page">
    <p>Stochas c node in the computa on graph</p>
    <p>Recall the stochas c objec ve:</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE (previous sec on). High variance.  An alterna ve is using the reparameteriza on trick [Kingma and Welling, 2014].</p>
    <p>deep-spin.github.io/tutorial 63</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on</p>
    <p>[Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)  What is the gradient of a sample z?!  Reparameteriza on: Move the stochas city out of the gradient path.  Makes z determinis c w.r.t. s!</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on</p>
    <p>[Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)</p>
    <p>What is the gradient of a sample z?!  Reparameteriza on: Move the stochas city out of the gradient path.  Makes z determinis c w.r.t. s!</p>
    <p>s z</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)  What is the gradient of a sample z?!</p>
    <p>Reparameteriza on: Move the stochas city out of the gradient path.  Makes z determinis c w.r.t. s!</p>
    <p>s z</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)  What is the gradient of a sample z?!  Reparameteriza on: Move the stochas city out of the gradient path.</p>
    <p>Makes z determinis c w.r.t. s!</p>
    <p>s</p>
    <p>(stochastic)</p>
    <p>z</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)  What is the gradient of a sample z?!  Reparameteriza on: Move the stochas city out of the gradient path.  Makes z determinis c w.r.t. s!</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)  What is the gradient of a sample z?!  Reparameteriza on: Move the stochas city out of the gradient path.  Makes z determinis c w.r.t. s!</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Sampling from a categorical value in the middle of the computa on graph. z  (z | x)  exps(z | x)  What is the gradient of a sample z?!  Reparameteriza on: Move the stochas city out of the gradient path.  Makes z determinis c w.r.t. s!</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>As a result:</p>
    <p>Stochas city is moved as an input.</p>
    <p>We can backpropagate through the determinis c input to z.</p>
    <p>deep-spin.github.io/tutorial 64</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>How do we sample from a categorical variable?</p>
    <p>deep-spin.github.io/tutorial 65</p>
  </div>
  <div class="page">
    <p>Categorical reparameteriza on [Jang et al., 2017, Maddison et al., 2016]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>How do we sample from a categorical variable?</p>
    <p>deep-spin.github.io/tutorial 65</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>p= softmax(s)  ci =ji pj  u  Uniform(0,1)  return z=et s.t. ct  u &lt; ct+1</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>ci =ji pj  u  Uniform(0,1)  return z=et s.t. ct  u &lt; ct+1</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>u  Uniform(0,1)  return z=et s.t. ct  u &lt; ct+1</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>return z=et s.t. ct  u &lt; ct+1</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>ui  Uniform(0,1)  i = log( log(ui))  z=arg max(s+)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>i = log( log(ui))  z=arg max(s+)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>z=arg max(s+)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.)</p>
    <p>Requires sampling from the Standard Gumbel Distribu on G(0,1). Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Sampling from a categorical variable We want to sample from a categorical variable with scores s (class i has a score si)</p>
    <p>The two methods are equivalent. (Not obvious, but we will not prove it now.) Requires sampling from the Standard Gumbel Distribu on G(0,1).</p>
    <p>Deriva on &amp; more info: [Adams, 2013, Vieira, 2014]</p>
    <p>s+s</p>
    <p>(stochastic)</p>
    <p>+</p>
    <p>z</p>
    <p>We have an argmax again and cannot backpropagate!</p>
    <p>deep-spin.github.io/tutorial 66</p>
  </div>
  <div class="page">
    <p>Straight-Through Gumbel Es mator Apply a variant of the Straight-Through Es mator to Gumbel-Max!</p>
    <p>[Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Forward: z=arg max(s+)  Backward: pretend we had done p= softmax(s+)</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 67</p>
  </div>
  <div class="page">
    <p>Straight-Through Gumbel Es mator Apply a variant of the Straight-Through Es mator to Gumbel-Max!</p>
    <p>[Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Forward: z=arg max(s+)</p>
    <p>Backward: pretend we had done p= softmax(s+)</p>
    <p>s+s</p>
    <p>i =-log(-log(ui)) ui~U(0,1)</p>
    <p>+</p>
    <p>z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 67</p>
  </div>
  <div class="page">
    <p>Straight-Through Gumbel Es mator Apply a variant of the Straight-Through Es mator to Gumbel-Max!</p>
    <p>[Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Forward: z=arg max(s+)  Backward: pretend we had done p= softmax(s+)</p>
    <p>s+s</p>
    <p>i =-log(-log(ui)) ui~U(0,1)</p>
    <p>+</p>
    <p>z</p>
    <p>p</p>
    <p>What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 67</p>
  </div>
  <div class="page">
    <p>Straight-Through Gumbel Es mator Apply a variant of the Straight-Through Es mator to Gumbel-Max!</p>
    <p>[Jang et al., 2017, Maddison et al., 2016]</p>
    <p>Forward: z=arg max(s+)  Backward: pretend we had done p= softmax(s+)</p>
    <p>s+s</p>
    <p>i =-log(-log(ui)) ui~U(0,1)</p>
    <p>+</p>
    <p>z</p>
    <p>p What about the structured case?</p>
    <p>deep-spin.github.io/tutorial 67</p>
  </div>
  <div class="page">
    <p>Dealing with the combinatorial explosion</p>
    <p>1. Incremental structures  Build structure greedily, as sequence of discrete choices (e.g., shi -reduce).  Scores (par al structure, ac on) tuples.  Advantages: exible, rich histories.  Disadvantages: greedy, local decisions are subop mal, error propaga on.</p>
    <p>max     , , , ,</p>
    <p>deep-spin.github.io/tutorial 68</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  Reparameterize the scores with Gumbel-Max - now we have a determinis c node.  Forward: the argmax from the reparameterized scores for each step  Backward: pretend we had used a di eren able surrogate func on Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)</p>
    <p>Assigns a score to any (par al structure, ac on) tuple.  Reparameterize the scores with Gumbel-Max - now we have a determinis c node.  Forward: the argmax from the reparameterized scores for each step  Backward: pretend we had used a di eren able surrogate func on Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.</p>
    <p>Reparameterize the scores with Gumbel-Max - now we have a determinis c node.  Forward: the argmax from the reparameterized scores for each step  Backward: pretend we had used a di eren able surrogate func on Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  Reparameterize the scores with Gumbel-Max - now we have a determinis c node.</p>
    <p>Forward: the argmax from the reparameterized scores for each step  Backward: pretend we had used a di eren able surrogate func on Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  Reparameterize the scores with Gumbel-Max - now we have a determinis c node.  Forward: the argmax from the reparameterized scores for each step</p>
    <p>Backward: pretend we had used a di eren able surrogate func on Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  Reparameterize the scores with Gumbel-Max - now we have a determinis c node.  Forward: the argmax from the reparameterized scores for each step  Backward: pretend we had used a di eren able surrogate func on</p>
    <p>Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Sampling from incremental structures</p>
    <p>Build a structure as a sequence of discrete choices (e.g., shi -reduce)  Assigns a score to any (par al structure, ac on) tuple.  Reparameterize the scores with Gumbel-Max - now we have a determinis c node.  Forward: the argmax from the reparameterized scores for each step  Backward: pretend we had used a di eren able surrogate func on Example: Gumbel Tree-LSTM [Choi et al., 2018].</p>
    <p>deep-spin.github.io/tutorial 69</p>
  </div>
  <div class="page">
    <p>Example: Gumbel Tree-LSTM [Choi et al., 2018]</p>
    <p>Building task-speci c tree structures.  Straight-Through Gumbel-So max at each step to select one arc.</p>
    <p>The cat sat on</p>
    <p>The cat cat sat sat on</p>
    <p>The cat sat on</p>
    <p>v1 = 0.5 v2 = 0.1 v2 = 0.4</p>
    <p>q</p>
    <p>deep-spin.github.io/tutorial 70</p>
  </div>
  <div class="page">
    <p>Sampling from factorized models Perturb-and-MAP</p>
    <p>[Papandreou and Yuille, 2011, Corro and Titov, 2019a,b]</p>
    <p>Reparameterize by perturbing the arc scores. (inexact!)</p>
    <p>Sample from the normal Gumbel distribu on.  Perturb the arc scores with the Gumbel noise.  Compute MAP (task-speci c algorithm).  Backward: we could use Straight-Through with Iden ty.</p>
    <p>G(0,1)  =+  arg maxzZ z</p>
    <p>deep-spin.github.io/tutorial 71</p>
  </div>
  <div class="page">
    <p>Sampling from factorized models Perturb-and-MAP</p>
    <p>[Papandreou and Yuille, 2011, Corro and Titov, 2019a,b]</p>
    <p>Reparameterize by perturbing the arc scores. (inexact!)</p>
    <p>Sample from the normal Gumbel distribu on.</p>
    <p>Perturb the arc scores with the Gumbel noise.  Compute MAP (task-speci c algorithm).  Backward: we could use Straight-Through with Iden ty.</p>
    <p>G(0,1)</p>
    <p>=+  arg maxzZ z</p>
    <p>deep-spin.github.io/tutorial 71</p>
  </div>
  <div class="page">
    <p>Sampling from factorized models Perturb-and-MAP</p>
    <p>[Papandreou and Yuille, 2011, Corro and Titov, 2019a,b]</p>
    <p>Reparameterize by perturbing the arc scores. (inexact!)</p>
    <p>Sample from the normal Gumbel distribu on.  Perturb the arc scores with the Gumbel noise.</p>
    <p>Compute MAP (task-speci c algorithm).  Backward: we could use Straight-Through with Iden ty.</p>
    <p>G(0,1)  =+</p>
    <p>arg maxzZ z</p>
    <p>deep-spin.github.io/tutorial 71</p>
  </div>
  <div class="page">
    <p>Sampling from factorized models Perturb-and-MAP</p>
    <p>[Papandreou and Yuille, 2011, Corro and Titov, 2019a,b]</p>
    <p>Reparameterize by perturbing the arc scores. (inexact!)</p>
    <p>Sample from the normal Gumbel distribu on.  Perturb the arc scores with the Gumbel noise.  Compute MAP (task-speci c algorithm).</p>
    <p>Backward: we could use Straight-Through with Iden ty.</p>
    <p>G(0,1)  =+  arg maxzZ z</p>
    <p>deep-spin.github.io/tutorial 71</p>
  </div>
  <div class="page">
    <p>Sampling from factorized models Perturb-and-MAP</p>
    <p>[Papandreou and Yuille, 2011, Corro and Titov, 2019a,b]</p>
    <p>Reparameterize by perturbing the arc scores. (inexact!)</p>
    <p>Sample from the normal Gumbel distribu on.  Perturb the arc scores with the Gumbel noise.  Compute MAP (task-speci c algorithm).  Backward: we could use Straight-Through with Iden ty.</p>
    <p>G(0,1)  =+  arg maxzZ z</p>
    <p>deep-spin.github.io/tutorial 71</p>
  </div>
  <div class="page">
    <p>Summary: Gradient surrogates</p>
    <p>Based on the Straight-Through Es mator.  Can be used for stochas c or determinis c computa on graphs.  Forward pass: Get an argmax (might be structured).  Backpropaga on: use a func on, which we hope is close to argmax.  Examples:  Argmax for itera ve structures and factoriza on into parts  Sampling from itera ve structures and factoriza on into parts</p>
    <p>deep-spin.github.io/tutorial 72</p>
  </div>
  <div class="page">
    <p>Gradient surrogates: Pros and cons</p>
    <p>Pros  Do not su er from the high variance problem of REINFORCE.  Allow for exibility to select or sample a latent structured in the middle of the computa on graph.  E cient computa on.</p>
    <p>Cons  The Gumbel sampling with Perturb-and-MAP is an approxima on.  Bias, due to func on mismatch on the backpropaga on</p>
    <p>(next sec on will address this problem.)</p>
    <p>deep-spin.github.io/tutorial 73</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE  Straight-Through Gumbel</p>
    <p>(Perturb &amp; MAP)</p>
    <p>SparseMAP</p>
    <p>L  arg maxz (z | x)</p>
    <p>Straight-Through  SPIGOT</p>
    <p>L  E(z|x)[z]</p>
    <p>Structured A n. Nets  SparseMAP</p>
    <p>And more, a er the break!</p>
    <p>deep-spin.github.io/tutorial 74</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE  Straight-Through Gumbel</p>
    <p>(Perturb &amp; MAP)</p>
    <p>SparseMAP</p>
    <p>L  arg maxz (z | x)</p>
    <p>Straight-Through  SPIGOT</p>
    <p>L  E(z|x)[z]</p>
    <p>Structured A n. Nets  SparseMAP</p>
    <p>And more, a er the break!</p>
    <p>deep-spin.github.io/tutorial 74</p>
  </div>
  <div class="page">
    <p>IV. End-to-end Di eren able Relaxa ons</p>
  </div>
  <div class="page">
    <p>End-to-end di eren able relaxa ons</p>
  </div>
  <div class="page">
    <p>Recall: Discrete choices &amp; di eren ability s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s z</p>
    <p>p</p>
    <p>z s= 0 or n/a</p>
    <p>input x</p>
    <p>outputby s= f(x) y=g(z,x)</p>
    <p>(argmax)</p>
    <p>p1</p>
    <p>s10</p>
    <p>s2 1 s2 s2 +1deep-spin.github.io/tutorial 76</p>
  </div>
  <div class="page">
    <p>One solu on: smooth relaxa on s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s</p>
    <p>z</p>
    <p>p</p>
    <p>p s=</p>
    <p>input x</p>
    <p>outputby s= f(x) y=g(z,x)</p>
    <p>(so max)</p>
    <p>p= softmax(s) =E[z], i.e. replace E  f(z)  with f  E[z]</p>
    <p>p1</p>
    <p>s10</p>
    <p>s2 1 s2 s2 +1deep-spin.github.io/tutorial 76</p>
  </div>
  <div class="page">
    <p>One solu on: smooth relaxa on s</p>
    <p>z=1 z=2</p>
    <p>z=N</p>
    <p>s</p>
    <p>z</p>
    <p>p</p>
    <p>p s=</p>
    <p>input x</p>
    <p>outputby s= f(x) y=g(z,x)</p>
    <p>(so max)</p>
    <p>p= softmax(s) =E[z], i.e. replace E  f(z)  with f  E[z]</p>
    <p>p1</p>
    <p>s10</p>
    <p>s2 1 s2 s2 +1deep-spin.github.io/tutorial 76</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE  Straight-Through Gumbel</p>
    <p>(Perturb &amp; MAP)</p>
    <p>SparseMAP</p>
    <p>L  arg maxz (z | x)</p>
    <p>Straight-Through  SPIGOT</p>
    <p>L  E(z|x)[z]</p>
    <p>Structured A n. Nets  SparseMAP</p>
    <p>And more, a er the break!</p>
    <p>deep-spin.github.io/tutorial 77</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices Expected score under p: Eip si =ps argmax</p>
    <p>maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices</p>
    <p>Expected score under p: Eip si =ps argmax</p>
    <p>maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices</p>
    <p>Expected score under p: Eip si =ps argmax</p>
    <p>maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>p= [0,1,0]</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices</p>
    <p>Expected score under p: Eip si =ps argmax</p>
    <p>maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices</p>
    <p>Expected score under p: Eip si =ps argmax</p>
    <p>maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>p= [1/3,1/3,1/3]</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices Expected score under p: Eip si =ps</p>
    <p>argmax</p>
    <p>maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices Expected score under p: Eip si =ps argmax</p>
    <p>maximizes expected score Shannon entropy of p: H(p)=</p>
    <p>i pi log pi</p>
    <p>so max maximizes expected score + entropy:</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices Expected score under p: Eip si =ps argmax maximizes expected score</p>
    <p>Shannon entropy of p: H(p)=</p>
    <p>i pi log pi so max maximizes expected score + entropy:</p>
    <p>p = [0,0,1]</p>
    <p>arg max p</p>
    <p>ps deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices Expected score under p: Eip si =ps argmax maximizes expected score Shannon entropy of p: H(p)=</p>
    <p>i pi log pi</p>
    <p>so max maximizes expected score + entropy:</p>
    <p>(1, 0, 0) (0, 1, 0)</p>
    <p>(0, 0, 1)</p>
    <p>p = [0,0,1]</p>
    <p>arg max p</p>
    <p>ps</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>What is so max? O en de ned via pi =</p>
    <p>expsi j expsj</p>
    <p>, but where does it come from?</p>
    <p>p : probability distribu on over choices Expected score under p: Eip si =ps argmax maximizes expected score Shannon entropy of p: H(p)=</p>
    <p>i pi log pi</p>
    <p>so max maximizes expected score + entropy:</p>
    <p>(1, 0, 0) (0, 1, 0)</p>
    <p>(0, 0, 1)</p>
    <p>arg max p</p>
    <p>ps+H(p)</p>
    <p>p = [0,0,1]</p>
    <p>arg max p</p>
    <p>ps</p>
    <p>deep-spin.github.io/tutorial 78</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1)</p>
    <p>if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1.</p>
    <p>Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Varia onal form of so max Proposi on. The unique solu on to arg max</p>
    <p>p ps+ H(p) is given by pj = expsj</p>
    <p>i expsi .</p>
    <p>Explicit form of the op miza on problem:</p>
    <p>maximize</p>
    <p>j pjsj pj log pj subject to p 0, p1=1</p>
    <p>Lagrangian:</p>
    <p>L(p,,) =</p>
    <p>j pjsj pj log pj p  +(p11)</p>
    <p>Op mality condi ons (KKT):</p>
    <p>p    0</p>
    <p>log pi =si +i  ( +1) if pi =0, r.h.s. must be , thus pi &gt; 0, so i =0.</p>
    <p>pi = exp(si)/exp(+1) = exp(si)/Z</p>
    <p>Must nd Z such that</p>
    <p>j pj =1. Answer: Z=</p>
    <p>j exp(sj)</p>
    <p>So, pi = exp(si) j exp(sj)</p>
    <p>.</p>
    <p>Classic result, e.g., [Boyd and Vandenberghe, 2004, Wainwright and Jordan, 2008]</p>
    <p>deep-spin.github.io/tutorial 79</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj sparsemax: (p)=1/2p22 -entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Astudillo, 2016][Niculae and Blondel, 2017, Blondel et al., 2019][Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj sparsemax: (p)=1/2p22 -entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Astudillo, 2016][Niculae and Blondel, 2017, Blondel et al., 2019][Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj sparsemax: (p)=1/2p22 -entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Astudillo, 2016][Niculae and Blondel, 2017, Blondel et al., 2019][Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj</p>
    <p>sparsemax: (p)=1/2p22 -entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Astudillo, 2016][Niculae and Blondel, 2017, Blondel et al., 2019][Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj sparsemax: (p)=1/2p22</p>
    <p>-entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Astudillo, 2016]</p>
    <p>[Niculae and Blondel, 2017, Blondel et al., 2019][Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj sparsemax: (p)=1/2p22 -entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>Generalized entropy interpolates in between [Tsallis, 1988] Used in Sparse Seq2Seq: [Peters et al., 2019]</p>
    <p>(Mon 13:50, poster session 2D)</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017][Niculae and Blondel, 2017, Mar ns and Astudillo, 2016]</p>
    <p>[Niculae and Blondel, 2017, Blondel et al., 2019]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>Generalizing so max: Smoothed argmaxes p(s)=arg max</p>
    <p>p ps(p)</p>
    <p>argmax: (p)=0</p>
    <p>so max: (p)=</p>
    <p>j pj log pj sparsemax: (p)=1/2p22 -entmax: (p)=1/(1)</p>
    <p>j p  i</p>
    <p>fusedmax: (p)=1/2p22 +</p>
    <p>j |pj pj1| csparsemax: (p)=1/2p22 + (a  p  b) cso max: (p)=</p>
    <p>j pj log pj + (a  p  b)</p>
    <p>p1</p>
    <p>s10</p>
    <p>1 0 1</p>
    <p>[0,0,1]</p>
    <p>[.3, .2, .5]</p>
    <p>[.3,0, .7]</p>
    <p>[Niculae and Blondel, 2017][Niculae and Blondel, 2017, Mar ns and Astudillo, 2016][Niculae and Blondel, 2017, Blondel et al., 2019]</p>
    <p>[Niculae and Blondel, 2017, Mar ns and Kreutzer, 2017, Malaviya et al., 2018]</p>
    <p>deep-spin.github.io/tutorial 80</p>
  </div>
  <div class="page">
    <p>The structured case: Marginal polytope [Wainwright and Jordan, 2008]</p>
    <p>Each vertex corresponds to one such bit vector z  Points inside correspond to marginal distribu ons: convex combina ons of structured objects</p>
    <p>= p1z1 + . . .+pNzN   exponen ally many terms</p>
    <p>, p  .</p>
    <p>p1 =0.2, z1 = [1,0,0,0,1,0,0,0,1] p2 =0.7, z2 = [0,0,1,0,0,1,1,0,0] p3 =0.1, z3 = [1,0,0,0,1,0,0,1,0]</p>
    <p>=[.3,0, .7,0, .3, .7, .7, .1, .2].</p>
    <p>M</p>
    <p>deep-spin.github.io/tutorial 81</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH() SparseMAP arg max</p>
    <p>M 1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH() SparseMAP arg max</p>
    <p>M 1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH() SparseMAP arg max</p>
    <p>M 1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH() SparseMAP arg max</p>
    <p>M 1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH()</p>
    <p>SparseMAP arg max M</p>
    <p>1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH()</p>
    <p>SparseMAP arg max M</p>
    <p>1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH()</p>
    <p>SparseMAP arg max M</p>
    <p>1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 82</p>
  </div>
  <div class="page">
    <p>Algorithms for speci c structures Best structure (MAP) Marginals</p>
    <p>Sequence tagging Viterbi[Rabiner, 1989] Forward-Backward</p>
    <p>[Rabiner, 1989]</p>
    <p>Cons tuent trees CKY</p>
    <p>[Kasami, 1966, Younger, 1967] [Cocke and Schwartz, 1970]</p>
    <p>Inside-Outside [Baker, 1979]</p>
    <p>Temporal alignments DTW[Sakoe and Chiba, 1978] So -DTW</p>
    <p>[Cuturi and Blondel, 2017]</p>
    <p>Dependency trees Max. Spanning Arborescence[Chu and Liu, 1965, Edmonds, 1967] Matrix-Tree [Kirchho , 1847]</p>
    <p>Assignments Kuhn-Munkres[Kuhn, 1955, Jonker and Volgenant, 1987] #P-complete</p>
    <p>[Valiant, 1979, Taskar, 2004]</p>
    <p>deep-spin.github.io/tutorial 83</p>
  </div>
  <div class="page">
    <p>Algorithms for speci c structures Best structure (MAP) Marginals</p>
    <p>Sequence tagging Viterbi[Rabiner, 1989] Forward-Backward</p>
    <p>[Rabiner, 1989]</p>
    <p>Cons tuent trees CKY</p>
    <p>[Kasami, 1966, Younger, 1967] [Cocke and Schwartz, 1970]</p>
    <p>Inside-Outside [Baker, 1979]</p>
    <p>Temporal alignments DTW[Sakoe and Chiba, 1978] So -DTW</p>
    <p>[Cuturi and Blondel, 2017]</p>
    <p>Dependency trees Max. Spanning Arborescence[Chu and Liu, 1965, Edmonds, 1967] Matrix-Tree [Kirchho , 1847]</p>
    <p>Assignments Kuhn-Munkres[Kuhn, 1955, Jonker and Volgenant, 1987] #P-complete</p>
    <p>[Valiant, 1979, Taskar, 2004]</p>
    <p>dyn. prog.</p>
    <p>deep-spin.github.io/tutorial 83</p>
  </div>
  <div class="page">
    <p>Deriva ves of marginals 1: DP Dynamic programming: marginals by Forward-Backward, Inside-Outside, etc.</p>
    <p>Alg. consists of di eren able ops: PyTorch autograd can handle it! (v. bad idea)  Be er book-keeping: Li and Eisner [2009], Mensch and Blondel [2018]  With circular dependencies, this breaks! Can get an approxima onStoyanov et al. [2011] Marginals in a sequence tagging model. 1 input: d tags, n tokens, U  Rnd,V  Rdd 2 ini alize 1 =0,n =0 3 for i 2, . . . ,n do # forward log-probabili es 4 i,k = log</p>
    <p>k exp  i1,k +(U)i,k +(V)k,k</p>
    <p>for all k</p>
    <p>k exp  i+1,k +(U)i+1,k +(V)k,k</p>
    <p>for all k</p>
    <p>k expn,k # par on func on 8 return =exp</p>
    <p>+ log Z</p>
    <p># marginals</p>
    <p>(U)1,NN</p>
    <p>(U)1,VB</p>
    <p>(U)1,PR</p>
    <p>(U)2,NN</p>
    <p>(U)2,VB</p>
    <p>(U)2,PR</p>
    <p>V</p>
    <p>(U)n,NN</p>
    <p>(U)n,VB</p>
    <p>(U)n,PR</p>
    <p>deep-spin.github.io/tutorial 84</p>
  </div>
  <div class="page">
    <p>Deriva ves of marginals 1: DP Dynamic programming: marginals by Forward-Backward, Inside-Outside, etc.</p>
    <p>Alg. consists of di eren able ops: PyTorch autograd can handle it! (v. bad idea)  Be er book-keeping: Li and Eisner [2009], Mensch and Blondel [2018]  With circular dependencies, this breaks! Can get an approxima onStoyanov et al. [2011]</p>
    <p>Marginals in a sequence tagging model. 1 input: d tags, n tokens, U  Rnd,V  Rdd 2 ini alize 1 =0,n =0 3 for i 2, . . . ,n do # forward log-probabili es 4 i,k = log</p>
    <p>k exp  i1,k +(U)i,k +(V)k,k</p>
    <p>for all k</p>
    <p>k exp  i+1,k +(U)i+1,k +(V)k,k</p>
    <p>for all k</p>
    <p>k expn,k # par on func on 8 return =exp</p>
    <p>+ log Z</p>
    <p># marginals</p>
    <p>(U)1,NN</p>
    <p>(U)1,VB</p>
    <p>(U)1,PR</p>
    <p>(U)2,NN</p>
    <p>(U)2,VB</p>
    <p>(U)2,PR</p>
    <p>V</p>
    <p>(U)n,NN</p>
    <p>(U)n,VB</p>
    <p>(U)n,PR</p>
    <p>deep-spin.github.io/tutorial 84</p>
  </div>
  <div class="page">
    <p>Deriva ves of marginals 1: DP Dynamic programming: marginals by Forward-Backward, Inside-Outside, etc.  Alg. consists of di eren able ops: PyTorch autograd can handle it! (v. bad idea)</p>
    <p>Be er book-keeping: Li and Eisner [2009], Mensch and Blondel [2018]  With circular dependencies, this breaks! Can get an approxima onStoyanov et al. [2011]</p>
    <p>Marginals in a sequence tagging model. 1 input: d tags, n tokens, U  Rnd,V  Rdd 2 ini alize 1 =0,n =0 3 for i 2, . . . ,n do # forward log-probabili es 4 i,k = log</p>
    <p>k exp  i1,k +(U)i,k +(V)k,k</p>
    <p>for all k</p>
    <p>k exp  i+1,k +(U)i+1,k +(V)k,k</p>
    <p>for all k</p>
    <p>k expn,k # par on func on 8 return =exp</p>
    <p>+ log Z</p>
    <p># marginals</p>
    <p>(U)1,NN</p>
    <p>(U)1,VB</p>
    <p>(U)1,PR</p>
    <p>(U)2,NN</p>
    <p>(U)2,VB</p>
    <p>(U)2,PR</p>
    <p>V</p>
    <p>(U)n,NN</p>
    <p>(U)n,VB</p>
    <p>(U)n,PR</p>
    <p>deep-spin.github.io/tutorial 84</p>
  </div>
  <div class="page">
    <p>Deriva ves of marginals 1: DP Dynamic programming: marginals by Forward-Backward, Inside-Outside, etc.  Alg. consists of di eren able ops: PyTorch autograd can handle it! (v. bad idea)  Be er book-keeping: Li and Eisner [2009], Mensch and Blondel [2018]</p>
    <p>With circular dependencies, this breaks! Can get an approxima onStoyanov et al. [2011]</p>
    <p>Marginals in a sequence tagging model. 1 input: d tags, n tokens, U  Rnd,V  Rdd 2 ini alize 1 =0,n =0 3 for i 2, . . . ,n do # forward log-probabili es 4 i,k = log</p>
    <p>k exp  i1,k +(U)i,k +(V)k,k</p>
    <p>for all k</p>
    <p>k exp  i+1,k +(U)i+1,k +(V)k,k</p>
    <p>for all k</p>
    <p>k expn,k # par on func on 8 return =exp</p>
    <p>+ log Z</p>
    <p># marginals</p>
    <p>(U)1,NN</p>
    <p>(U)1,VB</p>
    <p>(U)1,PR</p>
    <p>(U)2,NN</p>
    <p>(U)2,VB</p>
    <p>(U)2,PR</p>
    <p>V</p>
    <p>(U)n,NN</p>
    <p>(U)n,VB</p>
    <p>(U)n,PR</p>
    <p>deep-spin.github.io/tutorial 84</p>
  </div>
  <div class="page">
    <p>Deriva ves of marginals 1: DP Dynamic programming: marginals by Forward-Backward, Inside-Outside, etc.  Alg. consists of di eren able ops: PyTorch autograd can handle it! (v. bad idea)  Be er book-keeping: Li and Eisner [2009], Mensch and Blondel [2018]  With circular dependencies, this breaks! Can get an approxima onStoyanov et al. [2011] Marginals in a sequence tagging model. 1 input: d tags, n tokens, U  Rnd,V  Rdd 2 ini alize 1 =0,n =0 3 for i 2, . . . ,n do # forward log-probabili es 4 i,k = log</p>
    <p>k exp  i1,k +(U)i,k +(V)k,k</p>
    <p>for all k</p>
    <p>k exp  i+1,k +(U)i+1,k +(V)k,k</p>
    <p>for all k</p>
    <p>k expn,k # par on func on 8 return =exp</p>
    <p>+ log Z</p>
    <p># marginals</p>
    <p>(U)1,NN</p>
    <p>(U)1,VB</p>
    <p>(U)1,PR</p>
    <p>(U)2,NN</p>
    <p>(U)2,VB</p>
    <p>(U)2,PR</p>
    <p>V</p>
    <p>(U)n,NN</p>
    <p>(U)n,VB</p>
    <p>(U)n,PR</p>
    <p>deep-spin.github.io/tutorial 84</p>
  </div>
  <div class="page">
    <p>Deriva ves of marginals 2: Matrix-Tree</p>
    <p>L(s): Laplacian of the edge score graph</p>
    <p>Z=detL(s) =L(s)1</p>
    <p>=L1 =L1  L</p>
    <p>L1</p>
    <p>deep-spin.github.io/tutorial 85</p>
  </div>
  <div class="page">
    <p>Structured A en on Networks</p>
    <p>la</p>
    <p>coali on</p>
    <p>...</p>
    <p>aide</p>
    <p>input x</p>
    <p>output y</p>
    <p>CRF marginals (from forwardbackward) give a en on weights (0,1) Similar idea for projec ve dependency trees with insideoutside</p>
    <p>and non-projec ve with the Matrix-Tree theorem [Liu and Lapata, 2018].</p>
    <p>[Kim et al., 2017]</p>
    <p>deep-spin.github.io/tutorial 86</p>
  </div>
  <div class="page">
    <p>Structured A en on Networks  input</p>
    <p>x output</p>
    <p>y</p>
    <p>CRF marginals (from forwardbackward) give a en on weights (0,1) Similar idea for projec ve dependency trees with insideoutside</p>
    <p>and non-projec ve with the Matrix-Tree theorem [Liu and Lapata, 2018].</p>
    <p>[Kim et al., 2017]</p>
    <p>deep-spin.github.io/tutorial 86</p>
  </div>
  <div class="page">
    <p>Structured A en on Networks  input</p>
    <p>x output</p>
    <p>y</p>
    <p>(i): score of word i receiving a en on</p>
    <p>(i, i+1): score of consecu ve words receiving a en on</p>
    <p>(i): probability of word i ge ng a en on</p>
    <p>CRF marginals (from forwardbackward) give a en on weights (0,1) Similar idea for projec ve dependency trees with insideoutside</p>
    <p>and non-projec ve with the Matrix-Tree theorem [Liu and Lapata, 2018].</p>
    <p>[Kim et al., 2017]</p>
    <p>deep-spin.github.io/tutorial 86</p>
  </div>
  <div class="page">
    <p>Structured A en on Networks  input</p>
    <p>x output</p>
    <p>y</p>
    <p>(i): score of word i receiving a en on</p>
    <p>(i, i+1): score of consecu ve words receiving a en on</p>
    <p>(i): probability of word i ge ng a en on</p>
    <p>CRF marginals (from forwardbackward) give a en on weights (0,1)</p>
    <p>Similar idea for projec ve dependency trees with insideoutside</p>
    <p>and non-projec ve with the Matrix-Tree theorem [Liu and Lapata, 2018].</p>
    <p>[Kim et al., 2017]</p>
    <p>deep-spin.github.io/tutorial 86</p>
  </div>
  <div class="page">
    <p>Structured A en on Networks  input</p>
    <p>x output</p>
    <p>y</p>
    <p>dog on wheels</p>
    <p>(dogon): arc score (tree constraints)</p>
    <p>(dogon): probability of arc</p>
    <p>CRF marginals (from forwardbackward) give a en on weights (0,1) Similar idea for projec ve dependency trees with insideoutside</p>
    <p>and non-projec ve with the Matrix-Tree theorem [Liu and Lapata, 2018].</p>
    <p>[Kim et al., 2017]</p>
    <p>deep-spin.github.io/tutorial 86</p>
  </div>
  <div class="page">
    <p>Structured A en on Networks  input</p>
    <p>x output</p>
    <p>y</p>
    <p>dog on wheels</p>
    <p>(dogon): arc score (tree constraints)</p>
    <p>(dogon): probability of arc</p>
    <p>CRF marginals (from forwardbackward) give a en on weights (0,1) Similar idea for projec ve dependency trees with insideoutside and non-projec ve with the Matrix-Tree theorem [Liu and Lapata, 2018].</p>
    <p>[Kim et al., 2017]</p>
    <p>deep-spin.github.io/tutorial 86</p>
  </div>
  <div class="page">
    <p>Di eren able Perturb &amp; Parse Extending Gumbel-So max to structured stochas c models</p>
    <p>Forward pass: sample structure z (approximately) z=arg max</p>
    <p>zZ (+)z</p>
    <p>Backward pass: pretend we did marginal inference =arg max</p>
    <p>M (+)z+ H()</p>
    <p>(or some similar relaxa on)</p>
    <p>z</p>
    <p>[Corro and Titov, 2019a,b]</p>
    <p>deep-spin.github.io/tutorial 87</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:</p>
    <p>Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious.</p>
    <p>deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,</p>
    <p>(Structured A en on Networks:) All computa ons exact. Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious.</p>
    <p>deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious.</p>
    <p>deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious.</p>
    <p>deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious.</p>
    <p>deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.</p>
    <p>Case-by-case algorithms required, can get tedious.</p>
    <p>deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious. deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious. deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>Back-propaga ng through marginals Pros:  Familiar algorithms for NLPers,  (Structured A en on Networks:) All computa ons exact.</p>
    <p>Cons:</p>
    <p>(Structured A en on Networks:) forward pass marginals are dense; ( xed by Perturb &amp; MAP, at cost of rough approxima on)</p>
    <p>E cient &amp; numerically stable back-propaga on through DPs is tricky; (somewhat alleviated by Mensch and Blondel [2018])</p>
    <p>Not applicable when marginals are unavailable.  Case-by-case algorithms required, can get tedious. deep-spin.github.io/tutorial 88</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH()</p>
    <p>SparseMAP arg max M</p>
    <p>1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 89</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>argmax arg max p</p>
    <p>ps</p>
    <p>so max arg max p</p>
    <p>ps+H(p)</p>
    <p>sparsemax arg max p</p>
    <p>ps1/2p2</p>
    <p>MAP arg max M</p>
    <p>marginals arg max M</p>
    <p>+eH() SparseMAP arg max</p>
    <p>M 1/22</p>
    <p>M</p>
    <p>Just like so max relaxes argmax, marginals relax MAP di eren ably!</p>
    <p>Unlike argmax/so max, computa on is not obvious!</p>
    <p>deep-spin.github.io/tutorial 89</p>
  </div>
  <div class="page">
    <p>SparseMAP solu on [Niculae et al., 2018a]</p>
    <p>= arg max M</p>
    <p>1/22</p>
    <p>= = .6 + .4 ( is unique, but may have mul ple decomposi ons p. Ac ve Set recovers a sparse one.)</p>
    <p>deep-spin.github.io/tutorial 90</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p</p>
    <p>Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p</p>
    <p>Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p</p>
    <p>Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM</p>
    <p>update the (sparse) coe cients of p</p>
    <p>Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM</p>
    <p>update the (sparse) coe cients of p</p>
    <p>Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e</p>
    <p>Ac ve Set achieves nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p  Update rules: vanilla, away-step, pairwise</p>
    <p>Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p  Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p  Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e</p>
    <p>Ac ve Set achieves nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p  Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p  Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>Algorithms for SparseMAP  =arg max</p>
    <p>M 1/22</p>
    <p>Condi onal Gradient [Frank and Wolfe, 1956, Lacoste-Julien and Jaggi, 2015]</p>
    <p>select a new corner ofM  update the (sparse) coe cients of p  Update rules: vanilla, away-step, pairwise  Quadra c objec ve: Ac ve Set a.k.a. Min-Norm Point, [Wolfe, 1976] [Mar ns et al., 2015, Nocedal and Wright, 1999,</p>
    <p>Vinyes and Obozinski, 2017]</p>
    <p>Backward pass</p>
    <p>is sparse</p>
    <p>compu ng</p>
    <p>dy</p>
    <p>takes O(dim()nnz(p))</p>
    <p>quadra c objec velinear constraints (alas, exponen ally many!)</p>
    <p>arg max M</p>
    <p>((t1))  e Ac ve Set achieves</p>
    <p>nite &amp; linear convergence!</p>
    <p>Completely modular: just add MAP</p>
    <p>deep-spin.github.io/tutorial 91</p>
  </div>
  <div class="page">
    <p>[Chen et al., 2017]</p>
    <p>a</p>
    <p>gentleman</p>
    <p>overlooking</p>
    <p>a</p>
    <p>neighborhood</p>
    <p>situation</p>
    <p>.</p>
    <p>a</p>
    <p>po lic e</p>
    <p>of fic er</p>
    <p>w at ch es a</p>
    <p>si tu at io n</p>
    <p>cl os el y . a</p>
    <p>po lic e</p>
    <p>of fic er</p>
    <p>w at ch es a</p>
    <p>si tu at io n</p>
    <p>cl os el y .</p>
  </div>
  <div class="page">
    <p>[Niculae et al., 2018a]</p>
    <p>a</p>
    <p>po lic e</p>
    <p>of fic er</p>
    <p>w at ch es a</p>
    <p>si tu at io n</p>
    <p>cl os el y .</p>
    <p>a</p>
    <p>gentleman</p>
    <p>overlooking</p>
    <p>a</p>
    <p>neighborhood</p>
    <p>situation</p>
    <p>.</p>
    <p>A</p>
    <p>gentleman</p>
    <p>overlooking</p>
    <p>a</p>
    <p>neighborhood</p>
    <p>situa on</p>
    <p>.</p>
    <p>A</p>
    <p>police</p>
    <p>o cer</p>
    <p>watches</p>
    <p>a</p>
    <p>situa on</p>
    <p>closely</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE  Straight-Through Gumbel</p>
    <p>(Perturb &amp; MAP)</p>
    <p>SparseMAP</p>
    <p>L  arg maxz (z | x)</p>
    <p>Straight-Through  SPIGOT</p>
    <p>L  E(z|x)[z]</p>
    <p>Structured A n. Nets  SparseMAP</p>
    <p>And more, a er the break!</p>
    <p>deep-spin.github.io/tutorial 94</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y</p>
    <p>(z)</p>
    <p>(z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3 SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3 SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3 SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by z</p>
    <p>sum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3 SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by z</p>
    <p>sum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3 SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?</p>
    <p>hH</p>
    <p>E  L(z)</p>
    <p>idea 1</p>
    <p>(z)  exp  f(z)</p>
    <p>so max</p>
    <p>idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1</p>
    <p>(z)  exp  f(z)</p>
    <p>so max</p>
    <p>idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1</p>
    <p>(z)  exp  f(z)</p>
    <p>so max</p>
    <p>idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2</p>
    <p>(z)=1 if z=MAP(f()) else 0 argmax</p>
    <p>idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.</p>
    <p>STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3</p>
    <p>SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.</p>
    <p>STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>Ez  L(z)  =  zZ</p>
    <p>L  y(z)  (z | x)</p>
    <p>How to de ne ?  hH</p>
    <p>E  L(z)</p>
    <p>idea 1 (z)  exp</p>
    <p>f(z)</p>
    <p>so max idea 2 (z)=1 if z=MAP(f()) else 0 argmax idea 3 SparseMAP</p>
    <p>e.g., a TreeLSTM de ned by zsum over all possible trees</p>
    <p>parsing model, using some scorer f(z;x)</p>
    <p>Exponen ally large sum!</p>
    <p>All methods weve seen require sampling; hard in general.STE / SPIGOT relax y in backward.</p>
    <p>deep-spin.github.io/tutorial 95</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>= .7    + .3</p>
    <p>+0    + ... E[L(z)]= .7 L(   )+ .3 L(    )</p>
    <p>recall our shorthand L(z) =L(y(z),y)</p>
    <p>deep-spin.github.io/tutorial 96</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>= .7    + .3    +0    + ...</p>
    <p>E[L(z)]= .7 L(   )+ .3 L(    )</p>
    <p>recall our shorthand L(z) =L(y(z),y)</p>
    <p>deep-spin.github.io/tutorial 96</p>
  </div>
  <div class="page">
    <p>Structured latent variables without sampling</p>
    <p>= .7    + .3    +0    + ... E[L(z)]= .7 L(   )+ .3 L(    )</p>
    <p>recall our shorthand L(z) =L(y(z),y)</p>
    <p>deep-spin.github.io/tutorial 96</p>
  </div>
  <div class="page">
    <p>[Corro and Titov, 2019b]</p>
    <p>Stanford Sen ment (Accuracy) Socher et al Bigram Naive Bayes 83.1 [Niculae et al., 2018b] TreeLSTM w/ CoreNLP 83.2 TreeLSTM w/ SparseMAP 84.7 [Corro and Titov, 2019b] GCN w/ CoreNLP 83.8 GCN w/ Perturb-and-MAP 84.6</p>
    <p>Stanford Natural Language Inference (Accuracy) [Kim et al., 2017] Simple A en on 86.2 Structured A en on 86.8 [Liu and Lapata, 2018] 100D SAN - 86.8 Yogatama et al 100D RL-SPINN 80.5 [Choi et al., 2018] 100D ST Gumbel-Tree 82.6 300D - 85.6 600D - 86.0 [Corro and Titov, 2019b] Latent Tree + 1 GCN - 85.2 Latent Tree + 2 GCN - 86.2</p>
    <p>deep-spin.github.io/tutorial 97</p>
  </div>
  <div class="page">
    <p>V. Conclusions</p>
  </div>
  <div class="page">
    <p>Is it syntax?!</p>
    <p>Unlike e.g. unsupervised parsing, the structures we learn are guided by a downstream objec ve (typically discrimina ve).  They dont typically resemble gramma cal structure (yet) [Williams et al., 2018]</p>
    <p>(future work: more induc ve biases and constraints?)</p>
    <p>Common to compare latent structures with parser outputs. But is this always a meaningful comparison?</p>
    <p>deep-spin.github.io/tutorial 98</p>
  </div>
  <div class="page">
    <p>Is it syntax?!</p>
    <p>Unlike e.g. unsupervised parsing, the structures we learn are guided by a downstream objec ve (typically discrimina ve).  They dont typically resemble gramma cal structure (yet) [Williams et al., 2018]</p>
    <p>(future work: more induc ve biases and constraints?)  Common to compare latent structures with parser outputs.</p>
    <p>But is this always a meaningful comparison?</p>
    <p>deep-spin.github.io/tutorial 98</p>
  </div>
  <div class="page">
    <p>Syntax vs. Composi on Order [Niculae et al., 2018b]</p>
    <p>p=22.6%</p>
    <p>lovely and poignant .</p>
    <p>CoreNLP parse, p=21.4%</p>
    <p>lovely and poignant .</p>
    <p>deep-spin.github.io/tutorial 99</p>
  </div>
  <div class="page">
    <p>Syntax vs. Composi on Order [Niculae et al., 2018b]</p>
    <p>p=22.6%</p>
    <p>lovely and poignant .</p>
    <p>CoreNLP parse, p=21.4%</p>
    <p>lovely and poignant .</p>
    <p>deep-spin.github.io/tutorial 99</p>
  </div>
  <div class="page">
    <p>Syntax vs. Composi on Order [Niculae et al., 2018b]</p>
    <p>p=22.6%</p>
    <p>lovely and poignant .</p>
    <p>CoreNLP parse, p=21.4%</p>
    <p>lovely and poignant .</p>
    <p>p=15.33%</p>
    <p>a deep and meaningful lm . 1.0</p>
    <p>p=15.27%</p>
    <p>a deep and meaningful lm .</p>
    <p>CoreNLP parse, p=0%</p>
    <p>a deep and meaningful lm .</p>
    <p>deep-spin.github.io/tutorial 100</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCE  Straight-Through Gumbel</p>
    <p>(Perturb &amp; MAP)  SparseMAP</p>
    <p>L  arg maxz (z | x)</p>
    <p>Straight-Through  SPIGOT</p>
    <p>L  E(z|x)[z]</p>
    <p>Structured A n. Nets  SparseMAP</p>
    <p>And more, a er the break!</p>
    <p>deep-spin.github.io/tutorial 101</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>E(z|x)  L(z)</p>
    <p>REINFORCESPL  Straight-Through Gumbel</p>
    <p>(Perturb &amp; MAP)SPL,MRG</p>
    <p>SparseMAPMAP+</p>
    <p>L  arg maxz (z | x)</p>
    <p>Straight-ThroughMAP,MRG  SPIGOTMAP+</p>
    <p>L  E(z|x)[z]</p>
    <p>Structured A n. NetsMRG  SparseMAPMAP+</p>
    <p>Computa on:</p>
    <p>SPL: Sampling. (Simple in incremental/unstructured, hard for most global structures.) MAP: Finding the highest-scoring structure. MRG: Marginal inference.</p>
    <p>And more, a er the break!</p>
    <p>deep-spin.github.io/tutorial 101</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Latent structure models are desirable for interpretability, structural bias, and higher predic ve power with fewer parameters.  Stochas c latent variables can be dealt with RL or straight-through gradients.  Determinis c argmax requires surrogate gradients (e.g. SPIGOT).  Con nuous relaxa ons of argmax include SANs and SparseMAP.  Intui vely, some of these di erent methods are trying to do similar things or require the same building blocks (e.g. SPIGOT and SparseMAP).</p>
    <p>... we didnt even get into deep genera ve models! These tools apply, but there are new challenges. [Corro and Titov, 2019a, Kim et al., 2019a,b, Kawakami et al., 2019]</p>
    <p>deep-spin.github.io/tutorial 102</p>
  </div>
  <div class="page">
    <p>References I Ryan Adams. The gumbel-max trick for discrete distribu ons, 2013. URL</p>
    <p>https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/. Blog post. James K Baker. Trainable grammars for speech recogni on. The Journal of the Acous cal Society of America, 65(S1):S132S132, 1979.</p>
    <p>Yoshua Bengio, Nicholas Lonard, and Aaron Courville. Es ma ng or propaga ng gradients through stochas c neurons for condi onal computa on. arXiv preprint arXiv:1308.3432, 2013.</p>
    <p>Mathieu Blondel, Andr FT Mar ns, and Vlad Niculae. Learning classi ers with Fenchel-Young losses: Generalized entropies, margins, and algorithms. In Proc. of AISTATS, 2019.</p>
    <p>Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and Christopher Po s. A fast uni ed model for parsing and sentence understanding. In Proc. of ACL, 2016. doi: 10.18653/v1/P16-1139.</p>
    <p>Stephen Boyd and Lieven Vandenberghe. Convex op miza on. Cambridge University Press, 2004. Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. The mathema cs of sta s cal machine transla on: Parameter es ma on. Computa onal Linguis cs, 19(2):263311, 1993.</p>
    <p>Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced LSTM for natural language inference. In Proc. of ACL, 2017.</p>
    <p>Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to compose task-speci c tree structures. In Proc. of AAAI, 2018.</p>
    <p>deep-spin.github.io/tutorial 103</p>
  </div>
  <div class="page">
    <p>References II Yoeng-Jin Chu and Tseng-Hong Liu. On the shortest arborescence of a directed graph. Science Sinica, 14:13961400, 1965. William John Cocke and Jacob T Schwartz. Programming languages and their compilers. Courant Ins tute of Mathema cal Sciences., 1970.</p>
    <p>Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. Spectral learning of latent-variable PCFGs. In Proc. of ACL, 2012.</p>
    <p>Caio Corro and Ivan Titov. Di eren able Perturb-and-Parse: Semi-Supervised Parsing with a Structured Varia onal Autoencoder. In Proc. of ICLR, 2019a.</p>
    <p>Caio Corro and Ivan Titov. Learning latent trees with stochas c perturba ons and di eren able dynamic programming. In Proc. of ACL, 2019b.</p>
    <p>Marco Cuturi and Mathieu Blondel. So -DTW: a di eren able loss func on for me-series. In Proc. of ICML, 2017. Jack Edmonds. Op mum branchings. J. Res. Nat. Bur. Stand., 71B:233240, 1967. Marguerite Frank and Philip Wolfe. An algorithm for quadra c programming. Nav. Res. Log., 3(1-2):95110, 1956. Serhii Havrylov, Germn Kruszewski, and Armand Joulin. Coopera ve Learning of Disjoint Syntax and Seman cs. In Proc.</p>
    <p>NAACL-HLT, 2019. Geo rey Hinton. Neural networks for machine learning. In Coursera video lectures, 2012. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameteriza on with Gumbel-so max. In Proc. of ICLR, 2017.</p>
    <p>deep-spin.github.io/tutorial 104</p>
  </div>
  <div class="page">
    <p>References III Roy Jonker and Anton Volgenant. A shortest augmen ng path algorithm for dense and sparse linear assignment problems.</p>
    <p>Compu ng, 38(4):325340, 1987. Tadao Kasami. An e cient recogni on and syntax-analysis algorithm for context-free languages. Coordinated Science Laboratory</p>
    <p>Report no. R-257, 1966. Kazuya Kawakami, Chris Dyer, and Phil Blunsom. Learning to discover, ground and use words with segmental neural language models. In Proc. of ACL, 2019.</p>
    <p>Yoon Kim, Carl Denton, Loung Hoang, and Alexander M Rush. Structured a en on networks. In Proc. of ICLR, 2017. Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilis c context-free grammars for grammar induc on. In Proc. of</p>
    <p>ACL, 2019a. Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gbor Melis. Unsupervised recurrent neural network grammars. In Proc. of NAACL-HLT, 2019b.</p>
    <p>Diederik P Kingma and Max Welling. Auto-encoding Varia onal Bayes. 2014. Gustav Kirchho . Ueber die au sung der gleichungen, auf welche man bei der untersuchung der linearen vertheilung galvanischer strme gefhrt wird. Annalen der Physik, 148(12):497508, 1847.</p>
    <p>Harold W Kuhn. The Hungarian method for the assignment problem. Nav. Res. Log., 2(1-2):8397, 1955. Simon Lacoste-Julien and Mar n Jaggi. On the global linear convergence of Frank-Wolfe op miza on variants. In Proc. of</p>
    <p>NeurIPS, 2015.</p>
    <p>deep-spin.github.io/tutorial 105</p>
  </div>
  <div class="page">
    <p>References IV Zhifei Li and Jason Eisner. First-and second-order expecta on semirings with applica ons to minimum-risk training on transla on forests. In Proc. of EMNLP, 2009.</p>
    <p>Yang Liu and Mirella Lapata. Learning structured text representa ons. TACL, 6:6375, 2018. Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribu on: A con nuous relaxa on of discrete random variables. In Proc. of ICLR, 2016.</p>
    <p>Jean Maillard and Stephen Clark. Latent tree learning with di eren able parsers: Shi -Reduce parsing and chart parsing. arXiv preprint arXiv:1806.00840, 2018.</p>
    <p>Chaitanya Malaviya, Pedro Ferreira, and Andr FT Mar ns. Sparse and constrained a en on for neural machine transla on. In Proc. of ACL, 2018.</p>
    <p>Andr FT Mar ns and Ramn Fernandez Astudillo. From so max to sparsemax: A sparse model of a en on and mul -label classi ca on. In Proc. of ICML, 2016.</p>
    <p>Andr FT Mar ns and Julia Kreutzer. Learning whats easy: Fully di eren able neural easy- rst taggers. In Proc. of EMNLP, 2017. Andr FT Mar ns and Vlad Niculae. Notes on latent structure models and SPIGOT. preprint arXiv:1907.10348, 2019. Andr FT Mar ns, Mrio AT Figueiredo, Pedro MQ Aguiar, Noah A Smith, and Eric P Xing. AD3: Alterna ng direc ons dual decomposi on for MAP inference in graphical models. JMLR, 16(1):495545, 2015.</p>
    <p>Arthur Mensch and Mathieu Blondel. Di eren able dynamic programming for structured predic on and a en on. In Proc. of ICML, 2018.</p>
    <p>deep-spin.github.io/tutorial 106</p>
  </div>
  <div class="page">
    <p>References V Nikita Nangia and Samuel Bowman. ListOps: A diagnos c dataset for latent tree learning. In Proc. of NAACL SRW, 2018. Vlad Niculae and Mathieu Blondel. A regularized framework for sparse and structured neural a en on. In Proc. of NeurIPS, 2017. Vlad Niculae, Andr FT Mar ns, Mathieu Blondel, and Claire Cardie. SparseMAP: Di eren able sparse structured inference. In</p>
    <p>Proc. of ICML, 2018a. Vlad Niculae, Andr FT Mar ns, and Claire Cardie. Towards dynamic computa on graphs via sparse latent structure. In Proc. of</p>
    <p>EMNLP, 2018b. Jorge Nocedal and Stephen Wright. Numerical Op miza on. Springer New York, 1999. George Papandreou and Alan L Yuille. Perturb-and-MAP random elds: Using discrete op miza on to learn and sample from energy models. In Proc. of ICCV, 2011.</p>
    <p>Hao Peng, Sam Thomson, and Noah A Smith. Backpropaga ng through structured argmax using a SPIGOT. In Proc. of ACL, 2018. Ben Peters, Vlad Niculae, and Andr FT Mar ns. Sparse sequence-to-sequence models. In Proc. of ACL, 2019. Slav Petrov and Dan Klein. Discrimina ve log-linear grammars with latent variables. In Advances in neural informa on processing</p>
    <p>systems, pages 11531160, 2008. Ariadna Qua oni, Sybor Wang, Louis-Philippe Morency, Michael Collins, and Trevor Darrell. Hidden condi onal random elds.</p>
    <p>IEEE Transac ons on Pa ern Analysis &amp; Machine Intelligence, 29(10):18481852, 2007.</p>
    <p>deep-spin.github.io/tutorial 107</p>
  </div>
  <div class="page">
    <p>References VI Lawrence R. Rabiner. A tutorial on Hidden Markov Models and selected applica ons in speech recogni on. P. IEEE, 77(2): 257286, 1989.</p>
    <p>Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm op miza on for spoken word recogni on. IEEE Trans. on Acous cs, Speech, and Sig. Proc., 26:4349, 1978.</p>
    <p>Veselin Stoyanov, Alexander Ropson, and Jason Eisner. Empirical risk minimiza on of graphical model parameters given approximate inference, decoding, and model structure. In Proc. of AISTATS, 2011.</p>
    <p>Ben Taskar. Learning structured predic on models: A large margin approach. PhD thesis, Stanford University, 2004. Constan no Tsallis. Possible generaliza on of Boltzmann-Gibbs sta s cs. Journal of Sta s cal Physics, 52:479487, 1988. Leslie G Valiant. The complexity of compu ng the permanent. Theor. Comput. Sci., 8(2):189201, 1979. Tim Vieira. Gumbel-max trick, 2014. URL</p>
    <p>https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/. Blog post. Marina Vinyes and Guillaume Obozinski. Fast column genera on for atomic norm regulariza on. In Proc. of AISTATS, 2017. Mar n J Wainwright and Michael I Jordan. Graphical models, exponen al families, and varia onal inference., volume 1. Now Publishers, Inc., 2008.</p>
    <p>Adina Williams, Andrew Drozdov, and Samuel R Bowman. Do latent tree learning models iden fy meaningful structure in sentences? TACL, 2018.</p>
    <p>deep-spin.github.io/tutorial 108</p>
  </div>
  <div class="page">
    <p>References VII Ronald J. Williams. Simple sta s cal gradient-following algorithms for connec onist reinforcement learning. Mach. Learn., 8, 1992.</p>
    <p>Philip Wolfe. Finding the nearest point in a polytope. Mathema cal Programming, 11(1):128149, 1976. Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenste e, and Wang Ling. Learning to compose words into sentences with reinforcement learning. In Proc. of ICLR, 2017.</p>
    <p>Daniel H Younger. Recogni on and parsing of context-free languages in me n3. Informa on and Control, 10(2):189208, 1967.</p>
    <p>deep-spin.github.io/tutorial 109</p>
  </div>
</Presentation>
