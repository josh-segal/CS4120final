<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Daniel Crankshaw Xin Wang, Giulio Zhou,</p>
    <p>Michael Franklin, Joseph Gonzalez, Ion Stoica</p>
    <p>NSDI 2017 March 29, 2017</p>
    <p>A Low-Latency Online Prediction Serving System</p>
    <p>Clipper</p>
  </div>
  <div class="page">
    <p>Big Data</p>
    <p>Complex Model</p>
    <p>Training</p>
    <p>Learning</p>
  </div>
  <div class="page">
    <p>Big Data</p>
    <p>Complex Model</p>
    <p>Training</p>
    <p>Learning</p>
  </div>
  <div class="page">
    <p>Learning Produces a Trained Model</p>
    <p>CAT</p>
    <p>Query Decision</p>
    <p>Model</p>
  </div>
  <div class="page">
    <p>Big Data</p>
    <p>Training</p>
    <p>Learning</p>
    <p>Application</p>
    <p>Decision</p>
    <p>Query</p>
    <p>?</p>
    <p>Serving</p>
    <p>Model</p>
  </div>
  <div class="page">
    <p>Big Data</p>
    <p>Training</p>
    <p>Learning</p>
    <p>Application</p>
    <p>Decision</p>
    <p>Query</p>
    <p>Model</p>
    <p>Prediction-Serving for interactive applications Timescale: ~10s of milliseconds</p>
    <p>Serving</p>
  </div>
  <div class="page">
    <p>Prediction-Serving Raises New Challenges</p>
  </div>
  <div class="page">
    <p>Prediction-Serving Challenges</p>
    <p>Support low-latency, highthroughput serving workloads</p>
    <p>??? Create VWCaffe 8</p>
    <p>Large and growing ecosystem of ML models and frameworks</p>
  </div>
  <div class="page">
    <p>Models getting more complex  10s of GFLOPs [1]</p>
    <p>Support low-latency, high-throughput serving workloads</p>
    <p>Using specialized hardware for predictions</p>
    <p>Deployed on critical path  Maintain SLOs under heavy load</p>
    <p>[1] Deep Residual Learning for Image Recognition. He et al. CVPR 2015.</p>
  </div>
  <div class="page">
    <p>Google Translate</p>
    <p>Serving</p>
    <p>[1] https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html</p>
    <p>(TPU)</p>
  </div>
  <div class="page">
    <p>Big Companies Build One-Off Systems</p>
    <p>Problems:  Expensive to build and maintain</p>
    <p>Highly specialized and require ML and systems expertise</p>
    <p>Tightly-coupled model and application  Difficult to change or update model</p>
    <p>Only supports single ML framework</p>
  </div>
  <div class="page">
    <p>Prediction-Serving Challenges</p>
    <p>Support low-latency, highthroughput serving workloads</p>
    <p>??? Create VWCaffe 12</p>
    <p>Large and growing ecosystem of ML models and frameworks</p>
  </div>
  <div class="page">
    <p>Large and growing ecosystem of ML models and frameworks</p>
    <p>???</p>
    <p>Content Rec.</p>
    <p>Fraud Detection</p>
    <p>Personal Asst.</p>
    <p>Robotic Control</p>
    <p>Machine Translation</p>
    <p>Create VW Caffe 13</p>
  </div>
  <div class="page">
    <p>???</p>
    <p>Content Rec.</p>
    <p>Fraud Detection</p>
    <p>Personal Asst.</p>
    <p>Robotic Control</p>
    <p>Machine Translation</p>
    <p>Create VW Caffe</p>
    <p>Varying physical resource requirements</p>
    <p>Difficult to deploy and brittle to manage</p>
    <p>Large and growing ecosystem of ML models and frameworks</p>
  </div>
  <div class="page">
    <p>But most companies cant build new</p>
    <p>serving systems</p>
  </div>
  <div class="page">
    <p>Big Data</p>
    <p>Batch Analytics</p>
    <p>Model</p>
    <p>Training</p>
    <p>Use existing systems: Offline Scoring</p>
  </div>
  <div class="page">
    <p>Big Data</p>
    <p>Model</p>
    <p>Training</p>
    <p>Batch Analytics</p>
    <p>Scoring X Y</p>
    <p>Datastore Use existing systems: Offline Scoring</p>
  </div>
  <div class="page">
    <p>Application</p>
    <p>Decision</p>
    <p>Query</p>
    <p>Look up decision in datastore</p>
    <p>Low-Latency Serving</p>
    <p>X Y</p>
    <p>Use existing systems: Offline Scoring</p>
  </div>
  <div class="page">
    <p>Application</p>
    <p>Decision</p>
    <p>Query</p>
    <p>Look up decision in datastore</p>
    <p>Low-Latency Serving</p>
    <p>X Y</p>
    <p>Problems:  Requires full set of queries ahead of time</p>
    <p>Small and bounded input domain  Wasted computation and space</p>
    <p>Can render and store unneeded predictions  Costly to update</p>
    <p>Re-run batch job</p>
    <p>Use existing systems: Offline Scoring</p>
  </div>
  <div class="page">
    <p>Prediction-Serving Challenges</p>
    <p>Support low-latency, highthroughput serving workloads</p>
    <p>??? Create VWCaffe 20</p>
    <p>Large and growing ecosystem of ML models and frameworks</p>
  </div>
  <div class="page">
    <p>How does Clipper address these challenges?</p>
  </div>
  <div class="page">
    <p>q Simplifies deployment through layered architecture</p>
    <p>q Serves many models across ML frameworks concurrently</p>
    <p>q Employs caching, batching, scale-out for high-performance serving</p>
    <p>Clipper Solutions</p>
  </div>
  <div class="page">
    <p>Clipper</p>
    <p>Predict FeedbackRPC/REST Interface</p>
    <p>Caffe MC MC MC</p>
    <p>RPC RPC RPC RPC</p>
    <p>Clipper Decouples Applications and Models</p>
    <p>Applications</p>
    <p>Model Container (MC)</p>
  </div>
  <div class="page">
    <p>Clipper Architecture</p>
    <p>Clipper</p>
    <p>Caffe</p>
    <p>Applications Predict ObserveRPC/REST Interface</p>
    <p>MC MC MC RPC RPC RPC RPC</p>
    <p>Model Abstraction Layer Provide a common interface to models while bounding latency and maximizing throughput.</p>
    <p>Model Selection LayerImprove accuracy through bandit methods and ensembles, online learning, and personalization</p>
    <p>Model Container (MC)</p>
  </div>
  <div class="page">
    <p>Clipper Architecture</p>
    <p>Clipper</p>
    <p>Caffe</p>
    <p>Applications Predict ObserveRPC/REST Interface</p>
    <p>MC MC MC RPC RPC RPC RPC</p>
    <p>Model Selection LayerSelection Policy</p>
    <p>Model Abstraction Layer Caching</p>
    <p>Adaptive Batching</p>
    <p>Model Container (MC)</p>
  </div>
  <div class="page">
    <p>Clipper Implementation</p>
    <p>Clipper</p>
    <p>Caffe</p>
    <p>Applications Predict ObserveRPC/REST Interface</p>
    <p>MC MC MC RPC RPC RPC RPC</p>
    <p>Model Abstraction Layer Caching</p>
    <p>Adaptive Batching</p>
    <p>Model Container (MC)</p>
    <p>ustCore system: 5000 lines of Rust RPC:  100 lines of Python  250 lines of Rust  200 lines of C++</p>
  </div>
  <div class="page">
    <p>Model Container (MC)</p>
    <p>Caffe</p>
    <p>Correction LayerCorrection Policy</p>
    <p>MC MC MC RPC RPC RPC</p>
    <p>Model Abstraction Layer Caching</p>
    <p>Adaptive Batching</p>
    <p>Provide a common interface to models while</p>
    <p>RPC</p>
  </div>
  <div class="page">
    <p>Correction LayerCorrection Policy</p>
    <p>Model Container (MC) RPC</p>
    <p>Caffe MC</p>
    <p>RPC MC</p>
    <p>RPC MC</p>
    <p>RPC</p>
    <p>Model Abstraction Layer Caching</p>
    <p>Adaptive Batching</p>
    <p>Common Interface  Simplifies Deployment:  Evaluate models using original code &amp; systems</p>
  </div>
  <div class="page">
    <p>Container-based Model Deployment</p>
    <p>class ModelContainer: def __init__(model_data) def predict_batch(inputs)</p>
    <p>Implement Model API:</p>
  </div>
  <div class="page">
    <p>class ModelContainer: def __init__(model_data) def predict_batch(inputs)</p>
    <p>Implement Model API:</p>
    <p>Implemented in many languages  Python  Java  C/C++</p>
    <p>Container-based Model Deployment</p>
  </div>
  <div class="page">
    <p>Model implementation packaged in container</p>
    <p>Model Container (MC)</p>
    <p>Container-based Model Deployment</p>
    <p>class ModelContainer: def __init__(model_data) def predict_batch(inputs)</p>
  </div>
  <div class="page">
    <p>Clipper</p>
    <p>Caffe MC MC MC</p>
    <p>RPC RPC RPC RPC Model Container (MC)</p>
    <p>Container-based Model Deployment</p>
  </div>
  <div class="page">
    <p>Correction LayerCorrection Policy</p>
    <p>Model Container (MC) RPC</p>
    <p>Caffe MC</p>
    <p>RPC MC</p>
    <p>RPC MC</p>
    <p>RPC</p>
    <p>Model Abstraction Layer Caching</p>
    <p>Adaptive Batching</p>
    <p>Common Interface  Simplifies Deployment:  Evaluate models using original code &amp; systems  Models run in separate processes as Docker containers</p>
    <p>Resource isolation</p>
  </div>
  <div class="page">
    <p>Correction LayerCorrection Policy</p>
    <p>Model Abstraction Layer Caching</p>
    <p>Adaptive Batching</p>
    <p>Model Container (MC) RPC</p>
    <p>Caffe MC</p>
    <p>RPC MC</p>
    <p>RPC MC</p>
    <p>RPC MC</p>
    <p>RPC MC</p>
    <p>RPC</p>
    <p>Common Interface  Simplifies Deployment:  Evaluate models using original code &amp; systems  Models run in separate processes as Docker containers</p>
    <p>Resource isolation  Scale-out</p>
    <p>Problem: frameworks optimized for batch processing not latency</p>
  </div>
  <div class="page">
    <p>A single page load may generate many queries</p>
    <p>Batching to Improve Throughput  Optimal batch depends on:</p>
    <p>hardware configuration  model and framework  system load</p>
    <p>Why batching helps:</p>
    <p>Hardware Acceleration</p>
    <p>Helps amortize system overhead</p>
  </div>
  <div class="page">
    <p>A single page load may generate many queries</p>
    <p>Clipper Solution:</p>
    <p>Adaptively tradeoff latency and throughput</p>
    <p>Inc. batch size until the latency objective is exceeded (Additive Increase)</p>
    <p>If latency exceeds SLO cut batch size by a fraction (Multiplicative Decrease)</p>
    <p>Why batching helps:</p>
    <p>Hardware Acceleration</p>
    <p>Helps amortize system overhead</p>
    <p>Optimal batch depends on:  hardware configuration  model and framework  system load</p>
    <p>Batching to Improve ThroughputAdaptive</p>
  </div>
  <div class="page">
    <p>Throughput (Queries Per Second)</p>
    <p>Tensor Flow Conv. Net (GPU)</p>
    <p>Batch Size</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Tensor Flow Conv. Net (GPU)</p>
    <p>Batch Size</p>
    <p>Latency (ms)</p>
    <p>Throughput (Queries Per Second)</p>
    <p>Better</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Tensor Flow Conv. Net (GPU)</p>
    <p>Batch Size</p>
    <p>Optimal Batch Size</p>
    <p>Latency (ms)</p>
    <p>Throughput (Queries Per Second)</p>
    <p>Better</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Ra ndR</p>
    <p>P ) Rre</p>
    <p>Vt</p>
    <p>(6K Oea</p>
    <p>rn)LLn ear</p>
    <p>(3y 6S</p>
    <p>arN ) LLn</p>
    <p>ear 69</p>
    <p>(6K Lea</p>
    <p>rn) Ke</p>
    <p>rne O 69</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>LRg Re</p>
    <p>gre VVL</p>
    <p>Rn</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>Throughput (QPS)</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Throughput (QPS)</p>
    <p>P ) Rre</p>
    <p>Vt</p>
    <p>(6K Oea</p>
    <p>rn) LLn ear</p>
    <p>(3y 6S</p>
    <p>arN )</p>
    <p>LLn ear</p>
    <p>(6K Lea</p>
    <p>rn) Ke</p>
    <p>rne O 69</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>LRg 5e</p>
    <p>gre VVL</p>
    <p>Rn</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>AdaStLve 1R BatFKLng Better</p>
  </div>
  <div class="page">
    <p>AdaStLve 1R BatFKLng</p>
    <p>P ) Rre</p>
    <p>Vt</p>
    <p>(6K Oea</p>
    <p>rn) LLn ear</p>
    <p>(3y 6S</p>
    <p>arN )</p>
    <p>LLn ear</p>
    <p>(6K Lea</p>
    <p>rn) Ke</p>
    <p>rne O 69</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>LRg 5e</p>
    <p>gre VVL</p>
    <p>Rn</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>P99 Latency (ms)</p>
    <p>Throughput (QPS)</p>
    <p>Better</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Throughput (QPS)</p>
    <p>AdaStLve 1R BatFKLng</p>
    <p>P ) Rre</p>
    <p>Vt</p>
    <p>(6K Oea</p>
    <p>rn) LLn ear</p>
    <p>(3y 6S</p>
    <p>arN )</p>
    <p>LLn ear</p>
    <p>(6K Lea</p>
    <p>rn) Ke</p>
    <p>rne O 69</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>LRg 5e</p>
    <p>gre VVL</p>
    <p>Rn</p>
    <p>(6K Lea</p>
    <p>rn)</p>
    <p>P99 Latency (ms)</p>
    <p>Batch Size</p>
  </div>
  <div class="page">
    <p>Overhead of decoupled architecture</p>
    <p>Clipper</p>
    <p>Predict FeedbackRPC/REST Interface</p>
    <p>Caffe MC MC MC</p>
    <p>RPC RPC RPC RPC</p>
    <p>Applications</p>
    <p>MC</p>
  </div>
  <div class="page">
    <p>TensorFlowServing</p>
    <p>Predict RPC Interface</p>
    <p>Applications</p>
    <p>Overhead of decoupled architecture</p>
    <p>Clipper Predict FeedbackRPC/REST Interface</p>
    <p>Caffe MC MC MC</p>
    <p>RPC RPC RPC RPC</p>
    <p>Applications</p>
    <p>MC</p>
  </div>
  <div class="page">
    <p>Clipper Predict FeedbackRPC/REST Interface</p>
    <p>Caffe MC MC MC</p>
    <p>RPC RPC RPC RPC</p>
    <p>Applications</p>
    <p>Model Container</p>
    <p>TensorFlowServing</p>
    <p>Predict RPC Interface</p>
    <p>Applications</p>
    <p>Overhead of decoupled architecture</p>
  </div>
  <div class="page">
    <p>Overhead of decoupled architecture</p>
    <p>Throughput (QPS)</p>
    <p>Better P99 Latency (ms)</p>
    <p>Better</p>
    <p>Model: AlexNet trained on CIFAR-10</p>
  </div>
  <div class="page">
    <p>Clipper Architecture</p>
    <p>Clipper</p>
    <p>Caffe</p>
    <p>Applications Predict ObserveRPC/REST Interface</p>
    <p>MC MC MC RPC RPC RPC RPC</p>
    <p>Model Abstraction Layer Provide a common interface to models while bounding latency and maximizing throughput.</p>
    <p>Model Selection LayerImprove accuracy through bandit methods and ensembles, online learning, and personalization</p>
    <p>Model Container (MC)</p>
  </div>
  <div class="page">
    <p>Clipper</p>
    <p>Ca ffe</p>
    <p>Version 1</p>
    <p>Version 2</p>
    <p>Version 3</p>
    <p>Periodic retraining</p>
    <p>Experiment with new models and frameworks</p>
    <p>Model Selection LayerImprove accuracy through bandit methods and ensembles, online learning, and personalization</p>
  </div>
  <div class="page">
    <p>Ca ffe</p>
    <p>CAT CAT CAT CAT</p>
    <p>CAT CONFIDENT</p>
    <p>Selection Policy: Estimate confidence</p>
    <p>P olicy</p>
    <p>Version 2</p>
    <p>Version 3</p>
  </div>
  <div class="page">
    <p>Ca ffe</p>
    <p>CAT MAN CAT SPACE</p>
    <p>CAT UNSURE</p>
    <p>Selection Policy: Estimate confidence</p>
    <p>P olicy</p>
  </div>
  <div class="page">
    <p>ensemEle 4-agree 5-agree 0.0</p>
    <p>-5 E</p>
    <p>rr Rr</p>
    <p>e</p>
    <p>Image1et</p>
    <p>cRnIident unsurecRnIident unsure</p>
    <p>Better</p>
    <p>Selection Policy: Estimate confidence</p>
  </div>
  <div class="page">
    <p>ensemEle 4-agree 5-agree 0.0</p>
    <p>-5 E</p>
    <p>rr Rr</p>
    <p>e</p>
    <p>Image1et</p>
    <p>cRnIident unsurecRnIident unsure</p>
    <p>Better</p>
    <p>width is percentage of</p>
    <p>query workloads</p>
    <p>Selection Policy: Estimate confidence</p>
  </div>
  <div class="page">
    <p>Clipper Model Selection LayerSelection Policy</p>
    <p>Selection policies supported by Clipper  Exploit multiple models to estimate confidence  Use multi-armed bandit algorithms to learn</p>
    <p>optimal model-selection online  Online personalization across ML frameworks</p>
    <p>*See paper for details</p>
  </div>
  <div class="page">
    <p>Conclusion  Prediction-serving is an important and challenging area for systems</p>
    <p>research  Support low-latency, high-throughput serving workloads  Serve large and growing ecosystem of ML frameworks</p>
    <p>Clipper is a first step towards addressing these challenges  Simplifies deployment through layered architecture  Serves many models across ML frameworks concurrently  Employs caching, adaptive batching, container scale-out to meet interactive</p>
    <p>serving workload demands  Beyond academic prototype to build a real, open-source system</p>
    <p>https://github.com/ucbrise/clipper crankshaw@cs.berkeley.edu</p>
  </div>
  <div class="page">
    <p>Kr Ru</p>
    <p>gK pu</p>
    <p>t (1</p>
    <p>ps ) Agg 10Gbps</p>
    <p>Agg 1Gbps 0ean 10Gbps 0ean 1Gbps</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(P s) 0ean 10Gbps</p>
    <p>GPU Cluster Scaling</p>
  </div>
</Presentation>
