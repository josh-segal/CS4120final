<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SECTOR: A Neural Model for Coherent Topic Segmentation and Classification</p>
    <p>Sebastian Arnold, Rudolf Schneider, Philippe Cudr-Mauroux*, Felix A. Gers, Alexander Lser</p>
    <p>sarnold@beuth-hochschule.de @sebastianarnold</p>
    <p>Transactions of the Association for Computational Linguistics (TACL) Vol.7</p>
    <p>ACL 2019, Florence, Italy 29.07.2019</p>
    <p>Beuth University of Applied Sciences Berlin, Germany</p>
    <p>*eXascale Infolab University of Fribourg Fribourg, Switzerland</p>
  </div>
  <div class="page">
    <p>Challenge: understand the topics and structure of a document</p>
    <p>Treatment</p>
    <p>Diagnosis</p>
    <p>Symptoms</p>
    <p>Causes</p>
    <p>Type 1 diabetesDISEASE How can we represent a document with respect to the authors emphasis?</p>
    <p>topical information [Ma18] (e.g. semantic class labels)</p>
    <p>structural information [Ag09, Gla16] (e.g. coherent passages)</p>
    <p>in latent vector space [Le14, Bha16] (i.e. distributional embedding)</p>
    <p>required for TDT, QA &amp; IR downstream tasks [All02, Di07, Coh18]</p>
  </div>
  <div class="page">
    <p>Task: split a document into coherent sections with topic labels</p>
    <p>We aim to detect topics in a document that are expressed by the author as a coherent sequence of sentences (e.g., a passage or book chapter).</p>
  </div>
  <div class="page">
    <p>WikiSection: Wiki authors provide topics as section headings</p>
    <p>https://github.com/sebastianarnold/WikiSection</p>
    <p>en_disease de_disease en_city de_city</p>
  </div>
  <div class="page">
    <p>SECTOR sequential prediction approach</p>
    <p>Transform a document of N sentences s1...N into N topic distributions y1...N  Predict M sections T1...M based on coherence of the networks weights  Assign section-level topic labels y1...M</p>
    <p>Number and length of sections is unknown!</p>
  </div>
  <div class="page">
    <p>Objective: maximize the log likelihood of model parameters  per document on sentence-level</p>
    <p>Requires the entire document as input</p>
    <p>Long range dependencies</p>
    <p>Focus on sharp distinction at topic shifts</p>
    <p>Network architecture (0/4)  Overview</p>
  </div>
  <div class="page">
    <p>Input: Vector representation of a full document</p>
    <p>Split text into sequence of sentences s1...N  Encode sentence vectors x1...N using</p>
    <p>Bag-of-words (~56k english words)  Bloom filter (4096 bits) [Se17] or  Pre-trained sentence embeddings</p>
    <p>[Mik13, Aro17] (128 dim)  Use sentences as time-steps</p>
    <p>Network architecture (1/4)  Sentence encoding</p>
  </div>
  <div class="page">
    <p>Encoder: Bidirectional Long Short-Term Memory (BLSTM) [Ho97, Ge00, Gra12] + dense embedding layer</p>
    <p>independent fw and bw parameters , helps to sharpen left/right context</p>
    <p>embedding layer captures latent topics</p>
    <p>2x256 LSTM cells, 128 dim embedding layer, 16 docs per batch, 0.5 dropout, ADAM opt.</p>
    <p>Network architecture (2/4)  Topic embedding</p>
  </div>
  <div class="page">
    <p>Output layer: Classification</p>
    <p>Decodes target probabilities  Human-readable topic labels for 2 Tasks:</p>
    <p>topic classes y1...N (2530 topics) disease.symptom</p>
    <p>headline words z1...N (1.52.8k words) [ signs, symptoms]</p>
    <p>Network architecture (3/4)  Topic classification</p>
  </div>
  <div class="page">
    <p>Segmentation: based on topic coherence</p>
    <p>deviation d k : stepwise movement</p>
    <p>of the embedding between two sentences</p>
    <p>Network architecture (4/4)  Segmentation</p>
  </div>
  <div class="page">
    <p>We use the topic embedding deviation (emd) d k to start new segments on peaks.</p>
    <p>Idea adapted from image processing: we apply Laplacian-of-Gaussian edge detection [Zi98] to find local maxima on the emd curve</p>
    <p>Steps: dimensionality reduction (PCA), Gaussian smoothing, local maxima</p>
    <p>Bidirectional deviation (bemd) on fw and bw layers allows for sharper separation</p>
    <p>Coherent segmentation using edge detection</p>
  </div>
  <div class="page">
    <p>Experiments with 20 different models on 8 datasets</p>
    <p>Sentence Classification Baselines: ParVec [Le14], CNN [Kim14]</p>
    <p>Segmentation Models: C99 [Choi00], TopicTiling [Rie12], BayesSeg [Eis08], TextSeg [Kosh18]</p>
    <p>dataset articles article type headings topics segments</p>
    <p>WikiSection 38k train/test</p>
    <p>German/English diseases and cities</p>
    <p>X X X</p>
    <p>Wiki-50 [Kosh18] 50 test English generic X X</p>
    <p>Cities/Elements [Chen09]</p>
    <p>X</p>
    <p>Clinical Textbook [Eis08]</p>
  </div>
  <div class="page">
    <p>Experiment 1: segmentation and single-label classification</p>
    <p>Segment on sentence-level and assign one of 2530 supervised topic labels (F1)</p>
  </div>
  <div class="page">
    <p>Experiment 2: segmentation and multi-label classification</p>
    <p>Segment on sentence-level and rank 1.0k2.8k noisy topic words per section (MAP)</p>
  </div>
  <div class="page">
    <p>Experiment 3: segmentation without topic prediction (cross-dataset)</p>
    <p>P k score  lower is better</p>
  </div>
  <div class="page">
    <p>Insights: SECTOR captures topic distributions coherently</p>
    <p>Topic predictions on sentence level  top: ParVec [Le14]  bottom: SECTOR Segmentation  left: newlines in text (\n)  right: embedding deviation (emd)</p>
  </div>
  <div class="page">
    <p>SECTOR prediction on par with Wiki authors for dermatitis</p>
    <p>Source: https://en.wikipedia.org/w/index.php?title=Atopic_dermatitis&amp;diff=786969806&amp;oldid=772576326</p>
  </div>
  <div class="page">
    <p>Conclusion and future work</p>
    <p>SECTOR is designed as a building block for document-level knowledge representation</p>
    <p>Reading sentences in document context is an important step to capture both topical and structural information</p>
    <p>Training the topic embedding with distant-supervised complementary labels improves performance over self-supervised word embeddings</p>
    <p>In future work, we aim to apply the topic embedding for unsupervised passage retrieval and QA tasks</p>
    <p>q = therapy</p>
  </div>
  <div class="page">
    <p>Thanks &amp; Questions</p>
    <p>Code and dataset available on GitHub: https://github.com/sebastianarnold/SECTOR https://github.com/sebastianarnold/WikiSection</p>
    <p>Our work is funded by the German Federal Ministry of Economic Affairs and Energy (BMWi) under grant agreement 01MD16011E (Medical Allround-Care Service Solutions) and H2020 ICT-2016-1 grant agreement 732328 (FashionBrain).</p>
    <p>Speaker: Sebastian Arnold</p>
    <p>sarnold@beuth-hochschule.de @sebastianarnold</p>
    <p>Data Science and Text-based Information Systems (DATEXIS) Beuth University of Applied Sciences</p>
    <p>Berlin, Germany</p>
    <p>www.datexis.de</p>
    <p>SECTOR: A Neural Model for Coherent Topic Segmentation and Classification</p>
  </div>
  <div class="page">
    <p>[Ag09] Agarwal and Yu, 2009. Automatically classifying sentences in full-text biomedical articles into introduction, methods, results and discussion. Bioinformatics 25</p>
    <p>[All02] Allan, 2002. Introduction to topic detection and tracking. Topic Detection and Tracking [Aro17] Arora et al., 2017. A simple but tough-to-beat baseline for sentence embeddings. ICLR '17 [Bha16] Bhatia et al., 2016. Automatic labelling of topics with neural embeddings. COLING '16 [Chen09] Chen et al., 2009. Global models of document structure using latent permutations. HLT-NAACL '09 [Choi00] Choi, 2000. Advances in domain independent linear text segmentation. NAACL '00 [Coh18] Cohen et al., 2018. WikiPassageQA: A benchmark collection for research on non-factoid answer passage retrieval. SIGIR '18 [Di07] Dias et al., 2007. Topic segmentation algorithms for text summarization and passage retrieval: An exhaustive evaluation. AAAI '07 [Eis08] Eisenstein and Barzilay, 2008. Bayesian unsupervised topic segmentation. EMNLP '08 [Ge00] Gers et al., 2000. Learning to forget: Continual prediction with LSTM. Neural Computation 12 [Gla16] Glava et al., 2016. Unsupervised text segmentation using semantic relatedness graphs. SEM '16 [Gra12] Graves, 2012. Supervised Sequence Labelling with Recurrent Neural Networks. [Ho97] Hochreiter and Schmidhuber, 1997. Long short-term memory. Neural Computation 9 [Kosh18] Koshorek at al., 2018. Text segmentation as a supervised learning task. NAACL-HLT '18 [Le14] Le and Mikolov, 2014. Distributed representations of sentences and documents. ICML '14 [Ma18] MacAvaney et al., 2018. Characterizing question facets for complex answer retrieval. SIGIR '18 [Mik13] Mikolov et al., 2013. Efficient estimation of word representations in vector space. CoRR, cs.CL/1301.3781v3. [Rie12] Riedl and Biemann, 2012. Topic-Tiling: A text segmentation algorithm based on LDA. ACL '12 Student Research Workshop [Se17] Serr and Karatzoglou, 2017. Getting deep recommenders fit: Bloom embeddings for sparse</p>
    <p>binary input/output networks. RecSys '17 [Zi98] Ziou and Tabbone, 1998. Edge detection techniques  An overview. Pattern Recognition and Image Analysis 8</p>
    <p>References</p>
  </div>
</Presentation>
