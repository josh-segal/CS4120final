<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Download Estimation for KDD Cup 2003</p>
    <p>Janez Brank and Jure Leskovec Joef Stefan Institute Ljubljana, Slovenia</p>
  </div>
  <div class="page">
    <p>Task Description</p>
    <p>Inputs:  Approx. 29000 papers from the high energy</p>
    <p>physics  theory area of arxiv.org  For each paper:</p>
    <p>Full text (TeX file, often very messy)  Metadata in a nice, structured file (authors,</p>
    <p>title, abstract, journal, subject classes)  The citation graph (excludes citations pointing</p>
    <p>outside our dataset)</p>
  </div>
  <div class="page">
    <p>Task Description</p>
    <p>Inputs (continued):  For papers from 6 months</p>
    <p>(the training set, 1566 papers)  The number of times this paper was downloaded</p>
    <p>during its first two months in the archive</p>
    <p>Problem:  For papers from 3 months (the test set,</p>
    <p>Only the 50 most frequently downloaded papers from each month will be used for evaluation!</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>Textual documents have traditionally been treated as bags of words  The number of occurrences of each word matters,</p>
    <p>but the order of the words is ignored  Efficiently represented by sparse vectors</p>
    <p>We extend this to include other items besides words (bag of X)  Most of our work was spent trying various features</p>
    <p>and adjusting their weight (more on that later)  Use support vector regression to train a linear</p>
    <p>model, which is then used to predict the download counts on test papers</p>
  </div>
  <div class="page">
    <p>A Few Initial Observations</p>
    <p>Our predictions will be evaluated on 50 most downloaded papers from each month  about 20% of all papers from these months  Its OK to be horribly wrong on other papers  Thus we should be optimistic, treating every</p>
    <p>paper as if it was in the top 20%  Maybe we should train the model using only</p>
    <p>most downloaded test papers</p>
  </div>
  <div class="page">
    <p>Cross-Validation</p>
    <p>Model</p>
    <p>Lather, rinse, repeat (10 times)</p>
    <p>(approx. 423 papers)</p>
    <p>(approx. 31 papers)</p>
    <p>Train</p>
    <p>Evaluate</p>
    <p>Report average</p>
    <p>Split into 10 folds</p>
    <p>Labeled papers (1566)</p>
  </div>
  <div class="page">
    <p>A Few Initial Observations</p>
    <p>We are interested in the downloads within 60 days since inclusion in the archive  Most of the downloads</p>
    <p>occur within the first few days, perhaps a week</p>
    <p>Most are probably coming from the Whats new page, which contains only:</p>
    <p>Author names  Institution name (rarely)  Title  Abstract</p>
    <p>Citations probably dont directly influence downloads in the first 60 days</p>
    <p>But they show which papers are good, and the readers perhaps sense this in some other way from the authors / title / abstract</p>
  </div>
  <div class="page">
    <p>The Rock Bottom</p>
    <p>The trivial model: always predict the average download count (computed on the training data)  Average download count: 384.2  Average error: 152.5 downloads</p>
  </div>
  <div class="page">
    <p>Abstract</p>
    <p>Abstract: use the text of the abstract and title of the paper in the traditional bag-of-words style  19912 features  No further feature selection etc.  This part of the vector was normalized</p>
    <p>to unit length (Euclidean norm = 1)  Average error: 149.4</p>
  </div>
  <div class="page">
    <p>Author</p>
    <p>One attribute for each possible author  Preprocessing to tidy up the original</p>
    <p>metadata: Y.S. Myung and Gungwon Kang myung-y kang-g</p>
    <p>xa = nonzero iff. a is one of the authors of the paper x</p>
    <p>This part is normalized to unit length  5716 features  Average error: 146.4</p>
  </div>
  <div class="page">
    <p>Address</p>
    <p>Intuition: people are more likely to download a paper if the authors are from a reputable institution  Admittedly, the Whats new page usually</p>
    <p>doesnt mention the institution  Nor is it provided in the metadata,</p>
    <p>we had to extract it from TeX files (messy!)  Words from the address are represented using the</p>
    <p>bag-of-words model  But they get their own namespace,</p>
    <p>separate from the abstract and title words  This part of the vector is also normalized</p>
    <p>to unit length  Average error: 154.0 ( worse than useless)</p>
  </div>
  <div class="page">
    <p>Abstract, Author, Address</p>
    <p>Author</p>
    <p>Abstract</p>
    <p>Address</p>
    <p>Author Abstract</p>
    <p>Author Address</p>
    <p>Abstract Address</p>
    <p>All three</p>
    <p>Training set Test set</p>
    <p>We used Author + Abstract (AA for short) as the baseline for adding new features</p>
  </div>
  <div class="page">
    <p>Using the Citation Graph</p>
    <p>InDegree, OutDegree  These are quite large in comparison to the text</p>
    <p>based features (average indegree = approx. 10)  We must use weighting, otherwise they will appear</p>
    <p>too important to the learner</p>
    <p>W e ight of InDe gre e</p>
    <p>A v</p>
    <p>e ra</p>
    <p>g e</p>
    <p>e rr</p>
    <p>o r</p>
    <p>o n</p>
    <p>t e</p>
    <p>s t</p>
    <p>s e</p>
    <p>t</p>
    <p>InDegree is useful</p>
    <p>OutDegree is largely useless (which is reasonable)</p>
    <p>AA + InDegree</p>
  </div>
  <div class="page">
    <p>Using the Citation Graph</p>
    <p>InLinks = add one feature for each paper i; it will be nonzero in vector x iff. the paper x is referenced by the paper i  Normalize this part of the vector to unit length</p>
    <p>OutLinks = the same, nonzero iff. x references i</p>
    <p>(results on next slide)</p>
  </div>
  <div class="page">
    <p>InDegree, InLinks, OutLinks</p>
    <p>AA</p>
    <p>AA + InLinks</p>
    <p>AA + OutLinks</p>
    <p>AA + InLinks + OutLinks</p>
    <p>AA + 0.8 InLinks + 0.9 OutLinks</p>
    <p>AA + 0.004 InDeg + 0.8 InLinks + 0.9 OutLinks</p>
    <p>AA + 0.005 InDeg + 0.5 InLinks + 0.7 OutLinks</p>
    <p>Training set Test set</p>
  </div>
  <div class="page">
    <p>Using the Citation Graph</p>
    <p>Use HITS to compute a hub value and an authority value for each paper ( two new features)</p>
    <p>Compute PageRank and add this as a new feature  Bad: all links point backwards in time</p>
    <p>(unlike on the web)  PageRank accumulates in the earlier years</p>
    <p>InDegree, Authority, and PageRank are strongly correlated, no improvement over previous results</p>
    <p>Hub is strongly correlated with OutDegree, and is just as useless</p>
  </div>
  <div class="page">
    <p>Journal</p>
    <p>The Journal field in the metadata indicates that the paper has been (or will be?) published in a journal  Present in about 77% of the papers  Already in standardized form, e.g. Phys. Lett.</p>
    <p>(never Physics Letters, Phys. Letters, etc.)  There are over 50 journals, but only 4 have more</p>
    <p>than 100 training papers  Papers from some journals are downloaded</p>
    <p>more often than from others:  JHEP 248, J. Phys. 104, global average 194</p>
    <p>Introduce one binary feature for each journal (+ one for missing)</p>
  </div>
  <div class="page">
    <p>Journal</p>
    <p>W e ight of the Journa l a ttribute</p>
    <p>A v</p>
    <p>e ra</p>
    <p>g e</p>
    <p>e rr</p>
    <p>o r</p>
    <p>o n</p>
    <p>t h</p>
    <p>e t</p>
    <p>e s</p>
    <p>t s</p>
    <p>e t</p>
    <p>AA + Journal AA + 0.005 InDeg + 0.5 InLinks + 0.7 OutLinks + Journal</p>
  </div>
  <div class="page">
    <p>Miscellaneous Statistics</p>
    <p>TitleCc, TitleWc: number of characters/words in the title  The most frequently downloaded</p>
    <p>papers have relatively short titles:</p>
    <p>The holographic principle (2927 downloads) Twenty Years of Debate with Stephen (1540) Brane New World (1351) A tentative theory of large distance physics (1351) (De)Constructing Dimensions (1343) Lectures on supergravity (1308) A Short Survey of Noncommutative Geometry (1246)</p>
  </div>
  <div class="page">
    <p>Miscellaneous Statistics</p>
    <p>Average error: 119.561 for weight = 0.02  The model says that the number of downloads decreases by</p>
    <p>W e ight of Title Cc/5</p>
    <p>A v</p>
    <p>e ra</p>
    <p>g e</p>
    <p>e rr</p>
    <p>o r</p>
    <p>o n</p>
    <p>t h</p>
    <p>e t</p>
    <p>e s</p>
    <p>t s</p>
    <p>e t</p>
  </div>
  <div class="page">
    <p>Miscellaneous Statistics</p>
    <p>AbstractCc, AbstractWc: number of characters/words in the abstract  Both useless</p>
    <p>Number of authors (useless)  Year (actually Year  2000)</p>
    <p>Almost useless (reduces error from 119.56 to 119.28)</p>
  </div>
  <div class="page">
    <p>Clustering</p>
    <p>Each paper was represented by a sparse vector (bag-of-words, using the abstract + title)</p>
    <p>Use 2-means to split into two clusters, then split each of them recursively  Stop splitting if one of the two clusters would have</p>
    <p>&lt; 600 documents  We ended up with 18 clusters</p>
    <p>Hard to say if theyre meaningful (ask a physicist?)  Introduce one binary feature for each cluster</p>
    <p>(useless)  Also a feature (ClusDlAvg) to contain the average</p>
    <p>no. of downloads over all the training documents from the same cluster  Reduces error from 119.59 to 119.30</p>
  </div>
  <div class="page">
    <p>Tweaking and Tuning</p>
    <p>AA + 0.005 InDegree + 0.5 InLinks + 0.7 OutLinks + 0.3 Journal + 0.02 TitleCc/5 + 0.6 (Year  2000) + 0.15 ClusDlAvg: 29.544 / 119.072</p>
    <p>The C parameter for SVM regression was fixed at 1 so far</p>
    <p>C = 0.7, AA + 0.006 InDegree + 0.7 InLinks + 0.85 OutLinks + 0.35 Journal + 0.03 TitleCc/5 + 0.3 ClusDlAvg: 31.805 / 118.944  This is the one we submitted</p>
  </div>
  <div class="page">
    <p>A Look Back</p>
    <p>Trivial model</p>
    <p>Author + Abstract</p>
    <p>+ InDegree</p>
    <p>+ InLinks + OutLinks</p>
    <p>+ Journal</p>
    <p>+ TitleCc/5</p>
    <p>Best model found</p>
    <p>Average error on the training set on the test set</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Its a nasty dataset!  The best model is still disappointingly inaccurate  and not so much better than the trivial model  Weighting the features is very important  We tried several other features (not mentioned in</p>
    <p>this presentation) that were of no use  Whatever you do, theres still so much variance left</p>
    <p>SVM learns well enough here, but it cant generalize well  It isnt the trivial sort of overfitting that could be</p>
    <p>removed simply by decreasing the C parameter in SVMs optimization problem</p>
  </div>
  <div class="page">
    <p>Further Work</p>
    <p>What is it that influences readers decisions to download a paper?  We are mostly using things they can see</p>
    <p>directly: author, title, abstract  But readers are also influenced by their</p>
    <p>background knowledge:  Is X currently a hot topic within this</p>
    <p>community? ( Will reading this paper help me with my own research?)</p>
    <p>Is Y a well-known author? How likely is the paper to be any good?</p>
    <p>It isnt easy to catch these things, and there is a risk of ovefitting</p>
  </div>
</Presentation>
