<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Vector Lane Threading</p>
    <p>S. Rivoire, R. Schultz, T. Okuda, C. Kozyrakis</p>
    <p>Computer Systems Laboratory</p>
    <p>Stanford University</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Vector processors excel at data-level parallelism (DLP)</p>
    <p>What happens to program phases with little or no DLP?</p>
    <p>Vector Lane Threading (VLT)</p>
    <p>Leverage idle DLP resources to exploit thread-level parallelism (TLP)</p>
    <p>1.4-2.3x speedup on already optimized code</p>
    <p>Small increase in system cost</p>
    <p>VLT increases the applicability of vector processors</p>
    <p>Efficient for both regular &amp; irregular parallelism</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Vector processor background</p>
    <p>Generic vector microarchitecture</p>
    <p>Vector processors and DLP</p>
    <p>Vector lane threading (VLT)</p>
    <p>VLT evaluation</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Vector Microarchitecture Overview</p>
    <p>Scalar Unit</p>
    <p>Vector Unit</p>
    <p>L2 Cache / Memory Interface</p>
  </div>
  <div class="page">
    <p>Vector Microarchitecture Overview</p>
    <p>Vector Unit</p>
    <p>L2 Cache / Memory Interface</p>
    <p>Fetch</p>
    <p>Scalar Processor</p>
    <p>D$ ROB</p>
  </div>
  <div class="page">
    <p>VRF VRF</p>
    <p>Vector Microarchitecture Overview</p>
    <p>Scalar Unit</p>
    <p>L2 Cache / Memory Interface</p>
    <p>Fetch</p>
    <p>ROB</p>
    <p>Vector Control Logic</p>
    <p>LSU LSUFUs FUs</p>
  </div>
  <div class="page">
    <p>Vector Efficiency with High DLP</p>
    <p>Best case for vector processors: long vectors &amp; regular memory patterns</p>
    <p>Lanes execute data-parallel operations very efficiently</p>
    <p>Low instruction issue rate, simple control, compact code, power efficiency</p>
    <p>Simple model for scalable performance</p>
    <p>Current vector processors have 4 to 16 lanes</p>
    <p>for i = 1 to N for j = 1 to N for k = 1 to N C[i,j] = C[i,j]+A[i,k]*C[k,j]</p>
    <p>Vector Lanes</p>
    <p>S p e e d u p</p>
    <p>mxm sage</p>
  </div>
  <div class="page">
    <p>Vector Efficiency with Low DLP</p>
    <p>Low DLP  underutilized lanes &amp; memory ports</p>
    <p>Short vectors</p>
    <p>No vectors</p>
    <p>Vector length vs. stride in nested loops</p>
    <p>Can we improve efficiency for low-DLP cases?</p>
    <p>Vector Lanes</p>
    <p>S p e e d u p</p>
    <p>mpenc</p>
    <p>trfd</p>
    <p>multprec</p>
    <p>bt</p>
    <p>radix</p>
    <p>ocean</p>
    <p>barnes</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Vector processor background</p>
    <p>Vector lane threading (VLT)</p>
    <p>Overview</p>
    <p>Possible configurations</p>
    <p>Implementation</p>
    <p>VLT Evaluation</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>VLT Basics</p>
    <p>Idea: use idle lanes to exploit thread-level parallelism (TLP)</p>
    <p>Sources of TLP</p>
    <p>Outer loops in loop nests</p>
    <p>Non-vectorizable loops</p>
    <p>Task-level parallelism</p>
    <p>VLT benefits</p>
    <p>Does not harm high-DLP performance</p>
    <p>Higher utilization for DLP resources</p>
    <p>Vector unit can be shared between multiple threads</p>
    <p>From SMT or CMP scalar processors</p>
  </div>
  <div class="page">
    <p>VLT Configurations: Single Vector Thread</p>
    <p>Original configuration for long vector lengths (high DLP)</p>
    <p>1 thread running vector instructions on 8 lanes</p>
    <p>L0 L1 L2 L3 L4 L5 L6 L7</p>
    <p>Vector Lanes</p>
  </div>
  <div class="page">
    <p>VLT Configurations: 2 Vector Threads</p>
    <p>T0, L0</p>
    <p>T0, L1</p>
    <p>Vector Lanes</p>
    <p>T0, L2</p>
    <p>T0, L3</p>
    <p>T1, L0</p>
    <p>T1, L1</p>
    <p>T1, L2</p>
    <p>T1, L3</p>
    <p>Medium vector length configuration</p>
    <p>Two threads, each running vector instructions on 4 lanes</p>
    <p>Threads may be controlled by SMT or CMP scalar processor (more later)</p>
  </div>
  <div class="page">
    <p>VLT Configurations: 4 Vector Threads</p>
    <p>T0, L0</p>
    <p>T0, L1</p>
    <p>Vector Lanes</p>
    <p>T1, L0</p>
    <p>T1, L1</p>
    <p>T2, L0</p>
    <p>T2, L1</p>
    <p>T3, L0</p>
    <p>T3, L1</p>
    <p>Short vector length configuration</p>
    <p>Four threads, each running vector instructions on 2 lanes</p>
    <p>Threads may be controlled by SMT or CMP scalar processor (more later)</p>
  </div>
  <div class="page">
    <p>VLT Configurations: Pure Scalar Threads</p>
    <p>T0, L0</p>
    <p>T1, L0</p>
    <p>T2, L0</p>
    <p>T3, L0</p>
    <p>T4, L0</p>
    <p>T5, L0</p>
    <p>T6, L0</p>
    <p>T7, L0</p>
    <p>Vector Lanes</p>
    <p>Scalar (no-vector) configuration</p>
    <p>Eight threads, each running scalar instructions on 1 lane</p>
    <p>Each lane operates as a simple processor</p>
  </div>
  <div class="page">
    <p>VLT vs. SMT</p>
    <p>VLT: exploit TLP on idle DLP resources</p>
    <p>Different threads on different vector lanes</p>
    <p>Each thread uses all functional units within a lane</p>
    <p>SMT: exploit TLP on idle ILP resources</p>
    <p>Different threads on different functional units</p>
    <p>In a vector processor, each thread uses all lanes</p>
    <p>Notes</p>
    <p>VLT and SMT are orthogonal &amp; can be combined</p>
    <p>VLT is more important for a vector processor</p>
    <p>Several apps run efficiently on a 16-lane vector processor</p>
    <p>How many apps run efficiently on a 16-way issue processor?</p>
  </div>
  <div class="page">
    <p>VRF VRF</p>
    <p>VLT Implementation</p>
    <p>Scalar Unit</p>
    <p>L2 Cache / Memory Interface</p>
    <p>Fetch</p>
    <p>ROB</p>
    <p>Vector Control Logic</p>
    <p>LSU LSUFUs FUs</p>
  </div>
  <div class="page">
    <p>VRF VRF</p>
    <p>VLT Implementation: Execution Resources</p>
    <p>Functional units  already available</p>
    <p>Vector registers for additional threads  already available</p>
    <p>VRF slice in each lane can store the register elements for each thread</p>
    <p>Note that each thread uses shorter vectors</p>
    <p>Memory ports  already available</p>
    <p>Necessary to support indexed and strided patterns even with long vectors</p>
    <p>Vector Control Logic</p>
    <p>LSU LSUFUs FUs</p>
  </div>
  <div class="page">
    <p>VLT Implementation: Vector Instr. Bandwidth</p>
    <p>Vector instruction issue bandwidth</p>
    <p>Must issue vector instructions separately for each thread</p>
    <p>Multiplex single vector control block, or replicate the block</p>
    <p>Multiplexing is sufficient as each vector instruction takes multiple cycles</p>
    <p>Instruction set</p>
    <p>Minor addition to specify configuration (number of threads)</p>
    <p>VRF VRF</p>
    <p>Vector Control Logic</p>
    <p>LSU LSUFUs FUs</p>
  </div>
  <div class="page">
    <p>VLT Implementation: Scalar Instr. Bandwidth</p>
    <p>Must process scalar instructions for</p>
    <p>multiple vector threads</p>
    <p>Design alternatives</p>
    <p>Attach vector unit to SMT processor</p>
    <p>Attach vector unit to CMP processor</p>
    <p>Multiple cores share one vector unit</p>
    <p>Combination of the above (CMT)</p>
    <p>Heterogeneous CMP</p>
    <p>One complex &amp; multiple simpler cores</p>
    <p>share a vector unit</p>
    <p>Trade-off</p>
    <p>Cost vs. utilization of vector unit</p>
    <p>Fetch</p>
    <p>D$ ROB</p>
    <p>Scalar Processor</p>
  </div>
  <div class="page">
    <p>VLT Implementation: Scalar Threads on Vector Unit</p>
    <p>Challenge: each lane requires 1-2 instructions per clock cycle</p>
    <p>Much higher instruction bandwidth than with vector threads</p>
    <p>No point in using multiple scalar cores to feed the lanes</p>
    <p>Design approach</p>
    <p>Introduce a small instruction cache in each lane</p>
    <p>Single-ported, 1 to 4 Kbytes is sufficient</p>
    <p>Feeds the functional units with instructions in tight loops</p>
    <p>Cache misses in lanes handled by scalar core</p>
    <p>Scalar core instruction cache acts as a L2 instruction cache</p>
    <p>Low miss penalty</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Vector processor background</p>
    <p>Vector lane threading (VLT)</p>
    <p>VLT Evaluation</p>
    <p>Methodology</p>
    <p>Vector thread results</p>
    <p>Scalar thread results</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Simulated processor</p>
    <p>Based on Cray X1 ISA</p>
    <p>Scalar unit: 4-way OOO superscalar</p>
    <p>Vector registers: 32 vector registers, max. vector length of 64</p>
    <p>Vector execution resources: 8 lanes, 3 functional units per lane</p>
    <p>Software environment</p>
    <p>All code compiled with production Cray C and Fortran compilers</p>
    <p>Highest level of vector optimizations enabled  All speedups reported are in addition to vectorization</p>
    <p>Used ANL macros for multithreading  Could also use OpenMP or other mulithreading approaches</p>
    <p>Further details in the paper</p>
  </div>
  <div class="page">
    <p>Benchmarks</p>
    <p>Nine benchmarks, three categories</p>
    <p>High DLP (long vectors), medium DLP (short vectors), no DLP</p>
    <p>Examples</p>
    <p>High DLP: matrix multiply</p>
    <p>96% vectorized, average vector length of 64 (maximum)</p>
    <p>Medium DLP: trfd (two electron integral transformation)</p>
    <p>76% vectorized, average vector length of 11.2</p>
    <p>No DLP: ocean (eddy currents in ocean basin)</p>
    <p>0% vectorized</p>
    <p>Note</p>
    <p>Benchmarks include some sequential portions with no DLP, no TLP</p>
  </div>
  <div class="page">
    <p>Vector Thread Evaluation</p>
    <p>mpenc trfd multprec bt</p>
    <p>Base VLT - 2 threads VLT - 4 threads</p>
    <p>Results for medium-DLP benchmarks on best scalar-core configuration</p>
    <p>Up to 2.3x performance improvement</p>
    <p>On top of vectorization for these applications</p>
    <p>Limitations</p>
    <p>Saturation of vector resources</p>
    <p>Cache effects, thread overhead, purely sequential portions</p>
    <p>S p e e d u p</p>
  </div>
  <div class="page">
    <p>VLT Cost Evaluation</p>
    <p>Default configuration: SU (4-way OOO) + VU</p>
    <p>SMT scalar unit</p>
    <p>0.8% area increase for 2 VLT threads</p>
    <p>1.3% area increase for 4 VLT threads</p>
    <p>CMP scalar unit (4-way OOO cores)</p>
    <p>12.3% area increase for 2 VLT threads</p>
    <p>26.9% area increase for 4 VLT threads</p>
    <p>CMT scalar unit (4-way OOO cores, 2 threads/core)</p>
    <p>13.8% area increase for 4 VLT threads</p>
    <p>Heterogeneous CMP scalar unit (CMP-h)</p>
    <p>3.4% area increase for 2 VLT threads</p>
    <p>10.1% area increase for 4 VLT threads</p>
  </div>
  <div class="page">
    <p>VLT Design Space Evaluation</p>
    <p>mpenc trfd multprec bt</p>
    <p>S p e e d u p</p>
    <p>V2-SMT V2-CMP V4-SMT V4-CMT V4-CMP V4-CMP-h</p>
    <p>V4-CMT equal performance to V4-CMP</p>
    <p>At lower area increase (13.8% vs. 26.9%)</p>
    <p>Two 4-way OOO processors can saturate an 8-lane vector unit</p>
    <p>V4-SMT outperforms V4-CMP-h</p>
    <p>V4-CMP-h configuration suffers thread imbalance</p>
  </div>
  <div class="page">
    <p>Scalar Thread Evaluation</p>
    <p>8-lane VLT operates like a 8-way CMP with very simple cores</p>
    <p>No out-of-order execution, wide issues, branch prediction, etc</p>
    <p>Up to 2x compared to CMP (4-way cores with 2 threads/core)</p>
    <p>But performance may vary due to sequential code performance</p>
    <p>radix ocean barnes</p>
    <p>S p e e d u p</p>
    <p>Scalar CMP VLT</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Vector Lane Threading (VLT)</p>
    <p>Turn underutilized DLP resources (lanes) into TLP resources</p>
    <p>Multiple scalar processors share a vector unit</p>
    <p>Vector unit used as a CMP with very simple cores</p>
    <p>Results</p>
    <p>Up to 2.3x performance for applications with short vectors</p>
    <p>Up to 2.0x performance for applications with no vectors</p>
    <p>Cost-effective implementation alternatives</p>
    <p>Overall, VLT improves the applicability of vector processors</p>
    <p>Good efficiency with both high DLP and low DLP</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>Cray</p>
    <p>X1 compiler access</p>
    <p>Research funding through DARPA HPCS</p>
    <p>Stanford Graduate Fellowship</p>
    <p>National Science Foundation Fellowships</p>
    <p>Stanford Computer Forum</p>
  </div>
</Presentation>
