<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Question Answering over Implicitly Structured Web Content</p>
    <p>Eugene Agichtein* Emory University</p>
    <p>Chris Burges Microsoft Research</p>
    <p>Eric Brill Microsoft Research</p>
    <p>* Research done while at Microsoft Research</p>
  </div>
  <div class="page">
    <p>What was the name of president Fillmores cat?</p>
    <p>Who invented crocs?</p>
    <p>Questions are Problematic for Web Search</p>
  </div>
  <div class="page">
    <p>Web search: What was the name of president Fillmores cat?</p>
  </div>
  <div class="page">
    <p>Web Question Answering</p>
    <p>Why are questions problematic for web search engines?</p>
    <p>Search engines treat questions as keyword queries, ignoring the semantic relationships between words, and the explicitly stated information need</p>
    <p>Poor performance for long (&gt; 5 terms) queries</p>
    <p>Problem exacerbated when common keywords are included</p>
  </div>
  <div class="page">
    <p>Agichtein et al., WI 2007  and millions more of other tables and lists</p>
  </div>
  <div class="page">
    <p>Implicitly Structured Web Content HTML Tables, Lists</p>
    <p>Product descriptions  Example: Lists of favorite things, top 10 lists, etc.</p>
    <p>HTML Syntax (sometimes) reflects semantics  Authors imply semantic relationships, entity types by grouping  Can infer information about ambiguous entities from others in the</p>
    <p>same column</p>
    <p>Millions of HTML tables, lists on the surface web alone  No common schema  Keyword queries: primary access method.  How to exploit this structured content for good (e.g., for Question</p>
    <p>Answering) at web scale?</p>
  </div>
  <div class="page">
    <p>Related Work  Web Question Answering</p>
    <p>AskMSR (TREC 2001)  Aranea (TREC 2003)  Mulder (WWW 2001)  A No-Frills Architecture for Lightweight Answer Retrieval (WWW 2007)</p>
    <p>Web-scale Information Extraction  QXtract (ICDE 2003): learn keyword queries to retrieve content  KnowItAll (WWW 2004): minimal supervision, larger scale  TextRunner (IJCAI 2007): single pass scan, disambiguate at query time  Towards Domain-Independent Information Extraction from Web Tables</p>
    <p>(WWW 2007)</p>
  </div>
  <div class="page">
    <p>Our System TQA: Overview</p>
  </div>
  <div class="page">
    <p>TableQA: Indexing  Crawl the Web  Identify promising</p>
    <p>tables (heuristic, could be improved)</p>
    <p>Extract metadata for each table  Context  Document content  Document metadata</p>
    <p>Index extracted metadata</p>
  </div>
  <div class="page">
    <p>Table Metadata</p>
    <p>Combines information about the source document, and table context</p>
  </div>
  <div class="page">
    <p>TQA Question Processing</p>
  </div>
  <div class="page">
    <p>Table QA: Querying Overview</p>
  </div>
  <div class="page">
    <p>Features for Ranking Candidate Answers</p>
  </div>
  <div class="page">
    <p>Ranking Answer Candidates  Frequency-based (AskMSR):</p>
    <p>Heuristic weight assignment (AskMSR improved)</p>
    <p>Neither is robust or general</p>
  </div>
  <div class="page">
    <p>Ranking Answer Candidates (cont) Solution: machine learning-based ranking</p>
    <p>Nave Bayes:</p>
    <p>Score(answer) =</p>
    <p>RankNet (Burges et al. 2005): scalable Neural Net implementation:  Optimized for ranking  predicting an ordering of items,</p>
    <p>not scores for each  Trains on pairs (where first point is to be ranked higher</p>
    <p>or equal to second)  Uses cross entropy cost and gradient descent to set</p>
    <p>weights</p>
    <p>).|( i i</p>
    <p>Fanswerrelevantp</p>
  </div>
  <div class="page">
    <p>Some Implementation Details</p>
    <p>Lucene, distributed indices (20M tables per index)</p>
    <p>NLP Tools:  MS internal Named Entity tagger (many free ones exist)  Porter Stemmer</p>
    <p>Relatively light-weight architecture:  Client (question processing): desktop machine  Table index server: dual-processor, 8 Gb RAM, WinNT</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>Queries: TREC QA 2002, 2003 questions</p>
    <p>Corpus: 100M web pages (a random subset of an MSN Search crawl, from 2005)</p>
    <p>Evaluation: TREC QA factoid patterns  Minimal regular expressions to match only right</p>
    <p>answers  Not comprehensive (based on judgement pool)</p>
  </div>
  <div class="page">
    <p>Evaluation Metrics</p>
    <p>MRR (mean reciprocal rank):  MRR @ K = , averaged over all</p>
    <p>questions</p>
    <p>Recall @ K:  The fraction of the questions for which a system</p>
    <p>returned a correct answer ranked at or above K.</p>
    <p>Ki ianswerrel..1 )(</p>
  </div>
  <div class="page">
    <p>Results (1): Accuracy vs. Corpus Size</p>
  </div>
  <div class="page">
    <p>Results (2): Comparing Ranking Methods</p>
    <p>If output consumed by another system, large K ok</p>
  </div>
  <div class="page">
    <p>Results (3): Accuracy on Hard Questions</p>
    <p>TQA can retrieve answer in top 100 when best QA system not able to return any answer</p>
  </div>
  <div class="page">
    <p>Result Summary</p>
    <p>Requires indexing more than 150M tables before respectable accuracy achieved</p>
    <p>Performance was around median on TREC 2002, 2003 benchmarks</p>
    <p>Can be helpful for questions difficult for traditional QA systems</p>
  </div>
  <div class="page">
    <p>Promising Directions for Future Work</p>
    <p>Craw-time: aggressive pruning/classification  Index-time: Integration of related tables  Query-time: taxonomies integration/hypernimy</p>
    <p>User behavior modeling  Past clickthrough to rerank candidate tables,</p>
    <p>answers  Query reformulation</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Implicitly structured web content can be useful for web question answering</p>
    <p>We demonstrated scalability of a lightweight table-based web QA approach</p>
    <p>Much room for improvement, future research</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
    <p>E-mail: eugene@mathcs.emory.edu Plug: User Interactions for Web Question Answering:</p>
    <p>http://www.mathcs.emory.edu/~eugene/uqa/</p>
    <p>E. Agichtein, E. Brill, S. Dumais, Mining user behavior to improve web search ranking, SIGIR 2006</p>
    <p>E. Agichtein, User Behavior Mining and Information Extraction: Towards closing the gap, IEEE Data Engineering Bulletin, Dec. 2006</p>
    <p>E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne, Finding High Quality Content in Social Media with applications to Communitybased Question Answering, to appear WSDM 2008</p>
  </div>
</Presentation>
