<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Encoding of Phonology in an RNN model of Grounded Speech</p>
    <p>Afra Alishahi, Marie Barking, Grzegorz Chrupaa</p>
    <p>Representations of language in a model of visually</p>
    <p>grounded speech signal</p>
    <p>Grzegorz Chrupaa Lieke Gelderloos Afra Alishahi</p>
  </div>
  <div class="page">
    <p>A Realistic Language Learning Scenario</p>
    <p>Two men are washing an</p>
    <p>elephant.</p>
    <p>Grounded speech perception</p>
  </div>
  <div class="page">
    <p>Elman (1991) Mohamed et al. (2012)</p>
    <p>Frank et al. (2013) Kadar et al. (2016)</p>
    <p>Li et al. (2016) Gelderloos &amp; Chrupala</p>
    <p>(2016) Linzen et al. (2016)</p>
    <p>Adi et al. (2017)</p>
    <p>Roy &amp; Pentland (2002) Yu &amp; Ballard (2014)</p>
    <p>Harwath et al. (2016) Gelderloos &amp; Chrupala</p>
    <p>(2016) Harwath &amp; Glass (2017) Chrupala et al. (2017)</p>
    <p>Grounded Language Learning</p>
    <p>Analysis of Linguistic Knowledge</p>
    <p>We are here!</p>
  </div>
  <div class="page">
    <p>A Model of Grounded Speech Perception</p>
    <p>Grounded speech perception</p>
    <p>Image Model</p>
    <p>Speech Model</p>
    <p>Joint Semantic Space</p>
  </div>
  <div class="page">
    <p>Joint Semantic Space</p>
    <p>Project speech and image to joint space</p>
    <p>a bird walks on a beam</p>
    <p>bears play in water</p>
  </div>
  <div class="page">
    <p>Image Model</p>
    <p>Image model</p>
    <p>BOAT</p>
    <p>BIRD</p>
    <p>BOAR</p>
    <p>P re</p>
    <p>-c la</p>
    <p>s s</p>
    <p>if c a</p>
    <p>t io</p>
    <p>n la</p>
    <p>y e</p>
    <p>r</p>
    <p>VGG-16: Simonyan &amp; Zisserman (2014)</p>
  </div>
  <div class="page">
    <p>Speech Model</p>
    <p>Attention</p>
    <p>RHN #5</p>
    <p>RHN #4</p>
    <p>RHN #3</p>
    <p>RHN #2</p>
    <p>RHN #1</p>
    <p>Convolution</p>
    <p>MFCC</p>
    <p>Grounded speech perception</p>
    <p>Project to the joint semantic space</p>
    <p>Attention: weighted sum of last RHN layer units</p>
    <p>RHN: Recurrent Highway Networks (Zilly et al., 2016)</p>
    <p>Convolution: subsampling MFCC vector</p>
  </div>
  <div class="page">
    <p>Chrupaa et al., ACL2017</p>
    <p>Representation of language in a model of visually grounded speech signal</p>
    <p>Using hidden layer activations in a set of auxiliary tasks</p>
    <p>Predicting utterance length and content, measuring representational similarity and disambiguation of homonyms</p>
    <p>Main findings:</p>
    <p>Encodings of form and meaning emerge and evolve in hidden layers of stacked RNNs processing grounded speech</p>
  </div>
  <div class="page">
    <p>Current Study</p>
    <p>Questions: how is phonology encoded in  MFCC features extracted from speech signal?  activations of the layers of the model?</p>
    <p>Data: Synthetically Spoken COCO dataset</p>
    <p>Experiments:  Phoneme decoding and clustering  Phoneme discrimination  Synonym discrimination</p>
    <p>Grounded speech perception</p>
  </div>
  <div class="page">
    <p>Phoneme Decoding</p>
    <p>Identifying phonemes from speech signal/activation patterns: supervised classification of aligned phonemes</p>
    <p>Speech signal was aligned with phonemic transcription using Gentle toolkit (based on Kaldi, Povey et al., 2011)</p>
  </div>
  <div class="page">
    <p>Phoneme Decoding</p>
    <p>Identifying phonemes from speech signal/activation patterns: supervised classification of aligned phonemes</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>MFCC features and activations) are stored in a tr  Dr matrix, where tr and Dr are the number of times steps and the dimensionality, respectively, for each representation r. Given the alignment of each phoneme token to the underlying audio, we then infer the slice of the representation matrix corresponding to it.</p>
    <p>In this section we report on four experiments which we designed to elucidate to what extent information about phonology is represented in the activations of the layers of the COCO Speech model. In Section 5.1 we quantify how easy it is to decode phoneme identity from activations. In Section 5.2 we determine phoneme discriminability in a controlled task with minimal pair stimuli. Section 5.3 shows how the phoneme inventory is organized in the activation space of the model. Finally, in Section 5.4 we tackle the general issue of the representation of phonological form versus meaning with the controlled task of synonym discrimination.</p>
    <p>In this section we quantify to what extent phoneme identity can be decoded from the input MFCC features as compared to the representations extracted from the COCO speech. As explained in Section 4.3, we use phonemic transcriptions aligned to the corresponding audio in order to segment the signal into chunks corresponding to individual phonemes.</p>
    <p>We take a subset of 5000 utterances from the validation set of Synthetically Spoken COCO, and extract the force-aligned representations from the Speech COCO model. We split this data into 23 training and 13 heldout portions, and use supervised classification in order to quantify the recoverability of phoneme identities from the representations. Each phoneme slice is averaged over time, so that it becomes a Dr-dimensional vector. For each representation we then train L2-penalized logistic regression (with the fixed penalty weight 1.0) on the training data and measure classification error rate on the heldout portion.</p>
    <p>Figure 1 shows the results. As can be seen from this plot, phoneme recoverability is poor for the representations based on MFCC and the convolutional layer activations, but improves markedly for the recurrent layers. Phonemes are easiest recov</p>
    <p>ered from the activations at recurrent layers 1 and 2, and the accuracy decreases thereafter. This suggests that the bottom recurrent layers of the model specialize in recognizing this type of low-level phonological information. It is notable however that even the last recurrent layer encodes phoneme identity to a substantial degree.</p>
    <p>MFCC Conv Rec1 Rec2 Rec3 Rec4 Rec5 Representation</p>
    <p>E rr</p>
    <p>or ra</p>
    <p>te</p>
    <p>Figure 1: Accuracy of phoneme decoding with input MFCC features and COCO Speech model activations. The boxplot shows error rates bootstrapped with 1000 resamples.</p>
    <p>Schatz et al. (2013) propose a framework for evaluating speech features learned in an unsupervised setup that does not depend on phonetically labeled data. They propose a set of tasks called MinimalPair ABX tasks that allow to make linguistically precise comparisons between syllable pairs that only differ by one phoneme. They use variants of this task to study phoneme discrimination across talkers and phonetic contexts as well as talker discrimination across phonemes.</p>
    <p>Here we evaluate the COCO Speech model on the Phoneme across Context (PaC) task of Schatz et al. (2013). This task consists of presenting a series of equal-length tuples (A, B, X) to the model, where A and B differ by one phoneme (either a vowel or a consonant), as do B and X, but A and X are not minimal pairs. For example, in the tuple (be /bi/, me /mi/, my /maI/), human subjects are asked to identify which of the two syllables be or me are closest to my. The goal is to measure</p>
  </div>
  <div class="page">
    <p>ABX task (Schatz et al., 2013): discriminate minimal pairs; is X closer to A or to B?</p>
    <p>A, B and X are CV syllables</p>
    <p>(A,B) and (B,X) are minimum pairs, but (A,X) are not (34,288 tuples in total)</p>
    <p>Phoneme Discrimination</p>
    <p>A: be /bi/ B: me /mi/</p>
    <p>X: my /maI/</p>
  </div>
  <div class="page">
    <p>Phoneme Discrimination</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>Table 3: Accuracy of choosing the correct target in an ABX task using different representations.</p>
    <p>MFCC 0.71 Convolutional 0.73 Recurrent 1 0.82 Recurrent 2 0.82 Recurrent 3 0.80 Recurrent 4 0.76 Recurrent 5 0.74</p>
    <p>context invariance in phoneme discrimination by evaluating how often the model recognises X as the syllable closer to B than to A.</p>
    <p>We used a list of all attested consonant-vowel (CV) syllables of American English according to the syllabification method described in Gorman (2013). We excluded the ones which could not be unambiguously represented using English spelling for input to the TTS system (e.g. /baU/). We then compiled a list of all possible (A, B, X) tuples from this list where (A, B) and (B, X) are minimal pairs, but (A, X) are not. This resulted in 34,288 tuples in total. For each tuple, we measure sign(dist(A, X)  dist(B, X)), where dist(i, j) is the euclidean distance between the vector representations of syllables i and j. These representations are either the audio feature vectors or the layer activation vectors. A positive value for a tuple means that the model has correctly discriminated the phonemes that are shared or different across the syllables.</p>
    <p>Table 3 shows the discrimination accuracy in this task using various representations. The pattern is similar to what we observed in the phoneme identification task: best accuracy is achieved using representation vectors from recurrent layers 1 and 2, and it drops as we move further up in the model. The accuracy is lowest when MFCC features are used for this task.</p>
    <p>However, the PaC task is most meaningful and challenging where the target and the distractor phonemes belong to the same phoneme class. Figure 2 shows the accuracies for this subset of cases, broken down by class. As can be seen, the model can discriminate between phonemes with high accuracy across all the layers, and the layer activations are more informative for this task than the MFCC features. Again, phonemes seem to be represented more accurately in the lower layers, and the performance of the model in this task drops</p>
    <p>mfcc conv rec1 rec2 rec3 rec4 rec5 Representation</p>
    <p>A cc ur ac y</p>
    <p>Class</p>
    <p>affricate approximant</p>
    <p>fricative nasal</p>
    <p>plosive vowel</p>
    <p>Figure 2: Accuracies for the ABX CV task for the cases where the target and the distractor belong to the same phoneme class.</p>
    <p>as we move towards higher hidden layers. There are also clear differences in the pattern of discriminability for the phoneme classes. The vowels are especially easy to tell apart, but accuracy on vowels drops most acutely in the higher layers. Meanwhile the accuracy on fricatives and approximants starts low, but improves rapidly and peaks around recurrent layer 2.</p>
    <p>In this section we take a closer look at the underlying organization of phonemes in the model. Our experiment is inspired by Khalighinejad et al. (2017) who study how the speech signal is represented in the brain at different stages of the auditory pathway by collecting and analyzing electroencephalography responses from participants listening to continuous speech, and show that brain responses to different phoneme categories turn out to be organized by phonetic features.</p>
    <p>We carry out an analogous experiment by analyzing the hidden layer activations of our model in response to each phoneme in the input. First, we generated a distance matrix for every pair of phonemes by calculating the Euclidean distance between the phoneme pairs activation vectors for each layer separately, as well as a distance matrix for all phoneme pairs based on their MFCC features. Similar to what Khalighinejad et al. (2017) report, we observe that the phoneme activations on</p>
  </div>
  <div class="page">
    <p>Phoneme Discrimination by Class</p>
    <p>The task is most challenging when the target (B) and distractor (A) belong to the same phoneme class</p>
    <p>A: be /bi/ B: me /mi/</p>
    <p>X: my /maI/</p>
  </div>
  <div class="page">
    <p>Phoneme Discrimination by Class</p>
    <p>The task is most challenging when the target (B) and distractor (A) belong to the same phoneme class</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>cessively according to:</p>
    <p>encu(u) = unit(Attn(RHNk,L(Convs,d,z(u)))) (2)</p>
    <p>The first layer Convs,d,z is a one-dimensional convolution of size s which subsamples the input with stride z, and projects it to d dimensions. It is followed by RHNk,L which consists of k residualized recurrent layers. Specifically these are Recurrent Highway Network layers (Zilly et al., 2016), which are closely related to GRU networks, with the crucial difference that they increase the depth of the transform between timesteps; this is the recurrence depth L. The output of the final recurrent layer is passed through an attention-like lookback operator Attn which takes a weighted average of the activations across time steps. Finally, both utterance and image projections are L2-normalized. See Section 4.1 for details of the model configuration.</p>
    <p>The phoneme activations in each layer are calculated as the activations averaged over the duration of the phoneme token in the input. The average input vectors are similarly calculated as the MFCC vectors averaged over the time course of the articulation of the phoneme token. When we need to represent a phoneme type we do so by averaging the vectors of all its instances in the validation set. Table 1 shows the phoneme inventory we</p>
    <p>Vowels i I U u e E @  OI O o aI  2 A aU</p>
    <p>Approximants j  l w Nasals m n N Plosives p b t d k g Fricatives f v T D s z S Z h Affricates</p>
    <p>Table 1: Phonemes of General American English.</p>
    <p>work with; this is also the inventory used by Gentle/Kaldi (see Section 4.3).</p>
    <p>Attention: size 512 Recurrent 5: size 512 Recurrent 4: size 512 Recurrent 3: size 512 Recurrent 2: size 512 Recurrent 1: size 512</p>
    <p>Convolutional: size 64, length 6, stride 3 Input MFCC: size 13</p>
    <p>Table 2: COCO Speech utterance encoder architecture.</p>
    <p>are as follows: convolutional layer with length 6, size 64, stride 3, 5 Recurrent Highway Network layers with 512 dimensions and 2 microsteps, attention Multi-Layer Perceptron with 512 hidden units, Adam optimizer, initial learning rate 0.0002. The 4096-dimensional image feature vectors come from the final fully connect layer of VGG-16 (Simonyan and Zisserman, 2014) pretrained on Imagenet (Russakovsky et al., 2014), and are averages of feature vectors for ten crops of each image. The total number of learnable parameters is 9,784,193. Table 2 sketches the architecture of the utterance encoder part of the model.</p>
    <p>The Speech COCO model was trained on the Synthetically Spoken COCO dataset (Chrupaa et al., 2017b), which is a version of the MS COCO dataset (Lin et al., 2014) where speech was synthesized for the original image descriptions, using high-quality speech synthesis provided by gTTS.2</p>
    <p>We aligned the speech signal to the corresponding phonemic transcription with the Gentle toolkit,3</p>
    <p>which in turn is based on Kaldi (Povey et al., 2011). It uses a speech recognition model for English to transcribe the input audio signal, and then finds the optimal alignment of the transcription to the signal. This fails for a small number of utterances, which we remove from the data. In the next step we extract MFCC features from the audio signal and pass them through the COCO Speech utterance encoder, and record the activations for the convolutional layer as well as all the recurrent layers. For each utterance the representations (i.e.</p>
  </div>
  <div class="page">
    <p>Phoneme Discrimination by Class</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>Table 3: Accuracy of choosing the correct target in an ABX task using different representations.</p>
    <p>MFCC 0.71 Convolutional 0.73 Recurrent 1 0.82 Recurrent 2 0.82 Recurrent 3 0.80 Recurrent 4 0.76 Recurrent 5 0.74</p>
    <p>context invariance in phoneme discrimination by evaluating how often the model recognises X as the syllable closer to B than to A.</p>
    <p>We used a list of all attested consonant-vowel (CV) syllables of American English according to the syllabification method described in Gorman (2013). We excluded the ones which could not be unambiguously represented using English spelling for input to the TTS system (e.g. /baU/). We then compiled a list of all possible (A, B, X) tuples from this list where (A, B) and (B, X) are minimal pairs, but (A, X) are not. This resulted in 34,288 tuples in total. For each tuple, we measure sign(dist(A, X)  dist(B, X)), where dist(i, j) is the euclidean distance between the vector representations of syllables i and j. These representations are either the audio feature vectors or the layer activation vectors. A positive value for a tuple means that the model has correctly discriminated the phonemes that are shared or different across the syllables.</p>
    <p>Table 3 shows the discrimination accuracy in this task using various representations. The pattern is similar to what we observed in the phoneme identification task: best accuracy is achieved using representation vectors from recurrent layers 1 and 2, and it drops as we move further up in the model. The accuracy is lowest when MFCC features are used for this task.</p>
    <p>However, the PaC task is most meaningful and challenging where the target and the distractor phonemes belong to the same phoneme class. Figure 2 shows the accuracies for this subset of cases, broken down by class. As can be seen, the model can discriminate between phonemes with high accuracy across all the layers, and the layer activations are more informative for this task than the MFCC features. Again, phonemes seem to be represented more accurately in the lower layers, and the performance of the model in this task drops</p>
    <p>mfcc conv rec1 rec2 rec3 rec4 rec5 Representation</p>
    <p>A cc ur ac y</p>
    <p>Class</p>
    <p>affricate approximant</p>
    <p>fricative nasal</p>
    <p>plosive vowel</p>
    <p>Figure 2: Accuracies for the ABX CV task for the cases where the target and the distractor belong to the same phoneme class.</p>
    <p>as we move towards higher hidden layers. There are also clear differences in the pattern of discriminability for the phoneme classes. The vowels are especially easy to tell apart, but accuracy on vowels drops most acutely in the higher layers. Meanwhile the accuracy on fricatives and approximants starts low, but improves rapidly and peaks around recurrent layer 2.</p>
    <p>In this section we take a closer look at the underlying organization of phonemes in the model. Our experiment is inspired by Khalighinejad et al. (2017) who study how the speech signal is represented in the brain at different stages of the auditory pathway by collecting and analyzing electroencephalography responses from participants listening to continuous speech, and show that brain responses to different phoneme categories turn out to be organized by phonetic features.</p>
    <p>We carry out an analogous experiment by analyzing the hidden layer activations of our model in response to each phoneme in the input. First, we generated a distance matrix for every pair of phonemes by calculating the Euclidean distance between the phoneme pairs activation vectors for each layer separately, as well as a distance matrix for all phoneme pairs based on their MFCC features. Similar to what Khalighinejad et al. (2017) report, we observe that the phoneme activations on</p>
    <p>The task is most challenging when the target (B) and distractor (A) belong to the same phoneme class</p>
  </div>
  <div class="page">
    <p>Organization of Phonemes</p>
    <p>Agglomerative hierarchical clustering of phoneme activation vectors from the first hidden layer:</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>Figure 4: Hierarchical clustering of phoneme activation vectors on the first hidden layer.</p>
    <p>mfcc conv rec1 rec2 rec3 rec4 rec5 emb Representation</p>
    <p>E rr or</p>
    <p>Pair</p>
    <p>cut.slice make.prepare someone.person photo.picture picture.image kid.child photograph.picture slice.piece bicycle.bike photograph.photo couch.sofa tv.television vegetable.veggie</p>
    <p>sidewalk.pavement rock.stone store.shop purse.bag assortment.variety spot.place pier.dock direction.way carpet.rug bun.roll large.big small.little</p>
    <p>Figure 6: Synonym discrimination error rates, per representation and synonym pair.</p>
    <p>tence embeddings give relatively high error rates suggesting that the attention layer acts to focus on semantic information and filter out much of phonological form.</p>
    <p>Understanding distributed representations learned by neural networks is important but has the reputation of being hard to impossible. In this work we focus on making progress on this problem for a particular domain: representations of phonology</p>
    <p>in a multilayer recurrent neural network trained on grounded speech signal. We believe it is important to carry out multiple analyses using diverse methodology: any single experiment may be misleading as it depends on analytical choices such as the type of supervised model used for decoding, the algorithm used for clustering, or the similarity metric for representational similarity analysis. To the extent that more than one experiment points to the same conclusion our confidence in the reliability of the insights gained will be increased.</p>
    <p>The main high-level result of our study confirms earlier work: encoding of semantics becomes stronger in higher layer, while encoding of form becomes weaker. This general pattern is to be expected as the objective of the utterance encoder is to transform the input acoustic features in such a way that it can be matched to its counterpart in a completely separate modality. Many of the details of how this happens, however, are far from obvious: perhaps most surprisingly we found that large amount of phonological information persist up to the top recurrent layer. Evidence for this pattern emerges from the phoneme decoding task, the ABX task and the synonym discrimination task. The last one also shows that the attention layer filters out and significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy.</p>
    <p>In future work we would like to apply our methodology to other models and data, especially human speech data. We would also like to make comparisons to the results that emerge from similar analyses applied to neuroimaging data.</p>
  </div>
  <div class="page">
    <p>Synonym Discrimination</p>
    <p>Distinguishing between synonym pairs in the same context:</p>
    <p>A girl looking at a photo  A girl looking at a picture</p>
    <p>Synonyms were selected using WordNet synsets:</p>
    <p>The pair have the same POS tag and are interchangeable</p>
    <p>The pair clearly differ in form (not donut/doughnut)</p>
    <p>The more frequent token in a pair constitutes less than 95% of the occurrences.</p>
  </div>
  <div class="page">
    <p>Synonym Discrimination</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>Figure 4: Hierarchical clustering of phoneme activation vectors on the first hidden layer.</p>
    <p>mfcc conv rec1 rec2 rec3 rec4 rec5 emb Representation</p>
    <p>E rr or</p>
    <p>Pair</p>
    <p>cut.slice make.prepare someone.person photo.picture picture.image kid.child photograph.picture slice.piece bicycle.bike photograph.photo couch.sofa tv.television vegetable.veggie</p>
    <p>sidewalk.pavement rock.stone store.shop purse.bag assortment.variety spot.place pier.dock direction.way carpet.rug bun.roll large.big small.little</p>
    <p>Figure 6: Synonym discrimination error rates, per representation and synonym pair.</p>
    <p>tence embeddings give relatively high error rates suggesting that the attention layer acts to focus on semantic information and filter out much of phonological form.</p>
    <p>Understanding distributed representations learned by neural networks is important but has the reputation of being hard to impossible. In this work we focus on making progress on this problem for a particular domain: representations of phonology</p>
    <p>in a multilayer recurrent neural network trained on grounded speech signal. We believe it is important to carry out multiple analyses using diverse methodology: any single experiment may be misleading as it depends on analytical choices such as the type of supervised model used for decoding, the algorithm used for clustering, or the similarity metric for representational similarity analysis. To the extent that more than one experiment points to the same conclusion our confidence in the reliability of the insights gained will be increased.</p>
    <p>The main high-level result of our study confirms earlier work: encoding of semantics becomes stronger in higher layer, while encoding of form becomes weaker. This general pattern is to be expected as the objective of the utterance encoder is to transform the input acoustic features in such a way that it can be matched to its counterpart in a completely separate modality. Many of the details of how this happens, however, are far from obvious: perhaps most surprisingly we found that large amount of phonological information persist up to the top recurrent layer. Evidence for this pattern emerges from the phoneme decoding task, the ABX task and the synonym discrimination task. The last one also shows that the attention layer filters out and significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy.</p>
    <p>In future work we would like to apply our methodology to other models and data, especially human speech data. We would also like to make comparisons to the results that emerge from similar analyses applied to neuroimaging data.</p>
    <p>CoNLL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.</p>
    <p>Figure 4: Hierarchical clustering of phoneme activation vectors on the first hidden layer.</p>
    <p>mfcc conv rec1 rec2 rec3 rec4 rec5 emb Representation</p>
    <p>E rr or</p>
    <p>Pair</p>
    <p>cut.slice make.prepare someone.person photo.picture picture.image kid.child photograph.picture slice.piece bicycle.bike photograph.photo couch.sofa tv.television vegetable.veggie</p>
    <p>sidewalk.pavement rock.stone store.shop purse.bag assortment.variety spot.place pier.dock direction.way carpet.rug bun.roll large.big small.little</p>
    <p>Figure 6: Synonym discrimination error rates, per representation and synonym pair.</p>
    <p>tence embeddings give relatively high error rates suggesting that the attention layer acts to focus on semantic information and filter out much of phonological form.</p>
    <p>Understanding distributed representations learned by neural networks is important but has the reputation of being hard to impossible. In this work we focus on making progress on this problem for a particular domain: representations of phonology</p>
    <p>in a multilayer recurrent neural network trained on grounded speech signal. We believe it is important to carry out multiple analyses using diverse methodology: any single experiment may be misleading as it depends on analytical choices such as the type of supervised model used for decoding, the algorithm used for clustering, or the similarity metric for representational similarity analysis. To the extent that more than one experiment points to the same conclusion our confidence in the reliability of the insights gained will be increased.</p>
    <p>The main high-level result of our study confirms earlier work: encoding of semantics becomes stronger in higher layer, while encoding of form becomes weaker. This general pattern is to be expected as the objective of the utterance encoder is to transform the input acoustic features in such a way that it can be matched to its counterpart in a completely separate modality. Many of the details of how this happens, however, are far from obvious: perhaps most surprisingly we found that large amount of phonological information persist up to the top recurrent layer. Evidence for this pattern emerges from the phoneme decoding task, the ABX task and the synonym discrimination task. The last one also shows that the attention layer filters out and significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy.</p>
    <p>In future work we would like to apply our methodology to other models and data, especially human speech data. We would also like to make comparisons to the results that emerge from similar analyses applied to neuroimaging data.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Phoneme representations are most salient in lower layers</p>
    <p>Large amount of phonological information persists up to the top recurrent layer</p>
    <p>The attention layer filters out and significantly attenuates encoding of phonology and makes utterance embeddings more invariant to synonymy</p>
    <p>Code: https://github.com/gchrupala/encoding-of-phonology</p>
  </div>
</Presentation>
