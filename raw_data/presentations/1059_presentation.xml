<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>On the diversity of cluster workloads and its impact on research results</p>
    <p>George Amvrosiadis, Jun Woo Park, Greg Ganger, Garth Gibson, Elisabeth Baseman, Nathan DeBardeleben</p>
  </div>
  <div class="page">
    <p>Sources for cluster traces today</p>
    <p>Parallel Workload Archive (1993  2015)  38 HPC cluster traces</p>
    <p>(each: 1K+ cores, months long)</p>
    <p>Publications: 250+</p>
    <p>Google cluster trace (2011)  29 days of a 12,000-node cluster  Publications: 450+</p>
  </div>
  <div class="page">
    <p>Project Atlas</p>
    <p>Mandate: use historical data to improve cluster efficiency  LANL: scheduler logs, sensor data, OS logs,   TBs / day  Recently: data from Two Sigma, Pittsburgh Supercomputing Center</p>
    <p>Current goals:</p>
    <p>Investigate overfitting to existing traces in systems literature  Produce generalizable models of cluster workloads  Create trace repository and make data publicly available</p>
  </div>
  <div class="page">
    <p>Atlas repository: current traces</p>
    <p>Two Sigma business analytics clusters: 9 months (2016-2017)  1300 nodes, 31500 cores, 328TB RAM</p>
    <p>LANL Mustang general-purpose cluster: 5 years (2011-2016)  1600 nodes, 38400 cores, 100TB RAM</p>
    <p>LANL OpenTrinity capability cluster: 3 months (2017)  Trinity phase 1: 9400 nodes, 300000 cores, 1.15PB RAM</p>
    <p>Entire</p>
    <p>cluster lifetime</p>
    <p>Repository accessible thru project-atlas.org</p>
    <p>More traces coming soon! You can contribute!</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>www.pdl.cmu.edu/ATLAS 4</p>
    <p>Characteristic Google Two Sigma Mustang OpenTrinity</p>
    <p>Short jobs     Small jobs     Diurnal patterns     High job submission rate     Resource over-commitment     Sub-second interarrival periods     User request variability     High failure rates     Costly failures (wasted CPU hours)     Longer/larger jobs fail more often</p>
    <p>Failure analysis</p>
    <p>Resource utilization</p>
    <p>Workload heterogeneity</p>
    <p>Job characteristics</p>
  </div>
  <div class="page">
    <p>Job Sizes</p>
    <p>Google jobs request 3 - 406x fewer CPU cores  LANL request sizes more uniformly distributed</p>
    <p>Two Sigma LANL</p>
    <p>www.pdl.cmu.edu/ATLAS</p>
    <p>Number of cores in job</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f to</p>
    <p>ta l jo</p>
    <p>b s</p>
    <p>Mustang OpenTrinity TwoSigma Google</p>
    <p>Solving head-of-line blocking by dedicating resources to small jobs becomes challenging [Delgado et al.]</p>
  </div>
  <div class="page">
    <p>Job duration (hours)</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f to</p>
    <p>ta l jo</p>
    <p>b s</p>
    <p>Mustang OpenTrinity TwoSigma Google</p>
    <p>Job Duration</p>
    <p>Median Google job is 4 - 5x shorter  But: LANL jobs end at 16- hours, Google jobs dont</p>
    <p>Two Sigma LANL</p>
    <p>www.pdl.cmu.edu/ATLAS Mitigating straggler effect thru short task replication should be applied judiciously [Ananthanarayanan et al.]</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>www.pdl.cmu.edu/ATLAS 7</p>
    <p>Characteristic Google Two Sigma Mustang OpenTrinity</p>
    <p>Short jobs     Small jobs     Diurnal patterns     High job submission rate     Resource over-commitment     Sub-second interarrival periods     User request variability     High failure rates     Costly failures (wasted CPU hours)     Longer/larger jobs fail more often</p>
    <p>Failure analysis</p>
    <p>Resource utilization</p>
    <p>Workload heterogeneity</p>
  </div>
  <div class="page">
    <p>Workload Heterogeneity</p>
    <p>Reversed diurnal patterns  More/smaller Google jobs</p>
    <p>between midnight and 4AM</p>
    <p>Job submission rate  10-1000x more scheduling</p>
    <p>requests in Two Sigma, Google</p>
    <p>www.pdl.cmu.edu/ATLAS</p>
    <p>J o b s</p>
    <p>u b m</p>
    <p>is s io</p>
    <p>n s</p>
    <p>TwoSigma Google</p>
    <p>Day hour (12am - 11pm)</p>
    <p>J o b s</p>
    <p>u b m</p>
    <p>is s io</p>
    <p>n s</p>
    <p>Mustang OpenTrinity</p>
    <p>Task placement algorithms achieve subsecond latency today [Quincy, Firmament]</p>
    <p>but we should aim for msec latencies</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>www.pdl.cmu.edu/ATLAS 9</p>
    <p>Characteristic Google Two Sigma Mustang OpenTrinity</p>
    <p>Short jobs     Small jobs     Diurnal patterns     High job submission rate     Resource over-commitment     Sub-second interarrival periods     User request variability     High failure rates     Costly failures (wasted CPU hours)     Longer/larger jobs fail more often</p>
    <p>Failure analysis</p>
    <p>Resource utilization</p>
  </div>
  <div class="page">
    <p>Job interarrival period (seconds)</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f in</p>
    <p>te ra</p>
    <p>rr iv</p>
    <p>a ls</p>
    <p>Mustang OpenTrinity TwoSigma Google</p>
    <p>Resource utilization: intensity</p>
    <p>Only Google overcommits resources (others at 65-90%)  43-64% of inter-arrivals &lt;1sec long</p>
    <p>20% of inter-arrivals &gt;100sec at LANL  Maintenance</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>www.pdl.cmu.edu/ATLAS 11</p>
    <p>Characteristic Google Two Sigma Mustang OpenTrinity</p>
    <p>Short jobs     Small jobs     Diurnal patterns     High job submission rate     Resource over-commitment     Sub-second interarrival periods     User request variability     High failure rates     Costly failures (wasted CPU hours)     Longer/larger jobs fail more often</p>
    <p>Failure analysis</p>
  </div>
  <div class="page">
    <p>M us</p>
    <p>ta ng</p>
    <p>O pe</p>
    <p>nT rin</p>
    <p>ity</p>
    <p>Tw oS</p>
    <p>ig m</p>
    <p>a</p>
    <p>G oo</p>
    <p>gl e</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f jo</p>
    <p>b s (</p>
    <p>% ) Unsuccessful</p>
    <p>Timeouts Unsuccessful jobs</p>
    <p>Unsuccessful job rates at Google are significant  1.4-6.8x higher than other traces</p>
    <p>Highest efficiency: HPC clusters  34-80% fewer CPU hours wasted* at LANL  Time wasted decreases with job runtime</p>
    <p>Failed or</p>
    <p>Aborted</p>
    <p>Two Sigma LANL</p>
    <p>www.pdl.cmu.edu/ATLAS</p>
    <p>M us</p>
    <p>ta ng</p>
    <p>O pe</p>
    <p>nT rin</p>
    <p>ity</p>
    <p>Tw oS</p>
    <p>ig m</p>
    <p>a</p>
    <p>G oo</p>
    <p>gl e</p>
    <p>F ra</p>
    <p>c ti o n o</p>
    <p>f C</p>
    <p>P U</p>
    <p>t im</p>
    <p>e (</p>
    <p>% )</p>
    <p>Defining failure is crucial: software errors may be benign</p>
  </div>
  <div class="page">
    <p>www.pdl.cmu.edu/ATLAS 13</p>
    <p>A case for</p>
    <p>dataset pluralism</p>
  </div>
  <div class="page">
    <p>Estimating job runtimes</p>
    <p>Runtime estimates: improve cluster efficiency  Adjust to heterogeneous hardware  lower response times  Job packing  increased utilization</p>
    <p>How do we come up with runtime estimates?  User-provided (Moab, Slurm @ LANL)  mostly inaccurate  Leverage job repeats (Rayon in Hadoop)  effectiveness depends on workload</p>
    <p>JVuPredict/3Sigma: generate estimates automatically  Step 1: Use past runtimes of jobs with similar feature(s)  Step 2: Select predictor with highest accuracy</p>
    <p>[EuroSys 2018]</p>
    <p>www.pdl.cmu.edu/ATLAS</p>
  </div>
  <div class="page">
    <p>-80 -60 -40 -20 0 20 40 60 80 +</p>
    <p>Estimate error (%)  5%</p>
    <p>P e rc</p>
    <p>e n t o f jo</p>
    <p>b s (</p>
    <p>% ) Mustang</p>
    <p>OpenTrinity TwoSigma Google</p>
    <p>JVuPredict: Accuracy across traces</p>
    <p>Reliance on: user ID, number of cores, job name (if present)  Logical job names matter!  Need busy (100K+ jobs) or long (3+ months) traces for training</p>
    <p>Under</p>
    <p>estimations:</p>
    <p>bad!</p>
    <p>Over</p>
    <p>estimations:</p>
    <p>eh</p>
    <p>www.pdl.cmu.edu/ATLAS</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Characteristic Google Two Sigma Mustang OpenTrinity</p>
    <p>Short jobs     Small jobs     Diurnal patterns     High job submission rate     Resource over-commitment     Sub-second interarrival periods     User request variability     High failure rates     Costly failures (wasted CPU hours)     Longer/larger jobs fail more often</p>
    <p>Private more similar to HPC, except: Failure rates, Job submission rate</p>
  </div>
</Presentation>
