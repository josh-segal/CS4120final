<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ElCached: Elastic Multi-Level Key-Value Cache</p>
    <p>Rahman Lavaee, Stephen Choi, Yang-Suk Kee,</p>
    <p>Chen Ding November 1st, Savannah, GA</p>
  </div>
  <div class="page">
    <p>Backend database server</p>
    <p>Web App</p>
  </div>
  <div class="page">
    <p>Backend database server</p>
    <p>Web App</p>
  </div>
  <div class="page">
    <p>Web Caching</p>
    <p>Memcached nodes</p>
    <p>Backend database server</p>
    <p>Web App</p>
  </div>
  <div class="page">
    <p>Resource Provisioning for Memcached</p>
  </div>
  <div class="page">
    <p>Resource Provisioning for Memcached</p>
    <p>The service provider must wisely allocate the resource to guarantee each tenants SLA, while minimizing TCO.</p>
  </div>
  <div class="page">
    <p>Elastic Resource Provisioning</p>
    <p>Optimal resource provisioning requires elasticity  capability to adapt to workload changes by dynamic resource provisioning.</p>
  </div>
  <div class="page">
    <p>Multitenant Resource Provisioning</p>
    <p>The problem becomes more complex as  more tenants are added to the system.  more web caching layers are used.</p>
  </div>
  <div class="page">
    <p>MlCached [HotCloud16]</p>
    <p>A multi-level key-value caching system.  L1: DRAM-based Memcached  L2: exclusive NAND-flash-based key-value cache (SSD).</p>
    <p>MlCached implements direct key-to-PBA mappings on SSD.  Independent resource provisioning</p>
    <p>In this work, we extend MlCached by adding the elasticity feature.</p>
  </div>
  <div class="page">
    <p>Independent Resource Provisioning in MlCached</p>
    <p>MlCached implements direct key-to-PBA mappings on SSD.  This removes the need for storing redundant key-to-LBA mapping tables in memory</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Latency Model</p>
    <p>&quot; Latency of Memcached server</p>
    <p># Latency of SSD</p>
    <p>$% Latency of backend DB server</p>
    <p>M' Miss rate of Memcached</p>
    <p># Miss rate of SSD</p>
    <p>= &quot; + #.M' + $%.#</p>
  </div>
  <div class="page">
    <p>Cost Model</p>
    <p>p' Price per unit of DRAM</p>
    <p># Price per unit of SSD</p>
    <p>c' Size of DRAM</p>
    <p># Size of SSD</p>
    <p>= &quot;.&quot; + #.#</p>
  </div>
  <div class="page">
    <p>Latency Based on Miss Ratio Curve</p>
    <p>The key to optimal resource provisioning is to find the miss ratio curve (MRC).</p>
    <p>Capacity</p>
    <p>M is s Ra tio</p>
    <p>= &quot; + #.r c' + $%. #</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Reuse Distance</p>
    <p>We use the reuse distance theory to compute the miss ratio curve.  Reuse distance is the number of distinct memory locations accessed between two consecutive uses of the same memory location.</p>
    <p>a b c d b a c</p>
    <p>3 4 4</p>
  </div>
  <div class="page">
    <p>Reuse Distance Histogram</p>
    <p>The reuse distance information is best represented by the reuse distance histogram, which shows the frequency for every reuse distance.  The MRC can be computed from this histogram.</p>
    <p>a b c d b a c</p>
    <p>3 4 4</p>
    <p>fre qu</p>
    <p>en cy</p>
    <p>m is</p>
    <p>s ra</p>
    <p>tio</p>
  </div>
  <div class="page">
    <p>Reuse Distance Computation</p>
    <p>Olken tree [Olken 1981]  Approximate reuse distance [Ding+ 2001]  Footprint estimation [Xiang+ 2011]  Stack counters [Wires+ 2014]</p>
  </div>
  <div class="page">
    <p>Reuse Distance Computation</p>
    <p>Olken tree [Olken 1981]  Approximate reuse distance [Ding+ 2001]  Footprint estimation [Xiang+ 2011]  Stack counters [Wires+ 2014]</p>
    <p>Program Locality Analysis Using Reuse Distance  20:5</p>
    <p>Fig. 2. An example illustrating the reuse-distance measurement. Part (a) shows a reuse distance. Parts (b) and (c) show its measurement by the Bennett-Kruskal algorithm and the Olken algorithm. Part (d) shows our approximate measurement with a guaranteed precision of 33%.</p>
    <p>case, the previous access may occur at the beginning of the trace, the difference in access time is up to T  1, and the reuse distance is up to N  1. In large applications, T can be over 100 billion, and N is often in the tens of millions.</p>
    <p>We use the example in Figure 2 to introduce two previous solutions and then describe the basic idea for our solution. Part (a) shows an example trace. Suppose we want to find the reuse distance between the two accesses of b at time 4 and 12. A solution has to store enough information about the trace history before time 12. Bennett and Kruskal [1975] discovered that it is sufficient to store only the last access of each datum, as shown in Part (b) for the example trace. The reuse distance is measured by counting the number of last accesses, stored in a bit vector rather than using the original trace.</p>
    <p>The efficiency was improved by Olken [1981], who organized the last accesses as nodes in a search tree keyed by their access time. The Olken-style tree for the example trace has 7 nodes, one for the last access of each datum, as shown in Figure 2(c). The reuse distance is measured by counting the number of nodes whose key values are between 4 and 12. The counting can be done in a single tree search, first finding the node with key value 4 and then backing up to the root accumulating the subtree weights [Olken 1981]. Since the algorithm needs one tree node for each data location, the search tree can grow to a significant size when analyzing programs with a large amount of data.</p>
    <p>While it is costly to measure long reuse distances, we rarely need the exact length. Often the first few digits suffice. For example, if a reuse distance is about one million, it rarely matters whether the exact value is one million or one million and one. Next we describe two approximate algorithms that extend the Olken algorithm by adapting and trimming the search tree.</p>
    <p>The new algorithms guarantee two types of precision for the approximate distance, dapproximate, compared to the actual distance, dactual. In both types, the</p>
    <p>ACM Transactions on Programming Languages and Systems, Vol. 31, No. 6, Article 20, Pub. date: August 2009.</p>
  </div>
  <div class="page">
    <p>Reuse Distance for Memcached</p>
    <p>Memcached distributes items among different slab classes, according to their sizes.  Slab allocation is done during the cold start.</p>
  </div>
  <div class="page">
    <p>LRU Replacement in Memcached</p>
    <p>Once the Memcached system reaches its memory limit, LRU replacement is done separately for every slab class.</p>
    <p>Item Item . . .</p>
    <p>Item</p>
    <p>LRU Head LRU Tail</p>
    <p>Slab Class 1</p>
    <p>Slab Class 2</p>
    <p>......</p>
    <p>. . .</p>
    <p>Item Item . . .</p>
    <p>Item . . .</p>
  </div>
  <div class="page">
    <p>Slab-Aware Reuse Distance Profiling  Rather than analyzing the whole Memcached system in a single reuse distance model, we model each slab class separately.  We compose the MRCs from different slab classes.</p>
    <p>Item Item . . .</p>
    <p>Item</p>
    <p>LRU Head LRU Tail</p>
    <p>Slab Class 1</p>
    <p>Slab Class 2</p>
    <p>......</p>
    <p>. . .</p>
    <p>Item Item . . .</p>
    <p>Item . . .</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Resource Provisioning as a Linear Program  The resource provisioning problem can be described in one of the two ways.  Minimize Cost such that Lat  SLA.  Minimize Lat such that Cost  TCO.</p>
  </div>
  <div class="page">
    <p>Resource Provisioning as a Linear Program  The resource provisioning problem can be described in one of the two ways.  Minimize Cost such that Lat  SLA.  Minimize Lat such that Cost  TCO.</p>
  </div>
  <div class="page">
    <p>Resource Provisioning as a Linear Program  The resource provisioning problem can be described in one of the two ways.  Minimize Cost such that Lat  SLA.  Minimize Lat such that Cost  TCO.</p>
    <p>Cost is already linear in terms of DRAM/SSD capacities.</p>
  </div>
  <div class="page">
    <p>Resource Provisioning as a Linear Program  The resource provisioning problem can be described in one of the two ways.  Minimize Cost such that Lat  SLA.  Minimize Lat such that Cost  TCO.</p>
    <p>Cost is already linear in terms of DRAM/SSD capacities.  Latency is linear only in terms of the miss ratio function.</p>
  </div>
  <div class="page">
    <p>Resource Provisioning as a Linear Program  We observe that the miss ratio curves in our workloads are always convex.  We formulate the miss ratio curve using linear constraints.</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>We compare ElCached against a proportional approach that fixes the ratio between DRAM and SSD capacities to 1:4 (Pareto principle).</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>We compare ElCached against a proportional approach that fixes the ratio between DRAM and SSD capacities to 1:4 (Pareto principle).</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>We compare ElCached against a proportional approach that fixes the ratio between DRAM and SSD capacities to 1:4 (Pareto principle).</p>
    <p>Workloads:  Zipfian key distribution with  = 1.15  Exponential key distribution with  = 10?@  Both workloads issue 800 million requests to a range of 4 billion keys.</p>
  </div>
  <div class="page">
    <p>Miss Ratio Prediction Accuracy</p>
    <p>Mean relative error on Zipfian workload: 4%</p>
    <p>On the other hand, the elastic approach, the mechanism ElCached uses, changes the ratio adaptively based on our reuse-distance based miss-rate prediction to achieve better elasticity. That is, different data locality determines the optimal DRAM/SSD allocations to optimize either latency or cost. We first investigate the accuracy of our miss-rate prediction and then, show the increased elasticity of Web caching service with ElCached.</p>
    <p>For these experiments, we use Mutilate [1] to generate our workloads. Mutilate emulates the ETC workloads at Facebook using key size, value size, and interarrival time distributions given by Atikoglu et al. [2]. The mean key size and value sizes are respectively 31 and 247 bytes. We use Mutilate to generate two workloads. For the first workload, we use a Zipfian key distribution with a = 1.15. For the second workload, we use an exponential distribution with l = 106. We set the both workload to issue 800 million requests to a range of 4 billion keys. Note that this is a limit study to evaluate elasticity of ElCached compared to existing work. Therefore, it does not include the overheads and other tuning aspects of dynamic resource provisioning; data promotion, demotion, eviction overheads and profiling window, reconfiguration frequencies, and its associated efficiencies in time domain.</p>
    <p>Prediction Accuracy We profile the workloads using our model and predict miss rates on a logarithmic scale, and compare the results to measurement. Figure 2 shows the result for the Zipfian workload. The mean relative error of prediction is 4%.</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l l</p>
    <p>M is</p>
    <p>s R</p>
    <p>at e</p>
    <p>(% )</p>
    <p>l Predicted Measured</p>
    <p>Figure 2: Miss ratio prediction accuracy of our reuse distance model</p>
    <p>Elastic Resource Provisioning Although both are dynamically allocating/deallocating resources, the propor</p>
    <p>Memory + Remote Storage) are common from Amazon EC2, Google Cloud Platform, and Microsoft Azure. Remote storage is out of the scope of this work.</p>
    <p>tional approach does not change the ratio between DRAM and SSD. In particular, we use 1:4 ratio following the Pareto principle. On the other hand, the elastic approach continuously changes the ratio based on the reuse-distance based miss-rate prediction. Figure 3 shows minimum cost per latency for both techniques, and for both workloads. For example, to guarantee the average latency of 700 s for the Zipfian workload, elastic costs 0.7 and proportional costs 8.1 (cost is normalized with respect to the cost of proportional at 1ms). In this case, elastic allocates 60MB/11.7GB while proportional does 1.8GB/7.3GB for DRAM and SSD, respectively. Assuming that CSPs bill tenants proportional to actual cost, elasticity will save costs both for users and CSPs. In particular, for the Zipfian workload, elasticity saves around 60% of cost. Meanwhile, the Exponential workload exhibits around 10% reduction in cost until 250 s latency requirement, but this rapidly changes to more than 70% cost improvement after 300 s. This is because the Exponential workload has much stronger data locality than Zipfian. So DRAM allocation dominates the performance before 300 s.</p>
    <p>Zipfian Exponential l</p>
    <p>l l l l l</p>
    <p>l l</p>
    <p>l l l l l l l</p>
    <p>l l llll l l l l l l</p>
    <p>N orm</p>
    <p>alized C ost</p>
    <p>R elative C</p>
    <p>ost</p>
    <p>l Prop. Elastic Elastic/Prop.</p>
    <p>Figure 3: Elastic vs. proportional resource provisioning</p>
    <p>We also conduct a comparison between elastic and proportional in a multi-tenant case. Here we assume a virtual cloud instance with 3 GB memory. Figure 4a shows latency for the two tenants when partitioning the fixed amount of memory. In particular, proportional partitions memory and allocates each memory and SSD with the fixed ratio of 1:4. On the other hand, elastic predicts an optimal configuration per tenant and tests its feasibility. We compare the elastic approach to one particular memory partition which allocates 1.63 GBs of DRAM for tenant 1 (shown by a line in Figure 4a). As shown in Figure 4b, both approaches result in the same latency for tenant 2, whereas, for tenant 1, elastic improves latency by 5%. Moreover, elastic reduces the cost for both tenants by around 26%. Elastic also saves the total memory consumption by 46% by allocating only 1.63 GB</p>
  </div>
  <div class="page">
    <p>Experiment 1: Elastic Resource Provisioning</p>
    <p>For each workload  For each latency limit, we find the minimum cost resource provisioning.</p>
  </div>
  <div class="page">
    <p>Experiment 1: Elastic Resource Provisioning</p>
    <p>For each workload  For each latency limit, we find the minimum cost resource provisioning.</p>
    <p>Elastic saves cost by around 60% for both workloads.</p>
    <p>Latency(ms)</p>
    <p>Co st (N</p>
    <p>or m al iz ed</p>
    <p>) Re</p>
    <p>la tiv e Co</p>
    <p>st</p>
    <p>On the other hand, the elastic approach, the mechanism ElCached uses, changes the ratio adaptively based on our reuse-distance based miss-rate prediction to achieve better elasticity. That is, different data locality determines the optimal DRAM/SSD allocations to optimize either latency or cost. We first investigate the accuracy of our miss-rate prediction and then, show the increased elasticity of Web caching service with ElCached.</p>
    <p>For these experiments, we use Mutilate [1] to generate our workloads. Mutilate emulates the ETC workloads at Facebook using key size, value size, and interarrival time distributions given by Atikoglu et al. [2]. The mean key size and value sizes are respectively 31 and 247 bytes. We use Mutilate to generate two workloads. For the first workload, we use a Zipfian key distribution with a = 1.15. For the second workload, we use an exponential distribution with l = 106. We set the both workload to issue 800 million requests to a range of 4 billion keys. Note that this is a limit study to evaluate elasticity of ElCached compared to existing work. Therefore, it does not include the overheads and other tuning aspects of dynamic resource provisioning; data promotion, demotion, eviction overheads and profiling window, reconfiguration frequencies, and its associated efficiencies in time domain.</p>
    <p>Prediction Accuracy We profile the workloads using our model and predict miss rates on a logarithmic scale, and compare the results to measurement. Figure 2 shows the result for the Zipfian workload. The mean relative error of prediction is 4%.</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l l</p>
    <p>M is</p>
    <p>s R</p>
    <p>at e</p>
    <p>(% )</p>
    <p>l Predicted Measured</p>
    <p>Figure 2: Miss ratio prediction accuracy of our reuse distance model</p>
    <p>Elastic Resource Provisioning Although both are dynamically allocating/deallocating resources, the propor</p>
    <p>Memory + Remote Storage) are common from Amazon EC2, Google Cloud Platform, and Microsoft Azure. Remote storage is out of the scope of this work.</p>
    <p>tional approach does not change the ratio between DRAM and SSD. In particular, we use 1:4 ratio following the Pareto principle. On the other hand, the elastic approach continuously changes the ratio based on the reuse-distance based miss-rate prediction. Figure 3 shows minimum cost per latency for both techniques, and for both workloads. For example, to guarantee the average latency of 700 s for the Zipfian workload, elastic costs 0.7 and proportional costs 8.1 (cost is normalized with respect to the cost of proportional at 1ms). In this case, elastic allocates 60MB/11.7GB while proportional does 1.8GB/7.3GB for DRAM and SSD, respectively. Assuming that CSPs bill tenants proportional to actual cost, elasticity will save costs both for users and CSPs. In particular, for the Zipfian workload, elasticity saves around 60% of cost. Meanwhile, the Exponential workload exhibits around 10% reduction in cost until 250 s latency requirement, but this rapidly changes to more than 70% cost improvement after 300 s. This is because the Exponential workload has much stronger data locality than Zipfian. So DRAM allocation dominates the performance before 300 s.</p>
    <p>Zipfian Exponential l</p>
    <p>l l l l l</p>
    <p>l l</p>
    <p>l l l l l l l</p>
    <p>l l llll l l l l l l</p>
    <p>N orm</p>
    <p>alized C ost</p>
    <p>R elative C</p>
    <p>ost</p>
    <p>l Prop. Elastic Elastic/Prop.</p>
    <p>Figure 3: Elastic vs. proportional resource provisioning</p>
    <p>We also conduct a comparison between elastic and proportional in a multi-tenant case. Here we assume a virtual cloud instance with 3 GB memory. Figure 4a shows latency for the two tenants when partitioning the fixed amount of memory. In particular, proportional partitions memory and allocates each memory and SSD with the fixed ratio of 1:4. On the other hand, elastic predicts an optimal configuration per tenant and tests its feasibility. We compare the elastic approach to one particular memory partition which allocates 1.63 GBs of DRAM for tenant 1 (shown by a line in Figure 4a). As shown in Figure 4b, both approaches result in the same latency for tenant 2, whereas, for tenant 1, elastic improves latency by 5%. Moreover, elastic reduces the cost for both tenants by around 26%. Elastic also saves the total memory consumption by 46% by allocating only 1.63 GB</p>
    <p>On the other hand, the elastic approach, the mechanism ElCached uses, changes the ratio adaptively based on our reuse-distance based miss-rate prediction to achieve better elasticity. That is, different data locality determines the optimal DRAM/SSD allocations to optimize either latency or cost. We first investigate the accuracy of our miss-rate prediction and then, show the increased elasticity of Web caching service with ElCached.</p>
    <p>For these experiments, we use Mutilate [1] to generate our workloads. Mutilate emulates the ETC workloads at Facebook using key size, value size, and interarrival time distributions given by Atikoglu et al. [2]. The mean key size and value sizes are respectively 31 and 247 bytes. We use Mutilate to generate two workloads. For the first workload, we use a Zipfian key distribution with a = 1.15. For the second workload, we use an exponential distribution with l = 106. We set the both workload to issue 800 million requests to a range of 4 billion keys. Note that this is a limit study to evaluate elasticity of ElCached compared to existing work. Therefore, it does not include the overheads and other tuning aspects of dynamic resource provisioning; data promotion, demotion, eviction overheads and profiling window, reconfiguration frequencies, and its associated efficiencies in time domain.</p>
    <p>Prediction Accuracy We profile the workloads using our model and predict miss rates on a logarithmic scale, and compare the results to measurement. Figure 2 shows the result for the Zipfian workload. The mean relative error of prediction is 4%.</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l l</p>
    <p>M is</p>
    <p>s R</p>
    <p>at e</p>
    <p>(% )</p>
    <p>l Predicted Measured</p>
    <p>Figure 2: Miss ratio prediction accuracy of our reuse distance model</p>
    <p>Elastic Resource Provisioning Although both are dynamically allocating/deallocating resources, the propor</p>
    <p>Memory + Remote Storage) are common from Amazon EC2, Google Cloud Platform, and Microsoft Azure. Remote storage is out of the scope of this work.</p>
    <p>tional approach does not change the ratio between DRAM and SSD. In particular, we use 1:4 ratio following the Pareto principle. On the other hand, the elastic approach continuously changes the ratio based on the reuse-distance based miss-rate prediction. Figure 3 shows minimum cost per latency for both techniques, and for both workloads. For example, to guarantee the average latency of 700 s for the Zipfian workload, elastic costs 0.7 and proportional costs 8.1 (cost is normalized with respect to the cost of proportional at 1ms). In this case, elastic allocates 60MB/11.7GB while proportional does 1.8GB/7.3GB for DRAM and SSD, respectively. Assuming that CSPs bill tenants proportional to actual cost, elasticity will save costs both for users and CSPs. In particular, for the Zipfian workload, elasticity saves around 60% of cost. Meanwhile, the Exponential workload exhibits around 10% reduction in cost until 250 s latency requirement, but this rapidly changes to more than 70% cost improvement after 300 s. This is because the Exponential workload has much stronger data locality than Zipfian. So DRAM allocation dominates the performance before 300 s.</p>
    <p>Zipfian Exponential l</p>
    <p>l l l l l</p>
    <p>l l</p>
    <p>l l l l l l l</p>
    <p>l l llll l l l l l l</p>
    <p>N orm</p>
    <p>alized C ost</p>
    <p>R elative C</p>
    <p>ost</p>
    <p>l Prop. Elastic Elastic/Prop.</p>
    <p>Figure 3: Elastic vs. proportional resource provisioning</p>
    <p>We also conduct a comparison between elastic and proportional in a multi-tenant case. Here we assume a virtual cloud instance with 3 GB memory. Figure 4a shows latency for the two tenants when partitioning the fixed amount of memory. In particular, proportional partitions memory and allocates each memory and SSD with the fixed ratio of 1:4. On the other hand, elastic predicts an optimal configuration per tenant and tests its feasibility. We compare the elastic approach to one particular memory partition which allocates 1.63 GBs of DRAM for tenant 1 (shown by a line in Figure 4a). As shown in Figure 4b, both approaches result in the same latency for tenant 2, whereas, for tenant 1, elastic improves latency by 5%. Moreover, elastic reduces the cost for both tenants by around 26%. Elastic also saves the total memory consumption by 46% by allocating only 1.63 GB</p>
    <p>Co st (N</p>
    <p>or m al iz ed</p>
    <p>)</p>
    <p>On the other hand, the elastic approach, the mechanism ElCached uses, changes the ratio adaptively based on our reuse-distance based miss-rate prediction to achieve better elasticity. That is, different data locality determines the optimal DRAM/SSD allocations to optimize either latency or cost. We first investigate the accuracy of our miss-rate prediction and then, show the increased elasticity of Web caching service with ElCached.</p>
    <p>For these experiments, we use Mutilate [1] to generate our workloads. Mutilate emulates the ETC workloads at Facebook using key size, value size, and interarrival time distributions given by Atikoglu et al. [2]. The mean key size and value sizes are respectively 31 and 247 bytes. We use Mutilate to generate two workloads. For the first workload, we use a Zipfian key distribution with a = 1.15. For the second workload, we use an exponential distribution with l = 106. We set the both workload to issue 800 million requests to a range of 4 billion keys. Note that this is a limit study to evaluate elasticity of ElCached compared to existing work. Therefore, it does not include the overheads and other tuning aspects of dynamic resource provisioning; data promotion, demotion, eviction overheads and profiling window, reconfiguration frequencies, and its associated efficiencies in time domain.</p>
    <p>Prediction Accuracy We profile the workloads using our model and predict miss rates on a logarithmic scale, and compare the results to measurement. Figure 2 shows the result for the Zipfian workload. The mean relative error of prediction is 4%.</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l l</p>
    <p>M is</p>
    <p>s R</p>
    <p>at e</p>
    <p>(% )</p>
    <p>l Predicted Measured</p>
    <p>Figure 2: Miss ratio prediction accuracy of our reuse distance model</p>
    <p>Elastic Resource Provisioning Although both are dynamically allocating/deallocating resources, the propor</p>
    <p>Memory + Remote Storage) are common from Amazon EC2, Google Cloud Platform, and Microsoft Azure. Remote storage is out of the scope of this work.</p>
    <p>tional approach does not change the ratio between DRAM and SSD. In particular, we use 1:4 ratio following the Pareto principle. On the other hand, the elastic approach continuously changes the ratio based on the reuse-distance based miss-rate prediction. Figure 3 shows minimum cost per latency for both techniques, and for both workloads. For example, to guarantee the average latency of 700 s for the Zipfian workload, elastic costs 0.7 and proportional costs 8.1 (cost is normalized with respect to the cost of proportional at 1ms). In this case, elastic allocates 60MB/11.7GB while proportional does 1.8GB/7.3GB for DRAM and SSD, respectively. Assuming that CSPs bill tenants proportional to actual cost, elasticity will save costs both for users and CSPs. In particular, for the Zipfian workload, elasticity saves around 60% of cost. Meanwhile, the Exponential workload exhibits around 10% reduction in cost until 250 s latency requirement, but this rapidly changes to more than 70% cost improvement after 300 s. This is because the Exponential workload has much stronger data locality than Zipfian. So DRAM allocation dominates the performance before 300 s.</p>
    <p>Zipfian Exponential l</p>
    <p>l l l l l</p>
    <p>l l</p>
    <p>l l l l l l l</p>
    <p>l l llll l l l l l l</p>
    <p>N orm</p>
    <p>alized C ost</p>
    <p>R elative C</p>
    <p>ost</p>
    <p>l Prop. Elastic Elastic/Prop.</p>
    <p>Figure 3: Elastic vs. proportional resource provisioning</p>
    <p>We also conduct a comparison between elastic and proportional in a multi-tenant case. Here we assume a virtual cloud instance with 3 GB memory. Figure 4a shows latency for the two tenants when partitioning the fixed amount of memory. In particular, proportional partitions memory and allocates each memory and SSD with the fixed ratio of 1:4. On the other hand, elastic predicts an optimal configuration per tenant and tests its feasibility. We compare the elastic approach to one particular memory partition which allocates 1.63 GBs of DRAM for tenant 1 (shown by a line in Figure 4a). As shown in Figure 4b, both approaches result in the same latency for tenant 2, whereas, for tenant 1, elastic improves latency by 5%. Moreover, elastic reduces the cost for both tenants by around 26%. Elastic also saves the total memory consumption by 46% by allocating only 1.63 GB</p>
    <p>Re la tiv e Co</p>
    <p>st</p>
    <p>Latency(ms)</p>
    <p>On the other hand, the elastic approach, the mechanism ElCached uses, changes the ratio adaptively based on our reuse-distance based miss-rate prediction to achieve better elasticity. That is, different data locality determines the optimal DRAM/SSD allocations to optimize either latency or cost. We first investigate the accuracy of our miss-rate prediction and then, show the increased elasticity of Web caching service with ElCached.</p>
    <p>For these experiments, we use Mutilate [1] to generate our workloads. Mutilate emulates the ETC workloads at Facebook using key size, value size, and interarrival time distributions given by Atikoglu et al. [2]. The mean key size and value sizes are respectively 31 and 247 bytes. We use Mutilate to generate two workloads. For the first workload, we use a Zipfian key distribution with a = 1.15. For the second workload, we use an exponential distribution with l = 106. We set the both workload to issue 800 million requests to a range of 4 billion keys. Note that this is a limit study to evaluate elasticity of ElCached compared to existing work. Therefore, it does not include the overheads and other tuning aspects of dynamic resource provisioning; data promotion, demotion, eviction overheads and profiling window, reconfiguration frequencies, and its associated efficiencies in time domain.</p>
    <p>Prediction Accuracy We profile the workloads using our model and predict miss rates on a logarithmic scale, and compare the results to measurement. Figure 2 shows the result for the Zipfian workload. The mean relative error of prediction is 4%.</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l</p>
    <p>l l</p>
    <p>M is</p>
    <p>s R</p>
    <p>at e</p>
    <p>(% )</p>
    <p>l Predicted Measured</p>
    <p>Figure 2: Miss ratio prediction accuracy of our reuse distance model</p>
    <p>Elastic Resource Provisioning Although both are dynamically allocating/deallocating resources, the propor</p>
    <p>Memory + Remote Storage) are common from Amazon EC2, Google Cloud Platform, and Microsoft Azure. Remote storage is out of the scope of this work.</p>
    <p>tional approach does not change the ratio between DRAM and SSD. In particular, we use 1:4 ratio following the Pareto principle. On the other hand, the elastic approach continuously changes the ratio based on the reuse-distance based miss-rate prediction. Figure 3 shows minimum cost per latency for both techniques, and for both workloads. For example, to guarantee the average latency of 700 s for the Zipfian workload, elastic costs 0.7 and proportional costs 8.1 (cost is normalized with respect to the cost of proportional at 1ms). In this case, elastic allocates 60MB/11.7GB while proportional does 1.8GB/7.3GB for DRAM and SSD, respectively. Assuming that CSPs bill tenants proportional to actual cost, elasticity will save costs both for users and CSPs. In particular, for the Zipfian workload, elasticity saves around 60% of cost. Meanwhile, the Exponential workload exhibits around 10% reduction in cost until 250 s latency requirement, but this rapidly changes to more than 70% cost improvement after 300 s. This is because the Exponential workload has much stronger data locality than Zipfian. So DRAM allocation dominates the performance before 300 s.</p>
    <p>Zipfian Exponential l</p>
    <p>l l l l l</p>
    <p>l l</p>
    <p>l l l l l l l</p>
    <p>l l llll l l l l l l</p>
    <p>N orm</p>
    <p>alized C ost</p>
    <p>R elative C</p>
    <p>ost</p>
    <p>l Prop. Elastic Elastic/Prop.</p>
    <p>Figure 3: Elastic vs. proportional resource provisioning</p>
    <p>We also conduct a comparison between elastic and proportional in a multi-tenant case. Here we assume a virtual cloud instance with 3 GB memory. Figure 4a shows latency for the two tenants when partitioning the fixed amount of memory. In particular, proportional partitions memory and allocates each memory and SSD with the fixed ratio of 1:4. On the other hand, elastic predicts an optimal configuration per tenant and tests its feasibility. We compare the elastic approach to one particular memory partition which allocates 1.63 GBs of DRAM for tenant 1 (shown by a line in Figure 4a). As shown in Figure 4b, both approaches result in the same latency for tenant 2, whereas, for tenant 1, elastic improves latency by 5%. Moreover, elastic reduces the cost for both tenants by around 26%. Elastic also saves the total memory consumption by 46% by allocating only 1.63 GB</p>
  </div>
  <div class="page">
    <p>Experiment 2: Multitenant Resource Provisioning</p>
    <p>First, we use the proportional scheme to partition a fixed 3GB memory between the two tenants.</p>
    <p>compared to the 3 GB of physical memory. That is the memory consumption of tenant 1 alone, when taking the proportional approach. This means potential revenue for CSP because it could potentially host more tenants as long as other resources (CPU, network, etc.) are available.</p>
    <p>l</p>
    <p>l lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>l Tenant 1 Tenant 2</p>
    <p>(a)</p>
    <p>Proportional T1 T2</p>
    <p>DRAM 1.63 1.37 Lat. 0.71 0.20 Cost 7.20 5.41</p>
    <p>Elastic T1 T2</p>
    <p>DRAM 0.46 1.17 Lat. 0.67 0.20 Cost 5.30 4.01</p>
    <p>(b)</p>
    <p>Figure 4: Multi-tenant resource provisioning: (a) latency per DRAM partitioning with proportional SSD size (1:4 ratio) (b) elastic vs proportional with the specified memory partition</p>
    <p>Our study of online resource provisioning is preliminary and remains as future work. Here, we explain the basics of our online implementation.</p>
    <p>Memcached allocates memory in units of fixed size (called slabs) until it reaches its memory limit. Thus we can control resource provisioning by dynamically reconfiguring the memory limit parameter. Increasing this parameter allows Memcached to boost memory provision to the new limit. To lower the provision, we must find and deallocate victim slabs. Prior to deallocation, all the items in the victim slabs must be removed from the LRU chains. We choose victim slabs from different size classes to ensure that the number of slabs in different size classes remains proportional to the initial setting. To find a victim slab in each size class, we monitor the activity of all the slabs during a short period of time. Slab activity is defined as the number of distinct items accessed during that period. For each slab, we estimate its activity using a Hyperloglog counter [8] with logarithmic space overhead. After the monitoring period, we choose the least active slabs as the victim slabs. We keep deallocating slabs until the new memory limit is satisfied.</p>
    <p>Our online analysis consists of a profiling window followed by a release period. During the profiling period, we use the reuse distance analysis to find the miss rate</p>
    <p>for every cache size. At the end of the profiling window, we formulate and solve a linear program according to the constraints and the objective function. The solution gives the new Memcached and KVD sizes. Then, we reconfigure the ElCached system using our above-mentioned procedure.</p>
    <p>The study of online resource provisioning remains as future work. An important question is how the profiling window length affects the accuracy and optimality of solution. Our future work also includes analyzing the memory and time overheads of the online reuse distance analysis.</p>
    <p>Our study is related to previous work on optimizing Web caching applications. For example, Dynacache [4] optimizes Memcached by profiling workloads and dynamically adjusting memory resources and eviction policies. Saemundsson et al. [14] introduced MIMIR, a lightweight online profiler for Memcached. Analogous to us, they used a tree-based scheme to estimate reuse distance. To lower the overhead, MIMIR divides the LRU stack into variable-sized buckets. However, contrary to us, MIMIR and Dynacache both assume a fixed size for all cached elements, which isnt the case in Memcached. Hu et al. [10] introduced LAMA to improve slab allocation in Memcached. Like us, they also estimate the miss ratio for every slab class.</p>
    <p>Reuse distance computation has been well-studied in the past. Mattson et al. [12] introduced a basic stack simulation algorithm. Olken [13] gave the first tree-based method using an AVL tree. Ding and Zhong [6] used a similar technique to approximate reuse distance while reducing the time overhead. Xiang et al. [18] defined the footprint of a given trace to be the number of distinct elements accessed in the window. They showed that under a certain condition (reuse-window condition) reuse distance is equal to the finite differential of average footprint. We implemented the footprint approach and found its accuracy to be slightly lower than our tree-based algorithm. These techniques all require a linear space overhead. Wires et al. [17] used counter stacks to approximate the miss ratio curve with sublinear space overhead. These studies can be integrated into ElCached to reduce the space and time overhead in the online analysis.</p>
    <p>In this paper, we described ElCached, a multi-level keyvalue caching system that uses a reuse distance profiler to estimate the miss rate curve across all capacity limits with 4% relative error. ElCached solves the resource pro</p>
    <p>compared to the 3 GB of physical memory. That is the memory consumption of tenant 1 alone, when taking the proportional approach. This means potential revenue for CSP because it could potentially host more tenants as long as other resources (CPU, network, etc.) are available.</p>
    <p>l</p>
    <p>l lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>l Tenant 1 Tenant 2</p>
    <p>(a)</p>
    <p>Proportional T1 T2</p>
    <p>DRAM 1.63 1.37 Lat. 0.71 0.20 Cost 7.20 5.41</p>
    <p>Elastic T1 T2</p>
    <p>DRAM 0.46 1.17 Lat. 0.67 0.20 Cost 5.30 4.01</p>
    <p>(b)</p>
    <p>Figure 4: Multi-tenant resource provisioning: (a) latency per DRAM partitioning with proportional SSD size (1:4 ratio) (b) elastic vs proportional with the specified memory partition</p>
    <p>Our study of online resource provisioning is preliminary and remains as future work. Here, we explain the basics of our online implementation.</p>
    <p>Memcached allocates memory in units of fixed size (called slabs) until it reaches its memory limit. Thus we can control resource provisioning by dynamically reconfiguring the memory limit parameter. Increasing this parameter allows Memcached to boost memory provision to the new limit. To lower the provision, we must find and deallocate victim slabs. Prior to deallocation, all the items in the victim slabs must be removed from the LRU chains. We choose victim slabs from different size classes to ensure that the number of slabs in different size classes remains proportional to the initial setting. To find a victim slab in each size class, we monitor the activity of all the slabs during a short period of time. Slab activity is defined as the number of distinct items accessed during that period. For each slab, we estimate its activity using a Hyperloglog counter [8] with logarithmic space overhead. After the monitoring period, we choose the least active slabs as the victim slabs. We keep deallocating slabs until the new memory limit is satisfied.</p>
    <p>Our online analysis consists of a profiling window followed by a release period. During the profiling period, we use the reuse distance analysis to find the miss rate</p>
    <p>for every cache size. At the end of the profiling window, we formulate and solve a linear program according to the constraints and the objective function. The solution gives the new Memcached and KVD sizes. Then, we reconfigure the ElCached system using our above-mentioned procedure.</p>
    <p>The study of online resource provisioning remains as future work. An important question is how the profiling window length affects the accuracy and optimality of solution. Our future work also includes analyzing the memory and time overheads of the online reuse distance analysis.</p>
    <p>Our study is related to previous work on optimizing Web caching applications. For example, Dynacache [4] optimizes Memcached by profiling workloads and dynamically adjusting memory resources and eviction policies. Saemundsson et al. [14] introduced MIMIR, a lightweight online profiler for Memcached. Analogous to us, they used a tree-based scheme to estimate reuse distance. To lower the overhead, MIMIR divides the LRU stack into variable-sized buckets. However, contrary to us, MIMIR and Dynacache both assume a fixed size for all cached elements, which isnt the case in Memcached. Hu et al. [10] introduced LAMA to improve slab allocation in Memcached. Like us, they also estimate the miss ratio for every slab class.</p>
    <p>Reuse distance computation has been well-studied in the past. Mattson et al. [12] introduced a basic stack simulation algorithm. Olken [13] gave the first tree-based method using an AVL tree. Ding and Zhong [6] used a similar technique to approximate reuse distance while reducing the time overhead. Xiang et al. [18] defined the footprint of a given trace to be the number of distinct elements accessed in the window. They showed that under a certain condition (reuse-window condition) reuse distance is equal to the finite differential of average footprint. We implemented the footprint approach and found its accuracy to be slightly lower than our tree-based algorithm. These techniques all require a linear space overhead. Wires et al. [17] used counter stacks to approximate the miss ratio curve with sublinear space overhead. These studies can be integrated into ElCached to reduce the space and time overhead in the online analysis.</p>
    <p>In this paper, we described ElCached, a multi-level keyvalue caching system that uses a reuse distance profiler to estimate the miss rate curve across all capacity limits with 4% relative error. ElCached solves the resource pro</p>
  </div>
  <div class="page">
    <p>Experiment 2: Multitenant Resource Provisioning</p>
    <p>Then, we use ElCached to find the latency-optimal DRAM/SSD partitioning.</p>
    <p>compared to the 3 GB of physical memory. That is the memory consumption of tenant 1 alone, when taking the proportional approach. This means potential revenue for CSP because it could potentially host more tenants as long as other resources (CPU, network, etc.) are available.</p>
    <p>l</p>
    <p>l lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>l Tenant 1 Tenant 2</p>
    <p>(a)</p>
    <p>Proportional T1 T2</p>
    <p>DRAM 1.63 1.37 Lat. 0.71 0.20 Cost 7.20 5.41</p>
    <p>Elastic T1 T2</p>
    <p>DRAM 0.46 1.17 Lat. 0.67 0.20 Cost 5.30 4.01</p>
    <p>(b)</p>
    <p>Figure 4: Multi-tenant resource provisioning: (a) latency per DRAM partitioning with proportional SSD size (1:4 ratio) (b) elastic vs proportional with the specified memory partition</p>
    <p>Our study of online resource provisioning is preliminary and remains as future work. Here, we explain the basics of our online implementation.</p>
    <p>Memcached allocates memory in units of fixed size (called slabs) until it reaches its memory limit. Thus we can control resource provisioning by dynamically reconfiguring the memory limit parameter. Increasing this parameter allows Memcached to boost memory provision to the new limit. To lower the provision, we must find and deallocate victim slabs. Prior to deallocation, all the items in the victim slabs must be removed from the LRU chains. We choose victim slabs from different size classes to ensure that the number of slabs in different size classes remains proportional to the initial setting. To find a victim slab in each size class, we monitor the activity of all the slabs during a short period of time. Slab activity is defined as the number of distinct items accessed during that period. For each slab, we estimate its activity using a Hyperloglog counter [8] with logarithmic space overhead. After the monitoring period, we choose the least active slabs as the victim slabs. We keep deallocating slabs until the new memory limit is satisfied.</p>
    <p>Our online analysis consists of a profiling window followed by a release period. During the profiling period, we use the reuse distance analysis to find the miss rate</p>
    <p>for every cache size. At the end of the profiling window, we formulate and solve a linear program according to the constraints and the objective function. The solution gives the new Memcached and KVD sizes. Then, we reconfigure the ElCached system using our above-mentioned procedure.</p>
    <p>The study of online resource provisioning remains as future work. An important question is how the profiling window length affects the accuracy and optimality of solution. Our future work also includes analyzing the memory and time overheads of the online reuse distance analysis.</p>
    <p>Our study is related to previous work on optimizing Web caching applications. For example, Dynacache [4] optimizes Memcached by profiling workloads and dynamically adjusting memory resources and eviction policies. Saemundsson et al. [14] introduced MIMIR, a lightweight online profiler for Memcached. Analogous to us, they used a tree-based scheme to estimate reuse distance. To lower the overhead, MIMIR divides the LRU stack into variable-sized buckets. However, contrary to us, MIMIR and Dynacache both assume a fixed size for all cached elements, which isnt the case in Memcached. Hu et al. [10] introduced LAMA to improve slab allocation in Memcached. Like us, they also estimate the miss ratio for every slab class.</p>
    <p>Reuse distance computation has been well-studied in the past. Mattson et al. [12] introduced a basic stack simulation algorithm. Olken [13] gave the first tree-based method using an AVL tree. Ding and Zhong [6] used a similar technique to approximate reuse distance while reducing the time overhead. Xiang et al. [18] defined the footprint of a given trace to be the number of distinct elements accessed in the window. They showed that under a certain condition (reuse-window condition) reuse distance is equal to the finite differential of average footprint. We implemented the footprint approach and found its accuracy to be slightly lower than our tree-based algorithm. These techniques all require a linear space overhead. Wires et al. [17] used counter stacks to approximate the miss ratio curve with sublinear space overhead. These studies can be integrated into ElCached to reduce the space and time overhead in the online analysis.</p>
    <p>In this paper, we described ElCached, a multi-level keyvalue caching system that uses a reuse distance profiler to estimate the miss rate curve across all capacity limits with 4% relative error. ElCached solves the resource pro</p>
    <p>compared to the 3 GB of physical memory. That is the memory consumption of tenant 1 alone, when taking the proportional approach. This means potential revenue for CSP because it could potentially host more tenants as long as other resources (CPU, network, etc.) are available.</p>
    <p>l</p>
    <p>l lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>l Tenant 1 Tenant 2</p>
    <p>(a)</p>
    <p>Proportional T1 T2</p>
    <p>DRAM 1.63 1.37 Lat. 0.71 0.20 Cost 7.20 5.41</p>
    <p>Elastic T1 T2</p>
    <p>DRAM 0.46 1.17 Lat. 0.67 0.20 Cost 5.30 4.01</p>
    <p>(b)</p>
    <p>Figure 4: Multi-tenant resource provisioning: (a) latency per DRAM partitioning with proportional SSD size (1:4 ratio) (b) elastic vs proportional with the specified memory partition</p>
    <p>Our study of online resource provisioning is preliminary and remains as future work. Here, we explain the basics of our online implementation.</p>
    <p>Memcached allocates memory in units of fixed size (called slabs) until it reaches its memory limit. Thus we can control resource provisioning by dynamically reconfiguring the memory limit parameter. Increasing this parameter allows Memcached to boost memory provision to the new limit. To lower the provision, we must find and deallocate victim slabs. Prior to deallocation, all the items in the victim slabs must be removed from the LRU chains. We choose victim slabs from different size classes to ensure that the number of slabs in different size classes remains proportional to the initial setting. To find a victim slab in each size class, we monitor the activity of all the slabs during a short period of time. Slab activity is defined as the number of distinct items accessed during that period. For each slab, we estimate its activity using a Hyperloglog counter [8] with logarithmic space overhead. After the monitoring period, we choose the least active slabs as the victim slabs. We keep deallocating slabs until the new memory limit is satisfied.</p>
    <p>Our online analysis consists of a profiling window followed by a release period. During the profiling period, we use the reuse distance analysis to find the miss rate</p>
    <p>for every cache size. At the end of the profiling window, we formulate and solve a linear program according to the constraints and the objective function. The solution gives the new Memcached and KVD sizes. Then, we reconfigure the ElCached system using our above-mentioned procedure.</p>
    <p>The study of online resource provisioning remains as future work. An important question is how the profiling window length affects the accuracy and optimality of solution. Our future work also includes analyzing the memory and time overheads of the online reuse distance analysis.</p>
    <p>Our study is related to previous work on optimizing Web caching applications. For example, Dynacache [4] optimizes Memcached by profiling workloads and dynamically adjusting memory resources and eviction policies. Saemundsson et al. [14] introduced MIMIR, a lightweight online profiler for Memcached. Analogous to us, they used a tree-based scheme to estimate reuse distance. To lower the overhead, MIMIR divides the LRU stack into variable-sized buckets. However, contrary to us, MIMIR and Dynacache both assume a fixed size for all cached elements, which isnt the case in Memcached. Hu et al. [10] introduced LAMA to improve slab allocation in Memcached. Like us, they also estimate the miss ratio for every slab class.</p>
    <p>Reuse distance computation has been well-studied in the past. Mattson et al. [12] introduced a basic stack simulation algorithm. Olken [13] gave the first tree-based method using an AVL tree. Ding and Zhong [6] used a similar technique to approximate reuse distance while reducing the time overhead. Xiang et al. [18] defined the footprint of a given trace to be the number of distinct elements accessed in the window. They showed that under a certain condition (reuse-window condition) reuse distance is equal to the finite differential of average footprint. We implemented the footprint approach and found its accuracy to be slightly lower than our tree-based algorithm. These techniques all require a linear space overhead. Wires et al. [17] used counter stacks to approximate the miss ratio curve with sublinear space overhead. These studies can be integrated into ElCached to reduce the space and time overhead in the online analysis.</p>
    <p>In this paper, we described ElCached, a multi-level keyvalue caching system that uses a reuse distance profiler to estimate the miss rate curve across all capacity limits with 4% relative error. ElCached solves the resource pro</p>
    <p>compared to the 3 GB of physical memory. That is the memory consumption of tenant 1 alone, when taking the proportional approach. This means potential revenue for CSP because it could potentially host more tenants as long as other resources (CPU, network, etc.) are available.</p>
    <p>l</p>
    <p>l lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>m em</p>
    <p>or y</p>
    <p>pa rt</p>
    <p>iti on</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>l Tenant 1 Tenant 2</p>
    <p>(a)</p>
    <p>Proportional T1 T2</p>
    <p>DRAM 1.63 1.37 Lat. 0.71 0.20 Cost 7.20 5.41</p>
    <p>Elastic T1 T2</p>
    <p>DRAM 0.46 1.17 Lat. 0.67 0.20 Cost 5.30 4.01</p>
    <p>(b)</p>
    <p>Figure 4: Multi-tenant resource provisioning: (a) latency per DRAM partitioning with proportional SSD size (1:4 ratio) (b) elastic vs proportional with the specified memory partition</p>
    <p>Our study of online resource provisioning is preliminary and remains as future work. Here, we explain the basics of our online implementation.</p>
    <p>Memcached allocates memory in units of fixed size (called slabs) until it reaches its memory limit. Thus we can control resource provisioning by dynamically reconfiguring the memory limit parameter. Increasing this parameter allows Memcached to boost memory provision to the new limit. To lower the provision, we must find and deallocate victim slabs. Prior to deallocation, all the items in the victim slabs must be removed from the LRU chains. We choose victim slabs from different size classes to ensure that the number of slabs in different size classes remains proportional to the initial setting. To find a victim slab in each size class, we monitor the activity of all the slabs during a short period of time. Slab activity is defined as the number of distinct items accessed during that period. For each slab, we estimate its activity using a Hyperloglog counter [8] with logarithmic space overhead. After the monitoring period, we choose the least active slabs as the victim slabs. We keep deallocating slabs until the new memory limit is satisfied.</p>
    <p>Our online analysis consists of a profiling window followed by a release period. During the profiling period, we use the reuse distance analysis to find the miss rate</p>
    <p>for every cache size. At the end of the profiling window, we formulate and solve a linear program according to the constraints and the objective function. The solution gives the new Memcached and KVD sizes. Then, we reconfigure the ElCached system using our above-mentioned procedure.</p>
    <p>The study of online resource provisioning remains as future work. An important question is how the profiling window length affects the accuracy and optimality of solution. Our future work also includes analyzing the memory and time overheads of the online reuse distance analysis.</p>
    <p>Our study is related to previous work on optimizing Web caching applications. For example, Dynacache [4] optimizes Memcached by profiling workloads and dynamically adjusting memory resources and eviction policies. Saemundsson et al. [14] introduced MIMIR, a lightweight online profiler for Memcached. Analogous to us, they used a tree-based scheme to estimate reuse distance. To lower the overhead, MIMIR divides the LRU stack into variable-sized buckets. However, contrary to us, MIMIR and Dynacache both assume a fixed size for all cached elements, which isnt the case in Memcached. Hu et al. [10] introduced LAMA to improve slab allocation in Memcached. Like us, they also estimate the miss ratio for every slab class.</p>
    <p>Reuse distance computation has been well-studied in the past. Mattson et al. [12] introduced a basic stack simulation algorithm. Olken [13] gave the first tree-based method using an AVL tree. Ding and Zhong [6] used a similar technique to approximate reuse distance while reducing the time overhead. Xiang et al. [18] defined the footprint of a given trace to be the number of distinct elements accessed in the window. They showed that under a certain condition (reuse-window condition) reuse distance is equal to the finite differential of average footprint. We implemented the footprint approach and found its accuracy to be slightly lower than our tree-based algorithm. These techniques all require a linear space overhead. Wires et al. [17] used counter stacks to approximate the miss ratio curve with sublinear space overhead. These studies can be integrated into ElCached to reduce the space and time overhead in the online analysis.</p>
    <p>In this paper, we described ElCached, a multi-level keyvalue caching system that uses a reuse distance profiler to estimate the miss rate curve across all capacity limits with 4% relative error. ElCached solves the resource pro</p>
  </div>
  <div class="page">
    <p>Experiment 2: Multitenant Resource Provisioning  Elasticity improves  tenant 1s latency by 5%,  both tenants cost by 26%, and  total memory consumption by 46%.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>ElCached is a multi-level key-value caching system.  It uses a reuse distance profiler to estimate the miss rate curve across all capacity limits.  It reduces the total cost by up to around 60% compared to a proportional scheme.  Multi-tenant experiment indicates that we can improve latency, cost, and total DRAM usage, compared to the proportional scheme.</p>
  </div>
  <div class="page">
    <p>Thank You</p>
  </div>
</Presentation>
