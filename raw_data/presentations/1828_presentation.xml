<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Mobile App Acceleration via ! Fine-Grain Offloading to the Cloud</p>
    <p>Chit-Kwan Lin H. T. Kung</p>
  </div>
  <div class="page">
    <p>Confluence of Forces Points to ! Offloading to the Cloud</p>
    <p>Copyright  2014 UpShift Labs, Inc. 2</p>
    <p>Compute Offloading</p>
    <p>Devices  Proliferation of</p>
    <p>smartphones &amp; tablets</p>
    <p>Complex tasks ! (e.g., imaging, learning, recognition)</p>
    <p>Emphasis on battery efficiency</p>
    <p>Network</p>
    <p>Internet infrastructure</p>
    <p>Wireless infrastructure (Wi-Fi &amp; cellular)</p>
    <p>High bandwidths &amp; ! low latencies</p>
    <p>Cloud</p>
    <p>Cloud computing infrastructures</p>
    <p>Economies of scale</p>
    <p>On-demand provisioning</p>
  </div>
  <div class="page">
    <p>*</p>
    <p>Cloud Daemon</p>
    <p>A Simple DSM Supports Offloading</p>
    <p>Copyright  2014 UpShift Labs, Inc. 3</p>
    <p>Mobile App</p>
    <p>Distributed Shared Memory Memory Memory (replica)</p>
    <p>Continuous! Replication</p>
    <p>* See last slide for image source references</p>
    <p>Object</p>
    <p>variable_X</p>
    <p>method_Y()</p>
    <p>Object</p>
    <p>variable_X</p>
    <p>method_Y() Dynamic + Fine-grain Offloading</p>
  </div>
  <div class="page">
    <p>Advantages of Dynamic + Fine-Grain  Dynamic  Can offload arbitrary work at runtime  Can optimize resource utilization (e.g., battery) at</p>
    <p>runtime</p>
    <p>Fine-grain  At the level of a method invocation  Feels more responsive when failure requires local</p>
    <p>restart to fix  New class of small workloads for cloud providers</p>
    <p>Useful for leveling out utilization while providing low-latency services for end-users</p>
    <p>Copyright  2014 UpShift Labs, Inc. 4</p>
  </div>
  <div class="page">
    <p>Challenges  Latency</p>
    <p>Dictates granularity: if update takes 5s, then workloads that run &lt;5s dont benefit from offloading</p>
    <p>Bandwidth  We should only send deltas, but determining delta encoding has non</p>
    <p>trivial costs  e.g., rsync can take 3 round trips (weak hash comparison, strong hash</p>
    <p>comparison, data) to generate a delta encoding, on top of time to calculate hashes</p>
    <p>Compute  We should compress to save bandwidth, but compression can be</p>
    <p>computationally expensive</p>
    <p>Battery  Shouldnt end up consuming more battery budget</p>
    <p>Copyright  2014 UpShift Labs, Inc. 5</p>
  </div>
  <div class="page">
    <p>Compressive Sensing</p>
    <p>Randomly mix signal elements by random projection onto lowerdimensional space</p>
    <p>Random  preserves Euclidean length/distance of sparse vectors with high probability when M  O(K log N/K)</p>
    <p>Decode x from y by solving y = x via optimization (linear programming)</p>
    <p>=   y x</p>
    <p>M  N M  1</p>
    <p>N  1</p>
    <p>Random sampling! matrix !</p>
    <p>(M &lt; N)</p>
    <p>K-sparse signal</p>
    <p>Compressive ! samples</p>
  </div>
  <div class="page">
    <p>Key Insight</p>
    <p>Writes (deltas) to memory typically constitute a sparse signal that can be</p>
    <p>compressively sampled</p>
    <p>Copyright  2014 UpShift Labs, Inc. 7</p>
  </div>
  <div class="page">
    <p>Compressive Replication (1)</p>
    <p>Copyright  2014 UpShift Labs, Inc. 8</p>
    <p>Device Server</p>
    <p>x0</p>
    <p>Time t0</p>
    <p>Memory starts out synchronized, with byte values x0</p>
    <p>Both device and server know</p>
    <p>Server calculates y0 = x0 y0 = x0</p>
    <p>x0</p>
  </div>
  <div class="page">
    <p>Compressive Replication (2)</p>
    <p>Copyright  2014 UpShift Labs, Inc. 9</p>
    <p>Device Server</p>
    <p>x1</p>
    <p>Time t1</p>
    <p>x0</p>
    <p>Some values in memory change, resulting in x1</p>
    <p>Device calculates</p>
    <p>Device transmits y1 = x1 to server</p>
    <p>y0 = x0</p>
    <p>y1 = x1 y1 = x1</p>
    <p>y1 = x1</p>
  </div>
  <div class="page">
    <p>y = y0  y1 y' = x0  x1 y' = (x0  x1) y' = x</p>
    <p>Compressive Replication (3)</p>
    <p>Copyright  2014 UpShift Labs, Inc. 10</p>
    <p>Device Server</p>
    <p>x1</p>
    <p>Time t1</p>
    <p>x0</p>
    <p>y0 = x0 y1 = x1</p>
    <p>Server calculates</p>
    <p>y = x has same form as compressive sensing decoding problem, so server solves for x</p>
    <p>y = y0  y1 y = y0  y1 x = x0  x1 is the delta encoding!</p>
    <p>y = y0  y1</p>
  </div>
  <div class="page">
    <p>Compressive Replication (4)</p>
    <p>Copyright  2014 UpShift Labs, Inc. 11</p>
    <p>Device Server</p>
    <p>x1</p>
    <p>Time t1</p>
    <p>x1</p>
    <p>x0  x = x0  (x0  x1) = x1</p>
    <p>Server calculates x0  x = x1, which is the new memory state Server is now updated</p>
  </div>
  <div class="page">
    <p>Novel Characteristics  All-in-one  Delta encoding + compression</p>
    <p>Delta encoding figured out by server, not device  Automatically recovered during decoding  Just send compressive samples; no addl network costs</p>
    <p>Codec is resource-commensurate  Device: low-complexity encoder  Server: higher-complexity decoder  Unlike traditional compressors</p>
    <p>Copyright  2014 UpShift Labs, Inc. 12</p>
  </div>
  <div class="page">
    <p>What do we compare against?  No similar replication methods</p>
    <p>Compressive replication gives all-in-one delta encoding + compression  e.g., rsync</p>
    <p>Compression is an addl step  Needs multiple round-trips to determine delta encoding</p>
    <p>Compressed snapshots is a more appropriate comparison  No addl round-trip overheads  Just compress the whole memory page (snapshot)  snappy, zlib</p>
    <p>Metrics  Replicate 64KB memory block  Total Latency = encoding + network + decoding  Compression ratio (bandwidth cost)</p>
    <p>Copyright  2014 UpShift Labs, Inc. 13</p>
  </div>
  <div class="page">
    <p>Latency/Compression Trade-off</p>
    <p>Copyright  2014 UpShift Labs, Inc. 14</p>
    <p>Encoding (ms)</p>
    <p>Network (ms)</p>
    <p>Decoding (ms)</p>
    <p>Total Latency</p>
    <p>(ms)!</p>
    <p>Compression! Ratio!</p>
    <p>Update Size ! (KB)</p>
    <p>snappy! 4 15 -- 19 3.8 : 1 17.2</p>
    <p>zlib! 487 13 -- 500 6.0 : 1 10.9</p>
    <p>compressive replication! 53 12 70 135 7.3 : 1 9.0</p>
    <p>Near ~100ms threshold of userperceptible app delay</p>
    <p>Highest compression ratio</p>
  </div>
  <div class="page">
    <p>Latency/Compression Trade-off</p>
    <p>Copyright  2014 UpShift Labs, Inc. 15</p>
    <p>Encoding (ms)</p>
    <p>Network (ms)</p>
    <p>Decoding (ms)</p>
    <p>Total Latency</p>
    <p>(ms)!</p>
    <p>Compression! Ratio!</p>
    <p>Update Size ! (KB)</p>
    <p>snappy! 4 15 -- 19 3.8 : 1 17.2</p>
    <p>zlib! 487 13 -- 500 6.0 : 1 10.9</p>
    <p>compressive replication! 53 12 70 135 7.3 : 1 9.0</p>
    <p>Unlike snappy/zlib, comp ratio fixed &amp; b/w cost predictable because y = x is independent of inputs Kolmogorov complexity</p>
    <p>Remains good w/ high Kolmogorov complexity, which would confound snappy/zlib</p>
  </div>
  <div class="page">
    <p>Example Handwriting Recognition App  UpShift platform for compressive</p>
    <p>offloading of iOS apps</p>
    <p>SVM for Chinese handwriting recognition  Stroke count is measure of task</p>
    <p>complexity</p>
    <p>Device: iPad (3rd gen)  Server: Amazon g2.xlarge</p>
    <p>GPU for compressive replication decoding</p>
    <p>CPU for SVM evaluation  Network: 802.11g</p>
    <p>Office setting  19ms RTT to us-east-1a server</p>
    <p>Copyright  2014 UpShift Labs, Inc. 16</p>
  </div>
  <div class="page">
    <p>App Speedup</p>
    <p>Copyright  2014 UpShift Labs, Inc. 17</p>
    <p>More complex tasks have greater speedup</p>
    <p>On-device execution time scales poorly with task complexity</p>
    <p>Offloaded execution time stays short due to low overhead of compressive offloading. Users expect this.</p>
  </div>
  <div class="page">
    <p>Battery Savings</p>
    <p>Copyright  2014 UpShift Labs, Inc. 18</p>
    <p>Offloading allows 60% more work to be done with the same battery budget.</p>
    <p>High task complexity (25 strokes), 250 iterations</p>
  </div>
  <div class="page">
    <p>Summary  Low-latency DSM updates enable fine-grain offloading</p>
    <p>UpShift supports offloading ~100ms workloads, while keeping resource utilization low</p>
    <p>Achieves significant speedups and battery savings</p>
    <p>Key insight: memory writes are a sparse signal that can be compressively sampled</p>
    <p>Implications for future ARM-based DCs or x86-based devices</p>
    <p>Copyright  2014 UpShift Labs, Inc. 19</p>
  </div>
  <div class="page">
    <p>Copyright  2014 UpShift Labs, Inc. 20</p>
    <p>Questions?</p>
    <p>ck@upshiftlabs.com</p>
  </div>
</Presentation>
