<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Predicting Execution Bottlenecks</p>
    <p>in Map-Reduce Clusters</p>
    <p>Edward Bortnikov, Ari Frank, Eshcar Hillel, Sriram Rao Presenting: Alex Shraer</p>
    <p>Yahoo! Labs</p>
  </div>
  <div class="page">
    <p>- 2 - Yahoo! Confidential</p>
    <p>The Map Reduce (MR) Paradigm  Architecture for scalable information processing</p>
    <p>Simple API  Computation scales to Web-scale data collections</p>
    <p>Google MR  Pioneered the technology in early 2000s</p>
    <p>Hadoop: open-source implementation  In use at Amazon, eBay, Facebook, Yahoo!,   Scales to 10Ks nodes (Hadoop 2.0)</p>
    <p>Many proprietary implementations  MR technologies at Microsoft, Yandex,</p>
  </div>
  <div class="page">
    <p>- 3 - Yahoo! Confidential</p>
    <p>Computational Model</p>
    <p>M1</p>
    <p>M2</p>
    <p>M3</p>
    <p>M4</p>
    <p>R1</p>
    <p>R2</p>
    <p>O ut</p>
    <p>pu t</p>
    <p>(o n</p>
    <p>D FS</p>
    <p>)</p>
    <p>In pu</p>
    <p>t (o</p>
    <p>n D</p>
    <p>FS )</p>
    <p>Synchronous execution: every R starts computing</p>
    <p>after all Ms have completed</p>
    <p>Slowest task (straggler) affects the job latency</p>
  </div>
  <div class="page">
    <p>- 4 - Yahoo! Confidential</p>
    <p>Predicting Straggler Tasks</p>
    <p>Straggler tasks are an inherent bottleneck  Affect job latency, and to some extend throughput</p>
    <p>Two approaches to tackle stragglers  Avoidance  reduce the probability of straggler emergence  Detection  once a task goes astray, speculatively fire a</p>
    <p>duplicate task somewhere else</p>
    <p>This work  straggler prediction  Fits with both avoidance and detection scenarios</p>
  </div>
  <div class="page">
    <p>- 5 - Yahoo! Confidential</p>
    <p>Background  Detection, Speculative Execution</p>
    <p>First implemented in Google MR (OSDI 04)  Hadoop employs a crude detection heuristic</p>
    <p>LATE scheduler (OSDI 08) addresses the issues of heterogeneous hardware. Evaluated on small scale.</p>
    <p>Microsoft MR (Mantri project, OSDI 10)</p>
    <p>Avoidance  Local/rack-local data access is preferred for mappers</p>
    <p>Network less likely to become the bottleneck</p>
    <p>All optimizations are heuristic</p>
  </div>
  <div class="page">
    <p>- 6 - Yahoo! Confidential</p>
    <p>Machine-Learned vs Heuristic Prediction  Heuristics are hard to</p>
    <p>Tune for real workloads  Catch transient bottlenecks</p>
    <p>Some evidence from Hadoop grids at Yahoo!  Speculative scheduling is non-timely and wasteful</p>
    <p>90% of the fired duplicates are eventually killed  Data-local computation amplifies contention</p>
    <p>Can we use the wealth of historical grid performance data to train a machine-learned bottleneck classifier?</p>
  </div>
  <div class="page">
    <p>- 7 - Yahoo! Confidential</p>
    <p>Why Should Machine Learning Work?</p>
    <p>Recurrence</p>
    <p>(over 5 months)</p>
    <p>Fraction of jobs</p>
    <p>Fraction of maps</p>
    <p>Fraction of reduces</p>
    <p>Huge recurrence of large jobs in production grids</p>
    <p>Target workload</p>
  </div>
  <div class="page">
    <p>- 8 - Yahoo! Confidential</p>
    <p>The Slowdown Metric  Task slowdown factor</p>
    <p>Ratio between the tasks running time and the median running time among the sibling tasks in the same job.</p>
    <p>Root causes  Data skew  input or output significantly exceeds the</p>
    <p>median for the job  Tasks with skew &gt; 4x happen really seldom.</p>
    <p>Hotspots  all the other reasons  Congested/misconfigured/degraded nodes, disks, or network.  Typically transient. The resulting slow can be very high.</p>
  </div>
  <div class="page">
    <p>- 9 - Yahoo! Confidential</p>
    <p>Jobs with Mapper Slowdown &gt; 5x</p>
    <p>Mapper Slowdown</p>
    <p>Mapper Tasks per Job</p>
    <p>S lo</p>
    <p>w d</p>
    <p>o w</p>
    <p>n F</p>
    <p>a c to</p>
    <p>r</p>
    <p>1% among all jobs  5% among jobs with 1000 mappers or more  40% due to data skew (2x or above), 60% due to hotspots</p>
    <p>Sample of ~50K jobs</p>
  </div>
  <div class="page">
    <p>- 10 - Yahoo! Confidential</p>
    <p>Jobs with Reducer Slowdown &gt; 5x Reducer Slowdown</p>
    <p>Reducer tasks per job</p>
    <p>S lo</p>
    <p>w d</p>
    <p>o w</p>
    <p>n F</p>
    <p>a c to</p>
    <p>r</p>
    <p>5% among all jobs  50% among jobs with 1000 reducers or more  10% due to data skew (2x or above), 90% due to hotspots</p>
    <p>Sample of ~60K jobs</p>
  </div>
  <div class="page">
    <p>- 11 - Yahoo! Confidential</p>
    <p>Locality is No Silver Bullet</p>
    <p>T im</p>
    <p>e s</p>
    <p>ta m</p>
    <p>p</p>
    <p>Node id</p>
    <p>Outliers among Local Mappers</p>
    <p>The same nodes are constantly lagging behind  Weaker CPUs (grid HW is heterogeneous), data hotspots, etc.</p>
    <p>Pushing for locality too hard amplifies the problem!</p>
    <p>Top contributor of straggler tasks</p>
    <p>over 6 hours</p>
  </div>
  <div class="page">
    <p>- 12 - Yahoo! Confidential</p>
    <p>Slowdown Predictor  An oracle plugin into a Map Reduce system</p>
    <p>Input: node features + task features  Output: slowdown estimate</p>
    <p>Features  M/R metrics (job- and task-level)  DFS metrics (datanode-level)  System metrics (host-level: CPU, RAM, disk I/O, JVM, )  Network traffic (host-, rack- and cross-rack-level)</p>
  </div>
  <div class="page">
    <p>- 13 - Yahoo! Confidential</p>
    <p>Slowdown Prediction - Mappers Map Tasks</p>
    <p>R2 = 0.7978</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>-3 -2 -1 0 1 2 3</p>
    <p>Actual Slowdown (log scale)</p>
    <p>Pr ed</p>
    <p>ic te</p>
    <p>d Sl</p>
    <p>ow do</p>
    <p>w n</p>
    <p>(lo g</p>
    <p>sc al</p>
    <p>e)</p>
    <p>Mis-predicted, need improvement</p>
  </div>
  <div class="page">
    <p>- 14 - Yahoo! Confidential</p>
    <p>Slowdown Prediction - Reducers Reduce Tasks</p>
    <p>R2 = 0.4083</p>
    <p>-3</p>
    <p>-2</p>
    <p>-1</p>
    <p>-3 -2 -1 0 1 2 3</p>
    <p>Actual Slowdown (log scale)</p>
    <p>Pr ed</p>
    <p>ic te</p>
    <p>d Sl</p>
    <p>ow do</p>
    <p>w n</p>
    <p>(lo g</p>
    <p>sc al</p>
    <p>e)</p>
    <p>More dispersed than the mappers</p>
  </div>
  <div class="page">
    <p>- 15 - Yahoo! Confidential</p>
    <p>Some Conclusions  Data skew is the most important signal, but there are</p>
    <p>many more that are important  Node HW generation is a very significant signal for both</p>
    <p>mappers and reducers  Large grids undergo continuous HW upgrades</p>
    <p>Network traffic features (intra-rack and cross-rack) is much more important for reducers than for mappers  How to collect efficiently in a real-time setting?</p>
    <p>Need to enhance data sampling/weighting to capture outliers better</p>
  </div>
  <div class="page">
    <p>- 16 - Yahoo! Confidential</p>
    <p>Takeaways  Slowdown prediction</p>
    <p>ML approach to straggler avoidance and detection</p>
    <p>Initial evaluation showed viability  Need to enhance training to capture outliers better</p>
    <p>Challenge: runtime implementation  A good blend with the modern MR system architecture?</p>
  </div>
  <div class="page">
    <p>- 17 - Yahoo! Confidential</p>
    <p>Thank you</p>
  </div>
  <div class="page">
    <p>- 18 - Yahoo! Confidential</p>
    <p>Machine Learning Technique</p>
    <p>Gradient Boosted Decision Trees (GBDT)  Additive regression model  Based on ensemble of binary decision trees  100 trees, 10 leaf nodes each</p>
  </div>
  <div class="page">
    <p>- 19 - Yahoo! Confidential</p>
    <p>Challenges  Hadoop Use Case  Hadoop 1.0  centralized architecture</p>
    <p>The single Job Tracker process manages all task assignment and scheduling</p>
    <p>Full picture of Map and Reduce slots across the cluster</p>
    <p>Hadoop 2.0  distributed architecture  Resource management and scheduling functions split  Thin centralized Resource Manager (RM) creates application</p>
    <p>containers (e.g., for running a Map Reduce job)  Per-job App Master (AM) does scheduling within a container</p>
    <p>May negotiate resource allocation with the RM</p>
    <p>Challenge: working with a limited set of local signals</p>
  </div>
  <div class="page">
    <p>- 20 - Yahoo! Confidential</p>
    <p>Possible Design  Hadoop 2.0</p>
    <p>Application Master</p>
    <p>Resource Manager</p>
    <p>Node Manager</p>
    <p>Node Manager</p>
    <p>Node Manager</p>
    <p>Model</p>
    <p>Metrics Metrics Metrics</p>
    <p>Reso urce</p>
    <p>requ ests</p>
    <p>App Cont</p>
    <p>aine r Cre</p>
    <p>ation</p>
    <p>Metrics collection (extends the existing HB protocol)</p>
    <p>New component or API</p>
    <p>Job Execution Environment</p>
    <p>Some metrics already collected (CPU ticks, bytes R/W)</p>
    <p>Others might be collected either by NM, or externally</p>
    <p>Centralized prediction will not scale.</p>
    <p>Will distributed prediction be accurate</p>
    <p>enough?</p>
  </div>
</Presentation>
