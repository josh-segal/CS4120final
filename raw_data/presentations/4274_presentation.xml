<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards String-to-Tree Neural Machine Translation</p>
    <p>Roee Aharoni &amp; Yoav Goldberg NLP Lab, Bar Ilan University</p>
    <p>ACL 2017</p>
  </div>
  <div class="page">
    <p>NMT is all the rage!</p>
  </div>
  <div class="page">
    <p>NMT is all the rage!</p>
  </div>
  <div class="page">
    <p>NMT is all the rage!</p>
    <p>Driving the current state-of-the-art (Sennrich et al., 2016)</p>
  </div>
  <div class="page">
    <p>NMT is all the rage!</p>
    <p>Driving the current state-of-the-art (Sennrich et al., 2016)</p>
    <p>Widely adopted by the industry</p>
  </div>
  <div class="page">
    <p>Seq2Seq with Attention Bahdanau et al. (2015)</p>
  </div>
  <div class="page">
    <p>Seq2Seq with Attention</p>
    <p>f = argmax f0</p>
    <p>p(f0|e)</p>
    <p>Bahdanau et al. (2015)</p>
  </div>
  <div class="page">
    <p>Syntax was all the rage!</p>
  </div>
  <div class="page">
    <p>Syntax was all the rage!  The previous state-of-the</p>
    <p>art was syntax-based SMT</p>
    <p>From Rico Sennrich, NMT: Breaking the Performance Plateau, 2016</p>
  </div>
  <div class="page">
    <p>From Williams, Sennrich, Post &amp; Koehn (2016), Syntax-based Statistical Machine Translation</p>
    <p>Syntax was all the rage!  The previous state-of-the</p>
    <p>art was syntax-based SMT</p>
    <p>i.e. systems that used linguistic information (usually represented as parse trees)</p>
  </div>
  <div class="page">
    <p>Syntax was all the rage!  The previous state-of-the</p>
    <p>art was syntax-based SMT</p>
    <p>i.e. systems that used linguistic information (usually represented as parse trees)</p>
    <p>Beaten by NMT in 2016</p>
    <p>From Rico Sennrich, NMT: Breaking the Performance Plateau, 2016</p>
  </div>
  <div class="page">
    <p>Syntax was all the rage!  The previous state-of-the</p>
    <p>art was syntax-based SMT</p>
    <p>i.e. systems that used linguistic information (usually represented as parse trees)</p>
    <p>Beaten by NMT in 2016</p>
    <p>Can we bring the benefits of syntax into the recent neural systems? From Rico Sennrich, NMT: Breaking</p>
    <p>the Performance Plateau, 2016</p>
  </div>
  <div class="page">
    <p>Syntax: Constituency Structure</p>
  </div>
  <div class="page">
    <p>Syntax: Constituency Structure  A Constituency (a.k.a Phrase-Structure) grammar defines a set of</p>
    <p>rewrite rules which describe the structure of the language.</p>
  </div>
  <div class="page">
    <p>Syntax: Constituency Structure  A Constituency (a.k.a Phrase-Structure) grammar defines a set of</p>
    <p>rewrite rules which describe the structure of the language.  Groups words into larger units (constituents)</p>
  </div>
  <div class="page">
    <p>Syntax: Constituency Structure  A Constituency (a.k.a Phrase-Structure) grammar defines a set of</p>
    <p>rewrite rules which describe the structure of the language.  Groups words into larger units (constituents)  Defines a hierarchy between constituents</p>
  </div>
  <div class="page">
    <p>Syntax: Constituency Structure  A Constituency (a.k.a Phrase-Structure) grammar defines a set of</p>
    <p>rewrite rules which describe the structure of the language.  Groups words into larger units (constituents)  Defines a hierarchy between constituents  Draws relations between different constituents (words, phrases,</p>
    <p>clauses)</p>
  </div>
  <div class="page">
    <p>Why Syntax Can Help MT?</p>
  </div>
  <div class="page">
    <p>Hints as to which word sequences belong together</p>
    <p>Why Syntax Can Help MT?</p>
  </div>
  <div class="page">
    <p>Hints as to which word sequences belong together  Helps in producing well structured sentences</p>
    <p>Why Syntax Can Help MT?</p>
  </div>
  <div class="page">
    <p>Hints as to which word sequences belong together  Helps in producing well structured sentences  Allows informed reordering decisions according to the</p>
    <p>syntactic structure</p>
    <p>Why Syntax Can Help MT?</p>
  </div>
  <div class="page">
    <p>Hints as to which word sequences belong together  Helps in producing well structured sentences  Allows informed reordering decisions according to the</p>
    <p>syntactic structure  Encourages long-distance dependencies when</p>
    <p>selecting translations</p>
    <p>Why Syntax Can Help MT?</p>
  </div>
  <div class="page">
    <p>String-to-Tree Translation source</p>
    <p>target</p>
  </div>
  <div class="page">
    <p>Our Approach: String-to-Tree NMT</p>
  </div>
  <div class="page">
    <p>Our Approach: String-to-Tree NMT</p>
    <p>source</p>
  </div>
  <div class="page">
    <p>Our Approach: String-to-Tree NMT</p>
    <p>source target</p>
  </div>
  <div class="page">
    <p>Our Approach: String-to-Tree NMT</p>
    <p>Main idea: translate a source sentence into a linearized tree of the target sentence</p>
    <p>source target</p>
  </div>
  <div class="page">
    <p>Our Approach: String-to-Tree NMT</p>
    <p>Main idea: translate a source sentence into a linearized tree of the target sentence</p>
    <p>Inspired by works on RNN-based syntactic parsing (Vinyals et. al, 2015, Choe &amp; Charniak, 2016)</p>
    <p>source target</p>
  </div>
  <div class="page">
    <p>Our Approach: String-to-Tree NMT</p>
    <p>Main idea: translate a source sentence into a linearized tree of the target sentence</p>
    <p>Inspired by works on RNN-based syntactic parsing (Vinyals et. al, 2015, Choe &amp; Charniak, 2016)</p>
    <p>Allows using the seq2seq framework as-is</p>
    <p>source target</p>
  </div>
  <div class="page">
    <p>Experimental Details</p>
    <p>We used the Nematus toolkit (Sennrich et al. 2017)</p>
    <p>Joint BPE segmentation (Sennrich et al. 2016)</p>
    <p>For training, we parse the target side using the BLLIP parser (McClosky, Charniak and Johnson, 2006)</p>
    <p>Requires some care about making BPE, Tokenization and Parser work together</p>
  </div>
  <div class="page">
    <p>Experiments - Large Scale</p>
  </div>
  <div class="page">
    <p>Experiments - Large Scale  German to English, 4.5 million parallel training sentences from WMT16</p>
  </div>
  <div class="page">
    <p>Experiments - Large Scale  German to English, 4.5 million parallel training sentences from WMT16</p>
    <p>Train two NMT models using the same setup (same settings as the SOTA neural system in WMT16)</p>
  </div>
  <div class="page">
    <p>Experiments - Large Scale  German to English, 4.5 million parallel training sentences from WMT16</p>
    <p>Train two NMT models using the same setup (same settings as the SOTA neural system in WMT16)</p>
    <p>syntax-aware (bpe2tree)</p>
  </div>
  <div class="page">
    <p>Experiments - Large Scale  German to English, 4.5 million parallel training sentences from WMT16</p>
    <p>Train two NMT models using the same setup (same settings as the SOTA neural system in WMT16)</p>
    <p>syntax-aware (bpe2tree)</p>
    <p>syntax-agnostic baseline (bpe2bpe)</p>
  </div>
  <div class="page">
    <p>Experiments - Large Scale  German to English, 4.5 million parallel training sentences from WMT16</p>
    <p>Train two NMT models using the same setup (same settings as the SOTA neural system in WMT16)</p>
    <p>syntax-aware (bpe2tree)</p>
    <p>syntax-agnostic baseline (bpe2bpe)</p>
    <p>The syntax-aware model performs better in terms of BLEU</p>
    <p>Single Model</p>
  </div>
  <div class="page">
    <p>Experiments - Low Resource</p>
    <p>German/Russian/Czech to English - 180k-140k parallel training sentences (News Commentary v8)</p>
    <p>The syntax-aware model performs better in terms of BLEU in all cases (12 comparisons)</p>
    <p>Up to 2+ BLEU improvement</p>
  </div>
  <div class="page">
    <p>Looking Beyond BLEU</p>
  </div>
  <div class="page">
    <p>Accurate Trees  99% of the predicted trees in the development set had valid bracketing</p>
    <p>Eye-balling the predicted trees found them well-formed and following the syntax of English.</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produced more sensible alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produced more sensible alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produced more sensible alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produces more sensible alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produces more sensible alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produces more sensible alignments</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Alignments</p>
    <p>The attention based model induces soft alignments between the source and the target</p>
    <p>The syntax-aware model produces more sensible alignments</p>
  </div>
  <div class="page">
    <p>Attending to Source Syntax</p>
  </div>
  <div class="page">
    <p>Attending to Source Syntax  We inspected the attention weights</p>
    <p>during the production of the trees opening brackets</p>
  </div>
  <div class="page">
    <p>Attending to Source Syntax  We inspected the attention weights</p>
    <p>during the production of the trees opening brackets</p>
    <p>The model consistently attends to the main verb (hatte&quot;) or to structural markers (question marks, hyphens) in the source sentence</p>
  </div>
  <div class="page">
    <p>Attending to Source Syntax  We inspected the attention weights</p>
    <p>during the production of the trees opening brackets</p>
    <p>The model consistently attends to the main verb (hatte&quot;) or to structural markers (question marks, hyphens) in the source sentence</p>
    <p>Indicates the system implicitly learns source syntax to some extent (Shi, Padhi and Knight, 2016) and possibly plans the decoding accordingly</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Structure</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Structure</p>
  </div>
  <div class="page">
    <p>Where Syntax Helps? Structure</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering</p>
    <p>German to English translation requires a significant amount of reordering during translation</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering</p>
    <p>German to English translation requires a significant amount of reordering during translation</p>
    <p>Quantifying reordering shows that the syntax-aware system performs more reordering during the training process</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering  We would like to interpret the increased reordering from a syntactic</p>
    <p>perspective</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering  We would like to interpret the increased reordering from a syntactic</p>
    <p>perspective</p>
    <p>We extract GHKM rules (Galley et al., 2004) from the dev set using the predicted trees and attention-induced alignments</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering  We would like to interpret the increased reordering from a syntactic</p>
    <p>perspective</p>
    <p>We extract GHKM rules (Galley et al., 2004) from the dev set using the predicted trees and attention-induced alignments</p>
    <p>The most common rules reveal linguistically sensible transformations, like moving the verb from the end of a German constituent to the beginning of the matching English one</p>
    <p>German English:</p>
  </div>
  <div class="page">
    <p>Structure (I) - Reordering  We would like to interpret the increased reordering from a syntactic</p>
    <p>perspective</p>
    <p>We extract GHKM rules (Galley et al., 2004) from the dev set using the predicted trees and attention-induced alignments</p>
    <p>The most common rules reveal linguistically sensible transformations, like moving the verb from the end of a German constituent to the beginning of the matching English one</p>
    <p>More examples in the paper</p>
    <p>German English:</p>
  </div>
  <div class="page">
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Structure (II) - Relative Constructions</p>
    <p>A common linguistic structure is relative constructions, i.e. The XXX which YYY, A XXX whose YYY</p>
  </div>
  <div class="page">
    <p>Structure (II) - Relative Constructions</p>
    <p>A common linguistic structure is relative constructions, i.e. The XXX which YYY, A XXX whose YYY</p>
    <p>The words that connect the clauses in such constructions are called relative pronouns, i.e. who, which, whom</p>
  </div>
  <div class="page">
    <p>Structure (II) - Relative Constructions</p>
    <p>A common linguistic structure is relative constructions, i.e. The XXX which YYY, A XXX whose YYY</p>
    <p>The words that connect the clauses in such constructions are called relative pronouns, i.e. who, which, whom</p>
    <p>The syntax-aware system produced more relative pronouns due to the syntactic context</p>
  </div>
  <div class="page">
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Source:</p>
    <p>Guangzhou, das in Deutschland auch Kanton genannt wird</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Source:</p>
    <p>Guangzhou, das in Deutschland auch Kanton genannt wird</p>
    <p>Reference:</p>
    <p>Guangzhou, which is also known as Canton in Germany</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Syntax-Agnostic:</p>
    <p>Guangzhou, also known in Germany, is one of</p>
    <p>Source:</p>
    <p>Guangzhou, das in Deutschland auch Kanton genannt wird</p>
    <p>Reference:</p>
    <p>Guangzhou, which is also known as Canton in Germany</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Syntax-Agnostic:</p>
    <p>Guangzhou, also known in Germany, is one of</p>
    <p>Syntax-Based:</p>
    <p>Guangzhou, which is also known as the canton in Germany,</p>
    <p>Source:</p>
    <p>Guangzhou, das in Deutschland auch Kanton genannt wird</p>
    <p>Reference:</p>
    <p>Guangzhou, which is also known as Canton in Germany</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Source:</p>
    <p>Zugleich droht der stark von internationalen Firmen abhngigen Region ein Imageschaden</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Reference:</p>
    <p>At the same time, the image of the region, which is heavily reliant on international companies</p>
    <p>Source:</p>
    <p>Zugleich droht der stark von internationalen Firmen abhngigen Region ein Imageschaden</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Syntax-Agnostic:</p>
    <p>At the same time, the region's heavily dependent region</p>
    <p>Reference:</p>
    <p>At the same time, the image of the region, which is heavily reliant on international companies</p>
    <p>Source:</p>
    <p>Zugleich droht der stark von internationalen Firmen abhngigen Region ein Imageschaden</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Syntax-Agnostic:</p>
    <p>At the same time, the region's heavily dependent region</p>
    <p>Syntax-Based:</p>
    <p>At the same time, the region, which is heavily dependent on international firms</p>
    <p>Reference:</p>
    <p>At the same time, the image of the region, which is heavily reliant on international companies</p>
    <p>Source:</p>
    <p>Zugleich droht der stark von internationalen Firmen abhngigen Region ein Imageschaden</p>
    <p>Structure (II) - Relative Constructions</p>
  </div>
  <div class="page">
    <p>Human Evaluation</p>
  </div>
  <div class="page">
    <p>Human Evaluation  We performed a small-scale human-evaluation using mechanical</p>
    <p>turk on the first 500 sentences in newstest 2015</p>
  </div>
  <div class="page">
    <p>Human Evaluation  We performed a small-scale human-evaluation using mechanical</p>
    <p>turk on the first 500 sentences in newstest 2015</p>
    <p>Two turkers per sentence</p>
  </div>
  <div class="page">
    <p>Human Evaluation  We performed a small-scale human-evaluation using mechanical</p>
    <p>turk on the first 500 sentences in newstest 2015</p>
    <p>Two turkers per sentence</p>
    <p>The syntax-aware translations had an advantage over the baseline</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Conclusions  Neural machine translation can clearly benefit from target-side</p>
    <p>syntax Other recent work include:</p>
  </div>
  <div class="page">
    <p>Conclusions  Neural machine translation can clearly benefit from target-side</p>
    <p>syntax Other recent work include:  Eriguchi et al., 2017, Wu et al., 2017 (Dependency)</p>
  </div>
  <div class="page">
    <p>Conclusions  Neural machine translation can clearly benefit from target-side</p>
    <p>syntax Other recent work include:  Eriguchi et al., 2017, Wu et al., 2017 (Dependency)  Nadejde et al., 2017 (CCG)</p>
  </div>
  <div class="page">
    <p>Conclusions  Neural machine translation can clearly benefit from target-side</p>
    <p>syntax Other recent work include:  Eriguchi et al., 2017, Wu et al., 2017 (Dependency)  Nadejde et al., 2017 (CCG)</p>
    <p>A general approach - can be easily incorporated into other neural language generation tasks like summarization, image caption generation</p>
  </div>
  <div class="page">
    <p>Conclusions  Neural machine translation can clearly benefit from target-side</p>
    <p>syntax Other recent work include:  Eriguchi et al., 2017, Wu et al., 2017 (Dependency)  Nadejde et al., 2017 (CCG)</p>
    <p>A general approach - can be easily incorporated into other neural language generation tasks like summarization, image caption generation</p>
    <p>Larger picture: dont throw away your linguistics! Neural systems can also leverage symbolic linguistic information</p>
  </div>
  <div class="page"/>
</Presentation>
