<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Learning attention for historical text normalization by learning to pronounce</p>
    <p>Marcel Bollmann1 Joachim Bingel2 Anders Sgaard2</p>
    <p>ACL 2017, Vancouver, Canada</p>
    <p>July 31, 2017</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>What is historical text normalization? Previous work</p>
    <p>Motivation</p>
    <p>Sample of a manuscript from Early New High German</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>What is historical text normalization? Previous work</p>
    <p>A corpus of Early New High German</p>
    <p>I Medieval religious treatise InterrogatioSanctiAnselmidePassioneDomini</p>
    <p>I &gt; 50 manuscripts and prints (in German)</p>
    <p>I 14th16th century I Various dialects</p>
    <p>I Bavarian I MiddleGerman I LowGerman I . . .</p>
    <p>Sample from an Anselm manuscript</p>
    <p>http://www.linguistics.rub.de/anselm/</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>What is historical text normalization? Previous work</p>
    <p>Examples for historical spellings</p>
    <p>Frau (woman) fraw, frawe, frwe, frauwe, frawe, frow, frouw, vraw, vrow, vorwe, vrauwe, vrouwe</p>
    <p>Kind (child) chind, chinde, chindt, chint, kind, kinde, kindi, kindt, kint, kinth, kynde, kynt</p>
    <p>Mutter (mother) moder, moeder, mueter, meter, muoter, muotter, muter, mutter, mvoter, mvter, mweter</p>
    <p>Normalization as the mapping of historical spellings to their modern-day equivalents.</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>What is historical text normalization? Previous work</p>
    <p>Examples for historical spellings</p>
    <p>Frau (woman) fraw, frawe, frwe, frauwe, frawe, frow, frouw, vraw, vrow, vorwe, vrauwe, vrouwe</p>
    <p>Kind (child) chind, chinde, chindt, chint, kind, kinde, kindi, kindt, kint, kinth, kynde, kynt</p>
    <p>Mutter (mother) moder, moeder, mueter, meter, muoter, muotter, muter, mutter, mvoter, mvter, mweter</p>
    <p>Normalization as the mapping of historical spellings to their modern-day equivalents.</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>What is historical text normalization? Previous work</p>
    <p>Previous work</p>
    <p>I Hand-crafted algorithms I VARD (Baron &amp; Rayson, 2008) I Norma (Bollmann, 2012)</p>
    <p>I Character-based statistical machine translation (CSMT) I Scherrer and Erjavec (2013), Pettersson et al. (2013), . . .</p>
    <p>I Sequence labelling with neural networks I Bollmann and Sgaard (2016)</p>
    <p>I Now: Character-based neural machine translation</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>What is historical text normalization? Previous work</p>
    <p>Previous work</p>
    <p>I Hand-crafted algorithms I VARD (Baron &amp; Rayson, 2008) I Norma (Bollmann, 2012)</p>
    <p>I Character-based statistical machine translation (CSMT) I Scherrer and Erjavec (2013), Pettersson et al. (2013), . . .</p>
    <p>I Sequence labelling with neural networks I Bollmann and Sgaard (2016)</p>
    <p>I Now: Character-based neural machine translation</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>An encoder/decoder model</p>
    <p>Prediction layer</p>
    <p>LSTM</p>
    <p>Embeddings</p>
    <p>LSTM</p>
    <p>Embeddings c h i n t</p>
    <p>S k i n d</p>
    <p>Ek i n d</p>
    <p>Decoder</p>
    <p>Encoder</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>An encoder/decoder model</p>
    <p>Avg. Accuracy</p>
    <p>Bi-LSTM tagger (Bollmann&amp;Sgaard,2016) 79.91%</p>
    <p>Base model</p>
    <p>Greedy 78.91%</p>
    <p>Beam 79.27% Beam + Filter 80.46%</p>
    <p>Evaluationon43texts fromtheAnselmcorpus ( 4,00013,000tokenseach)</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>An encoder/decoder model</p>
    <p>Avg. Accuracy</p>
    <p>Bi-LSTM tagger (Bollmann&amp;Sgaard,2016) 79.91%</p>
    <p>Base model</p>
    <p>Greedy 78.91% Beam 79.27%</p>
    <p>Beam + Filter 80.46%</p>
    <p>Evaluationon43texts fromtheAnselmcorpus ( 4,00013,000tokenseach)</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>An encoder/decoder model</p>
    <p>Avg. Accuracy</p>
    <p>Bi-LSTM tagger (Bollmann&amp;Sgaard,2016) 79.91%</p>
    <p>Base model</p>
    <p>Greedy 78.91% Beam 79.27% Beam + Filter 80.46%</p>
    <p>Evaluationon43texts fromtheAnselmcorpus ( 4,00013,000tokenseach)</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Attentional model</p>
    <p>Prediction layer</p>
    <p>AttentionalLSTM</p>
    <p>Attentionmodel</p>
    <p>BidirectionalLSTM</p>
    <p>c h i n t</p>
    <p>S k i n d</p>
    <p>Ek i n d</p>
    <p>Decoder</p>
    <p>Encoder</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Attentional model</p>
    <p>Avg. Accuracy</p>
    <p>Bi-LSTM tagger (Bollmann&amp;Sgaard,2016) 79.91%</p>
    <p>Base model</p>
    <p>Greedy 78.91% Beam 79.27% Beam + Filter 80.46% Beam + Filter + Attention 82.72%</p>
    <p>Evaluationon43texts fromtheAnselmcorpus ( 4,00013,000tokenseach)</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Learning to pronounce</p>
    <p>Can we improve results with multi-task learning?</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Learning to pronounce</p>
    <p>I Idea: grapheme-to-phoneme mapping as auxiliary task</p>
    <p>I CELEX 2 lexical database (Baayen et al., 1995)</p>
    <p>I Sample mappings for German:</p>
    <p>Jungfrau  jUN-frB Abend  ab@nt nicht  nIxt</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Multi-task learning</p>
    <p>Prediction layer forCELEXtask</p>
    <p>Prediction layer forhistorical task</p>
    <p>DecoderLSTM</p>
    <p>EncoderLSTM</p>
    <p>c h i n t</p>
    <p>S k i n d</p>
    <p>Ek i n d</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Multi-task learning</p>
    <p>Prediction layer forCELEXtask</p>
    <p>Prediction layer forhistorical task</p>
    <p>DecoderLSTM</p>
    <p>EncoderLSTM</p>
    <p>n i c h t</p>
    <p>S n I x t</p>
    <p>En I x t</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Model description Attention mechanism Multi-task learning</p>
    <p>Multi-task learning</p>
    <p>Avg. Accuracy</p>
    <p>Bi-LSTM tagger (Bollmann&amp;Sgaard,2016) 79.91%</p>
    <p>Base model</p>
    <p>Greedy 78.91% Beam 79.27% Beam + Filter 80.46% Beam + Filter + Attention 82.72%</p>
    <p>MTL model</p>
    <p>Greedy 80.64% Beam 81.13% Beam + Filter 82.76% Beam + Filter + Attention 82.02%</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Analysis Conclusion</p>
    <p>Why does MTL not improve with attention?</p>
    <p>Hypothesis Attention and MTL learn similar functions of the input data.</p>
    <p>MTLcanbeusedtocoerce the learner toattendto patterns in the input itwouldotherwise ignore. This is doneby forcing it to learn internal representations to support related tasks thatdependonsuchpatterns.</p>
    <p>Caruana (1998), p. 112 f.</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Analysis Conclusion</p>
    <p>Comparing the model outputs</p>
    <p>gewarnet uberhbe scholt</p>
    <p>Base model</p>
    <p>G prandet berbroch sollt B prandert berbrche sollt B+F pranget ber sollt B+F+A gewarnt bergebe sollte</p>
    <p>MTL model</p>
    <p>G gewarntet berbeh sollte B gewarntet bereube sollte B+F gewarnt bergebe sollte B+F+A gewand ber sollte</p>
    <p>Target gewarnt berhob sollte</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Analysis Conclusion</p>
    <p>Saliency plots Li, Chen, Hovy, and Jurafsky (2016)</p>
    <p>Base Attention MTL</p>
    <p>for words  7 characters, Attention/MTL correlate most</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Analysis Conclusion</p>
    <p>Conclusion</p>
    <p>I Encoder/decoder models for historical text normalization are competitive</p>
    <p>I Despite small datasets ( 4,200  13,200 tokens per text) I Beam search &amp; attention improve results further</p>
    <p>I MTL with grapheme-to-phoneme task helps</p>
    <p>I Attention and MTL have a similar effect</p>
    <p>I Can this be reproduced on other tasks? I What factors affect this (choice of attention</p>
    <p>mechanism/auxiliary task/. . . )?</p>
  </div>
  <div class="page">
    <p>Historical text normalization Encoder/decoder models</p>
    <p>Attention vs. multi-task learning</p>
    <p>Thank you for listening!</p>
    <p>Code [ https://bitbucket.org/mbollmann/acl2017 Further Qs? R bollmann@linguistics.rub.de 7 @mmbollmann</p>
  </div>
  <div class="page">
    <p>References Appendix</p>
    <p>References I</p>
    <p>Baayen, R. H., Piepenbrock, R., &amp; Gulikers, L. (1995). TheCELEX lexical database (Release2) (CD-ROM). Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.</p>
    <p>Baron, A., &amp; Rayson, P. (2008). VARD 2: A tool for dealing with spelling variation in historical corpora. In Proceedingsof thePostgraduate Conference inCorpusLinguistics.</p>
    <p>Bollmann, M. (2012). (Semi-)automatic normalization of historical texts using distance measures and the Norma tool. In Proceedingsof theSecond WorkshoponAnnotationofCorpora forResearch in theHumanities (ACRH-2). Lisbon, Portugal.</p>
    <p>Bollmann, M., &amp; Sgaard, A. (2016). Improving historical spelling normalization with bi-directional lstms and multi-task learning. In Proceedingsof coling2016 (pp. 131139). Osaka, Japan.</p>
    <p>Caruana, R. (1998). Multitask learning. In Learning to learn (pp. 95133). Springer. Retrieved from http://dl.acm.org/citation.cfm?id=296635.296645</p>
  </div>
  <div class="page">
    <p>References Appendix</p>
    <p>References II</p>
    <p>Li, J., Chen, X., Hovy, E., &amp; Jurafsky, D. (2016). Visualizing and understanding neural models in NLP. In Proceedingsof the2016conferenceof the northamericanchapterof theassociation forcomputational linguistics: Humanlanguagetechnologies (pp. 681691). Association for Computational Linguistics. Retrieved from http://aclweb.org/anthology/N16-1082 doi: 10.18653/v1/N16-1082</p>
    <p>Pettersson, E., Megyesi, B., &amp; Tiedemann, J. (2013). An SMT approach to automatic annotation of historical text. In Proceedingsof thenodalida workshoponcomputationalhistorical linguistics. Oslo, Norway.</p>
    <p>Scherrer, Y., &amp; Erjavec, T. (2013). Modernizing historical Slovene words with character-based SMT. In Proceedingsof the4thbiennialworkshopon balto-slavicnatural languageprocessing. Sofia, Bulgaria.</p>
  </div>
  <div class="page">
    <p>References Appendix</p>
    <p>References III</p>
    <p>Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., . . . Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention. In Jmlrworkshopandconferenceproceedings: Proceedingsof the32nd internationalconferenceonmachine learning (Vol. 37, pp. 20482057). Lille, France. Retrieved from http://proceedings.mlr.press/v37/xuc15.pdf</p>
  </div>
  <div class="page">
    <p>References Appendix</p>
    <p>Dealing with spelling variation</p>
    <p>The problems. . .</p>
    <p>I Difficult to annotate with tools aimed at modern data</p>
    <p>I High variance in spelling I None/very little training</p>
    <p>data</p>
    <p>Normalization. . .</p>
    <p>I Removes variance I Enables re-using of</p>
    <p>existing tools I Useful annotation layer</p>
    <p>(e.g. for corpus query)</p>
    <p>Normalization as the mapping of historical spellings to their modern-day equivalents.</p>
  </div>
  <div class="page">
    <p>References Appendix</p>
    <p>Attention mechanism: details</p>
    <p>I Attention mechanism follows Xu et al. (2015)</p>
    <p>zt = n i=1</p>
    <p>iai (1)</p>
    <p>= softmax(fatt(a,ht1)) (2)</p>
    <p>it = (Wi[ht1,yt1, zt] + bi) ft = (Wf[ht1,yt1, zt] + bf) ot = (Wo[ht1,yt1, zt] + bo) gt = tanh(Wg[ht1,yt1, zt] + bg) ct = ft ct1 + it gt ht = ot  tanh(ct)</p>
    <p>(3)</p>
  </div>
  <div class="page">
    <p>References Appendix</p>
    <p>Differences of learned parameters</p>
  </div>
</Presentation>
