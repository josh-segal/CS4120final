<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Scott Wen-tau Yih &amp; Hao Ma</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Tom Cruise</p>
  </div>
  <div class="page">
    <p>His Wife</p>
  </div>
  <div class="page">
    <p>How tall is Katie</p>
  </div>
  <div class="page">
    <p>How about Nicole</p>
  </div>
  <div class="page">
    <p>Where was she born</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>http://csunplugged.org/turing-test</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>How many games did the Yankees play in July?</p>
    <p>Month = July</p>
    <p>Place1 = Boston</p>
    <p>Day1 = 7</p>
    <p>Game Serial Number = 96</p>
    <p>Team = Red Sox, Score = 5</p>
    <p>Team = Yankees, Score = 3</p>
  </div>
  <div class="page">
    <p>Is the statement true? All circles are black circles.</p>
    <p>P L M</p>
  </div>
  <div class="page">
    <p>Complete Agreement Partial Agreement</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>[Simmons 1965]</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>James the Turtle was always getting in trouble. Sometimes he'd reach into the freezer and empty out all the food. Other times he'd sled on the deck and get a splinter. His aunt Jane tried as hard as she could to keep him out of trouble, but he was sneaky and got into lots of trouble behind her back.</p>
    <p>One day, James thought he would go into town and see what kind of trouble he could get into. He went to the grocery store and pulled all the pudding off the shelves and ate two jars. Then he walked to the fast food restaurant and ordered 15 bags of fries. He didn't pay, and instead headed home.</p>
    <p>His aunt was waiting for him in his room. She told James that she loved him, but he would have to start acting like a well-behaved turtle.</p>
    <p>After about a month, and after getting into lots of trouble, James finally made up his mind to be a better turtle.</p>
    <p>A) Fries</p>
    <p>B) Pudding</p>
    <p>C) James</p>
    <p>D) Jane</p>
    <p>A) pudding</p>
    <p>B) fries</p>
    <p>C) food</p>
    <p>D) splinters</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Im an Expert</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Month = July</p>
    <p>Place1 = Boston</p>
    <p>Day1 = 7</p>
    <p>Game Serial Number = 96</p>
    <p>Team = Red Sox, Score = 5</p>
    <p>Team = Yankees, Score = 3</p>
  </div>
  <div class="page">
    <p>A 13,000 entry table of</p>
    <p>chemical, isotope and</p>
    <p>age analyses of the</p>
    <p>Apollo 11 samples.</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Freebase</p>
    <p>Knowledge Graph NELL: Never-Ending Language Learning</p>
    <p>OpenIE</p>
    <p>(Reverb, OLLIE)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Headquarters</p>
    <p>Home Field</p>
    <p>Location NFL championships: 2013</p>
    <p>Head coach: Pete Carroll</p>
    <p>Founded: 1976</p>
    <p>Division: NFC West</p>
    <p>Address: 400 Broad St, Seattle, 98109</p>
    <p>Phone: (800) 937-9582</p>
    <p>Opened: Apr 21, 1962</p>
    <p>Height: 605 feet (184.41 m)</p>
    <p>Floors: 6</p>
    <p>Founded: Mar 30, 1971  Pike Place Market</p>
    <p>Customer service: +1 800-782-7282</p>
    <p>CEO: Howard Schultz</p>
    <p>Founders: Jerry Baldwin  Zev Siegl  Gordon Bowker</p>
    <p>Population: 652,405 (2013)</p>
    <p>Area: 142.55 sq miles (369.20 km)</p>
    <p>Mayor: Ed Murray</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Player Number Position From To</p>
    <p>Russell Wilson 3 Quarterback 2012</p>
    <p>Alan Branch 99 Defensive tackle 2011 2012</p>
    <p>Marshawn Lynch 24 Running back 2010 2016</p>
    <p>Richard Sherman 25 Cornerback 2011</p>
  </div>
  <div class="page">
    <p>Player Number Position From To</p>
    <p>Russell Wilson 3 Quarterback 2012</p>
    <p>Alan Branch 99 Defensive tackle 2011 2012</p>
    <p>Marshawn Lynch 24 Running back 2010 2016</p>
    <p>Richard Sherman 25 Cornerback 2011</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>What character did Natalie Portman play in Star Wars? Padme Amidala</p>
    <p>What currency do you use in Costa Rica? Costa Rican colon</p>
    <p>What did Obama study in school? political science</p>
    <p>What do Michelle Obama do for a living? writer, lawyer</p>
    <p>What killed Sammy Davis Jr? throat cancer [Examples from Berant]</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Knowledge</p>
    <p>Base . sister_of(justin_bieber, )</p>
    <p>Who is Justin Biebers sister?</p>
    <p>. sibling_of(justin_bieber, x)  gender(x, female)</p>
    <p>semantic parsing</p>
    <p>querying matching</p>
    <p>Jazmyn Bieber</p>
  </div>
  <div class="page">
    <p>Knowledge</p>
    <p>Base</p>
    <p>Who is Justin Biebers sister?</p>
    <p>. sibling_of(justin_bieber, x)  gender(x, female)</p>
    <p>semantic parsing</p>
    <p>querying</p>
    <p>Jazmyn Bieber</p>
  </div>
  <div class="page">
    <p>Who played the role of Meg on Family Guy?</p>
    <p>What is the name of the actress for Meg on Family Guy?</p>
    <p>In the TV show Family Guy, who is the voice for Meg?</p>
    <p>tv.tv_program.regular_cast  tv.regular_tv_appearance.actor</p>
    <p>What movies are directed by the person who won the most Academy and Golden Globe awards combined?</p>
  </div>
  <div class="page">
    <p>Unary: Seattle . [ = Seattle]</p>
    <p>Binary: PlaceOfBirth . . PlaceOfBirth(, )</p>
    <p>Join: people born in Seattle PlaceOfBirth.Seattle . PlaceofBirth(, Seattle)</p>
    <p>Intersection: scientists born in Seattle Profession.Scientist  PlaceOfBirth.Seattle . Profession , Scientist  PlaceOfBirth(, Seattle)</p>
  </div>
  <div class="page">
    <p>Bridging: Hypothesizing predicates to be</p>
    <p>connected when the type constraints are</p>
    <p>satisfied</p>
    <p>What government does Chile have?</p>
    <p>What actors are in Top Gun?</p>
    <p>What is Italy money?</p>
    <p>Type.FormOfGovernment</p>
    <p>Chile</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Word Nodes (Ovals)</p>
    <p>word nodes are connected</p>
    <p>via syntactic dependencies</p>
    <p>Entity Nodes (Rectangles)</p>
    <p>Mediator Nodes (Circles)</p>
    <p>Represent events</p>
    <p>Binary predicates</p>
    <p>Type nodes (Rounded</p>
    <p>rectangles)</p>
    <p>Unary predicates</p>
    <p>Math nodes (Diamonds)</p>
    <p>e.g., Aggregation Functions</p>
    <p>Austin is the capital of Texas.</p>
    <p>What is the capital of Texas?</p>
  </div>
  <div class="page">
    <p>Each span is a mapping of</p>
    <p>a single-relation question:</p>
    <p>Question Pattern:</p>
    <p>Who is the director of</p>
    <p>Forrest Gump?</p>
    <p>(Forrest Gump, Director, ?)</p>
    <p>Patterns from mining</p>
    <p>Bing query logs.</p>
    <p>CYK Paring</p>
    <p>* Few questions in WebQuestions are with a long chain like this.</p>
  </div>
  <div class="page">
    <p>query graph</p>
    <p>directly</p>
    <p>search staged</p>
    <p>Core idea</p>
  </div>
  <div class="page">
    <p>Family Guy cast</p>
    <p>Meg Grif finargmin</p>
    <p>xy</p>
    <p>topic entity core inferential chain</p>
    <p>constraints</p>
  </div>
  <div class="page">
    <p>Meg Family Guy</p>
    <p>Family Guy cast</p>
    <p>Meg Grif finargmin</p>
    <p>xy</p>
    <p>topic entity</p>
  </div>
  <div class="page">
    <p>Family Guy s1</p>
    <p>Meg Grif fin s2</p>
    <p>s0</p>
  </div>
  <div class="page">
    <p>Who Meg Family Guy</p>
    <p>Family Guy cast</p>
    <p>Meg Grif finargmin</p>
    <p>xy</p>
    <p>{castactor, producer, awards_wonwinner}</p>
    <p>core inferential chain</p>
  </div>
  <div class="page">
    <p>Family Guy s1</p>
    <p>Family Guy cast actor xy s3</p>
    <p>Family Guy writer start xy s4</p>
    <p>Family Guy genre x s5</p>
    <p>Who first voiced Meg on Family Guy?</p>
    <p>{castactor, writerstart, genre}</p>
  </div>
  <div class="page">
    <p>Input is mapped to two -dimensional vectors</p>
    <p>= exp cos(, )</p>
    <p>exp cos(, )</p>
    <p>R    R</p>
    <p>who voiced meg on  castactor</p>
    <p>max max</p>
    <p>...</p>
    <p>...</p>
    <p>... max</p>
    <p>...</p>
    <p>...</p>
    <p>&lt;s&gt; w1 w2 wT &lt;/s&gt;</p>
    <p>... ...</p>
  </div>
  <div class="page">
    <p>first Meg</p>
    <p>Family Guy cast</p>
    <p>Meg Grif finargmin</p>
    <p>xy</p>
    <p>constraints</p>
  </div>
  <div class="page">
    <p>Who voiced Family Guy</p>
    <p>cast FamilyGuy  actor</p>
    <p>One or more constraint nodes can be added to  or    : Additional property of this event (e.g., character  MegGriffin )</p>
    <p>: Additional property of the answer entity (e.g., gender)</p>
    <p>Family Guy cast actor xy</p>
    <p>Family Guy cast actor xy</p>
    <p>Meg Grif fin</p>
    <p>Family Guy xy</p>
    <p>Meg Grif finargmin</p>
    <p>s3</p>
    <p>s6</p>
    <p>s7</p>
    <p>Family Guy cast actor xy s3</p>
  </div>
  <div class="page">
    <p>Who first voiced Meg on Family Guy?</p>
    <p>Family Guy cast actor xy s3</p>
    <p>Family Guy writer start xy s4</p>
  </div>
  <div class="page">
    <p>Who first voiced Meg on Family Guy?</p>
    <p>Family Guy cast actor xy s3</p>
    <p>Family Guy xy</p>
    <p>Meg Grif finargmin s7</p>
  </div>
  <div class="page">
    <p>=Who first voiced Meg on Family Guy?</p>
    <p>Family Guy cast</p>
    <p>Meg Grif finargmin</p>
    <p>xy</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>Relation Matching (Identifying Core Inferential Chain)</p>
    <p>Pattern Inferential Chain</p>
    <p>what was &lt;e&gt; known for people.person.profession</p>
    <p>what kind of government does &lt;e&gt; have location.country.form_of_government</p>
    <p>what year were the &lt;e&gt; established sports.sports_team.founded</p>
    <p>what city was &lt;e&gt; born in people.person.place_of_birth</p>
    <p>what did &lt;e&gt; die from people.deceased_person.cause_of_death</p>
    <p>who married &lt;e&gt; people.person.spouse_s</p>
    <p>people.marriage.spouse</p>
  </div>
  <div class="page">
    <p>Reward Function</p>
  </div>
  <div class="page">
    <p>Addresses Key Challenges</p>
  </div>
  <div class="page">
    <p>What is the name of Justin Bieber brother?</p>
    <p>Create lots of features; learn an answer classifier (L1-regularized LR)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Avg. F1 (Accuracy) on WebQuestions Test Set</p>
    <p>Yao-14 Berant-13 Bao-14 Bordes-14b Berant-14 Yang-14 Yih-15</p>
  </div>
  <div class="page">
    <p>Avg. F1 (Accuracy) on WebQuestions Test Set</p>
    <p>Yao-14 Berant-13 Bao-14 Bordes-14b Berant-14 Yang-14 Yao-15</p>
    <p>Wang-14 Yih-15 Berant-15 Bast-15 Reddy-16 Xu-16</p>
  </div>
  <div class="page">
    <p>http://aka.ms/WebQSP</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Who first landed on the Moon?</p>
    <p>SELECT ?p</p>
    <p>WHERE {?p land-on ?m . ?m target Moon . ?m date ?t .}</p>
    <p>ORDER BY ?t LIMIT 1</p>
    <p>Question</p>
    <p>Analysis</p>
    <p>Knowledge Base Apollo 11</p>
    <p>Michael</p>
    <p>Collins</p>
    <p>Buzz Aldrin</p>
    <p>Neil</p>
    <p>Armstrong Moon</p>
    <p>Person</p>
    <p>Spaceflight</p>
    <p>Celestial</p>
    <p>Object</p>
    <p>Issues:</p>
    <p>Semantic parsing is difficult due to ontology mismatch</p>
    <p>Knowledge base is incomplete (missing</p>
    <p>entities/relations/attribute values)</p>
  </div>
  <div class="page">
    <p>Knowledge Base Completion via Search-Based Question Answering [Robert West, et al., WWW 2014]</p>
  </div>
  <div class="page">
    <p>Answer</p>
    <p>The Cattedrale di Santa Maria del Fiore is</p>
    <p>the main church of Florence, Italy. Il</p>
    <p>Duomo di Firenze, as it is ordinarily called,</p>
    <p>was begun in 1296 in the Gothic style ... It</p>
    <p>remains the largest brick dome ever</p>
    <p>constructed.</p>
    <p>en.wikipedia.org</p>
    <p>Florence Cathedral</p>
    <p>Q: Where is the largest brick dome?</p>
  </div>
  <div class="page">
    <p>Issues:</p>
    <p>Semantic parsing is difficult due to</p>
    <p>ontology mismatch</p>
    <p>Knowledge base is incomplete</p>
    <p>(missing entities/relations/attribute</p>
    <p>values)</p>
    <p>Advantages:</p>
    <p>Contains abundant information</p>
    <p>Redundancy on the Web could help</p>
    <p>confirm the answers</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Narrative: What countries does Eurail operate in</p>
    <p>Target Entity Type: LocationInput Entity: Eurail</p>
    <p>Narrative: Airlines that currently use Boeing 747 planes</p>
    <p>Target Entity Type: OrganizationInput Entity: Boeing 747</p>
    <p>Narrative: Find companies that are included in the Dow</p>
    <p>Jones industrial average</p>
    <p>Target Entity Type: OrganizationInput Entity: Dow Jones</p>
    <p>Narrative: Chefs with a show on the food network</p>
    <p>Target Entity Type: Person Input Entity: The food</p>
    <p>network</p>
  </div>
  <div class="page">
    <p>Entity Linking and Retrieval for Semantic Search [Edgar Meij, et al., WSDM 2014]</p>
  </div>
  <div class="page">
    <p>Related Entity Finding Based on Co-Occurrence [Balog, et al., TREC 2009]</p>
  </div>
  <div class="page">
    <p>Related Entity Finding Based on Co-Occurrence [Balog, et al., TREC 2009]</p>
  </div>
  <div class="page">
    <p>Related Entity Finding by Unified Probabilistic Models [Yi Fang, et al., World Wide Web 2015]</p>
    <p>Model A</p>
    <p>Model B</p>
  </div>
  <div class="page">
    <p>Related Entity Finding by Unified Probabilistic Models [Yi Fang, et al., World Wide Web 2015]</p>
    <p>Narrative: Find companies that are included in the Dow</p>
    <p>Jones industrial average</p>
    <p>Target Entity Type: OrganizationInput Entity: Dow Jones</p>
  </div>
  <div class="page">
    <p>Knowledge Base Completion via Search-Based Question Answering [Robert West, et al., WWW 2014]</p>
    <p>Entity Retrieval/Finding</p>
    <p>techniques can be</p>
    <p>used in Knowledge</p>
    <p>Base Completion</p>
  </div>
  <div class="page">
    <p>Knowledge Base Completion via Search-Based Question Answering [Robert West, et al., WWW 2014]</p>
  </div>
  <div class="page">
    <p>Narrative: Find companies that are included in the Dow</p>
    <p>Jones industrial average</p>
    <p>Target Entity Type: OrganizationInput Entity: Dow Jones</p>
    <p>Companies in Dow Jones industrial</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Who first landed on the Moon?</p>
    <p>Type Detection,</p>
    <p>NER Parsing and</p>
    <p>Candidate Ranking</p>
    <p>Question</p>
    <p>Understanding</p>
    <p>Web Corpus</p>
    <p>Apollo 11 was the spaceflight that</p>
    <p>landed the first humans on the</p>
    <p>M o o n , A m e r i c a n s N e i l</p>
    <p>Armstrong and Buzz Aldrin, on July</p>
    <p>Open Domain Question and Answering via Semantic Enrichment [Huan Sun, et al., WWW 2015]</p>
  </div>
  <div class="page">
    <p>Document</p>
    <p>DocumentDocument</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Question Processing</p>
    <p>Passage Retrieval</p>
    <p>Query Formulation</p>
    <p>Answer Type Detection</p>
    <p>Question</p>
    <p>Passage Retrieval</p>
    <p>Document Retrieval</p>
    <p>Answer Processing</p>
    <p>Answer</p>
    <p>passages</p>
    <p>Indexing</p>
    <p>Relevant Docs</p>
    <p>Document Document</p>
    <p>Document</p>
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
  </div>
  <div class="page">
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
    <p>Document</p>
    <p>DocumentDocument</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Docume nt</p>
    <p>Question Processing</p>
    <p>Passage Retrieval</p>
    <p>Query Formulation</p>
    <p>Answer Type Detection</p>
    <p>Question</p>
    <p>Passage Retrieval</p>
    <p>Document Retrieval</p>
    <p>Answer Processing</p>
    <p>Answer</p>
    <p>passages</p>
    <p>Indexing</p>
    <p>Relevant Docs</p>
    <p>Document Document</p>
    <p>Document</p>
  </div>
  <div class="page">
    <p>Highest flying bird</p>
    <p>Animal/Bird</p>
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
  </div>
  <div class="page">
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
    <p>Learning Question Classifiers [Xin Li &amp; Dan Roth, COLING 2002]</p>
  </div>
  <div class="page">
    <p>LOCATION</p>
    <p>NUMERIC</p>
    <p>ENTITY HUMAN</p>
    <p>ABBREVIATION DESCRIPTION</p>
    <p>country city state</p>
    <p>date</p>
    <p>percent</p>
    <p>money</p>
    <p>sizedistance</p>
    <p>individual</p>
    <p>title</p>
    <p>group</p>
    <p>food</p>
    <p>currency</p>
    <p>animal</p>
    <p>definition</p>
    <p>reason expression</p>
    <p>abbreviation</p>
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
    <p>Learning Question Classifiers [Xin Li &amp; Dan Roth, COLING 2002]</p>
  </div>
  <div class="page">
    <p>city</p>
    <p>flower</p>
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
  </div>
  <div class="page">
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
  </div>
  <div class="page">
    <p>Number</p>
    <p>Question Answering [Dan Jurafsky, Stanford]</p>
  </div>
  <div class="page">
    <p>Knowledge Bases based QA</p>
    <p>Web Documents based QA</p>
    <p>Answer Sentence Selection</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA [Mengqiu Wang, et al., EMNLP-CoNLL 2007]</p>
  </div>
  <div class="page">
    <p>http://aclweb.org/aclwiki/index.php?title=Question_Answering_(State_of_the_art)</p>
  </div>
  <div class="page">
    <p>QASent</p>
    <p>WikiQA</p>
    <p>WikiQA: A Challenge Dataset for Open-Domain Question Answering [Yi Yang, et al., EMNLP 2015]</p>
  </div>
  <div class="page">
    <p>Web QnAKB QnA</p>
  </div>
  <div class="page">
    <p>Who first landed on the Moon?</p>
    <p>Entity Linking, Feature Construction,</p>
    <p>and Candidate Ranking</p>
    <p>Question</p>
    <p>Analysis</p>
    <p>Web Corpus</p>
    <p>Apollo 11 was the spaceflight that landed</p>
    <p>t h e f i r s t h u m a n s o n t h e M o o n ,</p>
    <p>Americans Neil Armstrong and Buzz Aldrin,</p>
    <p>o n J u l y 2 0 , 1 9 6 9 , a t 2 0 : 1 8 U T C .</p>
    <p>Knowledge Base</p>
    <p>Apollo 11</p>
    <p>Michael</p>
    <p>Collins</p>
    <p>Buzz Aldrin</p>
    <p>Neil</p>
    <p>Armstrong</p>
    <p>Moon</p>
    <p>Person</p>
    <p>Spaceflight</p>
    <p>Celestial</p>
    <p>Object</p>
    <p>Advantages:  Generate better answer candidates</p>
    <p>Entities in Freebase</p>
    <p>Mentions of the same entity merged to one</p>
    <p>candidate</p>
    <p>Able to leverage entity information in Freebase  Semantic text relevance features for ranking</p>
    <p>More fine-grained answer type checking</p>
    <p>Open Domain Question and Answering via Semantic Enrichment [Huan Sun, et al., WWW 2015]</p>
  </div>
  <div class="page">
    <p>Answer Candidate Pool</p>
    <p>Candidate</p>
    <p>Generation</p>
    <p>Via</p>
    <p>Entity</p>
    <p>Linking</p>
    <p>Freebase</p>
    <p>Question</p>
    <p>Who was the first</p>
    <p>American in space?</p>
    <p>Sentence</p>
    <p>Selection</p>
    <p>Via</p>
    <p>Search</p>
    <p>Engine</p>
    <p>Sentence Collection</p>
    <p>Shepard piloted</p>
    <p>the first American</p>
    <p>Top-K Answers</p>
    <p>Feature</p>
    <p>Generation</p>
    <p>&amp;</p>
    <p>Ranking</p>
    <p>Open Domain Question and Answering via Semantic Enrichment [Huan Sun, et al., WWW 2015]</p>
  </div>
  <div class="page">
    <p>TREC</p>
    <p>Bing</p>
    <p>Example questions: 1. What are pennies made of? 2. What is the tallest building in Japan? 3. Who sang Tennessee Waltz?</p>
    <p>Example queries: 1. the highest flying bird 2. indiana jones named after 3. designer of the golden gate bridge</p>
    <p>Open Domain Question and Answering via Semantic Enrichment [Huan Sun, et al., WWW 2015]</p>
  </div>
  <div class="page">
    <p>Open Domain Question and Answering via Semantic Enrichment [Huan Sun, et al., WWW 2015]</p>
  </div>
  <div class="page">
    <p>QuASE AskMSR+ SEMPRE</p>
    <p>TREC</p>
    <p>Bing</p>
    <p>MRR: Mean Reciprocal Rank</p>
    <p>Open Domain Question and Answering via Semantic Enrichment [Huan Sun, et al., WWW 2015]</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Q: Where is the largest brick dome?</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Semi-Structured Tables</p>
    <p>Table caption</p>
    <p>Column names</p>
    <p>Table cells</p>
    <p>List of universities in Canada</p>
    <p>University City Province Established</p>
    <p>University of Alberta Calgary Alberta 1906</p>
    <p>University of Toronto Toronto Ontario 1827</p>
    <p>University of Montreal Montreal Quebec 1878</p>
  </div>
  <div class="page">
    <p>Given:</p>
    <p>The goal: to find a table cell containing answers.</p>
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
  </div>
  <div class="page">
    <p>What languages do people in France speak?</p>
    <p>More than 100K tables contain France !</p>
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
    <p>What languages do people in France speak?</p>
    <p>Capital? Main Language? Currency?</p>
    <p>A list of countries and their capital, language etc.</p>
  </div>
  <div class="page">
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
    <p>Chain representation for What languages do people in France speak?:</p>
    <p>entity + question pattern</p>
    <p>Relational chain between France and French:</p>
    <p>Graph representation</p>
    <p>of a table row:</p>
  </div>
  <div class="page">
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
    <p>Question</p>
    <p>What languages do</p>
    <p>people in France speak?</p>
    <p>Step 1:</p>
    <p>Candidate</p>
    <p>Chain</p>
    <p>Generation</p>
    <p>Via</p>
    <p>Topic</p>
    <p>Entity</p>
    <p>Matching</p>
    <p>Candidate Chain Collection</p>
    <p>Top-K Chains</p>
    <p>Step 3:</p>
    <p>Deep Chain</p>
    <p>Inference</p>
    <p>Via</p>
    <p>Deep Neural</p>
    <p>Networks</p>
    <p>Country Player</p>
    <p>Country MainLanguage</p>
    <p>Pruned Chain Collection</p>
    <p>Step 2:</p>
    <p>Coarse-Grained</p>
    <p>Pruning</p>
    <p>Via</p>
    <p>Snippets Matching</p>
    <p>Country MainLanguage</p>
    <p>Country Population</p>
  </div>
  <div class="page">
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
    <p>Topic entity: France</p>
    <p>{ France ------------&gt;Table ID--------------------&gt; ? ;</p>
    <p>France -----------------&gt;Table ID-----------------&gt; ?;}</p>
    <p>What languages do people in France speak?</p>
    <p>String match with table cells</p>
    <p>Country MainLanguage</p>
    <p>Country Capital</p>
    <p>Generate an initial set of chains</p>
  </div>
  <div class="page">
    <p>Word vector</p>
    <p>Word vector</p>
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
  </div>
  <div class="page">
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
    <p>Question Candidate chain</p>
    <p>Country MainLanguage</p>
    <p>Country Population</p>
    <p>What languages do</p>
    <p>people in France</p>
    <p>speak?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Table Cell Search for Question Answering [Huan Sun, et al., WWW 2016]</p>
    <p>Question Candidate chain</p>
    <p>Country MainLanguage</p>
    <p>Country Population</p>
    <p>What languages do</p>
    <p>people in France</p>
    <p>speak?</p>
    <p>What languages</p>
    <p>do people in</p>
    <p>&lt;e&gt; speak</p>
  </div>
  <div class="page">
    <p>DSSM-Type</p>
    <p>DSSM-Predicate</p>
    <p>DSSM-EntityPairs</p>
    <p>(Q, A) cos ine(y , y )</p>
    <p>T</p>
    <p>Q A</p>
    <p>Q A</p>
    <p>Q A</p>
    <p>y y R</p>
    <p>y y  e.g., DSSM-Type:</p>
    <p>Question pattern Answer type</p>
    <p>y Q y A</p>
    <p>Input:</p>
  </div>
  <div class="page">
    <p>WebQuestions</p>
    <p>Bing Queries</p>
    <p>Example questions: 1. who did the voice for lola bunny? 2. in what countries do people speak danish?</p>
    <p>Example queries: 1. cherieff callie voice 2. boeing charleston sc plant location</p>
  </div>
  <div class="page">
    <p>~5M</p>
  </div>
  <div class="page">
    <p>top-1</p>
  </div>
  <div class="page">
    <p>F1</p>
  </div>
  <div class="page">
    <p>F1</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Story comprehension (MCTest)</p>
    <p>Fill-in-the-blank questions (MSR sentence completion, DeepMind Q&amp;A Dataset, Facebook Children Stories)</p>
    <p>Commonsense reasoning (Facebook bAbI)</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Timmy liked to play games and play</p>
    <p>sports but more than anything he</p>
    <p>liked to collect things. He collected</p>
    <p>bottle caps. He collected sea shells.</p>
    <p>He collected baseball cards. He has</p>
    <p>collected baseball cards the longest.</p>
    <p>He likes to collect the thing that he</p>
    <p>has collected the longest the most.</p>
    <p>He once thought about collecting</p>
    <p>stamps but never did. His most</p>
    <p>expensive collection was not his</p>
    <p>favorite collection. Timmy spent the</p>
    <p>most money on his bottle cap</p>
    <p>collection.</p>
    <p>A) Collect things</p>
    <p>B) Collect stamps</p>
    <p>C) Play games</p>
    <p>D) Play sports</p>
    <p>A) Stamps</p>
    <p>B) Baseball Cards</p>
    <p>C) Bottle Cap</p>
    <p>D) Sea Shells</p>
    <p>A) Bottle caps</p>
    <p>B) Baseball cards</p>
    <p>C) Stamps</p>
    <p>D) Sea shells</p>
    <p>A) Stamps</p>
    <p>B) Baseball cards</p>
    <p>C) Bottle caps</p>
    <p>D) Sea shells</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Lexical matching [Smith et al., 2015]</p>
    <p>Answer-entailing structures [Sachan et al., 2015]</p>
    <p>Attention-based CNNs [Yin et al., 2016]</p>
    <p>Parallel-Hierarchical NN [Trischler et al., 2016]</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>MSR Sentence Completion Challenge</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>[Hermann et al., NIPS-15. Fig 1a]</p>
  </div>
  <div class="page">
    <p>[Hermann et al., NIPS-15. Fig 1b]</p>
  </div>
  <div class="page">
    <p>CNN Daily Mail</p>
    <p>Accuracy</p>
    <p>Max Freq Excl. Freq. Frame-semantic Word Dist. Att. Reader Impat. Reader</p>
  </div>
  <div class="page">
    <p>CNN Daily Mail</p>
    <p>Accuracy</p>
    <p>Max Freq Excl. Freq. Frame-semantic Word Dist. Att. Reader Impat. Reader</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>CNN Daily Mail</p>
    <p>Accuracy</p>
    <p>Frame-semantic Word Dist. Ent. Classifier Att. Reader Att. Reader+</p>
  </div>
  <div class="page">
    <p>Category Ratio Classifier NN</p>
    <p>Exact match 13% 13 (100.0%) 13 (100.0%)</p>
    <p>Paraphrasing 41% 29 (70.7%) 39 (95.1%)</p>
    <p>Partial clue 19% 14 (73.7%) 17 (89.5%)</p>
    <p>Multiple sentences 2% 1 (50.0%) 1 (50.0%)</p>
    <p>Coreference errors 8% 3 (37.5%) 3 (37.5%)</p>
    <p>Ambiguous / hard (to human) 17% 2 (11.8%) 1 (5.9%)</p>
  </div>
  <div class="page">
    <p>Category Ratio Classifier NN</p>
    <p>Exact match 13% 13 (100.0%) 13 (100.0%)</p>
    <p>Paraphrasing 41% 29 (70.7%) 39 (95.1%)</p>
    <p>Partial clue 19% 14 (73.7%) 17 (89.5%)</p>
    <p>Multiple sentences 2% 1 (50.0%) 1 (50.0%)</p>
    <p>Coreference errors 8% 3 (37.5%) 3 (37.5%)</p>
    <p>Ambiguous / hard (to human) 17% 2 (11.8%) 1 (5.9%)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>ROCStories and Story Cloze Test Corpora</p>
  </div>
  <div class="page">
    <p>Russian-language QA dataset Sergey Nikolenko</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>I</p>
    <p>G</p>
    <p>O</p>
    <p>R</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p># Statements/Questions Encodings</p>
    <p>garden</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
</Presentation>
