<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Reading Thieves Cant: Automatically Identifying and Understanding Dark</p>
    <p>Jargons from Cybercrime Marketplaces Kan Yuan, Haoran Lu, Xiaojing Liao, and XiaoFeng Wang</p>
    <p>Indiana University Bloomington</p>
  </div>
  <div class="page">
    <p>coke</p>
  </div>
  <div class="page">
    <p>blueberry</p>
    <p>The second examples</p>
  </div>
  <div class="page">
    <p>rat</p>
    <p>blueberry</p>
    <p>coke</p>
    <p>One more example. Rat, also known as remote access trojan. You must be</p>
    <p>Words like rat, blueberry and coke are jargons. They have their ordinary meanings, but they are used differently by s particular group</p>
  </div>
  <div class="page">
    <p>Words like rat, blueberry and coke, that have the ordinary meanings, while are used differently by s particular profession or group are called jargons.</p>
    <p>In fact Jargons are extensively used in the underground forums by cyber-criminals for a variety of reasons. It has become an obstacle</p>
    <p>Such deceptive content makes underground communication less conspicuous and difficult to detect, and in some cases, even allows the criminals to communicate through public forums. Hence, automatic discovery and understanding of these dark jargons are highly valuable for understanding various cybercrime activities and mitigating the threats they pose.</p>
  </div>
  <div class="page">
    <p>CANTREADER</p>
    <p>Cantreader an unsupervised approach to automatically detect and understand dark jargon</p>
    <p>Lets start with the detection</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>context = semantics</p>
    <p>Key idea is simple, we are going to look into the semantics.</p>
    <p>Because communication traces from dark forums are partially obfuscated Where the key words are replaced with jargons. Althought the jargons themselves are hard to deal with directly. So we can still investigate the context to find the clues of jargons</p>
  </div>
  <div class="page">
    <p>My fav is slayers new rat, its open source, gonna have his rootkit implemented into it.</p>
    <p>Rat has been used as working animal. Tasks for working rats include the sniffing of gunpowder residue, demining, acting and animal-assisted therapy.</p>
    <p>Lets look at the two pieces of text, both using the jargon word rat, but with different meaning Rat used as jargon, context: opensource, rootkit, slayers, implement Rat means mouse, context: animal, working, therapy Therefore, if a word is used as a jargon in an underground forum, its context in that forum ought to be totally different from that in the legit communication traces. There is how we are going to detect dark jargons.</p>
    <p>To better extract a word context information, and directly use that information in the semantic comparison,</p>
  </div>
  <div class="page">
    <p>!&quot;#$%&amp;'( WORD2VEC</p>
    <p>2-layer neural network</p>
    <p>Fake task: language model prediction</p>
    <p>Embedded vectors = hidden layer state</p>
    <p>Comparable embedded vectors: vector distance  semantic difference</p>
    <p>Hidden layer</p>
    <p>Output (softmax) layer</p>
    <p>Input layer</p>
    <p>Feature extraction</p>
    <p>Language Model Reconstruction</p>
    <p>Embedded Vectors</p>
    <p>Word2vec (Tomas Mikolov 2013) is a word embedding technique it use a 2-layer shallow NN Fake task: language model prediction, Language model: predict the context of given word</p>
    <p>Idea is like auto-encoder, 1st layer extract features, 2nd layer reconstruction After training, 2nd layer ignored, the embedded vectors are not just the densened feature vectors of the words, they actually represent the semantics of the words in the numeric form. So it shows some interesting property:</p>
    <p>Comparable: we say two vectors are comparable, means we can use the distance of embedded vectors to estimate words Semantic difference With this property, it seems that we are already ready to find dark jargons</p>
  </div>
  <div class="page">
    <p>Vecreddit</p>
    <p>Vecdark</p>
    <p>Word2vec_1</p>
    <p>Word2vec_2</p>
    <p>COS</p>
    <p>)*+,- !&quot;#$%&amp;'( USING WORD2VEC</p>
    <p>This property sounds very promising, it seems that we are already ready to find dark jargons! The idea is differential analysis. Two corpora, Ordinary forum and underground forum Each word has two vectors, comparing the vectors to see if the word has different contexts/meaning between the two corpora. Problem, are vectors from tow separately trained models comparable</p>
  </div>
  <div class="page">
    <p>Vecreddit</p>
    <p>Vecdark</p>
    <p>Word2vec_1</p>
    <p>Word2vec_2</p>
    <p>COS</p>
    <p>Text8</p>
    <p>Text8</p>
    <p>'./'#+0',1 2 EXPERIMENT 1</p>
    <p>We investiage this with an experiment</p>
    <p>Since we use the same corpus, the context of the word should be same If truly comparable, cosine similarity of the vectors should be close 1.</p>
  </div>
  <div class="page">
    <p>'./'#+0',1 2 EXPERIMENT 1</p>
    <p>= 0.49,  = 0.078</p>
    <p>Cross-corpus Semantic Comparison: Word2vec</p>
    <p>SO we cannot estimate CROSS-CORPUS semantic difference with the distance of embedded vectors from two SEPARATELY TRAINED word2vec models. But we are actually very close to the solution. We just need to tweak the word2vec model a little bit to suit our task, which is the cross-corpus semantic comparison.</p>
  </div>
  <div class="page">
    <p>Vecreddit</p>
    <p>Vecdark</p>
    <p>Word2vec_1</p>
    <p>Word2vec_2</p>
    <p>COS</p>
    <p>)*+,- !&quot;#$%&amp;'( USING WORD2VEC</p>
    <p>SO we cannot estimate CROSS-CORPUS semantic difference with the distance of embedded vectors from two SEPARATELY TRAINED word2vec models. But we are actually very close to the solution. We just need to tweak the word2vec model a little bit to suit our task, which is the cross-corpus semantic comparison.</p>
  </div>
  <div class="page">
    <p>x1 x2 x3 xk x|V|</p>
    <p>y1 y2 y3 yj y|V|</p>
    <p>W|V||H|</p>
    <p>W|H||V|</p>
    <p>Input word one-hot encoding</p>
    <p>Input layer</p>
    <p>Hidden layer</p>
    <p>Output (softmax) layer</p>
    <p>wk</p>
    <p>H = X  W = Wk,*</p>
    <p>xi = {0, i  k1, i = k</p>
    <p>Y = H  W</p>
    <p>!&quot;#$%&amp;'( WORD2VEC</p>
    <p>If two words have similar meaning, then they have similar context in the corpus;</p>
    <p>If two word have similar context, the model gives similar predictions;</p>
    <p>The output weight matrix W is shared by all input words;</p>
    <p>Similar predictions iff. similar hidden-layer state (namely similar vectors)</p>
    <p>But we are actually very close to the solution. We just need to tweak the word2vec model a little bit to suit our task, which is the cross-corpus semantic comparison. To to this, we need to dig a little deeper into the Word2vec, and find out why word2vec doesnt work in this scenario</p>
    <p>Lets look at its the prediction stage:  A word in input with one-hot encoding  It actually select a row of the input layer weight matrix, and feed to hidden layer H (embedded vector of the word)</p>
  </div>
  <div class="page">
    <p>Reason of Word2vec fails</p>
  </div>
  <div class="page">
    <p>W1 W2</p>
    <p>W2W1</p>
    <p>Y1  Y2</p>
    <p>H1  H2 ?</p>
    <p>W1  W2</p>
    <p>Reason of Word2vec fails</p>
  </div>
  <div class="page">
    <p>Semantic difference = distance of vectors</p>
    <p>iff.</p>
    <p>Vectors are associated with the same output-layer matrix W</p>
    <p>To understand the reason why word2vec doesnt work in this scenario, we need to dig a little deeper into the Word2vec. Let look at its the prediction stage:</p>
    <p>A</p>
  </div>
  <div class="page">
    <p>*'03,1+( (&quot;0/3#+*&quot;, 0&quot;$'4 SEMANTIC COMPARISON MODEL</p>
    <p>Reason of Word2vec fails</p>
  </div>
  <div class="page">
    <p>W1 W2</p>
    <p>W</p>
    <p>*'03,1+( (&quot;0/3#+*&quot;, 0&quot;$'4 SEMANTIC COMPARISON MODEL</p>
    <p>Embedded Vectors 2</p>
    <p>Embedded Vectors 1</p>
    <p>Word2vec to Semantic Comparison Model</p>
  </div>
  <div class="page">
    <p>'./'#+0',1 2 EXPERIMENT 1</p>
    <p>Cross-corpus Semantic Comparison: Word2vec vs. SCM (training set: Text8 &amp; Text8)</p>
    <p>= 0.49/0.98,  = 0.078/0.006</p>
    <p>we used Text8 as both input corpora for our SCM. For each word in the vocab- ulary, the model generated a pair of vectors, each representing its semantics in the corresponding corpus. Since the two input corpora here are identical, the cosine similarity of every vector pair should all be close to 1, if SCM can capture the words semantics in both corpora correctly. Our experiment shows that for every word in the corpora, the average cosine similarity between its two vectors is 0.98, with a standard deviation 0.006. As a reference, we trained a Word2Vec model on the same corpus twice, and calculated the cosine similarities between the vectors of the same words. Here the average similarity is 0.49 and standard deviation 0.078, indicat- ing that the vectors from the two models cannot be compared, due to the training randomness</p>
  </div>
  <div class="page">
    <p>'./'#+0',1 % EXPERIMENT 2</p>
    <p>SCM Capturing Cross-corpus Semantic Difference</p>
    <p>(Training set: Text8 &amp; Text8syn)</p>
    <p>Synthesizing jargons using word replacement</p>
    <p>= 0.98,  = 0.01</p>
    <p>cross-corpora semantic difference experiment</p>
    <p>We randomly chose 5 words from the Text8 corpus and replaced them with 5 other words (see Table 2) to construct a new corpus Text8syn. In this way, these replacements become jargons of the original words in the new corpus Text8syn. Then we trained our architecture on Text8 and Text8syn,</p>
    <p>all the replaced words were found to have small similarities in two corpora: the average similarity is 0.98 with a standard deviation of 0.01. This experiment shows that our SCM is able to capture a words cross-corpora semantic difference.</p>
  </div>
  <div class="page">
    <p>'./'#+0',1 5 EXPERIMENT 3</p>
    <p>Vector quality: Word2vec vs. SCM</p>
    <p>Tomas Mikolovs evaluation code.</p>
    <p>Training set: Text8 vs. Text8+NULLED.</p>
    <p>Accuracy: 0.50 vs. 0.46.</p>
    <p>In this experiment, we trained an SCM using Text8 along with a snapshot of Nulled [12], a collection of communication traces from an underground forum.</p>
    <p>Tomas Mikolov [22] provides code and the test set for evaluating the quality of word vectors.</p>
  </div>
  <div class="page">
    <p>3#-&quot;, $+*(&quot;&amp;'#7 JARGON DISCOVERY</p>
    <p>Dark jargons +</p>
    <p>Reddit slangs</p>
    <p>Reddit slangs +</p>
    <p>Forum terms</p>
    <p>Dark jargons</p>
    <p>Reddit slangs: such as damage on reddit.com often appear during the discussion of VIDEO GAMES and as a result, its context becomes very much biased towards settings in the games (such as heal, stun and dps);</p>
    <p>This the basic idea of jargon discover algorithm. It actually involves quite a few Implementation details and I dont have time to cover all those in the talk, so plz refer to our paper for more details.</p>
  </div>
  <div class="page">
    <p>Key Idea: hypernymy</p>
    <p>3#-&quot;, ),$'#*13,$+,JARGON UNDERSTANDING</p>
    <p>Hypernym refers to a word with a broad meaning that more specific words fall under;. For example, color is a hypernym of red.</p>
    <p>Different levels of hypernyms, e.g. cocaine -&gt; stimulant -&gt; drug</p>
  </div>
  <div class="page">
    <p>Vking - Vman = Vqueen - Vwoman</p>
    <p>3#-&quot;, ),$'#*13,$+,JARGON UNDERSTANDING</p>
    <p>Another interesting feature of word2vec is that: some kind of semantic relations can be calculated by arithmetics of embedded vectors</p>
  </div>
  <div class="page">
    <p>Illicit product categories</p>
    <p>Jargon interpretations</p>
    <p>Dark jargons</p>
    <p>3#-&quot;, ),$'#*13,$+,JARGON UNDERSTANDING</p>
    <p>projection learningHypernym candidates</p>
    <p>This property was used by Fu in their 2014 work. They found</p>
  </div>
  <div class="page">
    <p>$313*'1* 8 '&amp;34)31+&quot;, DATASETS &amp; EVALUATION</p>
    <p>1.5 million communication traces, 117 million words</p>
    <p>3,462 dark jargons covers 5 categories of illicit products</p>
    <p>Precision 0.91, recall 0.77</p>
    <p>Dataset: DARKNET MARKET ARCHIVES + Identifying products in online cybercrime marketplaces: A dataset for fine-grained domain adaptation.  Silkroad: mostly drugs 6/2011 - 11/2013  Darkode: cybercriminal wares, e.g. exploit kits, spam services, ransomware, and botnets. 3/2008 - 3/2013  Hack Forum: cyber-security, hacking technology and others. 5/2008 - 3/2015  NULLED: data stealing tools and services 11/2012 - 5/2016</p>
    <p>We observe the 3,462 dark jargons covers 5 categories of illicit products: drugs has the most jargons.</p>
    <p>Evaluation Precision: random sample 200 detected jargons Ground truth set: Drug Enforcement Administration (DEA) drug codename list + 1,292 illegitimate products manually annotated Recall: 774 jargon words in the set, 598 were successfully detected by Cantreader</p>
    <p>FN: jargon car means cocaine, never used nowadays</p>
  </div>
  <div class="page">
    <p>marijuana</p>
    <p>DEA (Drug Enforcement Administration) drug code words (may 2017): we found many drug jargons are not included in the drug jargon lists recorded by DEA For example On average, around 25 dark jargons emerge each month on hack forums from 2010 to 2013.  cinderella - a kind of cannabis  pea - organic compound acts as a central nervous system stimulant  mango - a kind of marijuana</p>
  </div>
  <div class="page">
    <p>We observe the 3,462 dark jargons covers 5 categories of illicit products: drugs has the most jargons.</p>
    <p>Jargons can be used in the:  profile of dealers and customers of illicit products,  identify key players in the community and  recover the ecosystem</p>
  </div>
  <div class="page">
    <p>Where are the dark jargons chosen from?</p>
    <p>We observe cyber-criminals choose jargons from a variety of types of innocent-looking words (e.g. animal, plant, fictional character). 8 categories has over 30 jargons.</p>
    <p>drug dealers like fruit (pineapple, blueberry, lemon) hackers prefer mythological figures (zeus, loki , athena)</p>
  </div>
  <div class="page">
    <p>&quot;19'# &quot;:*'#&amp;31+&quot;,* OTHER OBSERVATIONS</p>
    <p>We observe dark jargons also used in benign forums. (675 communication traces in Reddit related to illicit activity)</p>
    <p>We observe that dark jargons can help us find black words (dedicated used by cyber-criminals). We discover 522 black words with the help of discovered.</p>
    <p>Measurement - the four forums</p>
    <p>(8 types have more than 30 jargons).</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Conclusion</p>
  </div>
</Presentation>
