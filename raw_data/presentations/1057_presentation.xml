<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Redesigning Protection Storage for Modern Workloads</p>
    <p>Cant we all get along?</p>
    <p>Yamini Allu Fred Douglis* Mahesh Kamat Ramya Prabhakar Philip Shilane Rahul Ugale</p>
    <p>* Perspecta Labs</p>
  </div>
  <div class="page">
    <p>Data Domain File System  Purpose-built backup appliance</p>
    <p>Designed to identify duplicate regions of files and replace with references</p>
    <p>Designed around backup workloads which is typically data written and read sequentially.</p>
    <p>De-duplication  Content-defined chunks, fingerprinted with</p>
    <p>secure hash  Generally claim 10-40x data reduction</p>
    <p>Avoiding disk bottlenecks in Data Domain[Zhu 08] - SISL: Stream informed segment layout - Locality preserved caching for segments</p>
    <p>NFS, CIFS, VTL, DDBOOST</p>
    <p>Partition data into chunks</p>
    <p>Fingerprint chunks uniquely</p>
    <p>Filter duplicates</p>
    <p>Locally compress</p>
    <p>Store to disk</p>
    <p>De-duplication storage pipeline</p>
  </div>
  <div class="page">
    <p>Traditional Backup/Restore Workloads</p>
    <p>Sequential Workload</p>
    <p>Few Large Files</p>
    <p>Offline Backup Image Transfer for Recovery</p>
    <p>Backup Data Format</p>
    <p>Throughput Oriented</p>
    <p>Weekly Full, Daily</p>
    <p>Incremental</p>
    <p>Modern Backup/Restore Workloads</p>
    <p>Non Sequential I/O (NSIO)</p>
    <p>+ Sequential</p>
    <p>Many Small Files</p>
    <p>Native Data Format</p>
    <p>Throughput and Latency Sensitive</p>
    <p>Shift in Data Protection Workloads</p>
    <p>Instant Data Access / Recovery</p>
    <p>Incremental Forever/Virtua</p>
    <p>l full</p>
    <p>Challenge was to enhance our filesystem stack to support BOTH traditional and modern workloads</p>
  </div>
  <div class="page">
    <p>HD DFPI Containers</p>
    <p>Index</p>
    <p>A B C</p>
    <p>D E</p>
    <p>Z</p>
    <p>ALfp 0Maps fingerprint of data chunks to</p>
    <p>containers</p>
    <p>BLfp 0 DLfp 1</p>
    <p>File represented as Merkle Tree</p>
    <p>Afp Bfp Cfp</p>
    <p>VM1</p>
    <p>Xfp Yfp Zfp</p>
    <p>Primary Storage/Backup Applications</p>
    <p>Access Protocols</p>
    <p>Directory Manager VM</p>
    <p>User2</p>
    <p>/</p>
    <p>User1 VM</p>
    <p>Traditional Workloads:  Large files amortize Directory Manager lookup cost</p>
    <p>File Metadata (FMD) can be pre-fetched into memory for sequential I/O</p>
    <p>Good data locality Only one FPI lookup for 1 MB</p>
    <p>Good data locality fewer I/Os to fetch data chunks number of data can be Pre-fetched to memory</p>
    <p>Modern Workloads:</p>
    <p>Increased Directory Manager lookups for native format backups (small files)</p>
    <p>FMD pre-fetch into memory is not efficient for NSIO</p>
    <p>Increased index lookups due to random locality (once every I/O)</p>
    <p>Random I/O to HDD for every request</p>
    <p>IO Profile for Traditional vs Modern</p>
    <p>p1 p2</p>
  </div>
  <div class="page">
    <p>I/O for DM lookup</p>
    <p>I/O for FMD chunk load</p>
    <p>I/O for FPI</p>
    <p>I/O for Chunk Access</p>
    <p>Total number of HDD I/Os = ~8 I/Os per 1MB</p>
    <p>Example</p>
    <p>HD DFPI Containers</p>
    <p>Index</p>
    <p>A B C</p>
    <p>D E</p>
    <p>Z</p>
    <p>ALfp 0Maps fingerprint of data chunks to</p>
    <p>containers</p>
    <p>BLfp 0 DLfp 1</p>
    <p>File represented as Merkle Tree</p>
    <p>Afp Bfp Cfp</p>
    <p>VM1</p>
    <p>Xfp Yfp Zfp</p>
    <p>Primary Storage/Backup Applications</p>
    <p>Access Protocols</p>
    <p>Directory Manager VM</p>
    <p>User2</p>
    <p>/</p>
    <p>User1 VM</p>
    <p>p1 p2</p>
    <p>I/O for data chunk load</p>
  </div>
  <div class="page">
    <p>I/O for DM lookup</p>
    <p>I/O for FMD chunk load</p>
    <p>I/O for FPI</p>
    <p>I/O for Chunk Access</p>
    <p>Total number of HDD I/Os = ~8 I/Os per 8KB ~1024 per 1MB</p>
    <p>Example</p>
    <p>HD DFPI Containers</p>
    <p>Index</p>
    <p>A B C</p>
    <p>D E</p>
    <p>Z</p>
    <p>ALfp 0Maps fingerprint of data chunks to</p>
    <p>containers</p>
    <p>BLfp 0 DLfp 1</p>
    <p>File represented as Merkle Tree</p>
    <p>Afp Bfp Cfp</p>
    <p>VM1</p>
    <p>Xfp Yfp Zfp</p>
    <p>Primary Storage/Backup Applications</p>
    <p>Access Protocols</p>
    <p>Directory Manager VM</p>
    <p>User2</p>
    <p>/</p>
    <p>User1 VM</p>
    <p>p1 p2 I/O for data chunk load</p>
  </div>
  <div class="page">
    <p>Fingerprint Index (FPI) Cache  Two level index requires 2 HDD I/Os per chunk</p>
    <p>read  For sequential restores, index I/Os are 25% of</p>
    <p>total I/Os, for NSIO, it is 50%  Index metadata is 1.5% of total physical space  Caching short fingerprints (4 bytes) in SSD</p>
    <p>reduces SSD space requirement to 0.4%, with a collision rate &lt; 0.01%.</p>
    <p>Collisions resolved by comparing full FP on disk  FPI cache improves sequential restore</p>
    <p>performance by up to 32% on I/O bound configurations</p>
    <p>FPI on SSD ALfp 0</p>
    <p>BLfp 0 CLfp 1 DLfp 1</p>
    <p>FPI on HDD</p>
    <p>IO PS</p>
    <p>Disk Only</p>
    <p>Disk + SSD for Fingerprint cache</p>
  </div>
  <div class="page">
    <p>SSD I/O for DM lookup</p>
    <p>Tr an sl at es</p>
    <p>HDD I/O for FMD lookup</p>
    <p>SSD I/O for FPI lookup</p>
    <p>HDD I/O for Data Access</p>
    <p>Total number of HDD I/Os = ~6 I/Os per 1MB</p>
    <p>Addition of SSD Cache for PBBA (dense drive support)</p>
    <p>HD DFPI Containers</p>
    <p>Index</p>
    <p>A B C</p>
    <p>D E</p>
    <p>Z</p>
    <p>ALfp 0Maps fingerprint of data chunks to</p>
    <p>containers</p>
    <p>BLfp 0 DLfp 1</p>
    <p>File represented as Merkle Tree</p>
    <p>Afp Bfp Cfp</p>
    <p>VM1</p>
    <p>Xfp Yfp Zfp</p>
    <p>Primary Storage/Backup Applications</p>
    <p>Access Protocols</p>
    <p>Directory Manager VM</p>
    <p>User2 /</p>
    <p>User1 VM</p>
    <p>p1 p2</p>
    <p>ASfp DM FMD AB C D E</p>
    <p>Z</p>
    <p>ASfp BSfp</p>
  </div>
  <div class="page">
    <p>File Metadata (FMD) Cache  IOs for file metadata account to 50% of total I/Os for</p>
    <p>NSIO, FMD is cached only on NSIO accesses  File metadata is variable sized (Average 16kb)</p>
    <p>Accounts to 0.05% of logical backup size  10% of available SSD is reserved as FMD Cache to</p>
    <p>account for high metadata churn during NSIO  FMD is packed into 1MB Write eviction units to SSD</p>
    <p>cache  It is a write through cache, populated on FMD</p>
    <p>updates and read misses  Eviction is LRU based, this keeps most relevant data</p>
    <p>in cache Just FMD Cache reduces HDD I/Os required for 8KB NSIO from8 I/Os to 4 I/Os</p>
  </div>
  <div class="page">
    <p>Data Cache on SSD  Cache chunks of data that are randomly accessed within a file  Data is cached on a read miss and on write  Chunks are gathered and written to cache as 1MB Eviction unit. Eviction</p>
    <p>algorithm is LRU  Larger chunks are read-ahead and cached until the cache is sufficiently</p>
    <p>warm  40% of SSD cache is allocated for Data, sufficient to instantly access up to</p>
    <p>Data Cache reduces NSIO I/Os to HDD to 0</p>
  </div>
  <div class="page">
    <p>SSD I/O for DM lookup</p>
    <p>Tr an sl at es</p>
    <p>SSD I/O for FMD lookup</p>
    <p>SSD I/O for Index lookup</p>
    <p>SSD I/O for Data Access</p>
    <p>Total number of HDD I/Os = 0* I/Os</p>
    <p>Addition of SSD Cache for NSIO</p>
    <p>HD DFPI Containers</p>
    <p>Index</p>
    <p>A B C</p>
    <p>D E</p>
    <p>Z</p>
    <p>ALfp 0Maps fingerprint of data chunks to</p>
    <p>containers</p>
    <p>BLfp 0 DLfp 1</p>
    <p>File represented as Merkle Tree</p>
    <p>Afp Bfp Cfp</p>
    <p>VM1</p>
    <p>Xfp Yfp Zfp</p>
    <p>Primary Storage/Backup Applications</p>
    <p>Access Protocols</p>
    <p>Directory Manager VM</p>
    <p>User2 /</p>
    <p>User 1 VM</p>
    <p>p1 p2</p>
    <p>ASfp DM FMD AB C D E</p>
    <p>Z</p>
    <p>ASfp BSfp</p>
    <p>*Assumes 100% cache hit, typical read/write workload sees &gt;95% cache hits</p>
    <p>Cache warmup</p>
  </div>
  <div class="page">
    <p>File-system Modifications For NSIO Support</p>
    <p>Access pattern detection  Detect Sequential, NSIO Monotonic and NSIO</p>
    <p>patterns  Detect multiple access patterns across different</p>
    <p>regions of a file  Based on a sufficient history of past I/Os within a</p>
    <p>region of a file</p>
    <p>Disable simple prefetch for NSIO Enable parallel FMD and FPI</p>
    <p>lookups for NSIO Fixed size chunking for image</p>
    <p>backups</p>
    <p>Disable container metadata read for NSIO</p>
    <p>Disable dedup for small NSIO Delayed FMD updates for NSIO</p>
    <p>Span: 0GB -2GB 2GB-4GB 4GB-6GB label: Sequential NSIO-Monotonic NSIO</p>
    <p>Access history per region</p>
  </div>
  <div class="page">
    <p>QoS For Mixed Workloads  Tunable shares for non sequential workloads,</p>
    <p>default is 20%  CPU scheduling based on least loaded CPU,</p>
    <p>previously round robin  Higher priority for random reads compared to</p>
    <p>writes  Edge throttling based on feedback from</p>
    <p>different modules and subsystem health  Increasing QoS share for NSIO workloads</p>
    <p>increased NSIO performance in our mixed workload experiments</p>
    <p>PBBA Shares 80</p>
    <p>NSIO Shares 20</p>
    <p>Root</p>
    <p>External (50)</p>
    <p>Internal (50)</p>
    <p>Backup (25)</p>
    <p>Restore (30)</p>
    <p>Repl (25)</p>
    <p>NSIO (20)</p>
  </div>
  <div class="page">
    <p>Mixed Workload Performance NSIO performance was capped at 10K IOPs</p>
    <p>QOS throttle for NSIO set at 10%, Backup/Restore impacted by at-most 10%</p>
    <p>*DDBOOST  Bandwidth Optimized Open Storage Protocol 0</p>
    <p>NFS Write DDBOOST Write</p>
    <p>DDBOOST Read</p>
    <p>NFS Read/Write</p>
    <p>DDBOOST Read/Write</p>
    <p>Th ro u gh</p>
    <p>p ut in M B /s</p>
    <p>Backup Workload</p>
    <p>Backup and NSIO workload with 100% read</p>
    <p>Backup and NSIO workload with 70% read</p>
  </div>
  <div class="page">
    <p>NSIO Performance Evaluation</p>
    <p>No. of VMs 1 8 16 24 32</p>
    <p>IO PS</p>
    <p>Without Flash Cache with software optimizations</p>
  </div>
  <div class="page">
    <p>NSIO Performance Evaluation</p>
    <p>No. of VMs 1 8 16 24 32</p>
    <p>IO PS</p>
    <p>Without Flash Cache with software optimizations</p>
    <p>With metadata in Flash Cache with software optimizations</p>
  </div>
  <div class="page">
    <p>NSIO Performance Evaluation</p>
    <p>No. of VMs 1 8 16 24 32</p>
    <p>IO PS</p>
    <p>Without Flash Cache with software optimizations</p>
    <p>With metadata in Flash Cache with software optimizations</p>
    <p>With data &amp; metadata in Flash Cache with software optimizations</p>
  </div>
  <div class="page">
    <p>NSIO Performance Evaluation</p>
    <p>No. of VMs 1 8 16 24 32</p>
    <p>IO PS</p>
    <p>Without Flash Cache with software optimizations</p>
    <p>With metadata in Flash Cache with software optimizations</p>
    <p>With data &amp; metadata in Flash Cache with software optimizations</p>
    <p>With data &amp; metadata in Flash Cache without sofware optimizations</p>
  </div>
  <div class="page">
    <p>Conclusion/Future Work Improvements to our software and the addition of SSD caches allow Data Domain to support both new and traditional workloads  With a NSIO workload, with SSDs for caching metadata, we measured a 5.7x IOPS improvement</p>
    <p>relative to a system without SSDs  Adding data cache improved performance by further 1.7x  Combining SSD caching with software optimizations throughout our system, added an additional</p>
    <p>Future work: Additional SSDs and IOPs based on use-case QOS/priorities within random workloads Optimizations to improve CPU and disk utilization as we support higher IOPs</p>
  </div>
  <div class="page">
    <p>THANK YOU</p>
  </div>
</Presentation>
