<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MicroMon: A Monitoring Framework for Tackling Distributed Heterogeneity</p>
    <p>Babar Khalid*+, Nolan Rudolph+,</p>
    <p>Ramakrishnan Durairajan, Sudarsun Kannan*</p>
    <p>*Rutgers University, University of Oregon</p>
    <p>(+co-primary authors)</p>
  </div>
  <div class="page">
    <p>Background  Modern applications are increasingly becoming geo-distributed</p>
    <p>- e.g., Cassandra, Apache Spark</p>
    <p>Geo-distributed datacenters (DCs) use heterogeneous resources - storage heterogeneity (e.g., SSD, NVMe, Harddisk) - WAN heterogeneity (e.g., fiber optics, InfiniBand)</p>
    <p>Hardware heterogeneity in DCs avoids vendor lockout and reduces operational cost (by combining older/cheaper and newer/expensive hardware)</p>
    <p>Careful provisioning can provide high performance at lower cost</p>
  </div>
  <div class="page">
    <p>Problem With Current Systems  Current monitoring frameworks for geo-distributed applications are</p>
    <p>unidimensional - can only monitor hosts, storage devices, networks in isolation</p>
    <p>Lack hardware heterogeneity awareness - e.g. no awareness for storage heterogeneity - could impact I/O intensive applications</p>
    <p>Coarse-granular monitoring - unaware of host-level micro-metrics in software and hardware - e.g. page cache, node-level I/O traffic, nodes network queue delays</p>
  </div>
  <div class="page">
    <p>Our Solution - MicroMon  MicroMon is a fined grained monitoring, dissemination, and inference</p>
    <p>framework  Collects fine-grained (micrometrics) software and hardware metrics in</p>
    <p>end-hosts and network</p>
    <p>- e.g., page cache utilization, disk read/write throughput in end host</p>
    <p>Filters micrometrics into anomalies to efficiently disseminate</p>
    <p>Enables replica selection for geo-distributed Cassandra  Preliminary study of Micromon integrated with geo-distributed</p>
    <p>Cassandra shows high throughput gains</p>
  </div>
  <div class="page">
    <p>Background  Case Study  Design  Evaluation  Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Case Study - Cassandra</p>
    <p>Distributed NoSQL database system deployed geographically</p>
    <p>Manages large amounts of structured data in commodity servers</p>
    <p>Provides highly available service and no single point of failure</p>
    <p>Typically focuses on availability and partition tolerance</p>
  </div>
  <div class="page">
    <p>Cassandra  Replication</p>
    <p>Node 1</p>
    <p>Node 2</p>
    <p>Node 3 Node 4</p>
    <p>Node 5</p>
    <p>Client</p>
    <p>Upd ate</p>
    <p>(key )</p>
    <p>Cassandra Cluster</p>
  </div>
  <div class="page">
    <p>Cassandra  Replication</p>
    <p>Node 1</p>
    <p>Node 2</p>
    <p>Node 3 Node 4</p>
    <p>Node 5</p>
    <p>Client</p>
    <p>Upd ate</p>
    <p>(key )</p>
    <p>Rack 1</p>
    <p>Rack 1</p>
    <p>Rack 2</p>
    <p>Rack Awareness</p>
  </div>
  <div class="page">
    <p>Cassandra  Replication</p>
    <p>Node 1</p>
    <p>Node 2</p>
    <p>Node 3 Node 4</p>
    <p>Node 5</p>
    <p>Client</p>
    <p>Upd ate</p>
    <p>(key )</p>
    <p>Rack 1</p>
    <p>Rack 1</p>
    <p>Rack 2</p>
    <p>Node 1</p>
    <p>Node 2</p>
    <p>Node 3 Node 4</p>
    <p>Node 5</p>
    <p>Rack 1</p>
    <p>Rack 1</p>
    <p>Rack 2</p>
    <p>DC: US DC: Europe</p>
    <p>DC Awareness</p>
  </div>
  <div class="page">
    <p>Cassandras Snitch Monitoring  Cassandra uses Snitch to monitor network topology and route requests</p>
    <p>across replicas</p>
    <p>Also provides capability to spread replicas across DCs to avoid correlated failures</p>
    <p>Snitch monitors (read) latencies to avoid non-responsive replicas</p>
    <p>Different types: Gossiping, MultiRegionSnitch - Gossiping uses rack and datacenter information to</p>
    <p>gossip across nodes and collect latency information</p>
    <p>Problem: No hardware heterogeneity awareness</p>
  </div>
  <div class="page">
    <p>Analysis Goal and Methodology  Goal: Highlight the lack of heterogeneity awareness</p>
    <p>Replica Configuration - SSD Replica: Sequential storage b/w - 600MB/s, rand b/w: 180 MB/s - HDD replica: Sequential storage b/w - 120MB/s, rand b/w: 10 MB/s</p>
    <p>Network latency across replicas same (for this analysis)</p>
    <p>Workload  YCSB benchmark - workload A (50% read and writes) - workload B (95% reads) - workload C (100% reads)</p>
  </div>
  <div class="page">
    <p>Impact of Storage Heterogeneity Awareness</p>
    <p>Significant performance impact over optimal SSD-only configuration</p>
    <p>Snitch: Lack of awareness to storage hardware heterogeneity</p>
    <p>A B C</p>
    <p>O PS</p>
    <p>/s ec</p>
    <p>YCSB Workloads</p>
    <p>HDD-only SSD-only Snitch</p>
  </div>
  <div class="page">
    <p>Background  Case Study  Design  Evaluation  Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Our Design: MicroMon  Monitoring and inference framework for geo-distributed applications</p>
    <p>Performs micro-metrics monitoring at the host and network-level - micro-metrics includes fine-grained software and hardware metrics</p>
    <p>Efficiently disseminates collected micro-metrics</p>
    <p>Ongoing - Distributed inference engines to guide application requests to the best replica</p>
  </div>
  <div class="page">
    <p>MicroMon Challenges</p>
    <p>Selection Problem: What micrometrics to consider?</p>
    <p>Dissemination Problem: How to send all micrometrics?</p>
    <p>Inference Problem: How to quickly infer from micrometrics?</p>
  </div>
  <div class="page">
    <p>Design - Micrometrics Selection  Huge combinations of micrometrics across app, host OS, and network</p>
    <p>Micrometrics could vary for different application-level metrics e.g. micrometrics for latency different than those for throughput</p>
    <p>Our approach: Start with storage and network micrometrics</p>
    <p>Identify hardware and software micrometrics using resource usage - e.g. high storage usage -&gt; monitor page cache, read/write latency</p>
  </div>
  <div class="page">
    <p>MicroMon High-level Design</p>
    <p>Enterprise Backbone Enterprise DC A</p>
    <p>Storage stack micrometrics at DC Page cache (SW) File system (SW) Block device driver (SW) Hard disk (HW)</p>
    <p>Networking stack micrometrics at DC ----- Transport ----Flags (syn, ack, etc.) Window size Goodput Bytes transmitted/received Round-trip time ----- Application ----Throughput</p>
    <p>Networking stack micrometrics at switches ----- Ingress/Egress ----Port Packet count Byte count Drop count Utilization ----- Buffer ----Avg. queue length Queue drop count Congestion status</p>
    <p>Collected micrometrics</p>
    <p>Server</p>
    <p>Enterprise DC B</p>
    <p>Switch</p>
  </div>
  <div class="page">
    <p>Reducing Dissemination  Anomaly Reports</p>
    <p>Problem: Prohibitive cost of dissemination across thousands of nodes - cost increases with hardware and software components</p>
    <p>- e.g., SSDs SMART counters contain close to 32 counters</p>
    <p>Observation: OSes already expose anomalies (indirectly) - e.g. high I/O wait time of process -&gt; higher page cache misses - e.g. sustained storage BW against max. hardware BW - e.g. network I/O queue wait time alludes to TCP congestion</p>
    <p>Proposed Idea: Instead of sending thousands of micrometrics to decision agent, only report OS perceived anomalies</p>
  </div>
  <div class="page">
    <p>Reducing Dissemination - Network Telemetry</p>
    <p>Network telemetry offers aggregated stats about state of the network</p>
    <p>Idea: co-design in-band network telemetry (INT) with end host OS - monitor packets at end host with anomaly reports as payload - get network anomaly reports using INT</p>
    <p>Pre-established anomaly thresholds reduce total aggregated stats further</p>
    <p>Network anomalies</p>
    <p>INT header INT payload</p>
    <p>End-host anomalies</p>
  </div>
  <div class="page">
    <p>Scalable Inference - Scoring-based Inference</p>
    <p>Simple scoringbased inference in Cassandra - replicas sorted and ranked by network latency</p>
    <p>Problem: for bandwidth sensitive applications, need higher weights for WAN-based micrometrics compared to host-level micrometrics</p>
    <p>Our approach: - we assign equal weights to all software and hardware micrometrics - use collected micrometrics to calculate a replica score - route request to replicas with higher scores - flexibility to assign higher weights for WAN-based micrometrics</p>
    <p>Ongoing: Designing a generic, self-adaptive inference engine</p>
  </div>
  <div class="page">
    <p>Background  Case Study  Design  Evaluation  Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Evaluation Goals Goals:</p>
    <p>Understand the impact of storage heterogeneity with Micromon</p>
    <p>Understand the impact of storage heterogeneity + network latency</p>
    <p>Analyze the page cache impact (see paper for details)</p>
  </div>
  <div class="page">
    <p>Analysis Methodology  Multiple DCs from CloudLab Infrastructure</p>
    <p>- three nodes located in UTAH, APT, and Emulab DCs</p>
    <p>Replica Configuration - UTAH replica: NVMe storage (seq bw: 600MB/s, rand bw: 180 MB/s) - APT replica: HDD (seq bw: 120 MB/s, rand bw: 10 MB/s) - Emulab master node: HDD (same as above)</p>
    <p>Network Latencies - 400us between UTAH (NVMe) replica and master node - 600us between APT (HDD) replica and master node</p>
    <p>Workload  YCSB benchmark - workload A (50% read and writes) - workload B (95% reads) - workload C (100% reads)</p>
  </div>
  <div class="page">
    <p>MicroMons - Storage Heterogeneity</p>
    <p>Snitch lacks storage heterogeneity awareness  MicroMons storage heterogeneity awareness provides performance</p>
    <p>close to SSD-only (optimal) configuration  Performance improves by up to 49% for large thread configuration</p>
    <p>Workload A Workload B Workload C</p>
    <p>O ps</p>
    <p>/s ec</p>
    <p>HDD-only SSD-only Snitch MicroMon</p>
  </div>
  <div class="page">
    <p>Storage Heterogeneity + Network Latency</p>
    <p>-1000</p>
    <p>T h</p>
    <p>ro u</p>
    <p>gh p</p>
    <p>u t</p>
    <p>(o p</p>
    <p>s/ s)</p>
    <p>Network Latency</p>
    <p>Snitch MicroMon</p>
    <p>Introduce network latency for SSD-only node</p>
    <p>For high network latencies (e.g., beyond 10ms) SSD benefits reduce</p>
  </div>
  <div class="page">
    <p>Conclusion  Datacenter systems are becoming more and more heterogeneous</p>
    <p>Deploying geo-distributed applications in heterogeneous datacenters requires redesign of monitoring mechanisms</p>
    <p>We propose MicroMon, a fine-grained micrometric monitoring, dissemination, and inference framework</p>
    <p>Our on-going work will focus on efficient dissemination and selfadaptive inference mechanisms</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Questions?</p>
    <p>Contact: sudarsun.kannan@rutgers.edu</p>
    <p>ram@cs.uoregon.edu</p>
  </div>
</Presentation>
