<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Polarity Inducing Latent Semantic Analysis</p>
    <p>Scott Wen-tau Yih Joint work with Geoffrey Zweig &amp; John Platt</p>
    <p>Microsoft Research</p>
    <p>A vector space model that can distinguish Antonyms from Synonyms!</p>
  </div>
  <div class="page">
    <p>Vector Space Model Text objects (e.g., words, phrases, sentences or documents) are represented as vectors</p>
    <p>High-dimensional sparse term-vectors Concept vectors from topic models or projection methods Constructed compositionally from word vectors [Socher et al. 12]</p>
    <p>Relations of the text objects are estimated by functions in the vector space</p>
    <p>Relatedness is measured by some distance function (e.g., cosine)</p>
    <p>cos()vq</p>
    <p>vd qv</p>
  </div>
  <div class="page">
    <p>Applications of Vector Space Models Document Level</p>
    <p>Information Retrieval [Salton &amp; McGill 83] Document Clustering [Deerwester et al. 90] Search Relevance Measurement [Baeza-Yates &amp; Riberio-Neto 99]</p>
    <p>Cross-lingual document retrieval [Platt et al. 10; Yih et al. 11]</p>
    <p>Word Level Language modeling [Bellegarda 00] Word similarity and relatedness [Deerwester et al. 90; Lin 98; Turney 01; Turney &amp; Littman 05; Agirre et al. 09; Reisinger &amp; Mooney 10; Yih &amp; Qazvinian 12]</p>
  </div>
  <div class="page">
    <p>Beyond General Similarity</p>
    <p>Existing VSMs cannot distinguish finer relations</p>
    <p>The antonym issue of distributional similarity The co-occurrence or distributional hypotheses</p>
    <p>Apply to near-synonyms, hypernyms and other semantically related words, including antonyms [Mohammad et al. 08] e.g., hot and cold occur in similar contexts</p>
    <p>LSA does not solve the issue Might assign a high degree of similarity to opposites as well as synonyms [Landauer &amp; Laham 98]</p>
  </div>
  <div class="page">
    <p>Approaches for Detecting Antonyms Separate antonyms from distributionally similar word pairs [Lin et al. 03]</p>
    <p>Patterns: from X to Y, either X or Y</p>
    <p>WordNet graph [Harabagiu et al. 06] Synsets connected by is-a links and exactly one antonymy link</p>
    <p>WordNet + affix rules + heuristics [Mohammad et al. 08]</p>
    <p>Distinguishing synonyms and antonyms is still perceived as a difficult open problem [Poon &amp; Domingos 09]</p>
  </div>
  <div class="page">
    <p>Our Contributions</p>
    <p>Polarity Inducing Latent Semantic Analysis (PILSA) A vector space model that encodes polarity information Synonyms cluster together in this space Antonyms lie at the opposite ends of a unit sphere</p>
    <p>hot burning</p>
    <p>cold freezing</p>
  </div>
  <div class="page">
    <p>Our Contributions</p>
    <p>Polarity Inducing Latent Semantic Analysis (PILSA) A vector space model that encodes polarity information Synonyms cluster together in this space Antonyms lie at the opposite ends of a unit sphere</p>
    <p>Significantly improved the prediction accuracy on a benchmark GRE dataset ()</p>
  </div>
  <div class="page">
    <p>Roadmap Introduction Polarity Inducing Latent Semantic Analysis</p>
    <p>Basic construction Extension 1: Improving accuracy Extension 2: Improving coverage</p>
    <p>Experimental evaluation Task &amp; datasets Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Input: A thesaurus (with synonyms &amp; antonyms)</p>
    <p>Create a document-term matrix Each group of words (synonyms and antonyms) is treated as a document</p>
    <p>Induce polarity by making antonyms have negative weights Apply SVD as in regular Latent Semantic Analysis</p>
    <p>The Core Method</p>
  </div>
  <div class="page">
    <p>Matrix Construction Acrimony: rancor, conflict, bitterness; goodwill, affection Affection: goodwill, tenderness, fondness; acrimony, rancor</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 4.73 6.01 5.81 4.86</p>
    <p>Group 2: affection 3.78 5.23 6.21 5.15</p>
    <p>Document: row-vector Term: column-vector</p>
    <p>TFIDF score</p>
  </div>
  <div class="page">
    <p>Matrix Construction Acrimony: rancor, conflict, bitterness; goodwill, affection Affection: goodwill, tenderness, fondness; acrimony, rancor</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 4.73 6.01 -5.81 -4.86</p>
    <p>Group 2: affection -3.78 -5.23 6.21 5.15</p>
    <p>Inducing polarity</p>
    <p>Cosine Score:</p>
  </div>
  <div class="page">
    <p>Effect of Inducing Polarity</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 4.73 6.01 5.81 4.86</p>
    <p>Group 2: affection 3.78 5.23 6.21 5.15</p>
  </div>
  <div class="page">
    <p>Effect of Inducing Polarity</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 1 1 1 1</p>
    <p>Group 2: affection 1 1 1 1</p>
    <p>Cosine similarity = 1</p>
  </div>
  <div class="page">
    <p>Effect of Inducing Polarity</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 1 1 1 1</p>
    <p>Group 2: affection 1 1 1 1</p>
    <p>Cosine similarity = 1</p>
    <p>Cannot distinguish antonyms from synonyms!</p>
  </div>
  <div class="page">
    <p>Effect of Inducing Polarity</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 1 1 1 1</p>
    <p>Group 2: affection 1 1 1 1</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 1 1 -1 -1</p>
    <p>Group 2: affection -1 -1 1 1</p>
    <p>Cosine similarity = 1</p>
  </div>
  <div class="page">
    <p>Effect of Inducing Polarity</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 1 1 1 1</p>
    <p>Group 2: affection 1 1 1 1</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 1 1 -1 -1</p>
    <p>Group 2: affection -1 -1 1 1</p>
    <p>Cosine similarity = -1</p>
  </div>
  <div class="page">
    <p>Mapping to Latent Space via SVD</p>
    <p>T</p>
    <p>words</p>
    <p>Word similarity: cosine of two columns in</p>
    <p>SVD generalizes and smooths the original data Uncovers relationships not explicit in the thesaurus</p>
  </div>
  <div class="page">
    <p>Mapping to Latent Space via SVD</p>
    <p>T</p>
    <p>words</p>
    <p>As , can be viewed as the projection matrix that maps the raw column-vector to the -dimensional latent space</p>
  </div>
  <div class="page">
    <p>Extension 1: Improve Accuracy Refine the projection matrix by discriminative training</p>
    <p>S2Net [Yih et al. 11]: very similar to RankNet [Burges et al. 05] but focuses on learning concept vectors</p>
    <p>1</p>
    <p>1</p>
    <p>=</p>
    <p>( ,)</p>
  </div>
  <div class="page">
    <p>Applying S2Net Training data: Antonym pairs from thesaurus Initialize model with the PILSA projection matrix</p>
    <p>Learning objective: cosine score of antonyms should be lower than other word pairs</p>
    <p>( ;)=log(1+exp ( ))</p>
    <p>-2 -1 0 1 2 0</p>
    <p>AntonymsOther word pair</p>
    <p>cos( T  ,</p>
    <p>T )cos (</p>
    <p>T ,</p>
    <p>T  )</p>
  </div>
  <div class="page">
    <p>Extension 2: Improve Coverage</p>
    <p>What to do with out-of-thesaurus words?</p>
    <p>Some lexical variations Encarta thesaurus contains corruptible and corruption, but not corruptibility</p>
    <p>Morphological analysis and stemming to find alternatives of an out-of-thesaurus target word</p>
    <p>Rare or offensive words e.g., froward and moronic</p>
    <p>Embedding out-of-thesaurus words by leveraging a general corpus</p>
  </div>
  <div class="page">
    <p>Embedding Out-of-thesaurus Words Create a context vector space model using a collection of documents (e.g., Wikipedia)</p>
    <p>Context: words within a window of [-10,10]</p>
    <p>Embed target word into the PILSA space by -NN Find nearby in-thesaurus words in the context space Remove words with inconsistent polarity Use the centroid of the corresponding PILSA vectors to represent the target word</p>
  </div>
  <div class="page">
    <p>Embedding Out-of-thesaurus Words Create a context vector space model using a collection of documents (e.g., Wikipedia)</p>
    <p>Context: words within a window of [-10,10]</p>
    <p>Embed target word into the PILSA space by -NN</p>
    <p>Context Vector Space PILSA Space</p>
    <p>sweltering</p>
    <p>burning</p>
    <p>hot</p>
    <p>cold</p>
  </div>
  <div class="page">
    <p>Roadmap Introduction Polarity Inducing Latent Semantic Analysis</p>
    <p>Basic construction Extension 1: Improving accuracy Extension 2: Improving coverage</p>
    <p>Experimental evaluation Task &amp; datasets Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Data for Building PILSA Models</p>
    <p>Encarta Thesaurus (for basic PILSA) 47k word categories (i.e., the documents) Vocabulary of 50k words 125,724 pairs of antonyms</p>
    <p>Wikipedia (for embedding out-of-thesaurus words) Sentences from a Nov-2010 snapshot 917M words after preprocessing</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>Task: GRE closest-opposite questions Which is the closest opposite of adulterate? (a) renounce (b) forbid (c) purify (d) criticize (e) correct Dev / Test: 162 / 950 questions [Mohammad et al. 08] Dev set is used for tuning the dimensionality of PILSA</p>
    <p>Evaluation metric Accuracy: #correct / #total questions Questions with unresolved out-of-thesaurus target words are treated answered incorrectly</p>
  </div>
  <div class="page">
    <p>Results on Test Set</p>
  </div>
  <div class="page">
    <p>Examples</p>
    <p>Target word: admirable No polarity  LSA</p>
    <p>Most Similar: commendable, creditable, despicable Least Similar: uninviting, dessert, seductive</p>
    <p>With polarity  PILSA Most Similar: commendable, creditable, laudable Least Similar: despicable, shameful, unworthy</p>
    <p>Full results on GRE test set are available online</p>
  </div>
  <div class="page">
    <p>Conclusion Polarity Inducing LSA</p>
    <p>Solves the open problem of antonyms/synonyms by making a vector space that can distinguish opposites Vector space designed so that synonyms/antonyms tend to have positive/negative cosine similarity</p>
    <p>Future Work New methods or representations for other word relations</p>
    <p>e.g., Part-Whole, Is-A, Attribute</p>
    <p>Applications e.g., Textual Entailment or Sentence Completion</p>
  </div>
</Presentation>
