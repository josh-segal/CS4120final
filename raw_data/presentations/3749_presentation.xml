<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Interpretable Deep Learning under Fire</p>
    <p>Xinyang Zhang1 Ningfei Wang2 Hua Shen1 Shouling Ji3,4 Xiapu Luo5 Ting Wang1</p>
  </div>
  <div class="page">
    <p>DNN Interpretability</p>
    <p>Lack of interpretability  How does a DNN arrive at a particular decision?</p>
    <p>Intensive research on interpreting DNNs  Backprop-guided  Representation-guided  Perturbation-guided  Model-based</p>
    <p>!2</p>
    <p>Interpretable Explanations of Black Boxes by Meaningful Perturbation</p>
    <p>Ruth C. Fong University of Oxford</p>
    <p>ruthfong@robots.ox.ac.uk</p>
    <p>Andrea Vedaldi University of Oxford</p>
    <p>vedaldi@robots.ox.ac.uk</p>
    <p>Abstract</p>
    <p>As machine learning algorithms are increasingly applied</p>
    <p>to high impact yet high risk tasks, such as medical diag</p>
    <p>nosis or autonomous driving, it is critical that researchers</p>
    <p>can explain how such algorithms arrived at their predic</p>
    <p>tions. In recent years, a number of image saliency methods</p>
    <p>have been developed to summarize where highly complex</p>
    <p>neural networks look in an image for evidence for their</p>
    <p>predictions. However, these techniques are limited by their</p>
    <p>heuristic nature and architectural constraints.</p>
    <p>In this paper, we make two main contributions: First, we</p>
    <p>propose a general framework for learning different kinds</p>
    <p>of explanations for any black box algorithm. Second, we</p>
    <p>specialise the framework to find the part of an image most</p>
    <p>responsible for a classifier decision. Unlike previous works,</p>
    <p>our method is model-agnostic and testable because it is</p>
    <p>grounded in explicit and interpretable image perturbations.</p>
    <p>ern black box predictors such as deep neural networks [4, 5], there is a considerable interest in explaining and understanding predictors a-posteriori, after they have been learned. This remains largely an open problem. One reason is that we lack a formal understanding of what it means to explain a classifier. Most of the existing approaches [19, 16, 8, 7, 9, 19], etc., often produce intuitive visualizations; however, since such visualizations are primarily heuristic, their meaning remains unclear.</p>
    <p>In this paper, we revisit the concept of explanation at a formal level, with the goal of developing principles and methods to explain any black box function f, e.g. a neural network object classifier. Since such a function is learned automatically from data, we would like to understand what f has learned to do and how it does it. Answering the what question means determining the properties of the map. The how question investigates the internal mechanisms that allow the map to achieve these properties. We focus mainly on the what question and argue that it can</p>
    <p>Figure 1. An example of a mask learned (right) by blurring an image (middle) to suppress the softmax probability of its target class (left: original image; softmax scores above images).</p>
    <p>be answered by providing interpretable rules that describe the input-output relationship captured by f. For example, one rule could be that f is rotation invariant, in the sense that f(x) = f(x0) whenever images x and x0 are related by a rotation.</p>
    <p>In this paper, we make several contributions. First, we propose the general framework of explanations as metapredictors (sec. 2), extending [18]s work. Second, we identify several pitfalls in designing automatic explanation systems. We show in particular that neural network artifacts are a major attractor for explanations. While artifacts are informative since they explain part of the network behavior, characterizing other properties of the network requires careful calibration of the generality and interpretability of explanations. Third, we reinterpret network saliency in our framework. We show that this provides a natural generalization of the gradient-based saliency technique of [15] by integrating information over several rounds of backpropagation in order to learn an explanation. We also compare this technique to other methods [15, 16, 20, 14, 19] in terms of their meaning and obtained results.</p>
    <p>backpropagates the gradient for a class label to the image layer. Other backpropagation methods include DeConvNet [19] and Guided Backprop [16, 8], which builds off of DeConvNet [19] and [15]s gradient method to produce sharper visualizations.</p>
    <p>Another set of techniques incorporate network activations into their visualizations: Class Activation Mapping</p>
    <p>ar X</p>
    <p>iv :1</p>
    <p>[c s.</p>
    <p>C V</p>
    <p>] 10</p>
    <p>J an</p>
    <p>Interpretable Explanations of Black Boxes by Meaningful Perturbation</p>
    <p>Ruth C. Fong University of Oxford</p>
    <p>ruthfong@robots.ox.ac.uk</p>
    <p>Andrea Vedaldi University of Oxford</p>
    <p>vedaldi@robots.ox.ac.uk</p>
    <p>Abstract</p>
    <p>As machine learning algorithms are increasingly applied</p>
    <p>to high impact yet high risk tasks, such as medical diag</p>
    <p>nosis or autonomous driving, it is critical that researchers</p>
    <p>can explain how such algorithms arrived at their predic</p>
    <p>tions. In recent years, a number of image saliency methods</p>
    <p>have been developed to summarize where highly complex</p>
    <p>neural networks look in an image for evidence for their</p>
    <p>predictions. However, these techniques are limited by their</p>
    <p>heuristic nature and architectural constraints.</p>
    <p>In this paper, we make two main contributions: First, we</p>
    <p>propose a general framework for learning different kinds</p>
    <p>of explanations for any black box algorithm. Second, we</p>
    <p>specialise the framework to find the part of an image most</p>
    <p>responsible for a classifier decision. Unlike previous works,</p>
    <p>our method is model-agnostic and testable because it is</p>
    <p>grounded in explicit and interpretable image perturbations.</p>
    <p>ern black box predictors such as deep neural networks [4, 5], there is a considerable interest in explaining and understanding predictors a-posteriori, after they have been learned. This remains largely an open problem. One reason is that we lack a formal understanding of what it means to explain a classifier. Most of the existing approaches [19, 16, 8, 7, 9, 19], etc., often produce intuitive visualizations; however, since such visualizations are primarily heuristic, their meaning remains unclear.</p>
    <p>In this paper, we revisit the concept of explanation at a formal level, with the goal of developing principles and methods to explain any black box function f, e.g. a neural network object classifier. Since such a function is learned automatically from data, we would like to understand what f has learned to do and how it does it. Answering the what question means determining the properties of the map. The how question investigates the internal mechanisms that allow the map to achieve these properties. We focus mainly on the what question and argue that it can</p>
    <p>Figure 1. An example of a mask learned (right) by blurring an image (middle) to suppress the softmax probability of its target class (left: original image; softmax scores above images).</p>
    <p>be answered by providing interpretable rules that describe the input-output relationship captured by f. For example, one rule could be that f is rotation invariant, in the sense that f(x) = f(x0) whenever images x and x0 are related by a rotation.</p>
    <p>In this paper, we make several contributions. First, we propose the general framework of explanations as metapredictors (sec. 2), extending [18]s work. Second, we identify several pitfalls in designing automatic explanation systems. We show in particular that neural network artifacts are a major attractor for explanations. While artifacts are informative since they explain part of the network behavior, characterizing other properties of the network requires careful calibration of the generality and interpretability of explanations. Third, we reinterpret network saliency in our framework. We show that this provides a natural generalization of the gradient-based saliency technique of [15] by integrating information over several rounds of backpropagation in order to learn an explanation. We also compare this technique to other methods [15, 16, 20, 14, 19] in terms of their meaning and obtained results.</p>
    <p>backpropagates the gradient for a class label to the image layer. Other backpropagation methods include DeConvNet [19] and Guided Backprop [16, 8], which builds off of DeConvNet [19] and [15]s gradient method to produce sharper visualizations.</p>
    <p>Another set of techniques incorporate network activations into their visualizations: Class Activation Mapping</p>
    <p>ar X</p>
    <p>iv :1</p>
    <p>[c s.</p>
    <p>C V</p>
    <p>] 10</p>
    <p>J an</p>
  </div>
  <div class="page">
    <p>Interpretable Deep Learning System</p>
    <p>Interpretable deep learning system (IDLS)  Consisting of DNN (classifier) and interpretation model (interpreter)  Involving humans in the decision-making process  Requiring the adversary to fool both classifier and interpreter</p>
    <p>!3</p>
    <p>Input Classifier Prediction f</p>
    <p>?</p>
    <p>Interpretation Interpreter</p>
  </div>
  <div class="page">
    <p>Interpretability = Security?</p>
    <p>Goal  Understanding the security vulnerabilities of IDLSes</p>
    <p>Approach  Developing attacks that simultaneously fool classifier and interpreter</p>
    <p>!4</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 99% 100% (0.98) (1.0) (0.95) (1.0)</p>
    <p>Table 6. ASR (MC) of ADV2 targeting random patch interpretations.</p>
    <p>fectiveness in terms of deceiving the classifiers, implying that the space of adversarial inputs is sufficiently large to contain ones with targeted interpretations.</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;5/mZiFD+Ihb0/jDnm5GodjBeKGQ=&quot;&gt;AAACBXicbVC7TsMwFHXKq5RXgBEGiwqJqUoKEozlMbAgFYk+pCZUjuu0Vm0nsh2kKsrCwq+wMIAQK//Axt/gph2g5UiWjs+5V/feE8SMKu0431ZhYXFpeaW4Wlpb39jcsrd3mipKJCYNHLFItgOkCKOCNDTVjLRjSRAPGGkFw8ux33ogUtFI3OlRTHyO+oKGFCNtpK6973GkBypMz6+a2X3Vg/lf8vQGxVnXLjsVJwecJ+6UlMEU9a795fUinHAiNGZIqY7rxNpPkdQUM5KVvESRGOEh6pOOoQJxovw0vyKDh0bpwTCS5gkNc/V3R4q4UiMemMp851lvLP7ndRIdnvkpFXGiicCTQWHCoI7gOBLYo5JgzUaGICyp2RXiAZIIaxNcyYTgzp48T5rVintcqd6elGsX0ziKYA8cgCPgglNQA9egDhoAg0fwDF7Bm/VkvVjv1sektGBNe3bBH1ifP2IfmIA=&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;+bXV59hkkJEfEzSI8p3tZABeoyk=&quot;&gt;AAACB3icbVDLSsNAFJ3UV62vqEtBBovgqiRV0GV9LHRXwT6giWUynbRDJ5MwMxFKyM6Nv+LGhSJu/QV3/o2TNAttPTBw5px7ufceL2JUKsv6NkoLi0vLK+XVytr6xuaWub3TlmEsMGnhkIWi6yFJGOWkpahipBsJggKPkY43vsz8zgMRkob8Tk0i4gZoyKlPMVJa6pv7ToDUSPrJ+VU7va87MP+LILnhUazSvlm1alYOOE/sglRBgWbf/HIGIY4DwhVmSMqebUXKTZBQFDOSVpxYkgjhMRqSnqYcBUS6SX5HCg+1MoB+KPTjCubq744EBVJOAk9X5lvPepn4n9eLlX/mJjQ7iXA8HeTHDKoQZqHAARUEKzbRBGFB9a4Qj5BAWOnoKjoEe/bkedKu1+zjWv32pNq4KOIogz1wAI6ADU5BA1yDJmgBDB7BM3gFb8aT8WK8Gx/T0pJR9OyCPzA+fwAxb5mG&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;QJ/hLC8WDWxr5siNC9xPMgd950w=&quot;&gt;AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5JUQZelbtwIFewDmlAm09t26GQSZiZCDcVfceNCEbf+hzv/xkmbhbYeGDiccy/3zAlizpR2nG+rsLK6tr5R3Cxtbe/s7tn7By0VJZJCk0Y8kp2AKOBMQFMzzaETSyBhwKEdjK8zv/0AUrFI3OtJDH5IhoINGCXaSD37yAuJHskwrYNgQ+HhWxJPe3bZqTgz4GXi5qSMcjR69pfXj2gSgtCUE6W6rhNrPyVSM8phWvISBTGhYzKErqGChKD8dJZ+ik+N0seDSJonNJ6pvzdSEio1CQMzmWVVi14m/ud1Ez248lMm4kSDoPNDg4RjHeGsCtxnEqjmE0MIlcxkxXREJKHaFFYyJbiLX14mrWrFPa9U7y7KtXpeRxEdoxN0hlx0iWroBjVQE1H0iJ7RK3qznqwX6936mI8WrHznEP2B9fkDgsqVQQ==&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;G+UhG8bCKcMc8EpoVI3zVEXidmY=&quot;&gt;AAAB/3icbVDLSsNAFJ3UV62vqODGzWARXJWkCrosdaO7CvYBTSiT6W07dDIJMxOhxC78FTcuFHHrb7jzb5y0WWjrgYHDOfcy554g5kxpx/m2Ciura+sbxc3S1vbO7p69f9BSUSIpNGnEI9kJiALOBDQ10xw6sQQSBhzawfg689sPIBWLxL2exOCHZCjYgFGijdSzj7yQ6JEM0zoINhQevhVxoqc9u+xUnBnwMnFzUkY5Gj37y+tHNAlBaMqJUl3XibWfEqkZ5TAteYmCmNAxGULXUEFCUH46yz/Fp0bp40EkzRMaz9TfGykJlZqEgZnM0qpFLxP/87qJHlz5KctOAkHnHw0SjnWEszJwn0mgmk8MIVQykxXTEZGEalNZyZTgLp68TFrVinteqd5dlGv1vI4iOkYn6Ay56BLV0A1qoCai6BE9o1f0Zj1ZL9a79TEfLVj5ziH6A+vzB0wclkc=&lt;/latexit&gt;</p>
    <p>T a rg</p>
    <p>et M</p>
    <p>a p</p>
    <p>&lt;latexit sha1_base64=&quot;wBcq9myxKvgSYxwPBMcNVLrx7Jw=&quot;&gt;AAAB/XicbVDLSsNAFL2pr1pf8bFzM1gEVyWpgi6LbtwIFfqCJpTJdNoOnUzCzESoofgrblwo4tb/cOffOGmz0NYDA4dz7uWeOUHMmdKO820VVlbX1jeKm6Wt7Z3dPXv/oKWiRBLaJBGPZCfAinImaFMzzWknlhSHAaftYHyT+e0HKhWLRENPYuqHeCjYgBGsjdSzj7wQ65EM0waWQ6o9dIfjac8uOxVnBrRM3JyUIUe9Z395/YgkIRWacKxU13Vi7adYakY4nZa8RNEYkzEe0q6hAodU+eks/RSdGqWPBpE0T2g0U39vpDhUahIGZjLLqha9TPzP6yZ6cOWnTMSJpoLMDw0SjnSEsipQn0lKNJ8YgolkJisiIywx0aawkinBXfzyMmlVK+55pXp/Ua5d53UU4RhO4AxcuIQa3EIdmkDgEZ7hFd6sJ+vFerc+5qMFK985hD+wPn8AoeKVVQ==&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
    <p>Figure 11: Visualization of ADV2 targeting random patch interpretations across different interpreters on ResNet.</p>
    <p>We then evaluate the effectiveness of ADV2 in terms of generating the target interpretations. For a given benign input x and a target random patch map mt , ADV2 attempts to generate an adversarial input ct with the interpretation similar to mt . Figure 11 visualizes a set of sample results. Note that in all the cases the ADV2 maps appear visually similar to the target maps, highlighting the attack effectiveness. This effectiveness is further validated in Table 7. Observe that across all the interpreters, an ADV2 map is much more similar to its target map, compared with its benign counterpart.</p>
    <p>GRAD CAM MASK RTS DbL1 0.16 0.50 0.42 0.49 DtL1 0.10 0.04 0.15 0.07</p>
    <p>Table 7. Comparison of ADV2 and target maps (Dt) and that of ADV2 and benign maps (Db), measured by L1 distance.</p>
    <p>Random Class Interpretation  In the second case, for a given input (with ct as the target class), we instantiate its target interpretation with the attribution map of a benign input randomly sampled from another class ct . We particularly enforce ct 6= ct ; in other words, the adversarial input is misclassified into one class but interpreted as another one.</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 100% 100% (0.99) (0.99) (0.99) (1.0)</p>
    <p>Table 8. ASR (MC) of ADV2 with random class interpretations.</p>
    <p>The ASR of ADV2 is summarized in Table 8. Observe that targeting random class interpretations has little influence on the attack effectiveness of deceiving the classifiers. Figure 12 visualizes a set of sample target and ADV2 inputs and their interpretations (DenseNet results in Appendix C4). Note that the target and ADV2 inputs are fairly distinct, but with highly</p>
    <p>similar interpretations. This is quantitatively validated by their L1 measures and IoU scores listed in Figure 13.</p>
    <p>Figure 12: Target and adversarial (ADV2) inputs and their attribution maps on ResNet.</p>
    <p>Figure 13: L1 measures (a) and IoU scores (b) of adversarial maps with respect to benign and target cases on ResNet.</p>
    <p>The experiments above show that it is practical to generate adversarial inputs targeting arbitrary predictions and interpretations. We can therefore conclude:</p>
    <p>Observation 6</p>
    <p>A DNN and its interpreter are often not fully aligned, allowing the adversary to exploit both models simultaneously.</p>
    <p>Q2. Root of Prediction-Interpretation Gap Next we explore the fundamental causes of this prediction</p>
    <p>interpretation gap. We speculate one following possible explanation as: existing interpretation models do not comprehensively capture the dynamics of DNNs, each only describing one aspect of their behavior.</p>
    <p>Specifically, GRAD solely relies on the gradient information; MASK focuses on the input-prediction correspondence while ignoring the internal representations; CAM leverages the deep representations at intermediate layers, but neglecting the input-prediction correspondence; RTS uses the internal representations in an auxiliary encoder and the inputinterpretation correspondence in the training data, which however may deviate from the true behavior of DNNs.</p>
    <p>Intuitively the exclusive focus on one aspect (e.g., inputprediction correspondence) of the DNN behavior results in loose constraints: when performing the attack, the adversary only needs to ensure that benign and adversarial inputs cause DNNs to behave similarly from one specific perspective. We validate this speculation from two observations, low attack</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 99% 100% (0.98) (1.0) (0.95) (1.0)</p>
    <p>Table 6. ASR (MC) of ADV2 targeting random patch interpretations.</p>
    <p>fectiveness in terms of deceiving the classifiers, implying that the space of adversarial inputs is sufficiently large to contain ones with targeted interpretations.</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;5/mZiFD+Ihb0/jDnm5GodjBeKGQ=&quot;&gt;AAACBXicbVC7TsMwFHXKq5RXgBEGiwqJqUoKEozlMbAgFYk+pCZUjuu0Vm0nsh2kKsrCwq+wMIAQK//Axt/gph2g5UiWjs+5V/feE8SMKu0431ZhYXFpeaW4Wlpb39jcsrd3mipKJCYNHLFItgOkCKOCNDTVjLRjSRAPGGkFw8ux33ogUtFI3OlRTHyO+oKGFCNtpK6973GkBypMz6+a2X3Vg/lf8vQGxVnXLjsVJwecJ+6UlMEU9a795fUinHAiNGZIqY7rxNpPkdQUM5KVvESRGOEh6pOOoQJxovw0vyKDh0bpwTCS5gkNc/V3R4q4UiMemMp851lvLP7ndRIdnvkpFXGiicCTQWHCoI7gOBLYo5JgzUaGICyp2RXiAZIIaxNcyYTgzp48T5rVintcqd6elGsX0ziKYA8cgCPgglNQA9egDhoAg0fwDF7Bm/VkvVjv1sektGBNe3bBH1ifP2IfmIA=&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;+bXV59hkkJEfEzSI8p3tZABeoyk=&quot;&gt;AAACB3icbVDLSsNAFJ3UV62vqEtBBovgqiRV0GV9LHRXwT6giWUynbRDJ5MwMxFKyM6Nv+LGhSJu/QV3/o2TNAttPTBw5px7ufceL2JUKsv6NkoLi0vLK+XVytr6xuaWub3TlmEsMGnhkIWi6yFJGOWkpahipBsJggKPkY43vsz8zgMRkob8Tk0i4gZoyKlPMVJa6pv7ToDUSPrJ+VU7va87MP+LILnhUazSvlm1alYOOE/sglRBgWbf/HIGIY4DwhVmSMqebUXKTZBQFDOSVpxYkgjhMRqSnqYcBUS6SX5HCg+1MoB+KPTjCubq744EBVJOAk9X5lvPepn4n9eLlX/mJjQ7iXA8HeTHDKoQZqHAARUEKzbRBGFB9a4Qj5BAWOnoKjoEe/bkedKu1+zjWv32pNq4KOIogz1wAI6ADU5BA1yDJmgBDB7BM3gFb8aT8WK8Gx/T0pJR9OyCPzA+fwAxb5mG&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;QJ/hLC8WDWxr5siNC9xPMgd950w=&quot;&gt;AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5JUQZelbtwIFewDmlAm09t26GQSZiZCDcVfceNCEbf+hzv/xkmbhbYeGDiccy/3zAlizpR2nG+rsLK6tr5R3Cxtbe/s7tn7By0VJZJCk0Y8kp2AKOBMQFMzzaETSyBhwKEdjK8zv/0AUrFI3OtJDH5IhoINGCXaSD37yAuJHskwrYNgQ+HhWxJPe3bZqTgz4GXi5qSMcjR69pfXj2gSgtCUE6W6rhNrPyVSM8phWvISBTGhYzKErqGChKD8dJZ+ik+N0seDSJonNJ6pvzdSEio1CQMzmWVVi14m/ud1Ez248lMm4kSDoPNDg4RjHeGsCtxnEqjmE0MIlcxkxXREJKHaFFYyJbiLX14mrWrFPa9U7y7KtXpeRxEdoxN0hlx0iWroBjVQE1H0iJ7RK3qznqwX6936mI8WrHznEP2B9fkDgsqVQQ==&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;G+UhG8bCKcMc8EpoVI3zVEXidmY=&quot;&gt;AAAB/3icbVDLSsNAFJ3UV62vqODGzWARXJWkCrosdaO7CvYBTSiT6W07dDIJMxOhxC78FTcuFHHrb7jzb5y0WWjrgYHDOfcy554g5kxpx/m2Ciura+sbxc3S1vbO7p69f9BSUSIpNGnEI9kJiALOBDQ10xw6sQQSBhzawfg689sPIBWLxL2exOCHZCjYgFGijdSzj7yQ6JEM0zoINhQevhVxoqc9u+xUnBnwMnFzUkY5Gj37y+tHNAlBaMqJUl3XibWfEqkZ5TAteYmCmNAxGULXUEFCUH46yz/Fp0bp40EkzRMaz9TfGykJlZqEgZnM0qpFLxP/87qJHlz5KctOAkHnHw0SjnWEszJwn0mgmk8MIVQykxXTEZGEalNZyZTgLp68TFrVinteqd5dlGv1vI4iOkYn6Ay56BLV0A1qoCai6BE9o1f0Zj1ZL9a79TEfLVj5ziH6A+vzB0wclkc=&lt;/latexit&gt;</p>
    <p>T a rg</p>
    <p>et M</p>
    <p>a p</p>
    <p>&lt;latexit sha1_base64=&quot;wBcq9myxKvgSYxwPBMcNVLrx7Jw=&quot;&gt;AAAB/XicbVDLSsNAFL2pr1pf8bFzM1gEVyWpgi6LbtwIFfqCJpTJdNoOnUzCzESoofgrblwo4tb/cOffOGmz0NYDA4dz7uWeOUHMmdKO820VVlbX1jeKm6Wt7Z3dPXv/oKWiRBLaJBGPZCfAinImaFMzzWknlhSHAaftYHyT+e0HKhWLRENPYuqHeCjYgBGsjdSzj7wQ65EM0waWQ6o9dIfjac8uOxVnBrRM3JyUIUe9Z395/YgkIRWacKxU13Vi7adYakY4nZa8RNEYkzEe0q6hAodU+eks/RSdGqWPBpE0T2g0U39vpDhUahIGZjLLqha9TPzP6yZ6cOWnTMSJpoLMDw0SjnSEsipQn0lKNJ8YgolkJisiIywx0aawkinBXfzyMmlVK+55pXp/Ua5d53UU4RhO4AxcuIQa3EIdmkDgEZ7hFd6sJ+vFerc+5qMFK985hD+wPn8AoeKVVQ==&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
    <p>Figure 11: Visualization of ADV2 targeting random patch interpretations across different interpreters on ResNet.</p>
    <p>We then evaluate the effectiveness of ADV2 in terms of generating the target interpretations. For a given benign input x and a target random patch map mt , ADV2 attempts to generate an adversarial input ct with the interpretation similar to mt . Figure 11 visualizes a set of sample results. Note that in all the cases the ADV2 maps appear visually similar to the target maps, highlighting the attack effectiveness. This effectiveness is further validated in Table 7. Observe that across all the interpreters, an ADV2 map is much more similar to its target map, compared with its benign counterpart.</p>
    <p>GRAD CAM MASK RTS DbL1 0.16 0.50 0.42 0.49 DtL1 0.10 0.04 0.15 0.07</p>
    <p>Table 7. Comparison of ADV2 and target maps (Dt) and that of ADV2 and benign maps (Db), measured by L1 distance.</p>
    <p>Random Class Interpretation  In the second case, for a given input (with ct as the target class), we instantiate its target interpretation with the attribution map of a benign input randomly sampled from another class ct . We particularly enforce ct 6= ct ; in other words, the adversarial input is misclassified into one class but interpreted as another one.</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 100% 100% (0.99) (0.99) (0.99) (1.0)</p>
    <p>Table 8. ASR (MC) of ADV2 with random class interpretations.</p>
    <p>The ASR of ADV2 is summarized in Table 8. Observe that targeting random class interpretations has little influence on the attack effectiveness of deceiving the classifiers. Figure 12 visualizes a set of sample target and ADV2 inputs and their interpretations (DenseNet results in Appendix C4). Note that the target and ADV2 inputs are fairly distinct, but with highly</p>
    <p>similar interpretations. This is quantitatively validated by their L1 measures and IoU scores listed in Figure 13.</p>
    <p>Figure 12: Target and adversarial (ADV2) inputs and their attribution maps on ResNet.</p>
    <p>Figure 13: L1 measures (a) and IoU scores (b) of adversarial maps with respect to benign and target cases on ResNet.</p>
    <p>The experiments above show that it is practical to generate adversarial inputs targeting arbitrary predictions and interpretations. We can therefore conclude:</p>
    <p>Observation 6</p>
    <p>A DNN and its interpreter are often not fully aligned, allowing the adversary to exploit both models simultaneously.</p>
    <p>Q2. Root of Prediction-Interpretation Gap Next we explore the fundamental causes of this prediction</p>
    <p>interpretation gap. We speculate one following possible explanation as: existing interpretation models do not comprehensively capture the dynamics of DNNs, each only describing one aspect of their behavior.</p>
    <p>Specifically, GRAD solely relies on the gradient information; MASK focuses on the input-prediction correspondence while ignoring the internal representations; CAM leverages the deep representations at intermediate layers, but neglecting the input-prediction correspondence; RTS uses the internal representations in an auxiliary encoder and the inputinterpretation correspondence in the training data, which however may deviate from the true behavior of DNNs.</p>
    <p>Intuitively the exclusive focus on one aspect (e.g., inputprediction correspondence) of the DNN behavior results in loose constraints: when performing the attack, the adversary only needs to ensure that benign and adversarial inputs cause DNNs to behave similarly from one specific perspective. We validate this speculation from two observations, low attack</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 99% 100% (0.98) (1.0) (0.95) (1.0)</p>
    <p>Table 6. ASR (MC) of ADV2 targeting random patch interpretations.</p>
    <p>fectiveness in terms of deceiving the classifiers, implying that the space of adversarial inputs is sufficiently large to contain ones with targeted interpretations.</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;5/mZiFD+Ihb0/jDnm5GodjBeKGQ=&quot;&gt;AAACBXicbVC7TsMwFHXKq5RXgBEGiwqJqUoKEozlMbAgFYk+pCZUjuu0Vm0nsh2kKsrCwq+wMIAQK//Axt/gph2g5UiWjs+5V/feE8SMKu0431ZhYXFpeaW4Wlpb39jcsrd3mipKJCYNHLFItgOkCKOCNDTVjLRjSRAPGGkFw8ux33ogUtFI3OlRTHyO+oKGFCNtpK6973GkBypMz6+a2X3Vg/lf8vQGxVnXLjsVJwecJ+6UlMEU9a795fUinHAiNGZIqY7rxNpPkdQUM5KVvESRGOEh6pOOoQJxovw0vyKDh0bpwTCS5gkNc/V3R4q4UiMemMp851lvLP7ndRIdnvkpFXGiicCTQWHCoI7gOBLYo5JgzUaGICyp2RXiAZIIaxNcyYTgzp48T5rVintcqd6elGsX0ziKYA8cgCPgglNQA9egDhoAg0fwDF7Bm/VkvVjv1sektGBNe3bBH1ifP2IfmIA=&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;+bXV59hkkJEfEzSI8p3tZABeoyk=&quot;&gt;AAACB3icbVDLSsNAFJ3UV62vqEtBBovgqiRV0GV9LHRXwT6giWUynbRDJ5MwMxFKyM6Nv+LGhSJu/QV3/o2TNAttPTBw5px7ufceL2JUKsv6NkoLi0vLK+XVytr6xuaWub3TlmEsMGnhkIWi6yFJGOWkpahipBsJggKPkY43vsz8zgMRkob8Tk0i4gZoyKlPMVJa6pv7ToDUSPrJ+VU7va87MP+LILnhUazSvlm1alYOOE/sglRBgWbf/HIGIY4DwhVmSMqebUXKTZBQFDOSVpxYkgjhMRqSnqYcBUS6SX5HCg+1MoB+KPTjCubq744EBVJOAk9X5lvPepn4n9eLlX/mJjQ7iXA8HeTHDKoQZqHAARUEKzbRBGFB9a4Qj5BAWOnoKjoEe/bkedKu1+zjWv32pNq4KOIogz1wAI6ADU5BA1yDJmgBDB7BM3gFb8aT8WK8Gx/T0pJR9OyCPzA+fwAxb5mG&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;QJ/hLC8WDWxr5siNC9xPMgd950w=&quot;&gt;AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5JUQZelbtwIFewDmlAm09t26GQSZiZCDcVfceNCEbf+hzv/xkmbhbYeGDiccy/3zAlizpR2nG+rsLK6tr5R3Cxtbe/s7tn7By0VJZJCk0Y8kp2AKOBMQFMzzaETSyBhwKEdjK8zv/0AUrFI3OtJDH5IhoINGCXaSD37yAuJHskwrYNgQ+HhWxJPe3bZqTgz4GXi5qSMcjR69pfXj2gSgtCUE6W6rhNrPyVSM8phWvISBTGhYzKErqGChKD8dJZ+ik+N0seDSJonNJ6pvzdSEio1CQMzmWVVi14m/ud1Ez248lMm4kSDoPNDg4RjHeGsCtxnEqjmE0MIlcxkxXREJKHaFFYyJbiLX14mrWrFPa9U7y7KtXpeRxEdoxN0hlx0iWroBjVQE1H0iJ7RK3qznqwX6936mI8WrHznEP2B9fkDgsqVQQ==&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;G+UhG8bCKcMc8EpoVI3zVEXidmY=&quot;&gt;AAAB/3icbVDLSsNAFJ3UV62vqODGzWARXJWkCrosdaO7CvYBTSiT6W07dDIJMxOhxC78FTcuFHHrb7jzb5y0WWjrgYHDOfcy554g5kxpx/m2Ciura+sbxc3S1vbO7p69f9BSUSIpNGnEI9kJiALOBDQ10xw6sQQSBhzawfg689sPIBWLxL2exOCHZCjYgFGijdSzj7yQ6JEM0zoINhQevhVxoqc9u+xUnBnwMnFzUkY5Gj37y+tHNAlBaMqJUl3XibWfEqkZ5TAteYmCmNAxGULXUEFCUH46yz/Fp0bp40EkzRMaz9TfGykJlZqEgZnM0qpFLxP/87qJHlz5KctOAkHnHw0SjnWEszJwn0mgmk8MIVQykxXTEZGEalNZyZTgLp68TFrVinteqd5dlGv1vI4iOkYn6Ay56BLV0A1qoCai6BE9o1f0Zj1ZL9a79TEfLVj5ziH6A+vzB0wclkc=&lt;/latexit&gt;</p>
    <p>T a rg</p>
    <p>et M</p>
    <p>a p</p>
    <p>&lt;latexit sha1_base64=&quot;wBcq9myxKvgSYxwPBMcNVLrx7Jw=&quot;&gt;AAAB/XicbVDLSsNAFL2pr1pf8bFzM1gEVyWpgi6LbtwIFfqCJpTJdNoOnUzCzESoofgrblwo4tb/cOffOGmz0NYDA4dz7uWeOUHMmdKO820VVlbX1jeKm6Wt7Z3dPXv/oKWiRBLaJBGPZCfAinImaFMzzWknlhSHAaftYHyT+e0HKhWLRENPYuqHeCjYgBGsjdSzj7wQ65EM0waWQ6o9dIfjac8uOxVnBrRM3JyUIUe9Z395/YgkIRWacKxU13Vi7adYakY4nZa8RNEYkzEe0q6hAodU+eks/RSdGqWPBpE0T2g0U39vpDhUahIGZjLLqha9TPzP6yZ6cOWnTMSJpoLMDw0SjnSEsipQn0lKNJ8YgolkJisiIywx0aawkinBXfzyMmlVK+55pXp/Ua5d53UU4RhO4AxcuIQa3EIdmkDgEZ7hFd6sJ+vFerc+5qMFK985hD+wPn8AoeKVVQ==&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
    <p>Figure 11: Visualization of ADV2 targeting random patch interpretations across different interpreters on ResNet.</p>
    <p>We then evaluate the effectiveness of ADV2 in terms of generating the target interpretations. For a given benign input x and a target random patch map mt , ADV2 attempts to generate an adversarial input ct with the interpretation similar to mt . Figure 11 visualizes a set of sample results. Note that in all the cases the ADV2 maps appear visually similar to the target maps, highlighting the attack effectiveness. This effectiveness is further validated in Table 7. Observe that across all the interpreters, an ADV2 map is much more similar to its target map, compared with its benign counterpart.</p>
    <p>GRAD CAM MASK RTS DbL1 0.16 0.50 0.42 0.49 DtL1 0.10 0.04 0.15 0.07</p>
    <p>Table 7. Comparison of ADV2 and target maps (Dt) and that of ADV2 and benign maps (Db), measured by L1 distance.</p>
    <p>Random Class Interpretation  In the second case, for a given input (with ct as the target class), we instantiate its target interpretation with the attribution map of a benign input randomly sampled from another class ct . We particularly enforce ct 6= ct ; in other words, the adversarial input is misclassified into one class but interpreted as another one.</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 100% 100% (0.99) (0.99) (0.99) (1.0)</p>
    <p>Table 8. ASR (MC) of ADV2 with random class interpretations.</p>
    <p>The ASR of ADV2 is summarized in Table 8. Observe that targeting random class interpretations has little influence on the attack effectiveness of deceiving the classifiers. Figure 12 visualizes a set of sample target and ADV2 inputs and their interpretations (DenseNet results in Appendix C4). Note that the target and ADV2 inputs are fairly distinct, but with highly</p>
    <p>similar interpretations. This is quantitatively validated by their L1 measures and IoU scores listed in Figure 13.</p>
    <p>Figure 12: Target and adversarial (ADV2) inputs and their attribution maps on ResNet.</p>
    <p>Figure 13: L1 measures (a) and IoU scores (b) of adversarial maps with respect to benign and target cases on ResNet.</p>
    <p>The experiments above show that it is practical to generate adversarial inputs targeting arbitrary predictions and interpretations. We can therefore conclude:</p>
    <p>Observation 6</p>
    <p>A DNN and its interpreter are often not fully aligned, allowing the adversary to exploit both models simultaneously.</p>
    <p>Q2. Root of Prediction-Interpretation Gap Next we explore the fundamental causes of this prediction</p>
    <p>interpretation gap. We speculate one following possible explanation as: existing interpretation models do not comprehensively capture the dynamics of DNNs, each only describing one aspect of their behavior.</p>
    <p>Specifically, GRAD solely relies on the gradient information; MASK focuses on the input-prediction correspondence while ignoring the internal representations; CAM leverages the deep representations at intermediate layers, but neglecting the input-prediction correspondence; RTS uses the internal representations in an auxiliary encoder and the inputinterpretation correspondence in the training data, which however may deviate from the true behavior of DNNs.</p>
    <p>Intuitively the exclusive focus on one aspect (e.g., inputprediction correspondence) of the DNN behavior results in loose constraints: when performing the attack, the adversary only needs to ensure that benign and adversarial inputs cause DNNs to behave similarly from one specific perspective. We validate this speculation from two observations, low attack</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 99% 100% (0.98) (1.0) (0.95) (1.0)</p>
    <p>Table 6. ASR (MC) of ADV2 targeting random patch interpretations.</p>
    <p>fectiveness in terms of deceiving the classifiers, implying that the space of adversarial inputs is sufficiently large to contain ones with targeted interpretations.</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;5/mZiFD+Ihb0/jDnm5GodjBeKGQ=&quot;&gt;AAACBXicbVC7TsMwFHXKq5RXgBEGiwqJqUoKEozlMbAgFYk+pCZUjuu0Vm0nsh2kKsrCwq+wMIAQK//Axt/gph2g5UiWjs+5V/feE8SMKu0431ZhYXFpeaW4Wlpb39jcsrd3mipKJCYNHLFItgOkCKOCNDTVjLRjSRAPGGkFw8ux33ogUtFI3OlRTHyO+oKGFCNtpK6973GkBypMz6+a2X3Vg/lf8vQGxVnXLjsVJwecJ+6UlMEU9a795fUinHAiNGZIqY7rxNpPkdQUM5KVvESRGOEh6pOOoQJxovw0vyKDh0bpwTCS5gkNc/V3R4q4UiMemMp851lvLP7ndRIdnvkpFXGiicCTQWHCoI7gOBLYo5JgzUaGICyp2RXiAZIIaxNcyYTgzp48T5rVintcqd6elGsX0ziKYA8cgCPgglNQA9egDhoAg0fwDF7Bm/VkvVjv1sektGBNe3bBH1ifP2IfmIA=&lt;/latexit&gt;</p>
    <p>A D</p>
    <p>V 2</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;+bXV59hkkJEfEzSI8p3tZABeoyk=&quot;&gt;AAACB3icbVDLSsNAFJ3UV62vqEtBBovgqiRV0GV9LHRXwT6giWUynbRDJ5MwMxFKyM6Nv+LGhSJu/QV3/o2TNAttPTBw5px7ufceL2JUKsv6NkoLi0vLK+XVytr6xuaWub3TlmEsMGnhkIWi6yFJGOWkpahipBsJggKPkY43vsz8zgMRkob8Tk0i4gZoyKlPMVJa6pv7ToDUSPrJ+VU7va87MP+LILnhUazSvlm1alYOOE/sglRBgWbf/HIGIY4DwhVmSMqebUXKTZBQFDOSVpxYkgjhMRqSnqYcBUS6SX5HCg+1MoB+KPTjCubq744EBVJOAk9X5lvPepn4n9eLlX/mJjQ7iXA8HeTHDKoQZqHAARUEKzbRBGFB9a4Qj5BAWOnoKjoEe/bkedKu1+zjWv32pNq4KOIogz1wAI6ADU5BA1yDJmgBDB7BM3gFb8aT8WK8Gx/T0pJR9OyCPzA+fwAxb5mG&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;QJ/hLC8WDWxr5siNC9xPMgd950w=&quot;&gt;AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5JUQZelbtwIFewDmlAm09t26GQSZiZCDcVfceNCEbf+hzv/xkmbhbYeGDiccy/3zAlizpR2nG+rsLK6tr5R3Cxtbe/s7tn7By0VJZJCk0Y8kp2AKOBMQFMzzaETSyBhwKEdjK8zv/0AUrFI3OtJDH5IhoINGCXaSD37yAuJHskwrYNgQ+HhWxJPe3bZqTgz4GXi5qSMcjR69pfXj2gSgtCUE6W6rhNrPyVSM8phWvISBTGhYzKErqGChKD8dJZ+ik+N0seDSJonNJ6pvzdSEio1CQMzmWVVi14m/ud1Ez248lMm4kSDoPNDg4RjHeGsCtxnEqjmE0MIlcxkxXREJKHaFFYyJbiLX14mrWrFPa9U7y7KtXpeRxEdoxN0hlx0iWroBjVQE1H0iJ7RK3qznqwX6936mI8WrHznEP2B9fkDgsqVQQ==&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>In p u t</p>
    <p>&lt;latexit sha1_base64=&quot;G+UhG8bCKcMc8EpoVI3zVEXidmY=&quot;&gt;AAAB/3icbVDLSsNAFJ3UV62vqODGzWARXJWkCrosdaO7CvYBTSiT6W07dDIJMxOhxC78FTcuFHHrb7jzb5y0WWjrgYHDOfcy554g5kxpx/m2Ciura+sbxc3S1vbO7p69f9BSUSIpNGnEI9kJiALOBDQ10xw6sQQSBhzawfg689sPIBWLxL2exOCHZCjYgFGijdSzj7yQ6JEM0zoINhQevhVxoqc9u+xUnBnwMnFzUkY5Gj37y+tHNAlBaMqJUl3XibWfEqkZ5TAteYmCmNAxGULXUEFCUH46yz/Fp0bp40EkzRMaz9TfGykJlZqEgZnM0qpFLxP/87qJHlz5KctOAkHnHw0SjnWEszJwn0mgmk8MIVQykxXTEZGEalNZyZTgLp68TFrVinteqd5dlGv1vI4iOkYn6Ay56BLV0A1qoCai6BE9o1f0Zj1ZL9a79TEfLVj5ziH6A+vzB0wclkc=&lt;/latexit&gt;</p>
    <p>T a rg</p>
    <p>et M</p>
    <p>a p</p>
    <p>&lt;latexit sha1_base64=&quot;wBcq9myxKvgSYxwPBMcNVLrx7Jw=&quot;&gt;AAAB/XicbVDLSsNAFL2pr1pf8bFzM1gEVyWpgi6LbtwIFfqCJpTJdNoOnUzCzESoofgrblwo4tb/cOffOGmz0NYDA4dz7uWeOUHMmdKO820VVlbX1jeKm6Wt7Z3dPXv/oKWiRBLaJBGPZCfAinImaFMzzWknlhSHAaftYHyT+e0HKhWLRENPYuqHeCjYgBGsjdSzj7wQ65EM0waWQ6o9dIfjac8uOxVnBrRM3JyUIUe9Z395/YgkIRWacKxU13Vi7adYakY4nZa8RNEYkzEe0q6hAodU+eks/RSdGqWPBpE0T2g0U39vpDhUahIGZjLLqha9TPzP6yZ6cOWnTMSJpoLMDw0SjnSEsipQn0lKNJ8YgolkJisiIywx0aawkinBXfzyMmlVK+55pXp/Ua5d53UU4RhO4AxcuIQa3EIdmkDgEZ7hFd6sJ+vFerc+5qMFK985hD+wPn8AoeKVVQ==&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
    <p>Figure 11: Visualization of ADV2 targeting random patch interpretations across different interpreters on ResNet.</p>
    <p>We then evaluate the effectiveness of ADV2 in terms of generating the target interpretations. For a given benign input x and a target random patch map mt , ADV2 attempts to generate an adversarial input ct with the interpretation similar to mt . Figure 11 visualizes a set of sample results. Note that in all the cases the ADV2 maps appear visually similar to the target maps, highlighting the attack effectiveness. This effectiveness is further validated in Table 7. Observe that across all the interpreters, an ADV2 map is much more similar to its target map, compared with its benign counterpart.</p>
    <p>GRAD CAM MASK RTS DbL1 0.16 0.50 0.42 0.49 DtL1 0.10 0.04 0.15 0.07</p>
    <p>Table 7. Comparison of ADV2 and target maps (Dt) and that of ADV2 and benign maps (Db), measured by L1 distance.</p>
    <p>Random Class Interpretation  In the second case, for a given input (with ct as the target class), we instantiate its target interpretation with the attribution map of a benign input randomly sampled from another class ct . We particularly enforce ct 6= ct ; in other words, the adversarial input is misclassified into one class but interpreted as another one.</p>
    <p>GRAD CAM MASK RTS</p>
    <p>ADV2 100% 100% 100% 100% (0.99) (0.99) (0.99) (1.0)</p>
    <p>Table 8. ASR (MC) of ADV2 with random class interpretations.</p>
    <p>The ASR of ADV2 is summarized in Table 8. Observe that targeting random class interpretations has little influence on the attack effectiveness of deceiving the classifiers. Figure 12 visualizes a set of sample target and ADV2 inputs and their interpretations (DenseNet results in Appendix C4). Note that the target and ADV2 inputs are fairly distinct, but with highly</p>
    <p>similar interpretations. This is quantitatively validated by their L1 measures and IoU scores listed in Figure 13.</p>
    <p>Figure 12: Target and adversarial (ADV2) inputs and their attribution maps on ResNet.</p>
    <p>Figure 13: L1 measures (a) and IoU scores (b) of adversarial maps with respect to benign and target cases on ResNet.</p>
    <p>The experiments above show that it is practical to generate adversarial inputs targeting arbitrary predictions and interpretations. We can therefore conclude:</p>
    <p>Observation 6</p>
    <p>A DNN and its interpreter are often not fully aligned, allowing the adversary to exploit both models simultaneously.</p>
    <p>Q2. Root of Prediction-Interpretation Gap Next we explore the fundamental causes of this prediction</p>
    <p>interpretation gap. We speculate one following possible explanation as: existing interpretation models do not comprehensively capture the dynamics of DNNs, each only describing one aspect of their behavior.</p>
    <p>Specifically, GRAD solely relies on the gradient information; MASK focuses on the input-prediction correspondence while ignoring the internal representations; CAM leverages the deep representations at intermediate layers, but neglecting the input-prediction correspondence; RTS uses the internal representations in an auxiliary encoder and the inputinterpretation correspondence in the training data, which however may deviate from the true behavior of DNNs.</p>
    <p>Intuitively the exclusive focus on one aspect (e.g., inputprediction correspondence) of the DNN behavior results in loose constraints: when performing the attack, the adversary only needs to ensure that benign and adversarial inputs cause DNNs to behave similarly from one specific perspective. We validate this speculation from two observations, low attack</p>
    <p>Benign Input</p>
    <p>f</p>
    <p>&lt;latexit sha1_base64=&quot;pGN8RfM0op437oNygVoOp321Iy4=&quot;&gt;AAAB+nicbZBNS8NAEIYn9avGr6pHL4tF8FQSEfRY9OKxBfsBbSib7bRdupuE3Y1QYn+BV/0D3sSrf8a7P8Rtm4NtfWHg4Z0ZZnjDRHBtPO/bKWxsbm3vFHfdvf2Dw6PS8UlTx6li2GCxiFU7pBoFj7BhuBHYThRSGQpsheP7Wb/1hErzOHo0kwQDSYcRH3BGjbXqw16p7FW8ucg6+DmUIVetV/rp9mOWSowME1Trju8lJsioMpwJnLrdVGNC2ZgOsWMxohJ1kM0fnZIL6/TJIFa2IkPm7t+NjEqtJzK0k5KakV7tzcx/e6Gcuu7ybTO4DTIeJanBiC1OD1JBTExmOZA+V8iMmFigTHH7PWEjqigzNi3XxuKvhrAOzauKb7l+Xa7e5QEV4QzO4RJ8uIEqPEANGsAA4QVe4c15dt6dD+dzMVpw8p1TWJLz9QvjFJOx&lt;/latexit&gt;</p>
    <p>Classifier Prediction</p>
    <p>fish</p>
    <p>Interpreter Interpretation</p>
    <p>Adversarial Input</p>
    <p>f</p>
    <p>&lt;latexit sha1_base64=&quot;pGN8RfM0op437oNygVoOp321Iy4=&quot;&gt;AAAB+nicbZBNS8NAEIYn9avGr6pHL4tF8FQSEfRY9OKxBfsBbSib7bRdupuE3Y1QYn+BV/0D3sSrf8a7P8Rtm4NtfWHg4Z0ZZnjDRHBtPO/bKWxsbm3vFHfdvf2Dw6PS8UlTx6li2GCxiFU7pBoFj7BhuBHYThRSGQpsheP7Wb/1hErzOHo0kwQDSYcRH3BGjbXqw16p7FW8ucg6+DmUIVetV/rp9mOWSowME1Trju8lJsioMpwJnLrdVGNC2ZgOsWMxohJ1kM0fnZIL6/TJIFa2IkPm7t+NjEqtJzK0k5KakV7tzcx/e6Gcuu7ybTO4DTIeJanBiC1OD1JBTExmOZA+V8iMmFigTHH7PWEjqigzNi3XxuKvhrAOzauKb7l+Xa7e5QEV4QzO4RJ8uIEqPEANGsAA4QVe4c15dt6dD+dzMVpw8p1TWJLz9QvjFJOx&lt;/latexit&gt;</p>
    <p>cat</p>
    <p>Perturbation</p>
  </div>
  <div class="page">
    <p>ADV2 Attack</p>
    <p>Overall formulation 1. Triggering target prediction  and target interpretation</p>
    <p>Regularized optimization</p>
    <p>ct mt (x, x)</p>
    <p>!5</p>
    <p>Consider PGD, a universal first-order adversarial attack, as a concrete case. At a high level, PGD implements a sequence of project gradient descent on the loss function:</p>
    <p>x(i+1) = PBe(x)  x(i)  a sgn</p>
    <p>x`prd</p>
    <p>f  x(i)</p>
    <p>,ct</p>
    <p>(1)</p>
    <p>where P is the projection operator, a represents the learning rate, the loss function `prd measures the difference of the model prediction f (x) and the class ct targeted by the adversary (e.g., cross entropy), and x(0) is initialized as x.</p>
    <p>Threat Model  Following the line of work on adversarial attacks [9, 19, 35, 56], we assume in this paper a white-box setting: the adversary has complete access to the classifier f and the interpreter g, including their architectures and parameters. This is a conservative and realistic assumption. Prior work has shown that it is possible to train a surrogate model f 0 given black-box access to a target DNN f [41]; given that the interpreter is often derived directly from the classifier (details in  3), the adversary may then train a substitution interpreter g0 based on f 0. We consider investigating such black-box attacks as our ongoing work.</p>
    <p>of security by involving human in the decision process [13, 17, 20, 57]; this belief has yet to be rigorously tested. We bridge this gap by presenting ADV2, a new class of attacks that deceive target DNNs and their interpreters simultaneously. Below we first give an overview of ADV2 and then detail its instantiations against four major types of interpreters.</p>
    <p>interpreter g. Specifically, ADV2 generates an adversarial input x by modifying a benign input x such that</p>
    <p>(i) x is misclassified by f to a target class ct , f (x) = ct ;</p>
    <p>(ii) x triggers g to generate a target attribution map mt , g(x; f ) = mt ;</p>
    <p>(iii) The difference between x and x, D(x,x), is imperceptible;</p>
    <p>where the distance function D depends on the concrete modification: for pixel perturbation (e.g., [35]), it is instantiated as Lp norm, while for spatial transformation (e.g., [60]), it is defined as the overall spatial distortion.</p>
    <p>In other words, the goal is to find sufficiently small perturbation to the benign input that leads to the prediction and interpretation desired by the adversary.</p>
    <p>At a high level, we formulate ADV2 using the following optimization framework:</p>
    <p>min x</p>
    <p>D(x,x) s.t.</p>
    <p>f (x) = ct g(x; f ) = mt</p>
    <p>(2)</p>
    <p>where the constraints ensure that (i) the adversarial input is misclassified as ct and (ii) it triggers g to generate the target attribution map mt .</p>
    <p>As the constraints of f (x) = ct and g(x; f ) = mt are highly non-linear for practical DNNs, we reformulate Eqn (2) in a form more suited for optimization:</p>
    <p>min x</p>
    <p>`prd( f (x),ct ) + l`int (g(x; f ),mt )</p>
    <p>s.t. D(x,x)  e (3)</p>
    <p>where the prediction loss `prd is the same as in Eqn (1), the interpretation loss `int measures the difference of adversarial map g(x; f ) and target map mt , and the hyper-parameter l balances the two factors. Below we use `adv(x) to denote the overall loss function defined in Eqn (3).</p>
    <p>We construct the solver of Eqn (3) upon an adversarial attack framework. While it is flexible to choose the concrete framework, below we primarily use PGD [35] as the reference and discuss the construction of ADV2 upon alternative frameworks (e.g., spatial transformation [60]) in  4.</p>
    <p>Under this setting, we define `prd( f (x),ct ) =  log( fct (x)) (i.e., the negative log likelihood of x with respect to the class ct ), D(x,x) = kx  xk, and `int(g(x; f ),mt ) = kg(x; f )  mt k22. In general, ADV</p>
    <p>x(i+1) = PBe(x)  x(i)  a sgn</p>
    <p>x`adv</p>
    <p>x(i)</p>
    <p>(4)</p>
    <p>However, directly applying Eqn (4) is often found ineffective, due to the unique characteristics of individual interpreters. In the following, we detail the instantiations of ADV2 against the back-propagation-, representation-, model-, and perturbation-guided interpreters, respectively.</p>
    <p>ants) of the model prediction with respect to a given input to derive the importance of each input feature. The hypothesis is that larger gradient magnitude indicates higher relevance of the feature to the prediction. We consider gradient saliency (GRAD) [50] as a representative of this class.</p>
    <p>Intuitively, GRAD considers a linear approximation of the model prediction (probability) fc(x) for a given input x and a given class c, and derives the attribution map m as:</p>
    <p>m =   fc(x)</p>
    <p>x</p>
    <p>(5)</p>
    <p>To attack GRAD-based IDLSes, we may search for x using a sequence of gradient descent updates as defined in Eqn (4). However, according to Eqn (5), computing the gradient of the attribution map g(x; f ) amounts to computing the Hessian matrix of fc(x), which is all-zero for DNNs with ReLU activation functions. Thus the gradient of the interpretation loss `int provides little information for updating x, which makes directly applying Eqn (4) ineffective.</p>
    <p>Consider PGD, a universal first-order adversarial attack, as a concrete case. At a high level, PGD implements a sequence of project gradient descent on the loss function:</p>
    <p>x(i+1) = PBe(x)  x(i)  a sgn</p>
    <p>x`prd</p>
    <p>f  x(i)</p>
    <p>,ct</p>
    <p>(1)</p>
    <p>where P is the projection operator, a represents the learning rate, the loss function `prd measures the difference of the model prediction f (x) and the class ct targeted by the adversary (e.g., cross entropy), and x(0) is initialized as x.</p>
    <p>Threat Model  Following the line of work on adversarial attacks [9, 19, 35, 56], we assume in this paper a white-box setting: the adversary has complete access to the classifier f and the interpreter g, including their architectures and parameters. This is a conservative and realistic assumption. Prior work has shown that it is possible to train a surrogate model f 0 given black-box access to a target DNN f [41]; given that the interpreter is often derived directly from the classifier (details in  3), the adversary may then train a substitution interpreter g0 based on f 0. We consider investigating such black-box attacks as our ongoing work.</p>
    <p>of security by involving human in the decision process [13, 17, 20, 57]; this belief has yet to be rigorously tested. We bridge this gap by presenting ADV2, a new class of attacks that deceive target DNNs and their interpreters simultaneously. Below we first give an overview of ADV2 and then detail its instantiations against four major types of interpreters.</p>
    <p>interpreter g. Specifically, ADV2 generates an adversarial input x by modifying a benign input x such that</p>
    <p>(i) x is misclassified by f to a target class ct , f (x) = ct ;</p>
    <p>(ii) x triggers g to generate a target attribution map mt , g(x; f ) = mt ;</p>
    <p>(iii) The difference between x and x, D(x,x), is imperceptible;</p>
    <p>where the distance function D depends on the concrete modification: for pixel perturbation (e.g., [35]), it is instantiated as Lp norm, while for spatial transformation (e.g., [60]), it is defined as the overall spatial distortion.</p>
    <p>In other words, the goal is to find sufficiently small perturbation to the benign input that leads to the prediction and interpretation desired by the adversary.</p>
    <p>At a high level, we formulate ADV2 using the following optimization framework:</p>
    <p>min x</p>
    <p>D(x,x) s.t.</p>
    <p>f (x) = ct g(x; f ) = mt</p>
    <p>(2)</p>
    <p>where the constraints ensure that (i) the adversarial input is misclassified as ct and (ii) it triggers g to generate the target attribution map mt .</p>
    <p>As the constraints of f (x) = ct and g(x; f ) = mt are highly non-linear for practical DNNs, we reformulate Eqn (2) in a form more suited for optimization:</p>
    <p>min x</p>
    <p>`prd( f (x),ct ) + l`int (g(x; f ),mt )</p>
    <p>s.t. D(x,x)  e (3)</p>
    <p>where the prediction loss `prd is the same as in Eqn (1), the interpretation loss `int measures the difference of adversarial map g(x; f ) and target map mt , and the hyper-parameter l balances the two factors. Below we use `adv(x) to denote the overall loss function defined in Eqn (3).</p>
    <p>We construct the solver of Eqn (3) upon an adversarial attack framework. While it is flexible to choose the concrete framework, below we primarily use PGD [35] as the reference and discuss the construction of ADV2 upon alternative frameworks (e.g., spatial transformation [60]) in  4.</p>
    <p>Under this setting, we define `prd( f (x),ct ) =  log( fct (x)) (i.e., the negative log likelihood of x with respect to the class ct ), D(x,x) = kx  xk, and `int(g(x; f ),mt ) = kg(x; f )  mt k22. In general, ADV</p>
    <p>x(i+1) = PBe(x)  x(i)  a sgn</p>
    <p>x`adv</p>
    <p>x(i)</p>
    <p>(4)</p>
    <p>However, directly applying Eqn (4) is often found ineffective, due to the unique characteristics of individual interpreters. In the following, we detail the instantiations of ADV2 against the back-propagation-, representation-, model-, and perturbation-guided interpreters, respectively.</p>
    <p>ants) of the model prediction with respect to a given input to derive the importance of each input feature. The hypothesis is that larger gradient magnitude indicates higher relevance of the feature to the prediction. We consider gradient saliency (GRAD) [50] as a representative of this class.</p>
    <p>Intuitively, GRAD considers a linear approximation of the model prediction (probability) fc(x) for a given input x and a given class c, and derives the attribution map m as:</p>
    <p>m =   fc(x)</p>
    <p>x</p>
    <p>(5)</p>
    <p>To attack GRAD-based IDLSes, we may search for x using a sequence of gradient descent updates as defined in Eqn (4). However, according to Eqn (5), computing the gradient of the attribution map g(x; f ) amounts to computing the Hessian matrix of fc(x), which is all-zero for DNNs with ReLU activation functions. Thus the gradient of the interpretation loss `int provides little information for updating x, which makes directly applying Eqn (4) ineffective.</p>
  </div>
  <div class="page">
    <p>Attack Instantiation</p>
    <p>Backprop-guided interpretation  Gradient saliency (GRAD) interpreter</p>
    <p>Gradient enhancement for ReLU</p>
    <p>Label smoothing to avoid gradient saturation</p>
    <p>!6</p>
    <p>m = fc(x)</p>
    <p>x</p>
    <p>Figure 3: Comparison of h(z), s(z), and r(z) near z = 0.</p>
    <p>To overcome this, when performing back-propagation, we smooth the gradient of ReLU, denoted by r(z), with a function h(z) defined as (t is a small constant, e.g., 104):</p>
    <p>h(z) , (</p>
    <p>(z + p</p>
    <p>z2 + t)0 = 1 + z/ p</p>
    <p>z2 + t (z &lt; 0) ( p</p>
    <p>z2 + t)0 = z/ p</p>
    <p>z2 + t (z  0)</p>
    <p>Intuitively, h(z) tightly approximates r(z), while its gradient is non-zero everywhere. Another possibility is the sigmoid function s(z) = 1/(1 + ez). Figure 3 compares different functions near z = 0. Our evaluation shows that h(z) significantly outperforms s(z) and r(z) in attacking GRAD.</p>
    <p>This attack is extensible to other back-propagation-based interpreters (e.g., DEEPLIFT [48], SMOOTHGRAD [51], and LRP [6]), due to their fundamentally equivalent, gradientcentric formulations [3].</p>
    <p>termediate layers of DNNs to generate attribution maps. We consider class activation mapping (CAM) [64] as a representative interpreter of this class.</p>
    <p>At a high level, CAM performs global average pooling [30] over the feature maps of the last convolutional layer, and uses the outputs as features for a linear layer with softmax activation to approximate the model predictions. Based on this connectivity structure, CAM computes the attribution maps by projecting the weights of the linear layer back to the convolutional feature maps.</p>
    <p>Formally, let ak[i, j] denote the activation of the k-th channel of the last convolutional layer at the spatial position (i, j). The output of global average pooling is defined as Ak = i, j ak[i, j]. Further let wk,c be the weight of the connection between the k-th input and the c-th output of the linear layer. The input to the softmax function for a class c with respect to a given input x is approximated by:</p>
    <p>zc(x)   k</p>
    <p>wk,c Ak =  i, j</p>
    <p>k</p>
    <p>wk,c ak[i, j] (6)</p>
    <p>The class activation map mc is then given by:</p>
    <p>mc[i, j] =  k</p>
    <p>wk,c ak[i, j] (7)</p>
    <p>Due to its use of deep representations at intermediate layers, CAM generates attribution maps of high visual quality and limited noise and artifacts [30].</p>
    <p>We instantiate g with a DNN that concatenates the part of f up to its last convolutional layer and a linear layer parameterized by {wk,c}. To attack CAM, we search for x using a sequence of gradient descent updates as defined in Eqn (4). This attack can be readily extended to other representation-guided interpreters (e.g., GRADCAM [47]), with details deferred to Appendix A1.</p>
    <p>layers, model-guided methods train a meta-model to directly predict the attribution map for any given input in a single feed-forward pass. We consider RTS [10] as a representative method in this category.</p>
    <p>For a given input x in a class c, RTS finds its attribution map m by solving the following optimization problem:</p>
    <p>minm l1rtv(m) + l2rav(m)  log ( fc (f(x; m))) +l3 fc (f(x; 1  m))l4</p>
    <p>s.t. 0  m  1 (8)</p>
    <p>Here rtv(m) denotes the total variation of m, which reduces noise and artifacts in m; rav(m) represents the average value of m, which minimizes the size of retained parts; f(x; m) is the operator using m as a mask to blend x with random colors and Gaussian blur, which captures the impact of retained parts (where the mask is non-zero) on the model prediction; the hyper-parameters {li}4i=1 balance these factors. Intuitively, this formulation finds the sufficient and necessary parts of x, based on which f is able to make the prediction f (x) with high confidence.</p>
    <p>However, solving Eqn (8) for every input during inference is fairly expensive. Instead, RTS trains a DNN to directly predict the attribution map for any given input, without accessing to the DNN f after training. In [44], this is achieved by composing a ResNet [22] pre-trained on ImageNet [12] as the encoder (which extracts feature maps of given inputs at different scales) and a U-NET [44] as the masking model, which is then trained to directly optimize Eqn (8). We consider the composition of this encoder and this masking model as the interpreter g.</p>
    <p>To attack RTS, one may directly apply Eqn (4). However, our evaluation shows that this strategy is often ineffective for finding desirable adversarial inputs. This is explained by that the encoder enc() plays a significant role in generating attribution maps, while solely relying on the outputs of the masking model is insufficient to guide the attack. We thus add to Eqn (3) an additional loss term `enc(enc(x),enc(ct )), which measures the difference of the encoders outputs for the adversarial input x and the target class ct .</p>
    <p>We then search for the adversarial input x with a sequence of gradient descent updates defined in Eqn (4). More implementation details are discussed in  3.6.</p>
    <p>Figure 3: Comparison of h(z), s(z), and r(z) near z = 0.</p>
    <p>To overcome this, when performing back-propagation, we smooth the gradient of ReLU, denoted by r(z), with a function h(z) defined as (t is a small constant, e.g., 104):</p>
    <p>h(z) , (</p>
    <p>(z + p</p>
    <p>z2 + t)0 = 1 + z/ p</p>
    <p>z2 + t (z &lt; 0) ( p</p>
    <p>z2 + t)0 = z/ p</p>
    <p>z2 + t (z  0)</p>
    <p>Intuitively, h(z) tightly approximates r(z), while its gradient is non-zero everywhere. Another possibility is the sigmoid function s(z) = 1/(1 + ez). Figure 3 compares different functions near z = 0. Our evaluation shows that h(z) significantly outperforms s(z) and r(z) in attacking GRAD.</p>
    <p>This attack is extensible to other back-propagation-based interpreters (e.g., DEEPLIFT [48], SMOOTHGRAD [51], and LRP [6]), due to their fundamentally equivalent, gradientcentric formulations [3].</p>
    <p>termediate layers of DNNs to generate attribution maps. We consider class activation mapping (CAM) [64] as a representative interpreter of this class.</p>
    <p>At a high level, CAM performs global average pooling [30] over the feature maps of the last convolutional layer, and uses the outputs as features for a linear layer with softmax activation to approximate the model predictions. Based on this connectivity structure, CAM computes the attribution maps by projecting the weights of the linear layer back to the convolutional feature maps.</p>
    <p>Formally, let ak[i, j] denote the activation of the k-th channel of the last convolutional layer at the spatial position (i, j). The output of global average pooling is defined as Ak = i, j ak[i, j]. Further let wk,c be the weight of the connection between the k-th input and the c-th output of the linear layer. The input to the softmax function for a class c with respect to a given input x is approximated by:</p>
    <p>zc(x)   k</p>
    <p>wk,c Ak =  i, j</p>
    <p>k</p>
    <p>wk,c ak[i, j] (6)</p>
    <p>The class activation map mc is then given by:</p>
    <p>mc[i, j] =  k</p>
    <p>wk,c ak[i, j] (7)</p>
    <p>Due to its use of deep representations at intermediate layers, CAM generates attribution maps of high visual quality and limited noise and artifacts [30].</p>
    <p>We instantiate g with a DNN that concatenates the part of f up to its last convolutional layer and a linear layer parameterized by {wk,c}. To attack CAM, we search for x using a sequence of gradient descent updates as defined in Eqn (4). This attack can be readily extended to other representation-guided interpreters (e.g., GRADCAM [47]), with details deferred to Appendix A1.</p>
    <p>layers, model-guided methods train a meta-model to directly predict the attribution map for any given input in a single feed-forward pass. We consider RTS [10] as a representative method in this category.</p>
    <p>For a given input x in a class c, RTS finds its attribution map m by solving the following optimization problem:</p>
    <p>minm l1rtv(m) + l2rav(m)  log ( fc (f(x; m))) +l3 fc (f(x; 1  m))l4</p>
    <p>s.t. 0  m  1 (8)</p>
    <p>Here rtv(m) denotes the total variation of m, which reduces noise and artifacts in m; rav(m) represents the average value of m, which minimizes the size of retained parts; f(x; m) is the operator using m as a mask to blend x with random colors and Gaussian blur, which captures the impact of retained parts (where the mask is non-zero) on the model prediction; the hyper-parameters {li}4i=1 balance these factors. Intuitively, this formulation finds the sufficient and necessary parts of x, based on which f is able to make the prediction f (x) with high confidence.</p>
    <p>However, solving Eqn (8) for every input during inference is fairly expensive. Instead, RTS trains a DNN to directly predict the attribution map for any given input, without accessing to the DNN f after training. In [44], this is achieved by composing a ResNet [22] pre-trained on ImageNet [12] as the encoder (which extracts feature maps of given inputs at different scales) and a U-NET [44] as the masking model, which is then trained to directly optimize Eqn (8). We consider the composition of this encoder and this masking model as the interpreter g.</p>
    <p>To attack RTS, one may directly apply Eqn (4). However, our evaluation shows that this strategy is often ineffective for finding desirable adversarial inputs. This is explained by that the encoder enc() plays a significant role in generating attribution maps, while solely relying on the outputs of the masking model is insufficient to guide the attack. We thus add to Eqn (3) an additional loss term `enc(enc(x),enc(ct )), which measures the difference of the encoders outputs for the adversarial input x and the target class ct .</p>
    <p>We then search for the adversarial input x with a sequence of gradient descent updates defined in Eqn (4). More implementation details are discussed in  3.6.</p>
  </div>
  <div class="page">
    <p>Attack Instantiation (cont.)</p>
    <p>Perturbation-guided interpretation  MASK interpreter</p>
    <p>A bi-level optimization formulation</p>
    <p>Updating  estimate and  alternatively  Stabilizing optimization with imbalanced update and</p>
    <p>periodical reset</p>
    <p>m* x</p>
    <p>!7</p>
    <p>bution map by perturbing the input with minimum noise and observing the change in the model prediction. We consider MASK [16] as a representative interpreter in this class.</p>
    <p>For a given input x, MASK identifies its most informative parts by checking whether changing such parts influences the prediction f (x). It learns a mask m, where m[i] = 0 if the i-th input feature is retained and m[i] = 1 if the feature is replaced with Gaussian noise. The optimal mask is found by solving an optimization problem:</p>
    <p>min m</p>
    <p>fc(f(x; m)) + lk1  mk1 s.t. 0  m  1 (9)</p>
    <p>where c denotes the current prediction c = f (x) and f(x; m) is the perturbation operator which blends x with Gaussian noise. The first term finds m that causes the probability of c to decrease significantly, while the second term encourages m to be sparse. Intuitively, solving Eqn (9) amounts to finding the most informative and necessary parts of x with respect to its prediction f (x). Note that this formulation may result in significant artifacts in m. A more refined formulation is given in Appendix A2.</p>
    <p>Unlike other classes of interpreters, to attack MASK, it is infeasible to directly optimize Eqn (3) with iterative gradient descent (Eqn (4)), because the interpreter g itself is formulated as an optimization procedure.</p>
    <p>Instead, we reformulate ADV2 using a bilevel optimization framework. For given x, ct , mt , f , and g, we re-define the adversarial loss function as `adv(x,m) , `prd( f (x),ct ) + l`int(m,mt ) by introducing m as an additional variable. Let `map(m; x) be the objective function defined in Eqn (9) (or its variant Eqn (16)). Note that m(x) = arg minm `map(m; x) is the attribution map found by MASK for a given input x. We then have the following attack framework:</p>
    <p>min x</p>
    <p>`adv (x, m(x))</p>
    <p>s.t. m(x) = arg min m</p>
    <p>`map(m; x) (10)</p>
    <p>Still, solving the bilevel optimization in Eqn (10) exactly is challenging, as it requires recomputing m(x) by solving the inner optimization problem whenever x is updated. We propose an approximate iterative procedure which optimizes x and m by alternating between gradient descent on `adv and `map respectively.</p>
    <p>More specifically, at the i-th iteration, given the current input x(i1), we compute its attribution map m(i) by updating m(i1) with gradient descent on `map</p>
    <p>m(i1); x(i1)</p>
    <p>; we then fix</p>
    <p>m(i) and obtain x(i) by minimizing `adv after a single step of gradient descent with respect to m(i). Formally, we define the objective function for updating x(i) as:</p>
    <p>`adv</p>
    <p>x(i1), m(i)  xm`map  m(i); x(i1)</p>
    <p>where x is the learning rate for this virtual gradient descent. The rationale behind this procedure is as follows. While it</p>
    <p>is difficult to directly minimizing `adv (x,m(x)) with respect to x, we use a single-step unrolled map as a surrogate of m(x). A similar approach is used in [15]. Essentially, this iterative optimization defines a Stackelberg game [46] between the optimizer for x (leader) and the optimizer for m (follower), which requires the leader to anticipate the followers next move to reach the equilibrium.</p>
    <p>Algorithm 1: ADV2 against MASK. Input: x: benign input; ct : target class; mt : target map; f : target DNN;</p>
    <p>g: MASK interpreter Output: x: adversarial input</p>
    <p>// update m 3 update m by gradient descent along m`map(m; x);</p>
    <p>// update x with single-step lookahead 4 update x by gradient descent along</p>
    <p>x`adv  x, m  xm`map (m; x)</p>
    <p>;</p>
    <p>Algorithm 1 sketches the attack against MASK. More implementation details are given in  3.6. The theoretical justification for its effectiveness is deferred to Appendix A3.</p>
    <p>a suite of optimizations to improve the attack effectiveness against specific interpreters.</p>
    <p>Iterative Optimizer  We build the optimizer based upon PGD [35], which iteratively updates the adversarial input using Eqn (4). By default, we use L norm to measure the perturbation magnitude. It is possible to adopt alternative frameworks if other perturbation metrics are considered. For instance, instead of modifying pixels directly, one may generate adversarial inputs via spatial transformation [2, 60], in which the perturbation magnitude is often measured by the overall spatial distortion. We detail and evaluate spatial transformationbased ADV2 in  4.</p>
    <p>Warm Start  It is observed in our evaluation that it is often inefficient to search for adversarial inputs by running the update steps of ADV2 (Eqn (4)) from scratch. Rather, first running a fixed number (e.g., 400) of update steps of the regular adversarial attack and then resuming the ADV2 update steps significantly improves the search efficiency. Intuitively, this strategy first quickly approaches the manifold of adversarial inputs, and then searches for inputs satisfying both prediction and interpretation constraints.</p>
    <p>Label Smoothing  Recall that we measure the prediction loss `prd( f (x),ct ) with cross entropy. When attacking GRAD, ADV2 may generate intermediate inputs that cause f to make over-confident predictions (e.g., with probability 1). The allzero gradient of `prd prevents the attack from finding inputs with desirable interpretations. To solve this, we refine cross</p>
    <p>bution map by perturbing the input with minimum noise and observing the change in the model prediction. We consider MASK [16] as a representative interpreter in this class.</p>
    <p>For a given input x, MASK identifies its most informative parts by checking whether changing such parts influences the prediction f (x). It learns a mask m, where m[i] = 0 if the i-th input feature is retained and m[i] = 1 if the feature is replaced with Gaussian noise. The optimal mask is found by solving an optimization problem:</p>
    <p>min m</p>
    <p>fc(f(x; m)) + lk1  mk1 s.t. 0  m  1 (9)</p>
    <p>where c denotes the current prediction c = f (x) and f(x; m) is the perturbation operator which blends x with Gaussian noise. The first term finds m that causes the probability of c to decrease significantly, while the second term encourages m to be sparse. Intuitively, solving Eqn (9) amounts to finding the most informative and necessary parts of x with respect to its prediction f (x). Note that this formulation may result in significant artifacts in m. A more refined formulation is given in Appendix A2.</p>
    <p>Unlike other classes of interpreters, to attack MASK, it is infeasible to directly optimize Eqn (3) with iterative gradient descent (Eqn (4)), because the interpreter g itself is formulated as an optimization procedure.</p>
    <p>Instead, we reformulate ADV2 using a bilevel optimization framework. For given x, ct , mt , f , and g, we re-define the adversarial loss function as `adv(x,m) , `prd( f (x),ct ) + l`int(m,mt ) by introducing m as an additional variable. Let `map(m; x) be the objective function defined in Eqn (9) (or its variant Eqn (16)). Note that m(x) = arg minm `map(m; x) is the attribution map found by MASK for a given input x. We then have the following attack framework:</p>
    <p>min x</p>
    <p>`adv (x, m(x))</p>
    <p>s.t. m(x) = arg min m</p>
    <p>`map(m; x) (10)</p>
    <p>Still, solving the bilevel optimization in Eqn (10) exactly is challenging, as it requires recomputing m(x) by solving the inner optimization problem whenever x is updated. We propose an approximate iterative procedure which optimizes x and m by alternating between gradient descent on `adv and `map respectively.</p>
    <p>More specifically, at the i-th iteration, given the current input x(i1), we compute its attribution map m(i) by updating m(i1) with gradient descent on `map</p>
    <p>m(i1); x(i1)</p>
    <p>; we then fix</p>
    <p>m(i) and obtain x(i) by minimizing `adv after a single step of gradient descent with respect to m(i). Formally, we define the objective function for updating x(i) as:</p>
    <p>`adv</p>
    <p>x(i1), m(i)  xm`map  m(i); x(i1)</p>
    <p>where x is the learning rate for this virtual gradient descent. The rationale behind this procedure is as follows. While it</p>
    <p>is difficult to directly minimizing `adv (x,m(x)) with respect to x, we use a single-step unrolled map as a surrogate of m(x). A similar approach is used in [15]. Essentially, this iterative optimization defines a Stackelberg game [46] between the optimizer for x (leader) and the optimizer for m (follower), which requires the leader to anticipate the followers next move to reach the equilibrium.</p>
    <p>Algorithm 1: ADV2 against MASK. Input: x: benign input; ct : target class; mt : target map; f : target DNN;</p>
    <p>g: MASK interpreter Output: x: adversarial input</p>
    <p>// update m 3 update m by gradient descent along m`map(m; x);</p>
    <p>// update x with single-step lookahead 4 update x by gradient descent along</p>
    <p>x`adv  x, m  xm`map (m; x)</p>
    <p>;</p>
    <p>Algorithm 1 sketches the attack against MASK. More implementation details are given in  3.6. The theoretical justification for its effectiveness is deferred to Appendix A3.</p>
    <p>a suite of optimizations to improve the attack effectiveness against specific interpreters.</p>
    <p>Iterative Optimizer  We build the optimizer based upon PGD [35], which iteratively updates the adversarial input using Eqn (4). By default, we use L norm to measure the perturbation magnitude. It is possible to adopt alternative frameworks if other perturbation metrics are considered. For instance, instead of modifying pixels directly, one may generate adversarial inputs via spatial transformation [2, 60], in which the perturbation magnitude is often measured by the overall spatial distortion. We detail and evaluate spatial transformationbased ADV2 in  4.</p>
    <p>Warm Start  It is observed in our evaluation that it is often inefficient to search for adversarial inputs by running the update steps of ADV2 (Eqn (4)) from scratch. Rather, first running a fixed number (e.g., 400) of update steps of the regular adversarial attack and then resuming the ADV2 update steps significantly improves the search efficiency. Intuitively, this strategy first quickly approaches the manifold of adversarial inputs, and then searches for inputs satisfying both prediction and interpretation constraints.</p>
    <p>Label Smoothing  Recall that we measure the prediction loss `prd( f (x),ct ) with cross entropy. When attacking GRAD, ADV2 may generate intermediate inputs that cause f to make over-confident predictions (e.g., with probability 1). The allzero gradient of `prd prevents the attack from finding inputs with desirable interpretations. To solve this, we refine cross</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>!8</p>
    <p>Attack effectiveness (misclassification)</p>
    <p>Attack effectiveness (misinterpretation)</p>
    <p>Classifier ResNet DenseNet</p>
    <p>Interpreter GRAD CAM MASK RTS GRAD CAM MASK RTS</p>
    <p>PGD 100% (1.0) 100% (1.0)</p>
    <p>ADV2 100% (0.99) 100% (1.0)</p>
    <p>Setting:  Dataset  ImageNet  Classifier  ResNet-50, DenseNet-169  Interpreter  GRAD, CAM, MASK, RTS  Attack model  PGD, ADV2  Target interpretation  benign attribute map</p>
    <p>GRAD CAM MASK RTS 0</p>
    <p>GRAD CAM MASK RTS</p>
    <p>(ResNet) (DenseNet)</p>
    <p>PGD ADV</p>
    <p>GRAD CAM MASK RTS (ResNet)</p>
    <p>Io U</p>
    <p>S c o</p>
    <p>re</p>
    <p>GRAD CAM MASK RTS (DenseNet)</p>
    <p>PGD ADV</p>
    <p>L1 distance between benign and adversarial attribution maps.</p>
    <p>Intersection-of-union (IOU) of benign and adversarial attribution maps.</p>
  </div>
  <div class="page">
    <p>Evaluation (cont.)</p>
    <p>!9</p>
    <p>Sample inputs, predictions, and interpretations Im ag e</p>
    <p>A D V 2</p>
    <p>&lt;latexit sha1_base64=&quot;ndlTBZhGdyfqfclpd+RDOxZSbho=&quot;&gt;AAACVHicbVBdTxNBFJ1dRKEiAj7ysrGY8NTsVo0+4seDj5jYQsIUMnt7l046H5uZu0gz2f/Bq/woEv+LD05LE7V4kklOzj33Y05ZK+kpz38m6dqj9cdPNjY7T7eebT/f2d0bets4wAFYZd1pKTwqaXBAkhSe1g6FLhWelNNP8/rJFTovrflGsxpHWlwaWUkQFKVzrgVNfBU+fB625/2LnW7eyxfIHpJiSbpsieOL3STnYwuNRkOghPdnRV7TKAhHEhS2Hd54rAVMxSWeRWqERj8Ki7Pb7FVUxlllXXyGsoX6d0cQ2vuZLqNzceZqbS7+t1bqlc1UvR8FaeqG0MD94qpRGdlsnkk2lg6B1CwSAU7G2zOYCCeAYnIdbvA7WK2FGQcOIB20gU/Rmbz3Fq/5FcTPowt8UtrrcMB9nFCTp5lCPjcftO0fd9uJGReriT4kw36veN3rf33TPfq4THuD7bOX7JAV7B07Yl/YMRswYI7dsB/sNrlLfqVr6fq9NU2WPS/YP0i3fwO2CbVe&lt;/latexit&gt;</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
    <p>P G D</p>
    <p>&lt;latexit sha1_base64=&quot;37lHHgOI5+NdtgstF/aZnejcenw=&quot;&gt;AAACUnicbVJNTxsxEPWmhUIKNLTHXlYNSJyiXaBqj6hFao9BagAJR8g7mSVW/LGyZymRtX+j1/KjuPBXeqoTIhVCR7L09N4bz/jJRaWkpyy7T1ovXq6svlpbb7/e2Nx609l+e+pt7QAHYJV154XwqKTBAUlSeF45FLpQeFZMvs70s2t0Xlrzg6YVDrW4MrKUIChSnGtBY1+G/rfj5rLTzXrZvNLnIF+ALltU/3I7yfjIQq3RECjh/UWeVTQMwpEEhU2b1x4rARNxhRcRGqHRD8N86SbdjcwoLa2Lx1A6Zx93BKG9n+oiOudLLmsz8r9aoZcmU/l5GKSpakIDD4PLWqVk01ki6Ug6BFLTCAQ4GXdPYSycAIq5tbnBn2C1FmYUOIB00AQ+QWey3ke84dcQH48u8HFhb8IO9/GGijxNFfKZeadp/rmbdsw4X070OTjd7+UHvf2Tw+7Rl0Xaa+w9+8D2WM4+sSP2nfXZgAGr2C/2m90md8mfVvwlD9ZWsuh5x55Ua+MvUfW0ug==&lt;/latexit&gt;</p>
    <p>B en</p>
    <p>ig n</p>
    <p>&lt;latexit sha1_base64=&quot;I5npcQN4LAtdmDP5Sk7C8uyw2sI=&quot;&gt;AAACV3icbVDLbhMxFPUM0IbwaAJLNiNSJFbRTAHBsiqbLotE2kp1FHlubhIrfozsO20ja76EbftR/RrqSSMBKUeydHTuuQ+fslLSU57fJemTp892djvPuy9evnq91+u/OfW2doAjsMq681J4VNLgiCQpPK8cCl0qPCuX39v62SU6L635SasKx1rMjZxJEBSlSW+Pa0ELp8MRGjk3zaQ3yIf5GtljUmzIgG1wMuknOZ9aqDUaAiW8vyjyisZBOJKgsOny2mMlYCnmeBGpERr9OKwvb7IPUZlmM+viM5St1b87gtDer3QZne2dfrvWiv+tlXprM82+jYM0VU1o4GHxrFYZ2ayNJZtKh0BqFYkAJ+PtGSyEE0AxvC43eAVWa2GmgQNIB03gS3QmH37Ba34J8fPoAl+U9jrscx8nVORppZC35v2m+eNuujHjYjvRx+T0YFh8Gh78+Dw4PNqk3WHv2Hv2kRXsKztkx+yEjRiwmv1iN+w2uUt+pztp58GaJpuet+wfpP17JmG2hw==&lt;/latexit&gt;</p>
    <p>Im a g e</p>
    <p>&lt;latexit sha1_base64=&quot;9iHrmTNcTY9jNG+6WX4G+SLCMv4=&quot;&gt;AAACVHicbVDLThsxFPUM5ZXybJdsRoRKXUUzQFWWqN20O5AIIOEUeW5uiBU/RvYdSmTNf3TbflSl/ksX9YRILaFXsnR0zrkPn7JS0lOe/0rSpRfLK6tr652XG5tb2zu7ry69rR1gH6yy7roUHpU02CdJCq8rh0KXCq/KycdWv7pH56U1FzStcKDFnZEjCYIi9YVrQWOnw+dIY3O70817+ayy56CYgy6b19ntbpLzoYVaoyFQwvubIq9oEIQjCQqbDq89VgImcfpNhEZo9IMwO7vJ3kRmmI2si89QNmP/7QhCez/VZXS2Z/pFrSX/q5V6YTONTgZBmqomNPC4eFSrjGzWZpINpUMgNY1AgJPx9gzGwgmgmFyHG/wKVmthhoEDSAdN4BN0Ju+9wwd+D/Hz6AIfl/YhHHAfJ1TkaaqQt+aDpvnrbjox42Ix0efg8rBXHPUOz4+7px/maa+xPbbP3rKCvWen7BM7Y30GzLFv7Dv7kfxMfqdL6fKjNU3mPa/Zk0q3/gCkw7Xc&lt;/latexit&gt;</p>
  </div>
  <div class="page">
    <p>Root of Attack Vulnerability</p>
    <p>Conjecture: prediction-interpretation gap  Interpreters explanations only partially describe classifiers predictions,</p>
    <p>making it practical to exploit both models simultaneously.</p>
    <p>Observation: random class interpretation</p>
    <p>!10</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>A D V 2 M a p</p>
    <p>&lt;latexit sha1_base64=&quot;5/mZiFD+Ihb0/jDnm5GodjBeKGQ=&quot;&gt;AAACBXicbVC7TsMwFHXKq5RXgBEGiwqJqUoKEozlMbAgFYk+pCZUjuu0Vm0nsh2kKsrCwq+wMIAQK//Axt/gph2g5UiWjs+5V/feE8SMKu0431ZhYXFpeaW4Wlpb39jcsrd3mipKJCYNHLFItgOkCKOCNDTVjLRjSRAPGGkFw8ux33ogUtFI3OlRTHyO+oKGFCNtpK6973GkBypMz6+a2X3Vg/lf8vQGxVnXLjsVJwecJ+6UlMEU9a795fUinHAiNGZIqY7rxNpPkdQUM5KVvESRGOEh6pOOoQJxovw0vyKDh0bpwTCS5gkNc/V3R4q4UiMemMp851lvLP7ndRIdnvkpFXGiicCTQWHCoI7gOBLYo5JgzUaGICyp2RXiAZIIaxNcyYTgzp48T5rVintcqd6elGsX0ziKYA8cgCPgglNQA9egDhoAg0fwDF7Bm/VkvVjv1sektGBNe3bBH1ifP2IfmIA=&lt;/latexit&gt;</p>
    <p>A D V 2 Im</p>
    <p>g &lt;latexit sha1_base64=&quot;9diO+gSujBUebJtrADp4GjiizMA=&quot;&gt;AAACBXicbVC7TsMwFHXKq5RXgBEGiwqJqUoKEozlMcBWJPqQmlA5rtNatZ3IdpCqKAsLv8LCAEKs/AMbf4ObdoCWI1k6Pude3XtPEDOqtON8W4WFxaXlleJqaW19Y3PL3t5pqiiRmDRwxCLZDpAijArS0FQz0o4lQTxgpBUML8d+64FIRSNxp0cx8TnqCxpSjLSRuva+x5EeqDA9v2pm91UP5n/J0xvez7p22ak4OeA8caekDKaod+0vrxfhhBOhMUNKdVwn1n6KpKaYkazkJYrECA9Rn3QMFYgT5af5FRk8NEoPhpE0T2iYq787UsSVGvHAVOY7z3pj8T+vk+jwzE+piBNNBJ4MChMGdQTHkcAelQRrNjIEYUnNrhAPkERYm+BKJgR39uR50qxW3ONK9fakXLuYxlEEe+AAHAEXnIIauAZ10AAYPIJn8ArerCfrxXq3PialBWvaswv+wPr8AWCemH8=&lt;/latexit&gt;</p>
    <p>T a rg et</p>
    <p>M a p</p>
    <p>&lt;latexit sha1_base64=&quot;wBcq9myxKvgSYxwPBMcNVLrx7Jw=&quot;&gt;AAAB/XicbVDLSsNAFL2pr1pf8bFzM1gEVyWpgi6LbtwIFfqCJpTJdNoOnUzCzESoofgrblwo4tb/cOffOGmz0NYDA4dz7uWeOUHMmdKO820VVlbX1jeKm6Wt7Z3dPXv/oKWiRBLaJBGPZCfAinImaFMzzWknlhSHAaftYHyT+e0HKhWLRENPYuqHeCjYgBGsjdSzj7wQ65EM0waWQ6o9dIfjac8uOxVnBrRM3JyUIUe9Z395/YgkIRWacKxU13Vi7adYakY4nZa8RNEYkzEe0q6hAodU+eks/RSdGqWPBpE0T2g0U39vpDhUahIGZjLLqha9TPzP6yZ6cOWnTMSJpoLMDw0SjnSEsipQn0lKNJ8YgolkJisiIywx0aawkinBXfzyMmlVK+55pXp/Ua5d53UU4RhO4AxcuIQa3EIdmkDgEZ7hFd6sJ+vFerc+5qMFK985hD+wPn8AoeKVVQ==&lt;/latexit&gt;</p>
    <p>T a rg et</p>
    <p>Im g</p>
    <p>&lt;latexit sha1_base64=&quot;+IXxUYjQg9dE1r8J2tWR7/6XgJ0=&quot;&gt;AAAB/XicbVDLSgMxFM3UV62v8bFzEyyCqzJTBV0W3eiuQl/QGUomzUxDk8yQZIQ6FH/FjQtF3Pof7vwbM+0stPVA4HDOvdyTEySMKu0431ZpZXVtfaO8Wdna3tnds/cPOipOJSZtHLNY9gKkCKOCtDXVjPQSSRAPGOkG45vc7z4QqWgsWnqSEJ+jSNCQYqSNNLCPPI70SPKshWREtAfveDQd2FWn5swAl4lbkCoo0BzYX94wxiknQmOGlOq7TqL9DElNMSPTipcqkiA8RhHpGyoQJ8rPZumn8NQoQxjG0jyh4Uz9vZEhrtSEB2Yyz6oWvVz8z+unOrzyMyqSVBOB54fClEEdw7wKOKSSYM0mhiAsqckK8QhJhLUprGJKcBe/vEw69Zp7XqvfX1Qb10UdZXAMTsAZcMElaIBb0ARtgMEjeAav4M16sl6sd+tjPlqyip1D8AfW5w+gYZVU&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;jijuc9C6B6tD5QNSewaHuI8qAjE=&quot;&gt;AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqszUgi7rA3RZxT6gHUomzbShmcyYZApl6He4caGIWz/GnX9jOp2Fth4IHM65l3tyvIgzpW3728qtrK6tb+Q3C1vbO7t7xf2DpgpjSWiDhDyUbQ8rypmgDc00p+1IUhx4nLa80fXMb42pVCwUj3oSUTfAA8F8RrA2ktsNsB4qP7l9uLyZ9oolu2ynQMvEyUgJMtR7xa9uPyRxQIUmHCvVcexIuwmWmhFOp4VurGiEyQgPaMdQgQOq3CQNPUUnRukjP5TmCY1S9fdGggOlJoFnJtOQi95M/M/rxNq/cBMmolhTQeaH/JgjHaJZA6jPJCWaTwzBRDKTFZEhlpho01PBlOAsfnmZNCtl56xcua+WaldZHXk4gmM4BQfOoQZ3UIcGEHiCZ3iFN2tsvVjv1sd8NGdlO4fwB9bnD4+bkfU=&lt;/latexit&gt;</p>
    <p>GRAD CAM MASK RTS (a)</p>
    <p>GRAD CAM MASK RTS (b)</p>
    <p>Io U</p>
    <p>S co</p>
    <p>re</p>
    <p>w.r.t. Benign w.r.t. Target</p>
    <p>w.r.t. Benign w.r.t. Target</p>
  </div>
  <div class="page">
    <p>Root of Prediction-Interpretation Gap Conjecture: limitations of existing interpretation models  Different interpreters focus on distinct aspects of DNN behaviors (e.g.,</p>
    <p>gradient, intermediate representations, etc.)</p>
    <p>Observation: low attack transferability</p>
    <p>!11</p>
    <p>CAM &lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>MASK &lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>C A M</p>
    <p>&lt;latexit sha1_base64=&quot;t9qJAjypfVgyUIJ4eLByrGfLUqE=&quot;&gt;AAACUnicbVJNTxsxEPWmfKZ8tsdeVoRKPUW7QFWOUC69VAKpASQcIe9klljxx8qeBSJr/0av7Y/qpX+FE06IVAgdydLTe2884ycXlZKesuxv0nqzsLi0vLLafru2vrG5tf3u3NvaAfbAKusuC+FRSYM9kqTwsnIodKHwohidTPSLW3ReWvODxhX2tbgxspQgKFKca0FDX4aT4+/N9VYn62bTSl+DfAY6bFan19tJxgcWao2GQAnvr/Kson4QjiQobNq89lgJGIkbvIrQCI2+H6ZLN+nHyAzS0rp4DKVT9nlHENr7sS6ic7rkvDYh/6sVem4ylYf9IE1VExp4GlzWKiWbThJJB9IhkBpHIMDJuHsKQ+EEUMytzQ3egdVamEHgANJBE/gIncm6n/Ge30J8PLrAh4W9D7vcxxsq8jRWyCfm3ab5527aMeN8PtHX4Hyvm+93984OOkdfZ2mvsA9sh31iOfvCjtg3dsp6DFjFfrJf7HfyJ3loxV/yZG0ls5737EW11h4BPu20sA==&lt;/latexit&gt;</p>
    <p>M A S K</p>
    <p>&lt;latexit sha1_base64=&quot;mR9A3drYeDdYcEdkw7znvgB+AII=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqBsSQoLBARKqQ6rv3GYqU4+26jYyqfR3uNWPcsG3sLFmmAQdPEklJ+ee+6hTNUp6yvOrJL23cv/Bw9VHvcdP1p4+W994fuRt6wCHYJV1J5XwqKTBIUlSeNI4FLpSeFxNPs/qxxfovLTmK00bLLU4N7KWIChKJdeCxr4O+x8P97qz9X4+yOfI7pJiQfpsgYOzjSTnIwutRkOghPenRd5QGYQjCQq7Hm89NgIm4hxPIzVCoy/D/OouexWVUVZbF5+hbK7+3RGE9n6qq+icX7lcm4n/rVV6aTPVH8ogTdMSGrhZXLcqI5vNIslG0iGQmkYiwMl4ewZj4QRQDK7HDX4Hq7Uwo8ABpIMu8Ak6kw/e4SW/gPh5dIGPK3sZtriPExryNFXIZ+atrrt1d72YcbGc6F1ytD0o3gy2v7zt73xapL3KXrCX7DUr2Hu2w3bZARsyYN/YD/aT/Up+J9dpmq7cWNNk0bPJ/kG69gcUsLUV&lt;/latexit&gt;</p>
    <p>R T S</p>
    <p>&lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>S o u rc e</p>
    <p>&lt;latexit sha1_base64=&quot;x8Qa/2d5vI4lgGkahLSIueuQjRQ=&quot;&gt;AAAB+HicbVC7TsMwFL0pr1IeDTCyWFRITFVSkGCsYGEsgj6kNqoc12mt2klkO0gl6pewMIAQK5/Cxt/gpBmg5UiWjs651z4+fsyZ0o7zbZXW1jc2t8rblZ3dvf2qfXDYUVEiCW2TiEey52NFOQtpWzPNaS+WFAuf064/vcn87iOVikXhg57F1BN4HLKAEayNNLSrA4H1RIr0Pr9xPrRrTt3JgVaJW5AaFGgN7a/BKCKJoKEmHCvVd51YeymWmhFO55VBomiMyRSPad/QEAuqvDQPPkenRhmhIJLmhBrl6u+NFAulZsI3k1lMtexl4n9eP9HBlZeyME40DcnioSDhSEcoawGNmKRE85khmEhmsiIywRITbbqqmBLc5S+vkk6j7p7XG3cXteZ1UUcZjuEEzsCFS2jCLbSgDQQSeIZXeLOerBfr3fpYjJasYucI/sD6/AFo2JOT&lt;/latexit&gt;</p>
    <p>Target &lt;latexit sha1_base64=&quot;9nrUCVEvVYd5wBP53f43EMr1XZE=&quot;&gt;AAAB+HicbVDLSgMxFM3UV62Pjrp0EyyCqzJTBV0W3bis0Be0Q8mkaRuaZIbkjlCHfokbF4q49VPc+Tdm2llo64HA4Zx7uScnjAU34HnfTmFjc2t7p7hb2ts/OCy7R8dtEyWashaNRKS7ITFMcMVawEGwbqwZkaFgnXB6l/mdR6YNj1QTZjELJBkrPuKUgJUGbrkvCUy0TJtEjxnMB27Fq3oL4HXi56SCcjQG7ld/GNFEMgVUEGN6vhdDkBINnAo2L/UTw2JCp2TMepYqIpkJ0kXwOT63yhCPIm2fArxQf2+kRBozk6GdzGKaVS8T//N6CYxugpSrOAGm6PLQKBEYIpy1gIdcMwpiZgmhmtusmE6IJhRsVyVbgr/65XXSrlX9y2rt4apSv83rKKJTdIYukI+uUR3dowZqIYoS9Ixe0Zvz5Lw4787HcrTg5Dsn6A+czx9ZVpOJ&lt;/latexit&gt;</p>
    <p>GRAD &lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
    <p>G R A D</p>
    <p>&lt;latexit sha1_base64=&quot;8M/5ECn0yx4s+c8rEiwRj7ahjPI=&quot;&gt;AAACU3icbVDLThRBFK1uUXEUBVm66TiYuJp0o0aXqCSwROIACdUh1XduM5WpR1t1G5lU+jvc6ke54FvYWDNMgg6epJKTc8991KkaJT3l+VWS3lu5/+Dh6qPe4ydrT5+tbzw/8rZ1gEOwyrqTSnhU0uCQJCk8aRwKXSk8riafZ/XjC3ReWvOVpg2WWpwbWUsQFKWSa0FjX4e9w4+73dl6Px/kc2R3SbEgfbbAwdlGkvORhVajIVDC+9Mib6gMwpEEhV2Ptx4bARNxjqeRGqHRl2F+dZe9isooq62Lz1A2V//uCEJ7P9VVdM6vXK7NxP/WKr20meoPZZCmaQkN3CyuW5WRzWaRZCPpEEhNIxHgZLw9g7FwAigG1+MGv4PVWphR4ADSQRf4BJ3JB+/wkl9A/Dy6wMeVvQxb3McJDXmaKuQz81bX3bq7Xsy4WE70LjnaHhRvBttf3vZ3Pi3SXmUv2Ev2mhXsPdth++yADRmwb+wH+8l+Jb+T6zRNV26sabLo2WT/IF37A/ontQc=&lt;/latexit&gt;</p>
  </div>
  <div class="page">
    <p>Potential Countermeasures Ensemble interpretation  Multiple, complimentary interpreters to fully cover DNN behaviors</p>
    <p>Adversarial interpretation  Minimizing prediction-interpretation gap using adversarial examples</p>
    <p>!12</p>
    <p>Input &lt;latexit sha1_base64=&quot;sswdsU9b9cSNor+qUqUMAkJOums=&quot;&gt;AAACVHicbVBNTxsxEPUuUGjaUijHXlaESj1Fu7QVHBG9tDeQGkDCKfJOJsSKP1b2LBBZ+z+4tj+qUv8LB7whUiH0SZae3rzxzLyyUtJTnv9N0qXllReray87r16/WX+7sfnuxNvaAfbBKuvOSuFRSYN9kqTwrHIodKnwtJx8beunV+i8tOYHTSscaHFp5EiCoCj95FrQ2Onw3VQ1NRcb3byXz5A9J8WcdNkcRxebSc6HFmqNhkAJ78+LvKJBEI4kKGw6vPZYCZiISzyP1AiNfhBmazfZh6gMs5F18RnKZurjjiC091NdRme7pl+steJ/a6VemEyj/UGQ7Y1o4GHwqFYZ2azNJBtKh0BqGokAJ+PuGYyFE0AxuQ43eA1Wa2GGgQNIB03gE3Qm733BG34F8Xh0gY9LexN2uI8/VORpqpC35p2m+eduOjHjYjHR5+Rkt1d86u0ef+4eHM7TXmPv2Tb7yAq2xw7YN3bE+gyYY7fsF/ud/Enu0qV05cGaJvOeLfYE6fo9+ga2CQ==&lt;/latexit&gt;</p>
    <p>Benign &lt;latexit sha1_base64=&quot;I5npcQN4LAtdmDP5Sk7C8uyw2sI=&quot;&gt;AAACV3icbVDLbhMxFPUM0IbwaAJLNiNSJFbRTAHBsiqbLotE2kp1FHlubhIrfozsO20ja76EbftR/RrqSSMBKUeydHTuuQ+fslLSU57fJemTp892djvPuy9evnq91+u/OfW2doAjsMq681J4VNLgiCQpPK8cCl0qPCuX39v62SU6L635SasKx1rMjZxJEBSlSW+Pa0ELp8MRGjk3zaQ3yIf5GtljUmzIgG1wMuknOZ9aqDUaAiW8vyjyisZBOJKgsOny2mMlYCnmeBGpERr9OKwvb7IPUZlmM+viM5St1b87gtDer3QZne2dfrvWiv+tlXprM82+jYM0VU1o4GHxrFYZ2ayNJZtKh0BqFYkAJ+PtGSyEE0AxvC43eAVWa2GmgQNIB03gS3QmH37Ba34J8fPoAl+U9jrscx8nVORppZC35v2m+eNuujHjYjvRx+T0YFh8Gh78+Dw4PNqk3WHv2Hv2kRXsKztkx+yEjRiwmv1iN+w2uUt+pztp58GaJpuet+wfpP17JmG2hw==&lt;/latexit&gt;</p>
    <p>ADV2 &lt;latexit sha1_base64=&quot;ZWHQm72Yv3xv1mA9WRicXNWuI64=&quot;&gt;AAACVHicbVBdTxNBFJ1dRKEiAj7ysrGY8NTsVo0+4seDj5jYQsIUMnt7Syedj83MXaSZ7P/gVX4Uif/FB2dLE7V4kklOzj33Y05ZKekpz38m6dqj9cdPNjY7T7eebT/f2d0bels7wAFYZd1pKTwqaXBAkhSeVg6FLhWelLNPbf3kCp2X1nyjeYUjLS6NnEgQFKVzrgVNnQ4fPg/P+83FTjfv5QtkD0mxJF22xPHFbpLzsYVaoyFQwvuzIq9oFIQjCQqbDq89VgJm4hLPIjVCox+FxdlN9ioq42xiXXyGsoX6d0cQ2vu5LqOzPdOv1lrxv7VSr2ymyftRkKaqCQ3cL57UKiObtZlkY+kQSM0jEeBkvD2DqXACKCbX4Qa/g9VamHHgANJBE/gMncl7b/GaX0H8PLrAp6W9DgfcxwkVeZor5K35oGn+uJtOzLhYTfQhGfZ7xete/+ub7tHHZdobbJ+9ZIesYO/YEfvCjtmAAXPshv1gt8ld8itdS9fvrWmy7HnB/kG6/RvBGrVk&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;lFxypesjedtwNDbnseQwEPV2zGw=&quot;&gt;AAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==&lt;/latexit&gt;</p>
    <p>RTSA &lt;latexit sha1_base64=&quot;s3ZcRV6L160OV+7FOgU8N3JLhfg=&quot;&gt;AAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WR&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;lFxypesjedtwNDbnseQwEPV2zGw=&quot;&gt;AAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==&lt;/latexit&gt;</p>
    <p>RTSA &lt;latexit sha1_base64=&quot;s3ZcRV6L160OV+7FOgU8N3JLhfg=&quot;&gt;AAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WR&lt;/latexit&gt;</p>
    <p>L1 measures &lt;latexit sha1_base64=&quot;qcjCSPy16smkXN/8tc8k+NkIMEI=&quot;&gt;AAACa3icbVDLbhMxFHWmPEp4tKU7YGGRVmKBopnSCpYV3bBgUSTSVqqj6M7NTWPFj5HtKY2s+YB+DVv4lH4E/4AnjQSkXMvS0bnnvk5ZKelDnt90srV79x88XH/Uffzk6bONza3nJ97WDmmAVll3VoInJQ0NggyKzipHoEtFp+XsqM2fXpLz0pqvYV7RUMOFkROJEBI12uwJDWGKoOLnZlRw8Ta9lnE6agJfO/JNUuX9fBH8LiiWoMeWcTza6uRibLHWZAIq8P68yKswjOCCREVNV9SeKsAZXNB5ggY0+WFcXNPw3cSM+cS69E3gC/bvigja+7kuk7Ld1K/mWvK/uVKvTA6TD8MoTVUHMng7eFIrHixvreJj6QiDmicA6GTaneMUHGBIhnaFoW9otQYzjgJROmyimJEzef+ArsQlpuPJRTEt7VXcET51qIIPc0WiFe80zR91000eF6uO3gUne/3iXX/vy37v8OPS7XX2kr1mb1jB3rND9okdswFDds2+sx/sZ+dXtp29yF7dSrPOsmab/RPZ7m9pEr2c&lt;/latexit&gt;</p>
    <p>the DNNs behavior change, by generating highly contrastive maps. This sensitivity is also quantitatively confirmed by the L1 distance between the clean and noisy attribution maps.</p>
    <p>Input &lt;latexit sha1_base64=&quot;sswdsU9b9cSNor+qUqUMAkJOums=&quot;&gt;AAACVHicbVBNTxsxEPUuUGjaUijHXlaESj1Fu7QVHBG9tDeQGkDCKfJOJsSKP1b2LBBZ+z+4tj+qUv8LB7whUiH0SZae3rzxzLyyUtJTnv9N0qXllReray87r16/WX+7sfnuxNvaAfbBKuvOSuFRSYN9kqTwrHIodKnwtJx8beunV+i8tOYHTSscaHFp5EiCoCj95FrQ2Onw3VQ1NRcb3byXz5A9J8WcdNkcRxebSc6HFmqNhkAJ78+LvKJBEI4kKGw6vPZYCZiISzyP1AiNfhBmazfZh6gMs5F18RnKZurjjiC091NdRme7pl+steJ/a6VemEyj/UGQ7Y1o4GHwqFYZ2azNJBtKh0BqGokAJ+PuGYyFE0AxuQ43eA1Wa2GGgQNIB03gE3Qm733BG34F8Xh0gY9LexN2uI8/VORpqpC35p2m+eduOjHjYjHR5+Rkt1d86u0ef+4eHM7TXmPv2Tb7yAq2xw7YN3bE+gyYY7fsF/ud/Enu0qV05cGaJvOeLfYE6fo9+ga2CQ==&lt;/latexit&gt;</p>
    <p>Benign &lt;latexit sha1_base64=&quot;I5npcQN4LAtdmDP5Sk7C8uyw2sI=&quot;&gt;AAACV3icbVDLbhMxFPUM0IbwaAJLNiNSJFbRTAHBsiqbLotE2kp1FHlubhIrfozsO20ja76EbftR/RrqSSMBKUeydHTuuQ+fslLSU57fJemTp892djvPuy9evnq91+u/OfW2doAjsMq681J4VNLgiCQpPK8cCl0qPCuX39v62SU6L635SasKx1rMjZxJEBSlSW+Pa0ELp8MRGjk3zaQ3yIf5GtljUmzIgG1wMuknOZ9aqDUaAiW8vyjyisZBOJKgsOny2mMlYCnmeBGpERr9OKwvb7IPUZlmM+viM5St1b87gtDer3QZne2dfrvWiv+tlXprM82+jYM0VU1o4GHxrFYZ2ayNJZtKh0BqFYkAJ+PtGSyEE0AxvC43eAVWa2GmgQNIB03gS3QmH37Ba34J8fPoAl+U9jrscx8nVORppZC35v2m+eNuujHjYjvRx+T0YFh8Gh78+Dw4PNqk3WHv2Hv2kRXsKztkx+yEjRiwmv1iN+w2uUt+pztp58GaJpuet+wfpP17JmG2hw==&lt;/latexit&gt;</p>
    <p>ADV2 &lt;latexit sha1_base64=&quot;ZWHQm72Yv3xv1mA9WRicXNWuI64=&quot;&gt;AAACVHicbVBdTxNBFJ1dRKEiAj7ysrGY8NTsVo0+4seDj5jYQsIUMnt7Syedj83MXaSZ7P/gVX4Uif/FB2dLE7V4kklOzj33Y05ZKekpz38m6dqj9cdPNjY7T7eebT/f2d0bels7wAFYZd1pKTwqaXBAkhSeVg6FLhWelLNPbf3kCp2X1nyjeYUjLS6NnEgQFKVzrgVNnQ4fPg/P+83FTjfv5QtkD0mxJF22xPHFbpLzsYVaoyFQwvuzIq9oFIQjCQqbDq89VgJm4hLPIjVCox+FxdlN9ioq42xiXXyGsoX6d0cQ2vu5LqOzPdOv1lrxv7VSr2ymyftRkKaqCQ3cL57UKiObtZlkY+kQSM0jEeBkvD2DqXACKCbX4Qa/g9VamHHgANJBE/gMncl7b/GaX0H8PLrAp6W9DgfcxwkVeZor5K35oGn+uJtOzLhYTfQhGfZ7xete/+ub7tHHZdobbJ+9ZIesYO/YEfvCjtmAAXPshv1gt8ld8itdS9fvrWmy7HnB/kG6/RvBGrVk&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;lFxypesjedtwNDbnseQwEPV2zGw=&quot;&gt;AAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==&lt;/latexit&gt;</p>
    <p>RTSA &lt;latexit sha1_base64=&quot;s3ZcRV6L160OV+7FOgU8N3JLhfg=&quot;&gt;AAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WR&lt;/latexit&gt;</p>
    <p>RTS &lt;latexit sha1_base64=&quot;lFxypesjedtwNDbnseQwEPV2zGw=&quot;&gt;AAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==&lt;/latexit&gt;</p>
    <p>RTSA &lt;latexit sha1_base64=&quot;s3ZcRV6L160OV+7FOgU8N3JLhfg=&quot;&gt;AAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WR&lt;/latexit&gt;</p>
    <p>the DNNs behavior change, by generating highly contrastive maps. This sensitivity is also quantitatively confirmed by the L1 distance between the clean and noisy attribution maps.</p>
    <p>R T S</p>
    <p>&lt;latexit sha1_base64=&quot;96Kp6KWXsT6/Gx6bjlx6ge0kIMI=&quot;&gt;AAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==&lt;/latexit&gt;</p>
    <p>Figure 16: Attribution maps of benign and adversarial (ADV2) inputs on RTS and RTSA.</p>
    <p>In the second case, we assess the resilience of RTSA against ADV2. In Figure ??, we compare the attribution maps of benign and adversarial inputs on RTS and RTSA. It is observed that while ADV2 generates adversarial inputs with interpretations fairly similar to benign cases on RTS, it fails to do so on RTSA: the maps of adversarial inputs are fairly distinguishable from their benign counterparts. Moreover, RTSA behaves almost identically to RTS on benign inputs, indicating that the AID training has little impact on benign cases. These observations are confirmed by the L1 measures as well.</p>
    <p>RTS RTSA</p>
    <p>Benign 0.03 ADV2 0.01 0.10</p>
    <p>Table 10. Comparison of AID and ADV2 with corresponded benign maps, measured by L1 distance.</p>
    <p>Overall we have the following conclusion.</p>
    <p>Observation 8</p>
    <p>It is possible to exploit ADV2 to reduce the predictioninterpretation gap in training interpreters.</p>
    <p>vant to this work, namely, adversarial attacks and defenses, transferability, and interpretability.</p>
    <p>Attacks and Defenses  Due to their widespread use in security-critical domains, machine learning models are increasingly becoming the targets of malicious attacks [?]. Two primary threat models are considered in literature. Poisoning attacks  the adversary pollutes the training data to eventually compromise the target models [?, ?, ?]; Evasion attacks  the adversary manipulates the input data during inference to trigger target models to misbehave [?, ?, ?].</p>
    <p>Compared with simple models (e.g., support vector machines), securing deep neural networks (DNNs) in adversarial settings entails more challenges due to their significantly higher model complexity [?]. One line of work focuses on developing new evasion attacks against DNNs [?, ?, ?, ?, ?].</p>
    <p>Another line of work attempts to improve DNN resilience against such attacks by inventing new training and inference strategies [?, ?, ?, ?]. Yet, such defenses are often circumvented by more powerful attacks [?] or adaptively engineered adversarial inputs [?, ?], resulting in a constant arms race between attackers and defenders [?].</p>
    <p>This work is among the first to explore attacks against DNNs with interpretability as a means of defense.</p>
    <p>Transferability  One intriguing property of adversarial attacks is their transferability [?]: adversarial inputs crafted against one DNN is often effective against another one. This property enables black-box attacks: the adversary generates adversarial inputs based on a surrogate DNN and apply them on the target model [?, ?, ?]. To defend against such attacks, the method of ensemble adversarial training [?] has been proposed, which trains DNNs using data augmented with adversarial inputs crafted on other models.</p>
    <p>This work complements this line of work by investigating the transferability of adversarial inputs across different interpretation models.</p>
    <p>Interpretability  A plethora of interpretation models have been proposed to provide interpretability for black-box DNNs, using techniques based on back-propagation [?, ?, ?], intermediate representations [?, ?, ?], input perturbation [?], and meta models [?].</p>
    <p>The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Existing work has exploited interpretability to debug DNNs [?], digest security analysis results [?], and detect adversarial inputs [?, ?]. Intuitively, as adversarial inputs cause unexpected DNN behaviors, the interpretation of DNN dynamics is expected to differ significantly between benign and adversarial inputs.</p>
    <p>However, recent work empirically shows that some interpretation models seem insensitive to either DNNs or data generation processes [?], while transformation with no effect on DNNs (e.g., constant shift) may significantly affect the behaviors of interpretation models [?].</p>
    <p>This work shows the possibility of deceiving DNNs and their coupled interpretation models simultaneously, implying that the improved interpretability only provides limited security assurance, which also complements prior work by examining the reliability of existing interpretation models from the perspective of adversarial vulnerability.</p>
    <p>of interpretable deep learning systems (IDLSes). We present ADV2, a general class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through extensive empirical evaluation, we show the effectiveness of ADV2 against a range of DNNs and interpretation models, implying that the interpretability of existing IDLSes may merely offer a false sense</p>
    <p>L1 measures &lt;latexit sha1_base64=&quot;qcjCSPy16smkXN/8tc8k+NkIMEI=&quot;&gt;AAACa3icbVDLbhMxFHWmPEp4tKU7YGGRVmKBopnSCpYV3bBgUSTSVqqj6M7NTWPFj5HtKY2s+YB+DVv4lH4E/4AnjQSkXMvS0bnnvk5ZKelDnt90srV79x88XH/Uffzk6bONza3nJ97WDmmAVll3VoInJQ0NggyKzipHoEtFp+XsqM2fXpLz0pqvYV7RUMOFkROJEBI12uwJDWGKoOLnZlRw8Ta9lnE6agJfO/JNUuX9fBH8LiiWoMeWcTza6uRibLHWZAIq8P68yKswjOCCREVNV9SeKsAZXNB5ggY0+WFcXNPw3cSM+cS69E3gC/bvigja+7kuk7Ld1K/mWvK/uVKvTA6TD8MoTVUHMng7eFIrHixvreJj6QiDmicA6GTaneMUHGBIhnaFoW9otQYzjgJROmyimJEzef+ArsQlpuPJRTEt7VXcET51qIIPc0WiFe80zR91000eF6uO3gUne/3iXX/vy37v8OPS7XX2kr1mb1jB3rND9okdswFDds2+sx/sZ+dXtp29yF7dSrPOsmab/RPZ7m9pEr2c&lt;/latexit&gt;</p>
    <p>Figure 16: Attribution maps of benign and adversarial (ADV2) inputs on RTS and RTSA.</p>
    <p>In the second case, we assess the resilience of RTSA against ADV2. In Figure 16, we compare the attribution maps of benign and adversarial inputs on RTS and RTSA. It is observed that while ADV2 generates adversarial inputs with interpretations fairly similar to benign cases on RTS, it fails to do so on RTSA: the maps of adversarial inputs are fairly distinguishable from their benign counterparts. Moreover, RTSA behaves almost identically to RTS on benign inputs, indicating that the AID training has little impact on benign cases. These observations are confirmed by the L1 measures as well.</p>
    <p>RTS RTSA</p>
    <p>Benign 0.03 ADV2 0.01 0.10</p>
    <p>Table 10. Comparison of AID and ADV2 with corresponded benign maps, measured by L1 distance.</p>
    <p>Overall we have the following conclusion. Observation 8</p>
    <p>It is possible to exploit ADV2 to reduce the predictioninterpretation gap in training interpreters.</p>
    <p>vant to this work, namely, adversarial attacks and defenses, transferability, and interpretability.</p>
    <p>Attacks and Defenses  Due to their widespread use in security-critical domains, machine learning models are increasingly becoming the targets of malicious attacks [9]. Two primary threat models are considered in literature. Poisoning attacks  the adversary pollutes the training data to eventually compromise the target models [8, 76, 46]; Evasion attacks  the adversary manipulates the input data during inference to trigger target models to misbehave [16, 40, 49].</p>
    <p>Compared with simple models (e.g., support vector machines), securing deep neural networks (DNNs) in adversarial settings entails more challenges due to their significantly higher model complexity [35]. One line of work focuses on developing new evasion attacks against DNNs [71, 24, 54,</p>
    <p>This work is among the first to explore attacks against DNNs with interpretability as a means of defense.</p>
    <p>Transferability  One intriguing property of adversarial attacks is their transferability [71]: adversarial inputs crafted against one DNN is often effective against another one. This property enables black-box attacks: the adversary generates adversarial inputs based on a surrogate DNN and apply them on the target model [51, 14, 39]. To defend against such attacks, the method of ensemble adversarial training [73] has been proposed, which trains DNNs using data augmented with adversarial inputs crafted on other models.</p>
    <p>This work complements this line of work by investigating the transferability of adversarial inputs across different interpretation models.</p>
    <p>Interpretability  A plethora of interpretation models have been proposed to provide interpretability for black-box DNNs, using techniques based on back-propagation [63, 66, 67], intermediate representations [80, 60, 19], input perturbation [21], and meta models [15].</p>
    <p>The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Existing work has exploited interpretability to debug DNNs [50], digest security analysis results [25], and detect adversarial inputs [38, 72]. Intuitively, as adversarial inputs cause unexpected DNN behaviors, the interpretation of DNN dynamics is expected to differ significantly between benign and adversarial inputs.</p>
    <p>However, recent work empirically shows that some interpretation models seem insensitive to either DNNs or data generation processes [1], while transformation with no effect on DNNs (e.g., constant shift) may significantly affect the behaviors of interpretation models [33].</p>
    <p>This work shows the possibility of deceiving DNNs and their coupled interpretation models simultaneously, implying that the improved interpretability only provides limited security assurance, which also complements prior work by examining the reliability of existing interpretation models from the perspective of adversarial vulnerability.</p>
    <p>of interpretable deep learning systems (IDLSes). We present ADV2, a general class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through extensive empirical evaluation, we show the effectiveness of ADV2 against a range of DNNs and interpretation models, implying that the interpretability of existing IDLSes may merely offer a false sense</p>
  </div>
  <div class="page">
    <p>Key Findings Finding 1</p>
    <p>The interpretability of existing interpretable deep learning systems merely provides limited security assurance.</p>
    <p>Finding 2</p>
    <p>The prediction-interpretation gap is one possible cause that the adversary is able to exploit both classifier and interpreter simultaneously.</p>
    <p>Finding 3</p>
    <p>Adversarial training aiming to minimize the prediction-interpretation gap potentially improves the robustness of interpreters.</p>
    <p>!13</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Please direct your questions to zxydi1992@hotmail.com</p>
  </div>
</Presentation>
