<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The 15th USENIX Conference on File and Storage Technologies (FAST17)</p>
    <p>DIDACache: A Deep Integra0on of Device and Applica0on for Flash based Key-value Caching</p>
    <p>Department of Computing, Hong Kong Polytechnic University Computer Science &amp; Engineering, Louisiana State University</p>
    <p>Zhaoyan Shen, Feng Chen, Yichen Jia, Zili Shao</p>
  </div>
  <div class="page">
    <p>Key-value Informa0on</p>
    <p>Key-value cache is the first line of defense  Benefits: improve throughput, reduce latency, reduce server load</p>
    <p>Flash based key-value cache: McDipper, Fatcache</p>
    <p>In-memory KV cache  High access speed  High power consump0on  High monetary cost  Capacity limita0on</p>
    <p>hit miss</p>
    <p>Browser</p>
    <p>Applica0on Server</p>
    <p>RDBMS RDBMS memcached memcached memcached memcached</p>
  </div>
  <div class="page">
    <p>Flash based Key-value Cache</p>
    <p>Current Prac0ce: Directly use commercial SSD as caching media</p>
    <p>Hash(key)</p>
    <p>Flash SSD</p>
    <p>KV Index KV Index</p>
    <p>md slab slot</p>
    <p>Sequen0al logical space</p>
    <p>Bucket</p>
    <p>Key-value Slabs (Flash LBA)</p>
    <p>Slab Slab</p>
    <p>Slab Slab</p>
    <p>Key-value Slabs (Flash LBA)</p>
    <p>In-memory hash table</p>
    <p>Log-structured slabs</p>
    <p>Out-of-place update</p>
  </div>
  <div class="page">
    <p>Research Issues</p>
    <p>Applica0on level  Key-value mapping: keyslab  Slab-level GC (item granularity)  Cache management</p>
    <p>Flash SSD (Hardware level) Page</p>
    <p>Mapping Garbage Collec0on</p>
    <p>Bad Block Mgm. Wear Leveling</p>
    <p>OPS Mgm.</p>
    <p>Others</p>
    <p>Opera0ng Systems</p>
    <p>Page Cache Device Driver</p>
    <p>I/O Scheduling</p>
    <p>Key-value Manager (Applica0on level)</p>
    <p>K/V Mapping Slab GC Cache Mgm.</p>
    <p>Slab Alloc. others</p>
    <p>K/V Mapping Slab GC</p>
    <p>Page Mapping</p>
    <p>Garbage Collec0on</p>
    <p>OPS Mgm.</p>
    <p>Cache Mgm.</p>
    <p>Seman0c Gap</p>
    <p>Hardware level  Page mapping  Flash page level GC  OPS management</p>
  </div>
  <div class="page">
    <p>Open-channel SSD</p>
    <p>Architecture &amp; IO Stack</p>
    <p>[1] Ouyang Jian, et al., SDF: SoYware-Defined Flash for Web-Scale Internet Storage Systems (ASPLOS14) 5</p>
    <p>SSD Controller</p>
    <p>Flash CH_0</p>
    <p>Flash CH_0</p>
    <p>Flash CH_0</p>
    <p>dev/sda</p>
    <p>Flash CH_0 Flash CH_0</p>
    <p>Flash CH_0 Flash CH_0</p>
    <p>Flash CH_0 Flash CH_0</p>
    <p>SSD Ctrl SSD Ctrl SSD Ctrl</p>
    <p>dev/sda0 ~ /dev/sdaN</p>
    <p>(a) System Architecture</p>
    <p>Conventional SSD Soft-Defined Flash</p>
    <p>read/write read/write/erase File System Block Device</p>
    <p>Page Cache</p>
    <p>SATA and SAS Translation</p>
    <p>SCSI Mid-Layer</p>
    <p>Low Level Device Driver</p>
    <p>Conventional SSD PCIE</p>
    <p>IO Scheduler Generic Block Layer</p>
    <p>VFS</p>
    <p>Direct IO Buffered IO</p>
    <p>User Space</p>
    <p>PCIE Driver</p>
    <p>SDF</p>
    <p>User Space</p>
    <p>Kernel Space IOCTRL</p>
    <p>(b) IO Stack</p>
    <p>Open-channel SSD provides us unprecedented new opportuni0es.</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Flash-based key-value cache  Fusion-IO and Memcached: more cache, less headache  SSD-assisted hybrid memory to accelerate Memcached over high performance</p>
    <p>networks (ICPP12)</p>
    <p>Open-channel SSD  SDF: SoYware-Defined Flash for Web-Scale Internet Storage Systems (ASPLOS14)  LOCS: An Efficient Design and Implementa0on of LSM-Tree based Key-Value Store on</p>
    <p>Open-Channel SSD (Eurosys14)  NVMKV: A Scalable, Lightweight, FTL-aware Key-Value Store (ATC15)</p>
    <p>Key-value caching + Open-channel SSD  Op0mizing flash-based key-value cache systems (Hotstorage16)</p>
  </div>
  <div class="page">
    <p>DIDACache: An Enhanced Flash-aware Key-value Cache</p>
    <p>Direct applica0on driven  Fully exploit applica0on seman0cs</p>
    <p>Hardware design simplified</p>
    <p>Non-essen0al components removed  Seman0c gap issue mi0gated</p>
    <p>A 0ght applica0on-device connec0on</p>
    <p>Flash SSD Page</p>
    <p>Mapping Garbage Collec0on</p>
    <p>Bad Block Manager Wear Leveling</p>
    <p>OPS Manager</p>
    <p>Others</p>
    <p>Opera0ng Systems</p>
    <p>Page Cache Device Driver</p>
    <p>I/O Scheduling</p>
    <p>Key-value Manager</p>
    <p>K/V Mapping Slab GC Cache</p>
    <p>Manager Slab</p>
    <p>Alloca0on others</p>
    <p>K/V Mapping Slab GC Cache</p>
    <p>Manager</p>
    <p>Page Mapping</p>
    <p>Garbage Collec0on OPS Manager Open-channel SSD</p>
    <p>Flash Opera0ons</p>
    <p>Opera0ng Systems</p>
    <p>Page Cache Device Driver</p>
    <p>I/O Scheduling</p>
    <p>Key-value Manager</p>
    <p>OPS Management</p>
    <p>Cache Manager Others</p>
    <p>Slab Manager Key/Slab Mapping</p>
    <p>Integrated GC</p>
  </div>
  <div class="page">
    <p>DIDACache: An Enhanced Flash-aware Key-value Cache</p>
    <p>Slab management  Unified direct mapping  Garbage collec0on  OPS management</p>
  </div>
  <div class="page">
    <p>Open-channel SSD</p>
    <p>Channel #1</p>
    <p>Slab Management: Slab buffer</p>
    <p>Merge small requests  Organize big log-like writes</p>
    <p>Asynchronized requests</p>
    <p>Hide I/Os from cri0cal path  Improve access speed</p>
    <p>Immediate return</p>
    <p>Flush Set(key)</p>
    <p>Slab buffer</p>
    <p>Channel #4 Channel #3</p>
    <p>Channel #2</p>
  </div>
  <div class="page">
    <p>Open-channel SSD</p>
    <p>Channel #1</p>
    <p>Channel #4 Channel #3</p>
    <p>Channel #2</p>
    <p>Slab Management: Slab-to-Channel Mapping</p>
    <p>Flush Set(key)</p>
    <p>Slab buffer</p>
    <p>Advantage:  Internal parallelism u0lized</p>
    <p>Disadvantages  Complex mapping/space management  Small chunks  Sub-block wri0ng/GC  Large chunks  Bad block, too big slab</p>
    <p>Cross-channel mapping:  Slab sliced to chunks  Stripe chunks to channels</p>
  </div>
  <div class="page">
    <p>Open-channel SSD</p>
    <p>Channel #1</p>
    <p>Channel #4 Channel #3</p>
    <p>Channel #2</p>
    <p>Slab Management: Slab-to-Channel Mapping</p>
    <p>Flush Set(key)</p>
    <p>Slab buffer</p>
    <p>Advantage:  No need of mapping structure  Transfer is efficient</p>
    <p>Per-channel mapping:  Slab size equals to one flash block  Sta0c map a slab to one block</p>
  </div>
  <div class="page">
    <p>Slab Management: Simplified Mapping</p>
    <p>Open-channel SSD</p>
    <p>Channel #1</p>
    <p>Channel #4 Channel #3</p>
    <p>Channel #2 Flush</p>
    <p>Set(key)</p>
    <p>Slab buffer</p>
    <p>KV Index</p>
    <p>md slab slot</p>
    <p>hash(key)</p>
    <p>Unified mapping structure:  Direct key-to flash mapping</p>
    <p>Advantages:  Eliminate intermediate layer  Reduce DRAM consump0on</p>
  </div>
  <div class="page">
    <p>Integrated Garbage Collec0on</p>
    <p>KV Item KV Item KV Item KV Item</p>
    <p>App level (Key-value item granularity)</p>
    <p>Device level (Flash page granularity)</p>
    <p>KV copy Slab Erase</p>
    <p>Vic0m Slab Target Slab</p>
    <p>Target Block Vic0m Block</p>
    <p>Flash page</p>
    <p>Flash page</p>
    <p>Flash page</p>
    <p>Flash page</p>
    <p>Block Erase</p>
    <p>page copy</p>
    <p>KV Item</p>
    <p>Flash page</p>
    <p>Double garbage collec0on problem</p>
    <p>Double GC processes at two levels  Run simultaneously and independently  Run with different granularity</p>
    <p>Problems of double GC  No coordina0on  Redundant data copy</p>
  </div>
  <div class="page">
    <p>KV Item KV Item</p>
    <p>KV Item KV Item</p>
    <p>KV Item KV Item</p>
    <p>KV Item KV Item</p>
    <p>Integrated Garbage Collec0on</p>
    <p>All writes in unit of flash blocks  Remove unnecessary device-level GC  Applica0on-driven fine-grained GC</p>
    <p>Vic0m Slab (Block)</p>
    <p>KV copy</p>
    <p>Slab Buffer</p>
    <p>Block Erase</p>
    <p>KV Item</p>
  </div>
  <div class="page">
    <p>Integrated Garbage Collec0on</p>
    <p>Light traffic: Space-based GC  Op0mize for high hit ra0o  Select the block with the most invalid items  Copy valid items and erase the slab</p>
    <p>Heavy traffic: Locality-based GC  Op0mize for low response 0me  Select the LRU block as the vic0m  Erase the en0re slab without item copy</p>
    <p>Fr ee s la bs (%</p>
    <p>)</p>
    <p>Time</p>
    <p>No GC</p>
    <p>Space-based GC</p>
    <p>Locality-based GC</p>
    <p>GC is a 0me consuming process (key-value copy and block erase)  Goal: retain high key-value cache hit ra0o and low latency</p>
  </div>
  <div class="page">
    <p>Integrated Garbage Collec0on</p>
    <p>Garbage collec0on overhead  DIDACache makes 86.6 % less key-value copies  DIDACache erases 30% less flash blocks on device</p>
    <p>DIDACache Fatcache</p>
    <p>Ke y- Va</p>
    <p>lu e Co</p>
    <p>py (G B)</p>
    <p>DIDACache Fatcache</p>
    <p>Bl oc k Er as e</p>
    <p>Key-Value item copy Block erase count</p>
    <p>* DIDACache: directly collect GC overhead, Fatcache: blktrace+ssdsim</p>
  </div>
  <div class="page">
    <p>Over-Provisioning Space Management</p>
    <p>OPS is a large (20-30%) reserved space for handling intensive writes  Goal: maximize the usable flash space for caching and keep just enough OPS</p>
    <p>Se t r eq</p>
    <p>ue st r at e</p>
    <p>Time</p>
    <p>Fr ee s la bs (%</p>
    <p>)</p>
    <p>OPS</p>
    <p>Over-Kill</p>
    <p>SSD is used as cache, not storage  Workload for Key-value cache is read intensive  20-30% OPS is an unnecessary over-kill</p>
    <p>Disadvantage of sta0c OPS  OPS not usable for key-value caching  Low hit ra0o with too large OPS</p>
  </div>
  <div class="page">
    <p>Over-Provisioning Space Management</p>
    <p>Queuing theory based OPS es0ma0on  Drain process: a Markov process with rate  ( )  GC process: a Markov process with rate ( )  Liples law:</p>
    <p>Drain process GC process</p>
    <p>Flush</p>
    <p>Evict</p>
    <p>OPS =  / ( )</p>
    <p>= KV SKV Sslab</p>
    <p>= 1</p>
    <p>tevict + tother</p>
  </div>
  <div class="page">
    <p>Sta0c policy</p>
    <p>S la</p>
    <p>b N</p>
    <p>um be</p>
    <p>r( %</p>
    <p>)</p>
    <p>Time</p>
    <p>OPS Free Slab Number</p>
    <p>R eq</p>
    <p>ue st</p>
    <p>R at</p>
    <p>e( 10</p>
    <p>^4 )</p>
    <p>Time</p>
    <p>Request Rate</p>
    <p>OPS Free Slab Number</p>
    <p>S la</p>
    <p>b N</p>
    <p>um be</p>
    <p>r( %</p>
    <p>)</p>
    <p>Time</p>
    <p>R eq</p>
    <p>ue st</p>
    <p>R at</p>
    <p>e( 10</p>
    <p>^4 )</p>
    <p>Time</p>
    <p>Request Rate</p>
    <p>Over-Provisioning Space Management</p>
    <p>Over-provisioning space with different policies</p>
  </div>
  <div class="page">
    <p>Over-Provisioning Space Management</p>
    <p>DIDACache improves hit ra0o with dynamic OPS management</p>
    <p>Time</p>
    <p>Sta0c policy Queuing Theory</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Implementa0on  Key-value cache on Twipers Fatcache to fit hardware  Schemes: Fatcache-Sync, Fatcache-Async[1], DIDACache</p>
    <p>Experimental Setup  Intel Xeon E-1225, 32GB Memory, 1TB Disk  Ubuntu 14.04 LTS, Linux 3.17.8, Ext4 filesystem  Database: MySQL 5.5  Workload: truncated Generalized Pareto distribu0on</p>
    <p>Storage  Open-channel SSD:</p>
    <p>A PCI-E based with 12 channel, and 192 LUNs  Direct control to the device (via ioctl interface)</p>
    <p>A conven0onal SSD with the same hardware configura0on</p>
    <p>[1] Fatcache-Async. hXps://github.com/polyu-szy/Fatcache-Async-2017 21</p>
  </div>
  <div class="page">
    <p>Fatcache-Sync Fatcache-Async DIDACache</p>
    <p>Cache Size(%)</p>
    <p>H it Ra</p>
    <p>)</p>
    <p>Overall Performance in a Data-center Environment</p>
    <p>MySQL + Key-value Cache + Client</p>
    <p>* A data-center environment running with 250GB MySQL database, 8 key-value caching servers and 32 clients 22</p>
    <p>As the cache size increases, all throughput improves substan0ally  DIDACache has the highest throughput among all the three cases</p>
    <p>Fatcache-Sync Fatcache-Async DIDACache</p>
    <p>Cache Size(%)</p>
    <p>Th ro ug hp</p>
    <p>ut (o ps /s ec )</p>
  </div>
  <div class="page">
    <p>Cache Server Performance</p>
    <p>Key-value Cache + Client: set opera0on</p>
    <p>* Directly SET 50GB key-value items (ranges from 64Bytes to 4KB) to the cache servers 23</p>
    <p>DIDACahce achieves the highest throughput and lowest latency.  With the item size of 64 bytes, throughput of DIDACache can be 35.5% higher than</p>
    <p>Fatcache-Async. Latency can be reduced by 23.6%.</p>
    <p>Fatcache-Sync Fatcache-Async DIDACache</p>
    <p>Th ro ug hp</p>
    <p>ut (o ps /s ec )</p>
    <p>KV Size</p>
    <p>Fatcache-Sync Fatcache-Async DIDACache</p>
    <p>KV Size</p>
    <p>La te nc y( us )</p>
  </div>
  <div class="page">
    <p>Cache Server Performance</p>
    <p>Key-value Cache + Client: mixed set/get opera0on</p>
    <p>* Mixed set/get opera0ons with key-value items of size 256bytes. 24</p>
    <p>DIDACache outperforms Fatcache-Sync and Fatcache-Async across the board.  As the ra0o of GET opera0ons increases, the related performance gain reduces.</p>
    <p>Fatcache-Sync Fatcache-Async DIDACache</p>
    <p>Set/Get Ra0o</p>
    <p>La te nc y( us )</p>
    <p>Fatcache-Sync Fatcache-Async DIDACache</p>
    <p>Set/Get Ra0o</p>
    <p>Th ro ug hp</p>
    <p>ut (o ps /s ec )</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>DIDACache deeply integrates the key-value cache system design with the Open-Channel SSD hardware.</p>
    <p>The prototype based on the Open-Channel SSD hardware shows that our approach can improve system performance significantly.</p>
  </div>
  <div class="page">
    <p>The 15th USENIX Conference on File and Storage Technologies (FAST17)</p>
    <p>Thank You ! Q&amp;A</p>
  </div>
</Presentation>
