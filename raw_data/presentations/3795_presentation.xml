<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Results of the fifth edition of the BioASQ Challenge</p>
    <p>A. Nentidis, K. Bougiatiotis, A. Krithara, G. Paliouras and I. Kakadiaris</p>
    <p>NCSR Demokritos, University of Houston</p>
    <p>BioNLP Workshop, Vancouver</p>
  </div>
  <div class="page">
    <p>Introduction What is BioASQ</p>
    <p>A competition</p>
    <p>I BioASQ is a series of challenges on biomedical semantic indexing and question answering (QA).</p>
    <p>I Participants are required to semantically index content from large-scale biomedical resources (e.g. MEDLINE) and/or</p>
    <p>I to assemble data from multiple heterogeneous sources (e.g. scientific articles, knowledge bases, databases)</p>
    <p>I to compose informative answers to biomedical natural language questions.</p>
  </div>
  <div class="page">
    <p>Presentation of the challenge Tasks</p>
    <p>Task A: Hierarchical text classification</p>
    <p>I Organizers distribute new unclassified MEDLINE articles. I Participants have 21 hours to assign MeSH terms to the articles. I Evaluation based on annotations of MEDLINE curators.</p>
    <p>Fe br</p>
    <p>ua ry</p>
    <p>Fe br</p>
    <p>ua ry</p>
    <p>Fe br</p>
    <p>ua ry</p>
    <p>M ar</p>
    <p>ch 1</p>
    <p>M ar</p>
    <p>ch 06</p>
    <p>M ar</p>
    <p>ch 13</p>
    <p>M ar</p>
    <p>ch 20</p>
    <p>M ar</p>
    <p>ch 27</p>
    <p>Ap ril</p>
    <p>Ap ril</p>
    <p>Ap ril</p>
    <p>M ay</p>
    <p>M ay</p>
    <p>M ay</p>
    <p>M ay</p>
  </div>
  <div class="page">
    <p>Presentation of the challenge Tasks</p>
    <p>Task B: IR, QA, summarization</p>
    <p>I Organizers distribute English biomedical questions. I Participants have 24 hours to provide: relevant articles,</p>
    <p>snippets, concepts, triples, exact answers, ideal answers. I Evaluation: both automatic (GMAP, MRR, Rouge etc.) and</p>
    <p>manual (by biomedical experts).</p>
    <p>M ar</p>
    <p>ch 08</p>
    <p>M ar</p>
    <p>ch 09</p>
    <p>M ar</p>
    <p>ch 22</p>
    <p>M ar</p>
    <p>ch 23</p>
    <p>Ap ril</p>
    <p>Ap ril</p>
    <p>Ap ril</p>
    <p>Ap ril</p>
    <p>ay 3</p>
    <p>M ay</p>
    <p>Phase A Phase B</p>
  </div>
  <div class="page">
    <p>Presentation of the challenge New task</p>
    <p>Task C: Funding Information Extraction</p>
    <p>I Organizers distribute PMC full-text articles. I Participants have 48 hours to extract: grant-IDs, funding</p>
    <p>agencies, full grants (i.e. the combination of a grant-ID and the corresponding funding agency).</p>
    <p>I Evaluation based on annotations of MEDLINE curators.</p>
    <p>Ap ril</p>
    <p>Ap ril</p>
    <p>Dry Run Test Batch</p>
  </div>
  <div class="page">
    <p>Presentation of the challenge BioASQ ecosystem</p>
  </div>
  <div class="page">
    <p>Presentation of the challenge BioASQ ecosystem</p>
  </div>
  <div class="page">
    <p>Presentation of the challenge Per task</p>
  </div>
  <div class="page">
    <p>Task 5A Hierarchical text classification</p>
    <p>I Training data</p>
    <p>version 2015 version 2016 version 2017 Articles 11,804,715 12,208,342 12,834,585 Total labels 27,097 27,301 27,773 Labels per article 12.61 12.62 12.66 Size in GB 19 19.4 20.5</p>
    <p>I Test data Week Batch 1 Batch 2 Batch 3</p>
    <p>Total 40,119 (34,272) 33,162 (30,934) 42,435 ( 21,323)</p>
    <p>The numbers in parentheses are the annotated articles for each test dataset.</p>
  </div>
  <div class="page">
    <p>Task 5A System approaches</p>
    <p>I Feature Extraction: Representing each abstract I tf-idf of words and bi-words I doc2vec embeddings of paragraphs</p>
    <p>I Concept Matching: Finding relevant MeSH labels I k-NN between article-vector representations I Linear SVM binary classifiers for each MESH label I Recurrent Neural Networks for sequence-to-sequence prediction I UIMA-ConceptMapper and MeSHLabeler tools for boosting NER</p>
    <p>and Entity-to-MeSH matching I Latend Dirichlet Allocation and Labeled LDA utilizing topics found</p>
    <p>in abstracts I Ensemble methodologies and stacking</p>
  </div>
  <div class="page">
    <p>Task 5A Evaluation Measures</p>
    <p>Flat measures</p>
    <p>I Accuracy (Acc.) I Example Based Precision (EBP) I Example Based Recall (EBR) I Example Based F-Measure (EBF) I Macro Precision/Recall/F-Measure</p>
    <p>(MaP, MaR,MaF) I Micro Precision/Recall/F-Measure</p>
    <p>(MiP,MIR,MiF)</p>
    <p>Hierarchical measures</p>
    <p>I Hierarchical Precision (HiP) I Hierarchical Recall (HiR) I Hierarchical F-Measure (HiF) I Lowest Common Ancestor Precision</p>
    <p>(LCA-P) I Lowest Common Ancestor Recall (LCA-R) I Lowest Common Ancestor F-measure</p>
    <p>(LCA-F)</p>
    <p>A. Kosmopoulos, I. Partalas, E. Gaussier, G. Paliouras and I. Androutsopoulos: Evaluation Measures for Hierarchical Classification: a unified view and novel approaches. Data Mining and Knowledge Discovery, 29:820-865, 2015.</p>
  </div>
  <div class="page">
    <p>Task 5A results Evaluation</p>
    <p>I Systems ranked using MiF (flat) and LCA-F (hierarchical). I Results, in all batches and for both measures :</p>
  </div>
  <div class="page">
    <p>Task 5A results</p>
  </div>
  <div class="page">
    <p>Task 5B Statistics on datasets</p>
    <p>Batch Size # of documents # of snippets Training 1,799 11.86 20.38 Test 1 100 4.87 6.03 Test 2 100 3.49 5.13 Test 3 100 4.03 5.47 Test 4 100 3.23 4.52 Test 5 100 3.61 5.01 total 2,299</p>
    <p>The numbers for the documents and snippets refer to averages</p>
  </div>
  <div class="page">
    <p>Task 5B Training Dataset Insights</p>
    <p>I 1799 Questions I 500 yes/no I 486 factoid I 413 list I 400 summary</p>
    <p>I 13 Experts I  3450 unique</p>
    <p>biomedical concepts</p>
    <p>A ve</p>
    <p>ra ge</p>
    <p>of ite</p>
    <p>m s</p>
    <p>pe r</p>
    <p>qu es</p>
    <p>tio n</p>
    <p>Concepts Documents</p>
    <p>Snippets</p>
  </div>
  <div class="page">
    <p>Task 5B Training Dataset Insights</p>
    <p>I Broad terms (e.g. proteins, syndromes) I More specific terms (e.g. cancer, heart, thyroid)</p>
  </div>
  <div class="page">
    <p>Task 5B Training Dataset Insights</p>
    <p>I Number of questions related to cancer vs thyroid per year I The numbers on top of the bars denote the contributing experts</p>
  </div>
  <div class="page">
    <p>Task 5B Evaluation measures</p>
    <p>I Evaluating Phase A (IR)</p>
    <p>Retrieved items Unordered retrieval measures Ordered retrieval measures</p>
    <p>concepts</p>
    <p>Mean Precision, Recall, F-Measure MAP, GMAP articles</p>
    <p>snippets</p>
    <p>triples I Evaluating the exact answers for Phase B (Traditional QA)</p>
    <p>Question type Participant response Evaluation measures</p>
    <p>yes/no yes or no Accuracy</p>
    <p>factoid up to 5 entity names strict and lenient accuracy, MRR</p>
    <p>list a list of entity names Mean Precision, Recall, F-measure I Evaluating the ideal answers for Phase B (Query-focused Summarization)</p>
    <p>Question type Participant response Evaluation measures</p>
    <p>any paragraph-sized text ROUGE-2, ROUGE-SU4, manual scores*</p>
    <p>(Readability, Recall, Precision, Repetition)</p>
    <p>*with the help of BioASQ Assessment tool.</p>
  </div>
  <div class="page">
    <p>Task 5B System approaches</p>
    <p>I Question analysis: Rule-based, regular expressions, ClearNLP, Semantic role labeling (SRL), Stanford Parser, tf-idf, SVD, word embeddings.</p>
    <p>I Query expansion: MetaMap, UMLS, sequential dependence models, ensembles, LingPipe.</p>
    <p>I Document retrieval: BM25, UMLS, SAP HANA database, Bag of Concepts (BoC), statistical language model.</p>
    <p>I Snippet selection: Agglomerative Clustering, Maximum Marginal Relevance, tf-idf, word embeddings.</p>
    <p>I Exact answer generation: Standford POS, PubTator, FastQA, SQuAD, Semantic role labeling (SRL), word frequencies, word embeddings, dictionaries, UMLS.</p>
    <p>I Ideal answer generation: Deep learning (LSTM, CNN, RNN), neural nets, Support Vector Regression.</p>
    <p>I Answer ranking: Word frequencies.</p>
  </div>
  <div class="page">
    <p>Task 5B Results</p>
    <p>I Our experts are currently assessing systems responses I The results will be announced in autumn</p>
  </div>
  <div class="page">
    <p>Task 5C Statistics on datasets</p>
    <p>Training Test Articles 62,952 22,610</p>
    <p>Grant IDs 111,528 42,711 Agencies 128,329 47,266</p>
    <p>Time Period 2005-13 2015-17</p>
    <p>I 104 unique agencies I 92,437 unique grant IDs</p>
  </div>
  <div class="page">
    <p>Task 5C Statistics on datasets</p>
    <p>Number of articles per agency in training dataset</p>
  </div>
  <div class="page">
    <p>Task 5C Evaluation measures</p>
    <p>I A subset of the Grant IDs and Agencies mentioned in full text are available in ground truth data Micro-Recall</p>
    <p>I Each Grant ID (or lone Agency) must exist verbatim in the text I Different scores for each subtask:</p>
    <p>I Grant IDs I Agencies I Full Grants</p>
  </div>
  <div class="page">
    <p>Task 5C System approaches</p>
    <p>I Grant Support Sentences: Identifying sentences containing grant information</p>
    <p>I Features: tf-idf of n-grams I Techniques: SVM and Naive Bayes for scoring, specific XML fields</p>
    <p>considered I Grant Information Extraction: Detecing Grant-IDs and</p>
    <p>Agencies I Manually crafted Regular Expressions I Heuristic Rules I Sequential Learning Models, such as Conditional Random Fields,</p>
    <p>Hidden Markov Models, Max Entropy Models I Ensemble of classifiers for pairing Grant-IDs to Agencies</p>
  </div>
  <div class="page">
    <p>Task 5C Results</p>
    <p>Grant-IDs Agencies Full-Grant 0.5</p>
    <p>M ic</p>
    <p>ro -R</p>
    <p>ec al</p>
    <p>l Fudan AUTH DZG</p>
  </div>
  <div class="page">
    <p>Challenge Participation Overall</p>
  </div>
  <div class="page">
    <p>Conclusions and Prespectives</p>
    <p>Goals and perspectives</p>
    <p>I BioASQ will run in 2018. I Continuous development of benchmark datasets.</p>
  </div>
  <div class="page">
    <p>Conclusions and Prespectives Oracle for continuous testing</p>
  </div>
  <div class="page">
    <p>Collaborations</p>
    <p>I NLM I Task A design and baselines I Task C design and baselines</p>
    <p>I CMU I OAQA Baselines for task B</p>
    <p>I DBCLS I BioASQ and PubAnnotation : Using linked</p>
    <p>annotations in biomedical question answering (BLAH3)</p>
    <p>I iASiS I Question answering over big</p>
    <p>heterogeneous biomedical data for precision medicine</p>
  </div>
  <div class="page">
    <p>Grateful to the BioASQ consortium BioASQ started as a European FP7 project, with the following</p>
    <p>partners:</p>
    <p>I National Centre for Scientific Research Demokritos (GR)</p>
    <p>I Transinsight GmbH (DE)</p>
    <p>I Universite Joseph Fourier (FR)</p>
    <p>I University Leipzig (DE)</p>
    <p>I Universite Pierre et Marie Curie Paris 6 (FR)</p>
    <p>I Athens University of Economics and Business Research Centre (GR)</p>
  </div>
  <div class="page">
    <p>Sponsors</p>
    <p>PLATINUM SPONSOR</p>
    <p>SILVER SPONSOR</p>
  </div>
  <div class="page">
    <p>Stay Tuned!</p>
    <p>Visit www.bioasq.org Follow @BioASQ</p>
    <p>BioASQ 6 to be announced soon!</p>
  </div>
</Presentation>
