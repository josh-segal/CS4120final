<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>(Semi-)Supervised Probabilistic</p>
    <p>Principal Component Analysis</p>
    <p>Shipeng Yu University of Munich, Germany Siemens Corporate Technology</p>
    <p>http://www.dbs.ifi.lmu.de/~spyu</p>
    <p>Joint work with Kai Yu, Volker Tresp, Hans-Peter Kriegel, Mingrui Wu</p>
  </div>
  <div class="page">
    <p>Dimensionality Reduction  We are dealing with high-dimensional data</p>
    <p>Texts: e.g. bag-of-words features  Images: color histogram, correlagram, etc.  Web pages: texts, linkages, structures, etc.</p>
    <p>Motivations  Noisy features: how to remove or down-weight them?  Learnability: curse of dimensionality  Inefficiency: high computational cost  Visualization</p>
    <p>A pre-processing for many data mining tasks</p>
  </div>
  <div class="page">
    <p>Unsupervised versus Supervised  Unsupervised Dimensionality Reduction</p>
    <p>Only the input data are given  PCA (principal component analysis)</p>
    <p>Supervised Dimensionality Reduction  Should be biased by the outputs</p>
    <p>Classification: FDA (Fisher discriminant analysis)  Regression: PLS (partial least squares)  RVs: CCA (canonical correlation analysis)</p>
    <p>More general solutions?  Semi-Supervised?</p>
  </div>
  <div class="page">
    <p>Our Settings and Notations  data points, input features, output labels  We aim to derive a mapping such that</p>
    <p>N</p>
    <p>: X 7! V V  RK , K &lt; M</p>
    <p>M L</p>
    <p>x1 ...</p>
    <p>xN 1 xN 1+1</p>
    <p>... xN</p>
    <p>y1 ...</p>
    <p>yN 1</p>
    <p>y1  yLx1  xM</p>
    <p>X</p>
    <p>Y</p>
    <p>Unlabeled Data!</p>
    <p>x1 x2  xM x1 x2 ...</p>
    <p>xN X</p>
    <p>y1 y2 ...</p>
    <p>yN</p>
    <p>x1 x2 ...</p>
    <p>xN</p>
    <p>x1  xM y1  yL</p>
    <p>X Y</p>
    <p>Unsupervised</p>
    <p>Supervised</p>
    <p>Semi-supervised</p>
  </div>
  <div class="page">
    <p>Outline  Principal Component Analysis  Probabilistic PCA  Supervised Probabilistic PCA  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>PCA: Motivation  Find the K orthogonal projection directions which</p>
    <p>have the most data variances</p>
    <p>Applications  Visualization  De-noising  Latent semantic indexing  Eigenfaces</p>
    <p>-6 -4 -2 0 2 4 6 -6</p>
    <p>-4</p>
    <p>-2</p>
  </div>
  <div class="page">
    <p>PCA: Algorithm  Basic Algorithm</p>
    <p>Centralize data</p>
    <p>Compute the sample covariance matrix</p>
    <p>Do eigen-decomposition (sort eigenvalues decreasingly)</p>
    <p>PCA directions are given in , the first K columns of</p>
    <p>The PCA projection of a test data is</p>
    <p>xi  xi   x ;  x = 1 N</p>
    <p>P N i =1</p>
    <p>xi</p>
    <p>S = 1N X &gt;X = 1N</p>
    <p>P N i =1</p>
    <p>xi x&gt;i</p>
    <p>v = U &gt;K x</p>
    <p>x U K U</p>
    <p>S = U  U &gt;</p>
  </div>
  <div class="page">
    <p>Supervised PCA?  PCA is unsupervised  When output information is available:</p>
    <p>Classification labels: 0/1  Regression responses: real values  Ranking orders: rank labels / preferences  Multi-outputs: output dimension &gt; 1  Structured outputs,</p>
    <p>Can PCA be biased by outputs?  And how?</p>
  </div>
  <div class="page">
    <p>Outline  Principal Component Analysis  Probabilistic PCA  Supervised Probabilistic PCA  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>Latent Variable Model for PCA  Another interpretation of PCA [Pearson 1901]</p>
    <p>PCA is minimizing the reconstruction error of :</p>
    <p>are latent variables: PCA projections of  are factor loadings: PCA mappings  Equivalent to PCA up to a scaling factor</p>
    <p>Lead to idea of PPCA</p>
    <p>X</p>
    <p>min A ;V</p>
    <p>kX  V A k2F</p>
    <p>s.t. V &gt;V = I</p>
    <p>V 2 RN  K</p>
    <p>A 2 RK  M X</p>
  </div>
  <div class="page">
    <p>Probabilistic PCA [TipBis99]</p>
    <p>Latent variable model</p>
    <p>Conditional independence:  If , PPCA leads to PCA solution (up to a rotation and</p>
    <p>scaling factor)  is Gaussian distributed:</p>
    <p>x = W x v +  x + x</p>
    <p>Mean vector</p>
    <p>Latent variables v  N (0; I )</p>
    <p>Noise process x  N (0; 2x I )</p>
    <p>x</p>
    <p>xjv  N (W x v +  x ;  2 x I )</p>
    <p>2x ! 0</p>
    <p>P (x) = R</p>
    <p>P (xjv)P (v) dv</p>
    <p>v</p>
    <p>xWx</p>
    <p>N</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>From Unsupervised to Supervised  Key insights of PPCA</p>
    <p>All the M input dimensions are conditionally independent given the K latent variables</p>
    <p>In PCA we are seeking the K latent variables that best explain the data covariance</p>
    <p>When we have outputs , we believe:  There are inter-covariances between and  There are intra-covariances within if</p>
    <p>Idea: Let the latent variables explain all of them!</p>
    <p>X Y Y</p>
    <p>Y 2 RN  L</p>
    <p>L &gt; 1</p>
  </div>
  <div class="page">
    <p>Outline  Principal Component Analysis  Probabilistic PCA  Supervised Probabilistic PCA  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>Supervised Probabilistic PCA</p>
    <p>Supervised latent variable model</p>
    <p>All input and output dimensions are conditionally independent</p>
    <p>are jointly Gaussian distributed:</p>
    <p>x = W xv +  x + x y = W yv +  y + y</p>
    <p>Noise process y  N (0; 2yI )</p>
    <p>xjv  N (W x v +  x ;  2 x I ); yjv  N (W yv +  y;</p>
    <p>(x; y)</p>
    <p>P (x; y) = R</p>
    <p>P (xjv)P (yjv)P (v) dv</p>
    <p>v</p>
    <p>xWx</p>
    <p>N</p>
    <p>x</p>
    <p>y Wy</p>
    <p>y</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Probabilistic PCA</p>
    <p>Idea: A SPPCA model with missing data!  Likelihood:</p>
    <p>x1 ...</p>
    <p>xN 1 xN 1+1</p>
    <p>... xN</p>
    <p>y1 ...</p>
    <p>yN 1</p>
    <p>y1  yLx1  xM</p>
    <p>v1  vK</p>
    <p>X</p>
    <p>Y</p>
    <p>P (X ; Y ) = N 1Y</p>
    <p>n=1</p>
    <p>P (xn ; yn ) NY</p>
    <p>n0=N 1 +1</p>
    <p>P (xn0)</p>
    <p>v</p>
    <p>x</p>
    <p>Wx</p>
    <p>N-N1</p>
    <p>x</p>
    <p>y Wy</p>
    <p>y</p>
    <p>v</p>
    <p>x</p>
    <p>N1</p>
  </div>
  <div class="page">
    <p>S2PPCA: EM Learning  Model parameters:  EM Learning</p>
    <p>E-step: estimate for each data point (a projection problem)</p>
    <p>M-step: maximize data log-likelihood w.r.t. parameters</p>
    <p>An extension of EM learning for PPCA model  Can be kernelized!</p>
    <p>By product: an EM learning algorithm for kernel PCA</p>
    <p>W x ; W y;  x ;  y ;  2 x ;</p>
    <p>v</p>
    <p>P (vn jxn ; yn ) / P (xn jvn )P (yn jvn )P (vn )</p>
    <p>P (vn 0jxn 0) / P (xn 0jvn 0)P (vn 0) Inference Problem</p>
  </div>
  <div class="page">
    <p>S2PPCA: Toy Problem Linear</p>
    <p>X Y X</p>
    <p>Y</p>
  </div>
  <div class="page">
    <p>S2PPCA: Toy Problem Nonlinear</p>
  </div>
  <div class="page">
    <p>S2PPCA: Toy Problem Nonlinear</p>
  </div>
  <div class="page">
    <p>S2PPCA: Properties  Semi-supervised projection</p>
    <p>Take PCA and kernel PCA as special cases  Applicable to large data sets</p>
    <p>Primal: O(t(M+L)NK) time, O((M+L)N) space  Dual: O(tN2K) time, O(N2) space</p>
    <p>A latent variable solution [Yu et al, SIGIR05]</p>
    <p>Cannot deal with semi-supervised setting!  Closed form solutions for SPPCA</p>
    <p>No closed form solutions for S2PPCA</p>
    <p>min A ;B ;W</p>
    <p>(1  )kX  V A k2F +  kY  V B k 2 F</p>
    <p>s.t. V &gt;V = I ; V = X W</p>
    <p>cheaper than Primal if M&gt;N</p>
  </div>
  <div class="page">
    <p>SPPCA: Primal Solution T heorem 1 (P rimal) Let S denote the normalized sample covariance matrix for centered observations f (xn ; yn )g</p>
    <p>N n =1</p>
    <p>,</p>
    <p>S = 1 N</p>
    <p>NX</p>
    <p>n =1</p>
    <p>1 2</p>
    <p>xn yn</p>
    <p>xn yn</p>
    <p>&gt;</p>
    <p>1</p>
    <p>2x Sx</p>
    <p>Sxy 1</p>
    <p>x y Syx</p>
    <p>Sy</p>
    <p>!</p>
    <p>;</p>
    <p>and 1  : : :  (M +L ) be its eigenvalues with eigenvectors u1; : :: ; u(M +L ) , then if the latent space is K -dimensional, the following results hold:</p>
    <p>(i) In SPPCA model W x and W y are calculated as</p>
    <p>W x = x U x ( K  I ) 1 2 R ; W y = y U y( K  I )</p>
    <p>where  K = (1; : : : ; K ), U x (U y ) contains the rst M (last L ) rows of [u1; : : : ; uK ], and R is an arbitrary K  K orthogonal rotation matrix.</p>
    <p>(ii) Projection v for centered new input x is given as</p>
    <p>v = 1 x</p>
    <p>R &gt; ( K  I )  1</p>
    <p>h U &gt;x U x + ( K  I )</p>
    <p>1 i  1</p>
    <p>U &gt;x x :</p>
    <p>(M+L)(M+L)</p>
  </div>
  <div class="page">
    <p>SPPCA: Dual Solution T heorem 1 (D ual) Let bK = 12x</p>
    <p>K + 1 2y</p>
    <p>Y Y &gt; , and 1  : : :  N be its eigenvalues with eigenvectors v1; : : : ; vN , then if the latent space in SPPCA is K -dimensional,</p>
    <p>(i) The projections of training data, which are encoded in rows of matrix Z, are calculated as</p>
    <p>Z = p</p>
    <p>N V K (I  N   1 K )</p>
    <p>where  K = (1; : : : ; K ), V K = [v1; : : : ; vK ], and R is an arbitrary K  K orthogonal matrix.</p>
    <p>(ii) Projections v for new input x is given as</p>
    <p>v = p</p>
    <p>N R &gt; D  1 2 (V &gt;K K V K + D )</p>
    <p>1V &gt;K k(X ; x );</p>
    <p>where D = I  N   1K .</p>
    <p>New kernel matrix!</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Train Nearest Neighbor classifier after projection  Evaluation metrics:</p>
    <p>Multi-class classification: error rate  Multi-label classification: F1-measure, AUC</p>
  </div>
  <div class="page">
    <p>Multi-class Classification</p>
    <p>S2PPCA is almost always better than SPPCA  LDA is very good for FACE data  S2PPCA is very good on TEXT data  S2PPCA has good scalability</p>
  </div>
  <div class="page">
    <p>Multi-label Classification</p>
    <p>S2PPCA is always better than SPPCA  S2PPCA is better in most cases  S2PPCA has good scalability</p>
  </div>
  <div class="page">
    <p>Extensions  Put priors on factor loading matrices</p>
    <p>Learn MAP estimates for them</p>
    <p>Relax Gaussian noise model for outputs  Better way to incorporate supervised information  We need to do more approximations (using, e.g. EP)  Directly predict missing outputs (i.e. single step)</p>
    <p>Mixture modeling in latent variable space  Achieve local PCA instead of global PCA</p>
    <p>Robust supervised PCA mapping  Replace Gaussian with Student-t  Outlier detection in PCA</p>
    <p>y = f (v)</p>
  </div>
  <div class="page">
    <p>Related Work  Fisher discriminant analysis (FDA)</p>
    <p>Goal: Find directions to maximize between-class distance while minimizing within-class distance</p>
    <p>Only deal with outputs of multi-class classification  Limited number of projection dimensions</p>
    <p>Canonical correlation analysis (CCA)  Goal: Maximize the correlation between inputs and outputs  Ignore intra-covariance of both inputs and outputs</p>
    <p>Partial least squares (PLS)  Goal: Sequentially find orthogonal directions to maximize</p>
    <p>covariance with respect to outputs  A penalized CCA; poor generalization on new output</p>
    <p>dimensions</p>
  </div>
  <div class="page">
    <p>Conclusion  A general framework for (semi-)supervised</p>
    <p>dimensionality reduction  We can solve the problem analytically (EIG) or via</p>
    <p>iterative algorithms (EM)  Trade-of</p>
    <p>EIG: optimization-based, easily extended to other loss  EM: semi-supervised projection, good scalability</p>
    <p>Both algorithms can be kernelized  PCA and kernel PCA are special cases  More applications expected</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
    <p>http://www.dbs.ifi.lmu.de/~spyu</p>
  </div>
</Presentation>
