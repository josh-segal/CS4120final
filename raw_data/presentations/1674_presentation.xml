<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>UTILITY-BASED CONTROL FEEDBACK IN A DIGITAL LIBRARY SEARCH ENGINE: CASES IN CITESEERX Jian Wu, Alexander Ororbia, Kyle Williams, Madian Khabsa, Zhaohui Wu, C. Lee Giles CiteSeerX</p>
    <p>Pennsylvania State University</p>
  </div>
  <div class="page">
    <p>Outline  Introduction</p>
    <p>Utility-based control feedback</p>
    <p>Three types of feedback paradigms  User-Correction (Metadata correction)  Ill-Conditioned Metadata Detection (Metadata correction)  Crawl Whitelist Generation (Crawl Coverage)</p>
    <p>Future Work  Crawl Scheduler  Dynamical Topic-Driven Crawling</p>
  </div>
  <div class="page">
    <p>Introduction  High-level policies:</p>
    <p>Figure: The utility-based control feedback loop.</p>
    <p>System State  S: service attribute vector  Utility function  U(S): maps any possible system state (S) to a</p>
    <p>scalar value  Agent: a controller that does the following jobs</p>
    <p>adapts by learning the mapping from actions to service level attributes  applies the utility function  chooses the action that maximizes utility</p>
  </div>
  <div class="page">
    <p>User-correction Feedback</p>
    <p>User-correction: allows users to directly correct paper metadata from the web interface  user-based feedback</p>
    <p>CiteSeerX Web service</p>
    <p>CiteSeerX registered users</p>
    <p>correct metadata</p>
    <p>paper metadata</p>
    <p>Figure: Feedback diagram for user-correction.</p>
  </div>
  <div class="page">
    <p>Ill-conditioned Metadata Detection Feedback</p>
    <p>Ill-conditioned Metadata Detection: detects papers with illconditioned metadata by checking citation and download history  long-term feedback</p>
    <p>CiteSeerX Database</p>
    <p>Metadata Checking Module</p>
    <p>automated correct metadata</p>
    <p>citation and download history</p>
    <p>Figure: Feedback diagram for Ill-conditioned metadata detection.</p>
  </div>
  <div class="page">
    <p>Crawl Whitelist Generation Feedback</p>
    <p>Crawl Whitelist Generation: selects high quality URLs based on the number of scholarly papers  automated feedback</p>
    <p>Focused Crawler + Doc</p>
    <p>Classifier</p>
    <p>Whitelist Generator</p>
    <p>Select URLs</p>
    <p>#scholarly documents of a URL</p>
    <p>Figure: Feedback diagram for whitelist generation.</p>
  </div>
  <div class="page">
    <p>Metadata Correction  Metadata in CiteSeerX</p>
    <p>Header: titles, authors, affiliations, year, venue, abstract etc.  Citations: titles, authors, year, venue, page, volume, issue, citation</p>
    <p>context etc.</p>
    <p>How does CiteSeerX acquire metadata  Actively crawling the Web  Automatically extracting metadata from scholarly documents</p>
    <p>Header  SVM based extraction tool  Citations  CRF-based parsing and tagging tool  CiteSeerX extracts acknowledgements, algorithms, figures and tables</p>
    <p>Challenges: near-duplication (ND), accuracy and efficiency  Mistakes occur in metadata extraction  Requires correction  user correction and automated correction</p>
  </div>
  <div class="page">
    <p>User-Correction (uC)</p>
    <p>Figure: user-correction Web Interface (WI) on a CiteSeerX paper summary page.</p>
    <p>Features  Users must login (limited crowd sourcing)  Users can change almost all metadata fields  New values are effective immediately after changes are submitted  Metadata can be changed multiple times  Version control</p>
  </div>
  <div class="page">
    <p>What happens behind the WI after a uC? 1)</p>
    <p>P.1 P.2 P.3 P.3</p>
  </div>
  <div class="page">
    <p>Evaluate the uC  CiteSeerX has received more than 277,000 user</p>
    <p>corrections on 251,000 papers since 2008  Tens to thousands of uCs daily  Preliminary evaluation:</p>
    <p>50 uC instances  Compare metadata before and after corrections  Four types of corrections:</p>
    <p>WC  real corrections, from wrong to correct  CW  correct to wrong  WW  wrong to wrong: metadata quality not improved  WD  a wrong value is deleted</p>
  </div>
  <div class="page">
    <p>Sample of uC tagging results metadata fields WC WW CW WD</p>
    <p>title 23 1 0 0 abstract 21 1 2 4</p>
    <p>(author) name 19 0 1 0 year 15 0 0 1</p>
    <p>(author) affiliation 13 0 0 1 (author) address 7 0 0 1</p>
    <p>(author) email 7 0 0 0 venue 5 0 2 0</p>
    <p>author** 0 0 0 8 #papers corrected 45 2 5 13</p>
    <p>**author: author blocks, including author names, affiliations, addresses and emails.</p>
  </div>
  <div class="page">
    <p>Implication of uC tagging results  Most uCs (90%) contain real corrections  Most corrected fields:</p>
    <p>titles, abstracts, author names, years, author affiliations  Fields deleted:</p>
    <p>Some redundant / wrong author blocks  Some abstract</p>
    <p>Most papers are corrected only once  Estimation</p>
    <p>About 116,000 paper titles are corrected (3% of all CiteSeerX papes)</p>
    <p>Author names are corrected in about 100,000 papers  These are among the most downloaded papers</p>
    <p>Overall, the uC is a useful method to improve metadata</p>
  </div>
  <div class="page">
    <p>Ill-Conditioned Metadata Detection  Access logs over long term can be used to detect anomalies</p>
    <p>and errors  Normal correlation  the number of times a given paper is</p>
    <p>downloaded and the number of citations it has received</p>
    <p>Abnormal documents: large number of downloads with zero citations</p>
    <p>Correct metadata from publisher website or a secondary digital library or use a better extractor</p>
    <p>#d ow</p>
    <p>nl oa</p>
    <p>d</p>
    <p>#citation</p>
    <p>normal correlation</p>
    <p>abnormal correlation</p>
  </div>
  <div class="page">
    <p>Why are (#download,#citation) used as feedback?</p>
    <p>users Google CiteSeerX</p>
    <p>PDF Files: Crawl/Extract/Index</p>
    <p>Search result</p>
    <p>PDF Files: Download A paper citing my paper Title: A Review of Feedback Computing in the last decade  Reference: [1] Jian Wu et al. Utility-based feedback in a digital library search engine: Cases In CiteSeerX, Feedback Computing 2014</p>
    <p>However, the title of my paper was not extracted correctly Title: We described a utility-based feedback control model and</p>
  </div>
  <div class="page">
    <p>Crawl Whitelist Generation  CiteSeerX focused crawler: citeseerxbot</p>
    <p>Targets: scholarly documents in PDF formats  Improve crawl efficiency  improve seed quality  Whitelist vs. blacklist:</p>
    <p>Blacklist  contains domains and URLs to be filtered out  A large fraction of non-scholarly documents  no feedback</p>
    <p>Whitelist  contains high quality URLs (URLs outside of whitelist domains are not crawled)  Example: http://clgiles.ist.psu.edu/papers/, whitelist domain: psu.edu</p>
    <p>Use #scholarly documents (npaper) as a feedback  URLs with npaper &gt; 1</p>
    <p>How does CiteSeerX select scholarly documents?  A rule-based filter  looking for keywords/keyphrases</p>
  </div>
  <div class="page">
    <p>Evaluate the Feedback Mechanism  Experimental setup</p>
    <p>Set P (10 experiments)  each experiment: 500 seed URLs randomly selected from 200,000</p>
    <p>parent URLs  Set W (10 experiments)</p>
    <p>each experiment: 500 seed URLs randomly selected from the whitelist (generated out of 200,000 parent URLs)</p>
    <p>Crawl efficiency is improved from 22.87% to 44.83% after using npaper as a feedback</p>
  </div>
  <div class="page">
    <p>Crawl Efficiency Comparison Experiments</p>
    <p>Set P (no feedback) Set W (with feedback)</p>
    <p>npaper nPDF npaper/nPDF npaper nPDF npaper/nPDF 1 6905 29276 23.58% 698 1308 53.36% 2 2088 8924 12.90% 1152 1735 66.38% 3 3784 16186 23.38% 575 1668 34.47% 4 2438 11141 19.61% 1002 2413 41.52% 5 2740 13974 19.61% 2362 3951 59.78% 6 2259 9395 24.04% 2126 4850 43.84% 7 1845 9873 18.69% 1498 3298 45.42% 8 3089 9432 32.75% 1252 4606 27.18% 9 2079 7486 27.77% 1214 4316 28.13% 10 1998 8284 24.12% 1298 2694 48.18%</p>
    <p>Average - - 22.875.04% - - 44.8312.14% npaper: # scholarly papers; nPDF: number of total PDF documents crawled</p>
  </div>
  <div class="page">
    <p>Future Work  Existing feedback</p>
    <p>Quantify the significance and importance  Improve the crawl scheduler by adopting more feedback</p>
    <p>Utilizes feedbacks from  npaper: # of scholarly papers  : updating rate  how long a scholarly web page is updated (based on the</p>
    <p>crawl history)  How to estimate</p>
    <p>X  the number of detected changes  n  the number of accesses within a time period of T</p>
    <p>Dynamical Topic-Driven Crawler  Train a artificial neural network (ANN) model based on a labeled</p>
    <p>sample  Automatically classify URLs on-the-fly  Dynamical crawl navigator</p>
    <p>! = !log X +0.5 n+0.5</p>
    <p>&quot;</p>
    <p># $</p>
    <p>%</p>
    <p>&amp; ',X = n! X</p>
  </div>
  <div class="page">
    <p>Summary  Three utility-based control feedback paradigms for a</p>
    <p>digital library search engine  The user-based feedback allows registered users to</p>
    <p>perform online changes to metadata. In more than 90% of cases, the users provides correct changes.</p>
    <p>The download and citation history are long-term feedback to detect ill-conditioned metadata and select those papers for corrections</p>
    <p>The # of scholarly documents of URLs (npaper) is used as feedback to generate a URL whitelist. The crawl efficiency is boosted by at least 20%.</p>
  </div>
</Presentation>
