<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>CRAID Online RAID Upgrades Using Dynamic Hot Data Reorganization</p>
    <p>Alberto Miranda! Toni Cortes</p>
    <p>Barcelona Supercomputing Center Technical University of Catalonia</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>RAID: building block for large-scale storage</p>
    <p>- Performance, reliability and capacity</p>
    <p>- Acceptable cost</p>
    <p>Client requirements  with time</p>
    <p>Solution: add disks to upgrade RAID</p>
    <p>- Significant data migration</p>
    <p>2</p>
  </div>
  <div class="page">
    <p>Upgrade challenges</p>
    <p>Uniformity of distribution</p>
    <p>- B blocks into n disks  B/n per disk</p>
    <p>Minimal (ideal) data migration (IM)</p>
    <p>- m additional disks  B m / (n+m) blocks to migrate</p>
    <p>Online rebalancing</p>
    <p>- 24/7 services</p>
    <p>3</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Existing methods with ideal migration</p>
    <p>- Uniformity  after several upgrades [Semi-RR, Goel02]</p>
    <p>- Limited performance scalability [GSR, Chang07]</p>
    <p>- No support for parities yet [FastScale, Zheng11]</p>
    <p>Rebalancing the ideal amount of data can be too expensive in some situations!</p>
    <p>4</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Existing methods with ideal migration</p>
    <p>- Uniformity  after several upgrades [Semi-RR, Goel02]</p>
    <p>- Limited performance scalability [GSR, Chang07]</p>
    <p>- No support for parities yet [FastScale, Zheng11]</p>
    <p>Rebalancing the ideal amount of data can be too expensive in some situations!</p>
    <p>5</p>
    <p>Can we migrate less than IM and still keep RAIDs benefits?</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Do we really need ALL DATA ideally distributed?</p>
    <p>Not all data is created equal</p>
    <p>- Frequently accessed data is a small % of total</p>
    <p>- Long-term locality</p>
    <p>- Really popular data can receive 90% accesses</p>
    <p>Rebalance only hot data in real time</p>
    <p>- Ideal RAID distribution of data actually in use</p>
    <p>- Upgrade overhead  (hot migration &lt; ideal migration)</p>
    <p>6</p>
  </div>
  <div class="page">
    <p>CRAID: Overview</p>
    <p>A cache partition (CP) is built from a % of all disks</p>
    <p>Original data remains in an archive partition (AP)</p>
    <p>Hot data is tracked</p>
    <p>and copied to CP</p>
    <p>When CP is full:</p>
    <p>- replace w/ hotter data</p>
    <p>- update original (if needed)</p>
    <p>7</p>
  </div>
  <div class="page">
    <p>CRAID: Overview</p>
    <p>A cache partition (CP) is built from a % of all disks</p>
    <p>Original data remains in an archive partition (AP)</p>
    <p>Hot data is tracked</p>
    <p>and copied to CP</p>
    <p>When CP is full:</p>
    <p>- replace w/ hotter data</p>
    <p>- update original (if needed)</p>
    <p>8</p>
  </div>
  <div class="page">
    <p>CRAID: Overview</p>
    <p>A cache partition (CP) is built from a % of all disks</p>
    <p>Original data remains in an archive partition (AP)</p>
    <p>Hot data is tracked</p>
    <p>and copied to CP</p>
    <p>When CP is full:</p>
    <p>- replace w/ hotter data</p>
    <p>- update original (if needed)</p>
    <p>9</p>
  </div>
  <div class="page">
    <p>CRAID: Advantages</p>
    <p>Large cache with small % per disk</p>
    <p>- Important data is resident longer</p>
    <p>Disk-based cache  persistent optimizations</p>
    <p>Clustering of hot data  sequentialization of access</p>
    <p>Benefits gained with in-place devices</p>
    <p>10</p>
  </div>
  <div class="page">
    <p>CRAID: Architecture</p>
    <p>I/O Monitor</p>
    <p>Mapping Cache</p>
    <p>I/O Redirector</p>
    <p>11</p>
    <p>I/O MONITOR</p>
    <p>I/O REDIRECTOR</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>MAPPING CACHE</p>
    <p>I/O request</p>
    <p>update</p>
    <p>lookup</p>
    <p>send I/O to CP/AP</p>
    <p>CRAID controllerA</p>
    <p>B.1</p>
    <p>B.2</p>
    <p>C.2</p>
    <p>CP AP copy</p>
    <p>C.1</p>
    <p>Storage Devices</p>
  </div>
  <div class="page">
    <p>I/O Monitor</p>
    <p>Analyzes REQS to identify hot data</p>
    <p>Schedules I/Os to/from partitions</p>
    <p>Chooses which data to replace</p>
    <p>- LRU, LFUDA, GDSF, ARC, WLRU</p>
    <p>- Simple, with high success rates</p>
    <p>12</p>
    <p>I/O MONITOR</p>
    <p>I/O REDIRECTOR</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>MAPPING CACHE</p>
    <p>I/O request</p>
    <p>update</p>
    <p>lookup</p>
    <p>send I/O to CP/AP</p>
    <p>CRAID controllerA</p>
    <p>B.1</p>
    <p>B.2</p>
    <p>C.2</p>
    <p>CP AP copy</p>
    <p>C.1</p>
    <p>Storage Devices</p>
  </div>
  <div class="page">
    <p>Mapping Cache</p>
    <p>Translates original LBAs to CP LBAs</p>
    <p>Lookup must be fast!</p>
    <p>- Tree structure O(log ||CP||)</p>
    <p>Memory usage:  5.9MB/GB</p>
    <p>Failure resilience: persistent log of dirty blocks and translations</p>
    <p>13</p>
    <p>I/O MONITOR</p>
    <p>I/O REDIRECTOR</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>MAPPING CACHE</p>
    <p>I/O request</p>
    <p>update</p>
    <p>lookup</p>
    <p>send I/O to CP/AP</p>
    <p>CRAID controllerA</p>
    <p>B.1</p>
    <p>B.2</p>
    <p>C.2</p>
    <p>CP AP copy</p>
    <p>C.1</p>
    <p>Storage Devices</p>
  </div>
  <div class="page">
    <p>I/O Redirector</p>
    <p>Intercepts I/Os and sends them to appropriate partition</p>
    <p>Lazy updates: writes directly to CP</p>
    <p>14</p>
    <p>I/O MONITOR</p>
    <p>I/O REDIRECTOR</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>LBAorig LBAcache</p>
    <p>MAPPING CACHE</p>
    <p>I/O request</p>
    <p>update</p>
    <p>lookup</p>
    <p>send I/O to CP/AP</p>
    <p>CRAID controllerA</p>
    <p>B.1</p>
    <p>B.2</p>
    <p>C.2</p>
    <p>CP AP copy</p>
    <p>C.1</p>
    <p>Storage Devices</p>
  </div>
  <div class="page">
    <p>CRAID: Rebalancing</p>
    <p>Done by I/O Monitor when new devices appear</p>
    <p>- Update originals of dirty blocks</p>
    <p>- Invalidate CP</p>
    <p>- Begin filling w/ currently hot working set</p>
    <p>Gradual, on-line rebalancing!</p>
    <p>15</p>
  </div>
  <div class="page">
    <p>CRAID: Rebalancing</p>
    <p>Pros:</p>
    <p>- New disks used at T=0</p>
    <p>- Long sequential chains of related blocks</p>
    <p>Cons:</p>
    <p>- Invalidation of the CP = work lost</p>
    <p>16</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Trace-based simulation (Disksim)</p>
    <p>- 7 datasets in paper (research, NFS, servers, )</p>
    <p>Here webusers (FIU) and proj (MSRC)</p>
    <p>- 1 week trace time</p>
    <p>- 50 disks (+5 SSDs in some cases)</p>
    <p>- 128KB block size</p>
    <p>- WLRU algorithm (best hit/miss % tradeoff)</p>
    <p>- CP cold at start</p>
    <p>17</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>RAID5: ideal, 50 disks, restriped</p>
    <p>!</p>
    <p>!</p>
    <p>RAID5+: ideal, 10 base disks + 30% growth, restriped</p>
    <p>18</p>
    <p>RAID5</p>
    <p>RAID5 RAID5 RAID5 RAID5</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>CRAID5: RAID5 (CP) + RAID5 (AP), not restriped</p>
    <p>!</p>
    <p>!</p>
    <p>CRAID5+: RAID5 (CP) + RAID5+ (AP), not restriped</p>
    <p>19</p>
    <p>RAID5</p>
    <p>RAID5 RAID5 RAID5 RAID5</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>CRAID5ssd: SSD dedicated CP, HDD AP (RAID5)</p>
    <p>!</p>
    <p>!</p>
    <p>CRAID5+ssd: SSD dedicated CP, HDD AP (RAID5+)</p>
    <p>20</p>
    <p>RAID5</p>
    <p>RAID5 RAID5 RAID5 RAID5</p>
  </div>
  <div class="page">
    <p>Response time (reads)</p>
    <p>21</p>
  </div>
  <div class="page">
    <p>Response time (reads)</p>
    <p>22</p>
  </div>
  <div class="page">
    <p>Response time (reads)</p>
    <p>23</p>
  </div>
  <div class="page">
    <p>Response time (writes)</p>
    <p>24</p>
  </div>
  <div class="page">
    <p>Response time (writes)</p>
    <p>25</p>
  </div>
  <div class="page">
    <p>Workload distribution (rw)</p>
    <p>26</p>
  </div>
  <div class="page">
    <p>Workload distribution (rw)</p>
    <p>27</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Low upgrade overhead</p>
    <p>- Increased locality and sequentiality of hot data</p>
    <p>- Improved read/write performance</p>
    <p>Good workload distribution (close to ideal RAID)</p>
    <p>Good results w/ 1.28% available capacity</p>
    <p>Alternative to SSD caching for less $</p>
    <p>- Should work with full-SSD RAIDs</p>
    <p>28</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Abandon simulations  HW prototype</p>
    <p>RAID6 and EC support</p>
    <p>Explore smarter prediction/rebalancing algorithms</p>
    <p>Extend beyond RAID storage</p>
    <p>29</p>
  </div>
  <div class="page">
    <p>Q&amp;A? THANK YOU!</p>
    <p>30</p>
  </div>
  <div class="page">
    <p>Sequentiality</p>
    <p>31</p>
  </div>
  <div class="page">
    <p>More information</p>
    <p>Long-term locality of hot data:</p>
    <p>- Analyzing Long-Term Access Locality to Find Ways to Improve Distributed Storage Systems PDP2012 A. Miranda, T. Cortes</p>
    <p>CRAID over RAID0</p>
    <p>- Performance Optimized Lustre PRACE-2IP white paper (2012) E. Artiaga, A. Miranda http://www.prace-ri.eu/IMG/pdf/D12-4_2ip.pdf</p>
    <p>32</p>
  </div>
</Presentation>
