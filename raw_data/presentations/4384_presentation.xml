<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>!1</p>
    <p>Learning to Ask Questions in Opendomain Conversational Systems with</p>
    <p>Typed Decoders</p>
    <p>Yansen Wang1, Chenyi Liu1, Minlie Huang1, Liqiang Nie2</p>
    <p>aihuang@tsinghua.edu.cn</p>
    <p>http://coai.cs.tsinghua.edu.cn/hml</p>
  </div>
  <div class="page">
    <p>!2</p>
    <p>Outline</p>
    <p>Background  Problem &amp; task definition  Motivation  Related works  Method  Experiments  Summary</p>
  </div>
  <div class="page">
    <p>!3</p>
    <p>Good Examples from DL Models</p>
    <p>User:  Time is a killer Machine:  I am the one who was killed.</p>
    <p>User</p>
    <p>The happiest person is not the one who gets the best thing, instead who cherishes what she/he already has.</p>
    <p>Machine:  Agree, I will cherish my happiness too.</p>
    <p>User:  Afraid to be lonely, but wont give up my little freedom. Machine:  I want to wander, but no one accompany me.</p>
  </div>
  <div class="page">
    <p>!4</p>
    <p>Bad Examples are More!</p>
    <p>User:  Ohh, he does not love you that much. Machine:  A happy day starts.</p>
    <p>User:  Back to Beijing now. So cold so happy! Machine:  When will you come to Beijing?</p>
    <p>User:  Mans who can cook are the most sexy. Machine:  Mans who can cook are the most sexy women.</p>
  </div>
  <div class="page">
    <p>!5</p>
    <p>Bad Examples are More!</p>
    <p>User:  :  User:  : 75 User: 75 :</p>
    <p>User:  How large is your house? :  So soon to reveal my age?</p>
    <p>User:  Are you vegan or meatatarian? :  How can I lose weight?</p>
    <p>User:  :  User:  :  User:  :</p>
    <p>Context issue</p>
    <p>Semantic understanding</p>
    <p>Inconsistency in personality</p>
    <p>Xiaoice by MSRA</p>
  </div>
  <div class="page">
    <p>!6</p>
    <p>Bad Examples (AI Ethics)</p>
    <p>Picture from Prof. Frank Rudzicz, University of Toronto</p>
  </div>
  <div class="page">
    <p>!7</p>
    <p>Challenges in Chatting Machines</p>
    <p>Semantic s</p>
    <p>Consistenc y</p>
    <p>Interactiveness</p>
    <p>Content, Context,</p>
    <p>Scene</p>
    <p>Personality, Personalizatio</p>
    <p>n, Language</p>
    <p>Style</p>
    <p>Emotion &amp; Sentiment</p>
    <p>Strategy &amp;</p>
    <p>Behavior</p>
  </div>
  <div class="page">
    <p>!8</p>
    <p>More Intelligent Chatting Machines</p>
    <p>Behaving more interactively: Emotional Chatting Machine (AAAI 2018) Proactive Behavior by Asking Good Questions (ACL 2018) Controlling sentence function (ACL 2018)</p>
    <p>Behaving more consistently: Explicit Personality Assignment (IJCAI-ECAI 2018)</p>
    <p>Behaving more intelligently with semantics: Better Understanding and Generation Using Commonsense Knowledge (IJCAI-ECAI 2018 Distinguished Paper)</p>
    <p>References:  Emotional Chatting Machine: Emotional Conversation Generation with Internal and External</p>
    <p>Memory. AAAI 2018.  Assigning personality/identity to a chatting machine for coherent conversation generation. IJCAI</p>
    <p>ECAI 2018.  Commonsense Knowledge Aware Conversation Generation with Graph Attention. IJCAI-ECAI 2018.  Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. ACL 2018.  Generating Informative Responses with Controlled Sentence Function. ACL 2018.</p>
  </div>
  <div class="page">
    <p>!9</p>
    <p>Post: I went to dinner yesterday night.</p>
    <p>Problem &amp; Task Definition</p>
    <p>How to ask good questions in open-domain conversational systems?</p>
  </div>
  <div class="page">
    <p>!10</p>
    <p>Post: I went to dinner yesterday night.</p>
    <p>Problem &amp; Task Definition</p>
    <p>Who were you with?  Where did you have the dinner?  How about the food?  How many friends?  Who paid the bill?  Is it an Italian restaurant?</p>
    <p>Friends? Place? Food? Persons? Bill?</p>
    <p>WHO WHERE HOW-MANYHOW-ABOUT WHO</p>
  </div>
  <div class="page">
    <p>!11</p>
    <p>Post: I went to dinner yesterday night.</p>
    <p>Scene: Dining at a restaurant</p>
    <p>Problem &amp; Task Definition</p>
    <p>Asking good questions requires scene understanding</p>
    <p>Friends? Place? Food? Persons? Bill?</p>
    <p>WHO WHERE HOW-MANYHOW-ABOUT WHO</p>
  </div>
  <div class="page">
    <p>!12</p>
    <p>Motivation</p>
    <p>Responding + asking (Li et al., 2016)  More interactive chatting machines</p>
    <p>Key proactive behaviors (Yu et al., 2016)  Less dialogue breakdowns</p>
    <p>Asking good questions is indication of understanding  As in course teaching  Scene understanding in this paper</p>
  </div>
  <div class="page">
    <p>!13</p>
    <p>Related Work</p>
    <p>Traditional question generation (Andrenucci and Sneiders, 2005; Popowich and Winne, 2013)</p>
    <p>Syntactic Transformation</p>
    <p>Given context: As recently as 12,500 years ago, the Earth was in the midst of a glacial age referred to as the Last Ice Age.</p>
    <p>Generated question: How would you describe the Last Ice Age?</p>
  </div>
  <div class="page">
    <p>!14</p>
    <p>Related Work</p>
    <p>A few neural models for question generation in reading comprehension (Du et al., 2017; Zhou et al., 2017; Yuan et al., 2017)</p>
    <p>Given  Passage: Oxygen is used in cellular respiration</p>
    <p>and released by photosynthesis, which uses the energy of sunlight to produce oxygen from water.</p>
    <p>Answer: photosynthesis  Generated question: What life process produces</p>
    <p>oxygen in the presence of light?</p>
  </div>
  <div class="page">
    <p>!15</p>
    <p>Related Work</p>
    <p>Visual question generation for eliciting interactions (Mostafazadeh, 2016): beyond image captioning</p>
    <p>Given image:</p>
    <p>Generated question: What happened?</p>
  </div>
  <div class="page">
    <p>!16</p>
    <p>Difference to Existing Works</p>
    <p>Different goals:  To enhance interactiveness and persistence of</p>
    <p>human-machine interactions  Information seeking in read comprehension</p>
    <p>Various patterns: YES-NO, WH-, HOW-ABOUT, etc.</p>
    <p>Topic transition: from topics in post to topics in response  Dinner!food; fat ! climbing; sports !</p>
    <p>soccer</p>
  </div>
  <div class="page">
    <p>!17</p>
    <p>Key Observations</p>
    <p>A good question is a natural composition of  Interrogatives for using various questioning</p>
    <p>patterns  Topic words for addressing interesting yet novel</p>
    <p>topics  Ordinary words for playing grammar or</p>
    <p>syntactic roles</p>
  </div>
  <div class="page">
    <p>!18</p>
    <p>Hard/Soft Typed Decoders (HTD/STD)</p>
  </div>
  <div class="page">
    <p>!19</p>
    <p>Encoder-decoder Framework</p>
  </div>
  <div class="page">
    <p>!20</p>
    <p>Soft Typed Decoder(STD)</p>
    <p>Decoding state</p>
  </div>
  <div class="page">
    <p>!21</p>
    <p>Soft Typed Decoder(STD)</p>
    <p>Applying multiple type-specific generation distributions over the same vocabulary</p>
    <p>Each word has a latent distribution among the set</p>
    <p>type(w){interrogative, topic word, ordinary word}</p>
    <p>STD is a very simple mixture model</p>
    <p>type-specific generation distribution</p>
    <p>word type distribution</p>
  </div>
  <div class="page">
    <p>!22</p>
    <p>Soft Typed Decoder(STD)</p>
    <p>Estimate the type distribution of each word:</p>
    <p>The final generation distribution is a mixture of the three type-specific generation distribution.</p>
    <p>Estimate the type-specific generation distribution of each word:</p>
  </div>
  <div class="page">
    <p>!23</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>In soft typed decoder, word types are modeled in a latent, implicit way</p>
    <p>Can we control the word type more explicitly in generation?  Stronger control</p>
  </div>
  <div class="page">
    <p>!24</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>Decoding state</p>
  </div>
  <div class="page">
    <p>!25</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>Estimate the generation probability distribution</p>
    <p>Modulate words probability by its corresponding type probability:</p>
    <p>m(yt) is related to the type probability of word yt</p>
    <p>Estimate the type probability distribution</p>
  </div>
  <div class="page">
    <p>!26</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>Argmax? (firstly select largest type prob. then sample word from generation dist.)  Indifferentiable  Serious grammar errors if word type is</p>
    <p>wrongly selected</p>
    <p>what 0.3 Tinterrogative 0.7 what 0.8 food 0.2 X Ttopic 0.1  food 0.05 is 0.4 Tordinary 0.2 is 0.09</p>
    <p>Generation distr. Type distr. Modulated distr.</p>
  </div>
  <div class="page">
    <p>!27</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>Gumble-Softmax:  A differentiable surrogate to the argmax</p>
    <p>function.</p>
  </div>
  <div class="page">
    <p>!28</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>In HTD, the types of words are given in advance.  How to determine the word types?</p>
  </div>
  <div class="page">
    <p>!29</p>
    <p>Hard Typed Decoder(HTD)</p>
    <p>Interrogatives:  A list of about 20 interrogatives are given by</p>
    <p>hand.  Topic words:  Training: all nouns and verbs in response are</p>
    <p>topic words.  Test: 20 words are predicted by PMI.</p>
    <p>Ordinary words:  All other words, for grammar or syntactic</p>
    <p>roles</p>
  </div>
  <div class="page">
    <p>!30</p>
    <p>Loss Function</p>
    <p>Cross entropy  Supervisions are on both final probability and</p>
    <p>the type distribution:</p>
    <p>is a term to balance the two kinds of losses.</p>
  </div>
  <div class="page">
    <p>!31</p>
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>!32</p>
    <p>Dataset</p>
    <p>PMI estimation: calculated from 9 million postresponse pairs from Weibo.</p>
    <p>Dialogue Question Generation Dataset(DQG), about 491,000 pairs  Distilled questioning responses using about</p>
    <p>hml/dataset/</p>
  </div>
  <div class="page">
    <p>!33</p>
    <p>Baselines</p>
    <p>Seq2Seq: A simple encoder-decoder model (Luong et al., 2015)</p>
    <p>Mechanism-Aware (MA): Multiple responding mechanisms represented by real-valued vectors (Zhou et al., 2017)</p>
    <p>Topic-Aware (TA): Topic Aware Model by incorporating topic words (Xing et al., 2017)</p>
    <p>Elastic Responding Machine (ERM): Enhanced MA using reinforcement learning (Zhou et al., 2018)</p>
  </div>
  <div class="page">
    <p>!34</p>
    <p>Automatic Evaluation</p>
    <p>Evaluation metrics  Perplexity &amp; Distinct  TRR (Topical Response Ratio):</p>
    <p>20 topic words are predicted with PMI for each post.  TRR is the proportion of the responses containing at</p>
    <p>least one topic word.</p>
  </div>
  <div class="page">
    <p>!35</p>
    <p>Manual Evaluation</p>
    <p>Pair-wise comparison: win, loss, tie  Three evaluation criteria:  Appropriateness: whether a question is</p>
    <p>reasonable in logic and content, and has key info.</p>
    <p>Richness: containing topic words or not  Willingness to respond to a generated</p>
    <p>question</p>
  </div>
  <div class="page">
    <p>!36</p>
    <p>Manual Evaluation(Pairwise)</p>
    <p>Score: the probability of win/lose/tie of our model vs. baseline</p>
  </div>
  <div class="page">
    <p>!37</p>
    <p>Examples</p>
  </div>
  <div class="page">
    <p>!38</p>
    <p>More Examples</p>
    <p>Different questioning patterns and topic transition:</p>
    <p>Work!Department Sports! College</p>
    <p>Suchi!Treat Suchi!Try</p>
  </div>
  <div class="page">
    <p>!39</p>
    <p>Visualization</p>
    <p>Type prediction at each decoding position</p>
  </div>
  <div class="page">
    <p>!40</p>
    <p>Summary</p>
    <p>Stronger control in language generation via word semantic type</p>
    <p>Whats new  A new task: question generation in open-domain</p>
    <p>dialogue systems  A new dataset: Dialog Question Generation Dataset  A new model with two variants: possibly applicable to</p>
    <p>other generation tasks if word semantic types can be easily identified</p>
    <p>The compatibility issue between topic control and other word type control is NOT well solved  Bad grammar or not reasonable responses</p>
  </div>
  <div class="page">
    <p>!41</p>
    <p>Thanks for your attentions</p>
    <p>Dataset: http://coai.cs.tsinghua.edu.cn/hml/dataset/</p>
    <p>Codes: https://github.com/victorywys/ Learning2Ask_TypedDecoder</p>
    <p>Homepage: http://coai.cs.tsinghua.edu.cn/hml</p>
    <p>Recruiting post-doctors!</p>
  </div>
  <div class="page">
    <p>!42</p>
    <p>Error Analysis</p>
    <p>Main error types  No topic words (NoT) in a response  Wrong topics (WrT) where topic words are</p>
    <p>irrelevant  Type generation error (TGE) where a wrong word</p>
    <p>type is predicted</p>
  </div>
  <div class="page">
    <p>!43</p>
    <p>Error Analysis: Examples</p>
    <p>No topic words</p>
    <p>Wrong topics</p>
    <p>Type generation</p>
    <p>error</p>
  </div>
</Presentation>
