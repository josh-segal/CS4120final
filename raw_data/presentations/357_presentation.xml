<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Data Swapping: Variations on a Theme by Dalenius and Reiss</p>
    <p>By Stephen Fienberg, Julie Mclntyre</p>
    <p>Hilal Akay</p>
  </div>
  <div class="page">
    <p>Abstract</p>
    <p>The paper revisited the original published version (A Technique for Disclosure by Dalenius and Reiss, 1978) Examine the original proposal The variations and refinements of data</p>
    <p>swapping Model based methods for statistical disclosure</p>
    <p>limitation</p>
  </div>
  <div class="page">
    <p>Statistical disclosure problem</p>
    <p>providing quality data to users while protecting the identities of subjects.</p>
    <p>concerned with the possibility of a person inferring sensitive information from a database.  A value of some variable is compromisable if it can be</p>
    <p>uniquely determined from the information that is released.</p>
    <p>Data swapping: introduced by Dalenius and Reiss  disclosure protection and release of statistically</p>
    <p>usable databases</p>
  </div>
  <div class="page">
    <p>Data swapping</p>
    <p>a method of preserving confidentiality in data sets that contain categorical variables,i.e, for contingency tables  Categorical variables: it makes no sense to say that</p>
    <p>one category of data is more or less than another (ex: Mr or Mrs, hair colors, religions) X quantitive variables</p>
    <p>transform a database by exchanging values of sensitive variables among individual records  in such a way to maintain lower order frequency</p>
    <p>counts and marginals</p>
  </div>
  <div class="page">
    <p>Data swapping</p>
    <p>confidentiality  introducing uncertainty about sensitive data values</p>
    <p>preserving certain statistics of the data  preserves lower order marginal totals, no impact on inferences</p>
    <p>that derive from these statistics  a unique approach</p>
    <p>they cast disclosure limitation problem firmly as a statistical problem.</p>
    <p>The release of data is justified if one can show that the probability of any individual's data being compromised is small.</p>
    <p>The usefulness of data by focusing on the type and amount of distortion introduced by the data.</p>
  </div>
  <div class="page">
    <p>Theorical justification</p>
    <p>Consider a set of N individuals. With each individual, associate data observed with respect to V variables, X, Y ..... U, some of which may be sensitive.</p>
    <p>it is all categorical: values in the domain {0, 1 ..... (r-l)} for some r, denote the categories into which the N individuals are classified.</p>
    <p>a categorical database by an N  V matrix, the original data matrix mo.</p>
    <p>using this matrix, mo, produce another data matrix, me, to be used as the basis for producing statistics by way of tabulations</p>
  </div>
  <div class="page">
    <p>t-order statistic</p>
    <p>if the tabulation involves data for one variable only, the statistic will be referred to as an 1-order statistic; the simplest case in kind is #(x = 0)</p>
    <p>if the tabulation involves data for two or more variables, the statistic will be referred to as a 2-order statistic, a 3order statistic, etc., as the case may be; a simple example is #(x = 0, y = 0), which is a 2- order statistic.</p>
    <p>This matrix me,while not identical with mo, is t-order equivalent with mo,so that t-order statistics are preseved (yield the same t-order statistics).</p>
  </div>
  <div class="page">
    <p>Data swapping</p>
    <p>swap the values of sensitive variables among records in such a way that the t-order frequency counts, i.e., the entries in the t-way marginal table, are preserved: t-order equivalent to the original database.</p>
    <p>justification: existence of sufficient number of t-order equivalent databases to introduce uncertainty about the true values of sensitive variables.  the sensitive variable is protected from compromise if there is at</p>
    <p>least one other database or table, t-order equivalent of the original one, that assigns it a different value.</p>
  </div>
  <div class="page">
    <p>Example1</p>
    <p>Swap values of X 1 with 5 4 with 7</p>
    <p>data in tabular form the two-way marginal tables have not changed from the original data</p>
  </div>
  <div class="page">
    <p>Example1</p>
    <p>the two-way marginal tables have not changed from the original data  Summing over any dimension results in the same 2-way totals for the</p>
    <p>swapped data as for the original data  there are at least two data bases that could have generated the</p>
    <p>same set of two-way tables  The data for any single individual cannot be determined with certainty</p>
    <p>from the release of this information alone.</p>
  </div>
  <div class="page">
    <p>Example1</p>
    <p>construct a new database that is equivalent to the original one in terms of t-order statistics where the new data is sufficiently different from the original so that compromise is not possible</p>
    <p>a database presented only in terms of t-order statistics is unlikely to be compromised  every sensitive variable of every individual is almost</p>
    <p>certainly involved in at least one swap and hence cannot be determined.</p>
  </div>
  <div class="page">
    <p>Example2</p>
    <p>Swap data for the X-variable for k=4 individuals, number 1 and three others</p>
    <p>Find the 2-order equivalence  n this case, five possible matrices</p>
  </div>
  <div class="page">
    <p>Example2</p>
    <p>This matrix yields 1- and 2-order statistics identical with the corresponding statistics computed from mo</p>
  </div>
  <div class="page">
    <p>Probability</p>
    <p>The probability that the swap will result in a 2-equivalent database is</p>
    <p>Two variables: To protect data in both the X and Y variables , swap data for both.  Starting with mo, swap data for the X variable</p>
    <p>and get mex. Next, starting with mex, swap data for the Y variable and get mexy</p>
  </div>
  <div class="page">
    <p>Observations</p>
    <p>focus primarily on the release of data in the form of 2-way marginal totals</p>
    <p>details and proofs in the original text are unclear  They do not actually swap data but only ask</p>
    <p>about possible data swaps.  Their sole purpose appears to have been to provide a</p>
    <p>framework for evaluating the likelihood of disclosure.</p>
  </div>
  <div class="page">
    <p>Observations</p>
    <p>the concept of disclosure is probabilistic and not absolute  Data release should be based on an assessment of the</p>
    <p>probability of the occurrence of disclosure.</p>
    <p>tradeoff between protection and utility  no release of information without some possibility o disclosure  responsibility of data managers to weigh the risks</p>
    <p>data utility is defined statistically  The requirement to maintain a set of marginal totals places the</p>
    <p>emphasis on statistical utility by preserving certain types of inferences (t-way and lower marginal totals)</p>
  </div>
  <div class="page">
    <p>The form of released data</p>
    <p>microdata releases:requires that enough data are swapped to introduce sufficient uncertainty about the true values of the individual's data.</p>
    <p>tabulation: All marginal tables up to order t are unchanged by the transformation</p>
    <p>Identifiying enough swaps to protect every value in the database turns out to be computationaly impractical  an approximate data swapping approach for the release of</p>
    <p>microdata from categorical databases that aproximately preservers t-order marginal totals</p>
  </div>
  <div class="page">
    <p>Approximation</p>
    <p>a more feasible approach  data swapping is performed so that t-order frequency</p>
    <p>counts are approximately preserved  compute relevant frequency tables from the original</p>
    <p>database, and then construct a new database to be consistent with these tables, according to probability distribution derived from the original frequency tables</p>
    <p>extended for containing continues variables, an alogrithm for approximately preserving generalized kth order moments (Reiss, Post, and Dalenius )  for k=2</p>
  </div>
  <div class="page">
    <p>Applying data swapping to census data releases  The US Census Bereau began using a variant of data swapping for</p>
    <p>data releases from the 1990 decennial census.  Before implementation the method was tested with extensive</p>
    <p>simulations and the release of both tabulations and microdata was considered</p>
    <p>Records are swapped between census blocks between individuals or households that have been matched on a predetermined set of k variables.  The (k + 1)-way marginals involving the matching variables and census</p>
    <p>block totals are guaranteed to remain the same; however, marginals for tables involving other variables are subject to change at any level of tabulation.</p>
    <p>U.S 2000 decennial census : unique records that were more at risk of disclosure were targeted to be involved in swaps.</p>
  </div>
  <div class="page">
    <p>Variations on a Theme-Extensions and alternatives</p>
    <p>Rank swaping  NISS Web-based Data Swapping  Data Swapping and Local Recoding  Data Shuffling</p>
  </div>
  <div class="page">
    <p>Rank swaping by Moore  Rank-based proximity swapping algorithm  Find swaps for a continuous variable in a way that swapped records</p>
    <p>are guaranteed to be within a specified rank-distance of one another.</p>
    <p>Multivariate statistics computed from data swapped with this algorithm will be less distorted than those computed after an unconstrained swap.</p>
    <p>Certain summary statistics are preserved within a specified interval  Values of a swapped variable are uniformly distributed on the</p>
    <p>interval between its bottom-coded and top-coded values.</p>
  </div>
  <div class="page">
    <p>Rank swaping by Carlson and Salabasis  For continuous or ordinally scaled variables  Variables: X and Y Databases: S1 = [X1, Y1] and S2 =</p>
    <p>[X2, Y2]  Databases are ranked with respect to X, so for large</p>
    <p>sample sizes, the corresponding ordered values of X1 and X2 should be approximately equal</p>
    <p>Swap X1 and X2, S1 = [X1, Y2] and S 2 = [X2Y1].  randomly dividing the database into two equal parts,</p>
    <p>ranking and performing the swap, and then recombining  wasteful of the data  theory apply only to bivariate correlation coefficients</p>
  </div>
  <div class="page">
    <p>NISS Web-based Data Swapping</p>
    <p>By user-specified parameters, produces a data set for release as microdata  swap variables and the swap rate, i.e., the proportion of records to be</p>
    <p>involved in swaps  pairs of records are randomly selected and values for that variable</p>
    <p>exchanged  Both risk and utility decrease as the number of swap variables and</p>
    <p>the swap rate increase  high swapping rate implies that data are well-protected from</p>
    <p>compromise, but inferential properties are more likely to be distorted  The risk-utility frontier identifies the greatest amount of protection</p>
    <p>achievable for any set of swap variables and swap rate  for a variety variables generate and test</p>
  </div>
  <div class="page">
    <p>Data Swapping and Local Recoding by Takemura  a disclosure limitation procedure for microdata that combines data</p>
    <p>swapping and local recoding  identifies groups of individuals in the database with similar records</p>
    <p>different matching algorithms to identify and pair similar individuals for swapping, ex:clustering,Edmonds algorithm</p>
    <p>obscuring the values of sensitive variables either by swapping records among individuals within groups, or recoding the sensitive variables for the entire group</p>
    <p>The method works for both continuous and categorical variables.  The swapping version of the method resemblaces to rank swapping,</p>
    <p>but the criterion for swapping varies across individuals.</p>
  </div>
  <div class="page">
    <p>Data Shuffling by Mulalidhar and Sarathy  replace sensitive data by simulated data with similar</p>
    <p>distributional properties  suppose that X represents sensitive variables and S</p>
    <p>non-sensitive variables 1. Generate new data Y to replace X by using the conditional</p>
    <p>distribution of X given S, f(X|S), so that f(X|S,Y) = f(X|S). The released versions of the sensitive data, i.e., Y, provide an intruder with no additional information about f(X|S).</p>
    <p>One of the problems is that f is unknown and thus there is information in Y</p>
  </div>
  <div class="page">
    <p>Data swapping and Model-based statistical Methods  methods that use a specific model to perturb or</p>
    <p>transform data to protect confidentiality  Post Randomization MethodPRAM</p>
    <p>methods that involve some perturbation or transformation to protect confidentiality, but preserve minimal sufficient statistics for a specific model, thereby maintaining the data users inferences under that model  Model-based Approaches for the Release of</p>
    <p>Marginals and Other statistics</p>
  </div>
  <div class="page">
    <p>Post Randomization Method PRAM  Suppose that a sensitive variable has categories 1, . . . ,m.  each value of the variable in the database is altered according to a</p>
    <p>predefined transition probability (Markov) matrix.  conditional on its observed value, each value of the variable is</p>
    <p>assigned one of 1, . . . ,m.  observations either remain the same or are changed to another</p>
    <p>possible value, all with known probability  The degree of protection depends on the probabilities in the</p>
    <p>transition matrix and the frequencies of observations in the original database.</p>
  </div>
  <div class="page">
    <p>Model-based Approaches</p>
    <p>bootstrap-like sampling from the empirical distribution of the data and then releasing the sampled data for analysis (Fienberg, Steele, and Makov)</p>
    <p>In the case of categorical data, this procedure is closely related to the problem of generating entries in a contingency table given a fixed set of marginals.</p>
    <p>Preserving marginal totals is equivalent to preserving sufficient statistics of certain log-linear models.</p>
    <p>The Dalenius and Reiss data swap preserves marginal totals of tables up to order t, and so can be viewed as a model-based method with respect to a log-linear model.</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Data swapping is used for the release of marginals in a contingency table that are useful for statistical analysis.  This leads to a consideration of log-linear models for which marginal totals are</p>
    <p>minimal sufficient statistics  Although Dalenius and Reiss made no references to log-linear models, they</p>
    <p>appear to provide the justification for much of the original paper.  Data swapping as originally proposed by Dalenius and Reiss does not</p>
    <p>generalize in ways that they thought because of the relationship between  the calculation of bounds for cell entries in contingency tables given a set of</p>
    <p>released marginals  the generation of tables from the exact distribution of a log-linear model given its</p>
    <p>minimal sufficient statistics marginals.  Original paper</p>
    <p>focus primarily on the release of data in the form of 2-way marginal totals  details and proofs in the original text are unclear</p>
    <p>Future work : probabilistic justifications and more systematic methods</p>
  </div>
  <div class="page">
    <p>Thank you.</p>
    <p>Questions?</p>
  </div>
</Presentation>
