<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sidi Lu1, Bing Luo1, Tirthak Patel2, Yongtao Yao1, Devesh Tiwari2, Weisong Shi1</p>
    <p>Making Disk Failure Prediction SMARTer!</p>
  </div>
  <div class="page">
    <p>Hard disk drives (HDD)</p>
    <p>Key driving factor of data centers</p>
    <p>frequently replaced hardware components</p>
    <p>main reason behind server failures</p>
    <p>Introduction</p>
    <p>Data loss Service</p>
    <p>unavailability</p>
    <p>Operational cost</p>
    <p>Economic loss</p>
  </div>
  <div class="page">
    <p>Source: https://medium.com/genaro-network/tencent-was-claimed-ten-million-for-data-loss-due-to-cloud-hard-drive-glitch-344a26449fe2</p>
    <p>Tencent Cloud Storage: An incident resulted in the loss of around 10 million RMB of data</p>
    <p>A startup company named Front Edge: Basically lost the entire database that had been accumulating since its establishment</p>
  </div>
  <div class="page">
    <p>Significance</p>
    <p>Storage community benefits from the disk reliability field-studies</p>
    <p>Gap</p>
    <p>Disk reliability field-studies are infrequent and limited in sample size</p>
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>To bridge this gap  One of the largest disk failure analysis studies</p>
    <p>- 380,000 HDDs - 10,000 server racks - 64 data center sites - Over 2 months</p>
    <p>Hosted by a large enterprise data center operator</p>
    <p>Goal  predict disk failure accurately with long prediction horizon</p>
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>Key Concepts</p>
    <p>For the first time to demonstrate disk failure prediction can be highly accurate by combining  Disk performance data (e.g., capacity, throughput-related attributes)</p>
    <p>Disk location data (disk/server/rack/room/site)</p>
    <p>Disk monitoring data (Self-Monitoring, Analysis, and Reporting Technology - SMART)</p>
    <p>Conventional knowledge holds true</p>
    <p>Why we still consider other data?</p>
    <p>Traditional work</p>
    <p>Focused on SMART data only</p>
    <p>- e.g., correctable errors, disk spin-up time</p>
    <p>- indicative of eventual failure</p>
  </div>
  <div class="page">
    <p>Why not only SMART data?</p>
    <p>SMART attributes do not always have the strong predictive capability at long prediction horizon windows for all disks</p>
    <p>Value often do not change frequently enough before the actual failure</p>
    <p>Change is often noticeable only few hours before the actual failure</p>
  </div>
  <div class="page">
    <p>Why add performance data?</p>
    <p>The value of performance metrics (related to capacity, throughput, etc.)</p>
    <p>Exhibit more variations before the actual drive failure</p>
    <p>Performance increases the coverage in capturing the workload characteristics beyond what SMART attributes cover</p>
    <p>Show distinguishable behavior from healthy disks</p>
  </div>
  <div class="page">
    <p>Why add location data?</p>
    <p>Prediction can be further improved by incorporating the location information</p>
    <p>Disks in close spatial neighborhood</p>
    <p>- Affected by the same environmental factors (such as humidity and temperature)</p>
    <p>- Experience similar vibration level (known to affect the reliability of disks)</p>
    <p>Location information increases our coverage of the operating conditions of disks</p>
    <p>Site Room</p>
    <p>Rack</p>
    <p>Server</p>
  </div>
  <div class="page">
    <p>Disk SMART Data</p>
    <p>Select SMART attributes</p>
    <p>(1) Raw value: specific to the disk manufacturer</p>
    <p>(2) Normalized value: mapping corresponding raw value to 1-byte</p>
    <p>Reported at per-day granularity</p>
  </div>
  <div class="page">
    <p>Performance Data</p>
    <p>Selected disk-level performance metrics (per-hour granularity)</p>
    <p>Selected server-level performance metrics (per-hour granularity)</p>
  </div>
  <div class="page">
    <p>Disk Spatial Location Data</p>
    <p>Location markers</p>
    <p>Each disk has four levels of location markers associated with it:</p>
    <p>site, room, rack, and server</p>
    <p>Capture the concept of neighborhood</p>
    <p>(physical distance is not captured by our location coordinates)  Do not explicitly indicate the actual physical proximity between two disks</p>
  </div>
  <div class="page">
    <p>Selection of Attributes</p>
    <p>Min-max normalization:</p>
    <p>For a given feature:</p>
    <p>(1) Set a series of threshold candidates with the step of 0.01,</p>
    <p>(2) Calculate their corresponding J-Indexes</p>
    <p>J-Index classification:</p>
  </div>
  <div class="page">
    <p>Selection of Attributes</p>
    <p>Min-max normalization:</p>
    <p>J-Index classification:</p>
    <p>Higher J-Index: more distinguishable</p>
    <p>The threshold candidate with the highest J-Index: the best (final) threshold</p>
    <p>Select features with highest J-Indexes as the informative features</p>
  </div>
  <div class="page">
    <p>Selection of Attributes</p>
    <p>Highest J-Indexes for SMART attributes</p>
    <p>Highest J-Indexes for performance metrics</p>
    <p>Single performance metric has an overall higher J-Index than a SMART attribute</p>
    <p>Performance metrics are likely to be predictive for disk failures</p>
  </div>
  <div class="page">
    <p>Patterns of Performance Metrics</p>
    <p>Performance metrics are likely to be predictive for disk failures</p>
    <p>240 hours before actual failures</p>
    <p>Raw value of the failed disk (RFD)</p>
    <p>Average value of all healthy disks (AHD)</p>
    <p>RFD - AHD</p>
    <p>The difference between the signatures of failed and healthy disks on the same server</p>
  </div>
  <div class="page">
    <p>Patterns of Performance Metrics</p>
    <p>Top two graphs:</p>
    <p>Some failed disks have a similar value to healthy disks at first, but then their behavior becomes unstable</p>
    <p>Bottom two graphs:</p>
    <p>Some failed disks report a sharp impulse before they fail</p>
  </div>
  <div class="page">
    <p>Effective Measurements</p>
    <p>To evaluate the effectiveness of our prediction approaches</p>
    <p>(Matthews correlation coefficient)</p>
  </div>
  <div class="page">
    <p>ML Models</p>
    <p>Bayes classifier (Bayes)</p>
    <p>Random forest (RF)</p>
    <p>Gradient boosted decision trees (GBDT)</p>
    <p>Long short-term memory network (LSTM)</p>
    <p>Convolutional neural network with long short-term memory (CNN-LSTM)</p>
    <p>Employ five ML methods:</p>
  </div>
  <div class="page">
    <p>Feature Group Sets</p>
    <p>Construct six groups using different feature combinations</p>
    <p>- to evaluate the effectives of SMART (S), performance (P) and location (L) data</p>
  </div>
  <div class="page">
    <p>Prediction Horizon Selection</p>
    <p>Predict if a given disk will fail within the next 10 days</p>
    <p>- Long enough for IT operators to conduct early countermeasures</p>
    <p>Sensitivity study (Mean Squared Error) - Derivative of MSE reaches the minimum on the 10th day</p>
    <p>- MSE increases after 10 days</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>Results of 6 groups by employing 5 machine learning methods:</p>
  </div>
  <div class="page">
    <p>Observation #1</p>
    <p>- performance and location features improve the effectiveness of prediction</p>
  </div>
  <div class="page">
    <p>Observation #2</p>
    <p>- Without location markers: 10% reduction for CNN-LSTM in terms of MCC</p>
    <p>- Location info may help ML models amplify the hidden patterns in performance metrics</p>
  </div>
  <div class="page">
    <p>Observation #3</p>
    <p>- Achieve MCC score of 0.95 for SPL group</p>
    <p>- LSTM could be further improved by taking better features as the input, which could be provided by CNN through dimensionality reduction</p>
  </div>
  <div class="page">
    <p>Observation #4 4. Trade-off between models with respect to different availability of feature sets</p>
    <p>In absence of performance and location features</p>
    <p>- Traditional tree-based ML models (RF and GBDT) can provide equally accurate predictions as complicated neural network based model (CNN-LSTM or LSTM)</p>
  </div>
  <div class="page">
    <p>Further Exploration</p>
    <p>False positive and false negative rates for different ML models and different feature groups</p>
    <p>SMART attribute based models: - High FNR (failed disks predicted healthy) across all models</p>
    <p>Adding performance and location features - Decreases FNR significantly - Prediction quality goes up</p>
  </div>
  <div class="page">
    <p>Where do ML models perform poorly and why?</p>
    <p>Mispredicted failures (blue) tend to occur in low failure locations for all models</p>
    <p>Where: Concentration of failures is relatively lower</p>
    <p>Why: ML models are not able to collect enough failed disk samples</p>
    <p>Significance: Emphasizes the need for adding location markers in disk failure prediction models</p>
  </div>
  <div class="page">
    <p>When do ML models fail to predict and why?</p>
    <p>When: The number of false positives is very low initially as it predicts many disks as healthy although they eventually failed in that window</p>
    <p>- and, this is why the false negatives are high initially</p>
    <p>Why: ML model does not have enough data and (conservatively) predicts that disks are healthy</p>
    <p>Significance: The need for sufficiently long testing periods before concluding the prediction quality</p>
    <p>false positives (healthy disks predicted as failed) categorized in 20-day windows for CNN-LSTM model</p>
    <p>false negative (failed disks predicted healthy) categorized in 20-day windows for CNN-LSTM model</p>
  </div>
  <div class="page">
    <p>If simply to train on one data center site and port it to another</p>
    <p>- disk failure prediction model may not work</p>
    <p>Training on multiple data sites before testing on a new unseen data site provides reasonable accuracy</p>
    <p>Tested on two unseen data center sites A and B, while training models on rest of the 62 sites</p>
    <p>Prefer CNN-LSTM model if portability is a requirement</p>
    <p>Is the prediction model portable across data center sites?</p>
  </div>
  <div class="page">
    <p>Is the prediction model effective at different prediction horizon (lead time)?</p>
    <p>Prediction quality indeed goes down with increasing prediction horizon window</p>
    <p>Rate of decrease is not steep for any model</p>
  </div>
  <div class="page">
    <p>Does J-Index classification for feature selection degrade the overall prediction accuracy compared to models trained with all features?</p>
    <p>Provide similar quality results</p>
    <p>Suggestion: use J-Index to manage the storage overhead of storing attributes</p>
    <p>without risking the prediction quality significantly</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>One of the largest disk failure prediction studies - Covering 380,000 hard drives across 64 sites of a leading e-commerce site</p>
    <p>Performance and location attributes are effective in improving the disk failure prediction quality</p>
    <p>No single machine learning model is a winner across all scenarios, although CNN-LSTM is fairly effective across different situations</p>
    <p>Train models up to 0.95 F-measure and 0.95 MCC score for a 10-day prediction horizon</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
    <p>Our disk failure prediction framework and the dataset used are hosted at http://codegreen.cs.wayne.edu/wizard</p>
    <p>Email: lu.sidi@wayne.edu</p>
  </div>
</Presentation>
