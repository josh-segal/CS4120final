<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Telekine: Secure Computing with Cloud GPUs</p>
    <p>Tyler Hunt, Zhipeng Jia, Vance Miller, Ariel Szekely, Yige Hu, Christopher J. Rossbach, Emmett Witchel</p>
    <p>NSDI 2020</p>
  </div>
  <div class="page">
    <p>Trusting the cloud provider is difficult</p>
    <p>Attackers can exploit system bugs to steal data  The cloud provider has their own interests (e.g., monetizing user data)  Many administrators; some may be malicious</p>
    <p>Recognition Results</p>
  </div>
  <div class="page">
    <p>Avoiding trust in the cloud provider is difficult</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>Neural Net Computation</p>
    <p>OS/Hypervisor allows provider to see users secret data</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>Introduce TEEs to isolate computation</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>TEEs cannot be bypassed by software - Hardware root of trust (e.g., SGX on Intel, TrustZone on ARM)</p>
    <p>Protect communication from the provider with cryptography  Research proposals exist for GPU TEEs</p>
    <p>- Graviton [OSDI`18], HIX [ASPLOS`19] - Performance critical hardware unchanged</p>
    <p>(TEE is Trusted Execution Environment)</p>
    <p>TEE</p>
    <p>Neural Net Computation TEE</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>TEEs have limitations</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>Neural Net Computation</p>
    <p>TEE</p>
    <p>TEE</p>
    <p>CPU TEES: Cache Side Channels,</p>
    <p>Spectre attacks</p>
    <p>Mitigations require heroic effort, especially for complex software</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>Telekine addresses TEE limitations</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>Neural Net Computation</p>
    <p>TEE GPU API</p>
    <p>Proxy</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>Telekine uses API-remoting instead of CPU TEEs</p>
    <p>Interpose on GPU API calls  Application does not have to be modified, user does not need GPU  Turn every API call into an RPC, executed by the remote machine  Traffic is encrypted/authenticated; the proxy does not need protection</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>Neural Net Computation</p>
    <p>TEE GPU API</p>
    <p>Proxy</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>TEEs still have limitations</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>Neural Net Computation TEE</p>
    <p>GPU API</p>
    <p>Proxy</p>
    <p>All TEES: Communication timing</p>
    <p>attacks</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>Telekine addresses communication timing channels</p>
    <p>TEEs do not consider communication side channels - Securing the processor (CPU/GPU) does not secure communication</p>
    <p>GPU programing paradigm features frequent communication - CPU-to-CPU communication is also vulnerable</p>
    <p>Communication patterns tend to leak timing information - E.g., GPU kernel execution time</p>
    <p>TEE TEE</p>
    <p>TEE</p>
    <p>TEE</p>
    <p>TEE</p>
    <p>TEETEE</p>
  </div>
  <div class="page">
    <p>In the rest of the talk we will answer:</p>
    <p>Can information be extracted from GPU communication patterns?</p>
    <p>How does Telekine remove that information?</p>
    <p>What are Telekines overheads? Overheads are reasonable: ~20% for neural network training</p>
    <p>Replace GPU streams with new data-oblivious streams</p>
    <p>Yes, we demonstrate a communication timing attack</p>
  </div>
  <div class="page">
    <p>Expanded image recognition</p>
    <p>Tensorflow/MXNet</p>
    <p>Recognition Results</p>
    <p>Neural Net Computation</p>
    <p>GPU TEE GPU API</p>
    <p>Proxy</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>Tensorflow/MXNet</p>
    <p>Expanded image recognition</p>
    <p>Recognition Results</p>
    <p>GPU TEE</p>
    <p>Neural Net State/Code</p>
    <p>memcpy()</p>
    <p>launchKernel(1) . .</p>
    <p>launchKernel(n)</p>
    <p>memcpy() Recognition Results</p>
    <p>Start</p>
    <p>Start</p>
    <p>Kernel execution times!</p>
    <p>[Encrypted Result]</p>
    <p>Done</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
    <p>Done</p>
    <p>GPU API</p>
    <p>Proxy</p>
    <p>GPU raises interrupt on kernel completionGPU</p>
    <p>Stream</p>
  </div>
  <div class="page">
    <p>Attack</p>
    <p>Application</p>
    <p>Information gained from kernel execution time</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y (%</p>
    <p>)</p>
    <p>Number of Classes</p>
    <p>Classification Accuracy</p>
    <p>GPU Kernel timing classifier Random Guess</p>
    <p>Kernel timing classifier</p>
    <p>Image class</p>
    <p>Start</p>
    <p>Done</p>
  </div>
  <div class="page">
    <p>In the rest of the talk we will answer:</p>
    <p>Can information be extracted from GPU communication patterns?</p>
    <p>How does Telekine remove that information?</p>
    <p>What are Telekines overheads? Overheads are reasonable: ~20% for neural network training</p>
    <p>Replace GPU streams with new data-oblivious streams</p>
    <p>Yes, we demonstrate a communication timing attack</p>
  </div>
  <div class="page">
    <p>Tensorflow/MXNet</p>
    <p>Timing information is abundant</p>
    <p>Recognition Results</p>
    <p>GPU TEE</p>
    <p>Neural Net State/Code</p>
    <p>memcpy()</p>
    <p>launchKernel(1) . .</p>
    <p>launchKernel(n)</p>
    <p>memcpy() Recognition Results</p>
    <p>Done</p>
    <p>Start</p>
    <p>Start</p>
    <p>Done GPU API</p>
    <p>Proxy</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
    <p>GPU Stream</p>
  </div>
  <div class="page">
    <p>GPU API Proxy</p>
    <p>Application</p>
    <p>Timing information is abundant</p>
    <p>memcpy()</p>
    <p>Start</p>
    <p>Done launchKernel()</p>
    <p>memcpy()</p>
    <p>GPU TEE</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
    <p>GPU Stream</p>
    <p>Operations are distinguishable because of</p>
    <p>the hardware they use</p>
    <p>MMIO</p>
    <p>DMA</p>
    <p>DMA</p>
    <p>GPU raises interrupt on kernel completion</p>
    <p>Other potential timing channel sources:  Commands may be different sizes  Applications API use pattern may depend on secret data</p>
  </div>
  <div class="page">
    <p>Application</p>
    <p>GPU API Proxy</p>
    <p>Data-oblivious streams</p>
    <p>GPU TEE</p>
    <p>memcpy()</p>
    <p>launchKernel()</p>
    <p>memcpy()</p>
    <p>LibTelekine</p>
    <p>memcpy queue</p>
    <p>launchKernel queue</p>
    <p>GPU stream</p>
    <p>GPU stream</p>
    <p>MMIO</p>
    <p>DMA</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
    <p>GPU Stream</p>
    <p>Data-oblivious Stream</p>
  </div>
  <div class="page">
    <p>Data-oblivious streams  Divide commands by type so they can be scheduled independently</p>
    <p>- Adversary sees two independent streams of operations - Telekine manages data dependencies between types</p>
    <p>Split and pad commands as necessary; enforce a uniform size  Queue commands and send them (or no-ops) out deterministically</p>
    <p>- E.g., launch 32 kernels every 15ms, memcpy 1MB both directions every 30ms</p>
    <p>GPU API Proxy</p>
    <p>Application</p>
    <p>GPU TEE</p>
    <p>memcpy()</p>
    <p>launchKernel()</p>
    <p>memcpy()</p>
    <p>LibTelekine</p>
    <p>memcpy queue</p>
    <p>launchKernel queue</p>
    <p>GPU stream</p>
    <p>GPU stream</p>
    <p>MMIO</p>
    <p>DMA</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
    <p>Data-oblivious Stream</p>
  </div>
  <div class="page">
    <p>Data-oblivious streams  Divide commands by type so they can be scheduled independently</p>
    <p>- Adversary sees two independent streams of operations - Telekine manages data dependencies between types</p>
    <p>Split and pad commands as necessary; enforce a uniform size  Queue commands and send them (or no-ops) out deterministically</p>
    <p>- E.g., launch 32 kernels every 15ms, memcpy 1MB both directions every 30ms</p>
    <p>GPU API Proxy</p>
    <p>Application</p>
    <p>GPU TEE</p>
    <p>memcpy()</p>
    <p>launchKernel()</p>
    <p>memcpy()</p>
    <p>LibTelekine</p>
    <p>memcpy queue</p>
    <p>launchKernel queue</p>
    <p>GPU stream</p>
    <p>GPU stream</p>
    <p>MMIO</p>
    <p>DMA</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
    <p>Data-oblivious Stream</p>
  </div>
  <div class="page">
    <p>In the rest of the talk we will answer:</p>
    <p>Can information be extracted from GPU communication patterns?</p>
    <p>How does Telekine remove that information?</p>
    <p>What are Telekines overheads? Overheads are reasonable: ~20% for neural network training</p>
    <p>Replace GPU streams with new data-oblivious streams</p>
    <p>Yes, we demonstrate a communication timing attack</p>
  </div>
  <div class="page">
    <p>Testbeds</p>
    <p>The cloud machine (Austin, Texas): Intel i9-9900K, 8 cores @3.60GHz 32GB of RAM Radeon RX VEGA 64 GPU with 8GB of RAM</p>
    <p>Client (Austin, Texas): Intel Xeon E3-1270 v6, 4 cores @3.8GHz 32GB of RAM</p>
    <p>Geo-distributed client (Dallas, Texas): Vultr cloud VM, 8 vCPUS 32GB of RAM</p>
    <p>Real WAN testbed Simulated WAN testbed</p>
    <p>(RTT is Roundtrip Time)</p>
  </div>
  <div class="page">
    <p>We compare Telekine to an insecure baseline: running on the GPU server without protections</p>
    <p>Workloads:</p>
    <p>Data movement vs. GPU work microbenchmark  Neural net inference on MXNet:</p>
    <p>- ResNet50[He et. al 2016], InceptionV3[Szegedy et. al 2016], DenseNet [Huang et. al 2017]</p>
    <p>Neural net training on MXNet: - (Same networks as above)</p>
    <p>Graph analytics on Galois: - BFS, PageRank, SSSP (across 1 and 2 GPUs)</p>
  </div>
  <div class="page">
    <p>MXNet neural net inference (Real WAN)</p>
    <p>User sends a batch of images to be classified  Baseline: user sends batch to remote MXNet  Telekine: user sends batch to local MXNet</p>
    <p>- Telekine remotes computation to the GPU</p>
    <p>Batch Size ResNet50 InceptionV3 DenseNet 1 8</p>
    <p>ResNet50 InceptionV3 DenseNet 10.0X 6.6X 7.7X</p>
    <p>ResNet50 InceptionV3 DenseNet 10.0X 6.6X 7.7X 3.4X 2.2X 2.5X</p>
    <p>MXNet</p>
    <p>MXNet GPU TEE</p>
    <p>Trusted Untrusted Legend:</p>
    <p>Data Trusted</p>
  </div>
  <div class="page">
    <p>ResNet50 InceptionV3 DenseNetResNet50 InceptionV3 DenseNet</p>
    <p>MXNet neural net training</p>
    <p>Large dataset of images, processed in batches of size 64  Baseline: data set is on the cloud machine, passed to MXNet  Telekine: data set is on a client, passed to MXNet instance</p>
    <p>- Telekine connects that instance to the remote GPU - As a result Telekine uses a consistent 533 Mb/s network bandwidth</p>
    <p>(Real WAN)</p>
    <p>MXNet</p>
    <p>MXNet GPU TEE</p>
    <p>Overheads are low because GPUs overlap the extra work with computation  E.g., CUs can keep processing while the DMA engine performs transfers</p>
  </div>
  <div class="page">
    <p>MXNet neural net training breakdown (Simulated WAN)</p>
    <p>ResNet50 InceptionV3 DenseNet</p>
    <p>ResNet50 InceptionV3 DenseNet</p>
    <p>Sl o w</p>
    <p>do w</p>
    <p>n</p>
    <p>Baseline Add API Remoting Add Encryption Telekine (Real WAN)</p>
  </div>
  <div class="page">
    <p>Telekine: Secure Computing with Cloud GPUs</p>
    <p>Eliminates communication timing channels with data-oblivious streams</p>
    <p>Transparent to applications because it maintains GPU API semantics</p>
    <p>Has modest performance overheads for level of security provided</p>
    <p>Thanks!</p>
  </div>
  <div class="page">
    <p>Backup slides follow</p>
  </div>
  <div class="page">
    <p>MXNet training RTT sensitivity (Simulated WAN)</p>
    <p>RTT to cloud provider can vary  The effect in performance depends on the workload</p>
    <p>RTT ResNet50 InceptionV3 Densenet 10ms 1.19X 1.10X 1.22X 20ms 1.29X 1.13X 1.37X 30ms 1.44X 1.16X 1.49X 40ms 1.53X 1.18X 1.66X 50ms 1.62X 1.30X 2.09X</p>
  </div>
  <div class="page">
    <p>Attack accuracy for batched inference  GPU kernels operate on an entire batch</p>
    <p>- Cannot measure kernel execution time for individual images</p>
    <p>Task: correctly identify the class with the most images - Accuracy varies with how many more images</p>
    <p>there are (purity) - Batches of 32, four classes - Images selected from target class up to Purity - Batch filled out with images from other three</p>
    <p>classes</p>
    <p>Batch size Purity Accuracy</p>
  </div>
  <div class="page">
    <p>Communication vs GPU work (Simulated WAN)</p>
    <p>Copy16MB to the GPU  Compute for x-axis seconds  Copy16MB from the GPU</p>
    <p>S lo</p>
    <p>w d</p>
    <p>ow n</p>
    <p>GPU Computation (in seconds, logscale) Telekine</p>
  </div>
</Presentation>
