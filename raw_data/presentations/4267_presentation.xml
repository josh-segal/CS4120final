<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Abigail See* Peter J. Liu+ Christopher Manning*</p>
    <p>*Stanford NLP +Google Brain</p>
    <p>Get To The Point: Summarization with Pointer-Generator Networks</p>
  </div>
  <div class="page">
    <p>More difficult  More flexible and human  Necessary for future progress</p>
    <p>Easier  Too restrictive (no paraphrasing)  Most past work is extractive</p>
    <p>Two approaches to summarization</p>
    <p>Extractive Summarization</p>
    <p>Select parts (typically sentences) of the original text to form a summary.</p>
    <p>Abstractive Summarization</p>
    <p>Generate novel sentences using natural language generation techniques.</p>
  </div>
  <div class="page">
    <p>Long news articles (average ~800 words)</p>
    <p>Multi-sentence summaries (usually 3 or 4 sentences, average 56 words)</p>
    <p>Summary contains information from throughout the article</p>
    <p>CNN / Daily Mail dataset</p>
  </div>
  <div class="page">
    <p>Sequence-to-sequence + attention model</p>
    <p>&lt;START&gt;</p>
    <p>Context Vector</p>
    <p>Germany</p>
    <p>V ocabulary</p>
    <p>D istributiona zoo</p>
    <p>A tt</p>
    <p>en tio</p>
    <p>n D</p>
    <p>is tr</p>
    <p>ib ut</p>
    <p>io n</p>
    <p>&quot;beat&quot;</p>
    <p>...</p>
    <p>En co</p>
    <p>de r</p>
    <p>H id</p>
    <p>de n</p>
    <p>St at</p>
    <p>es</p>
    <p>D ecoder</p>
    <p>H idden States</p>
    <p>Germany emerge victorious in 2-0 win against Argentina on Saturday ...</p>
    <p>Source Text</p>
    <p>weig hted</p>
    <p>sum weighted sum</p>
    <p>Partial Summary</p>
  </div>
  <div class="page">
    <p>Sequence-to-sequence + attention model</p>
    <p>&lt;START&gt; Germany</p>
    <p>...</p>
    <p>En co</p>
    <p>de r</p>
    <p>H id</p>
    <p>de n</p>
    <p>St at</p>
    <p>es</p>
    <p>Germany emerge victorious in 2-0 win against Argentina on Saturday ...</p>
    <p>Source Text</p>
    <p>beat</p>
    <p>D ecoder</p>
    <p>H idden States</p>
    <p>Partial Summary</p>
  </div>
  <div class="page">
    <p>Sequence-to-sequence + attention model</p>
    <p>&lt;START&gt;</p>
    <p>...</p>
    <p>En co</p>
    <p>de r</p>
    <p>H id</p>
    <p>de n</p>
    <p>St at</p>
    <p>es</p>
    <p>Germany emerge victorious in 2-0 win against Argentina on Saturday ...</p>
    <p>Source Text</p>
    <p>Argentina 2-0 &lt;STOP&gt;beatGermany</p>
  </div>
  <div class="page">
    <p>Problem 1: The summaries sometimes reproduce factual details inaccurately.</p>
    <p>e.g. Germany beat Argentina 3-2</p>
    <p>Problem 2: The summaries sometimes repeat themselves.</p>
    <p>e.g. Germany beat Germany beat Germany beat</p>
    <p>Two Problems</p>
    <p>Incorrect rare or out-of-vocabulary word</p>
  </div>
  <div class="page">
    <p>Problem 1: The summaries sometimes reproduce factual details inaccurately.</p>
    <p>e.g. Germany beat Argentina 3-2</p>
    <p>Problem 2: The summaries sometimes repeat themselves.</p>
    <p>e.g. Germany beat Germany beat Germany beat</p>
    <p>Two Problems</p>
    <p>Incorrect rare or out-of-vocabulary word</p>
    <p>Solution: Use a pointer to copy words.</p>
  </div>
  <div class="page">
    <p>Get to the point!</p>
    <p>Source Text</p>
    <p>Germany emerge victorious in 2-0 win against Argentina on Saturday ...</p>
    <p>Germany</p>
    <p>... ...</p>
    <p>beat Argentina 2-0</p>
    <p>point!</p>
    <p>point! point!</p>
    <p>generate!</p>
    <p>...</p>
    <p>Best of both worlds: extraction + abstraction</p>
    <p>[1] Incorporating copying mechanism in sequence-to-sequence learning. Gu et al., 2016. [2] Language as a latent variable: Discrete generative models for sentence compression. Miao and Blunsom, 2016.</p>
  </div>
  <div class="page">
    <p>Source Text</p>
    <p>Germany emerge victorious in 2-0 win against Argentina on Saturday ...</p>
    <p>...</p>
    <p>&lt;START&gt; Germany</p>
    <p>V ocabulary</p>
    <p>D istributiona zoo</p>
    <p>beat</p>
    <p>Partial Summary</p>
    <p>Final Distribution &quot;Argentina&quot;</p>
    <p>a zoo</p>
    <p>&quot;2-0&quot;</p>
    <p>Context Vector</p>
    <p>A tt</p>
    <p>en tio</p>
    <p>n D</p>
    <p>is tr</p>
    <p>ib ut</p>
    <p>io n</p>
    <p>En co</p>
    <p>de r</p>
    <p>H id</p>
    <p>de n</p>
    <p>St at</p>
    <p>es</p>
    <p>D ecoder</p>
    <p>H idden States</p>
    <p>Pointer-generator network</p>
  </div>
  <div class="page">
    <p>Improvements</p>
    <p>Before After</p>
    <p>UNK UNK was expelled from the dubai open chess tournament</p>
    <p>gaioz nigalidze was expelled from the dubai open chess tournament</p>
    <p>the 2015 rio olympic games the 2016 rio olympic games</p>
  </div>
  <div class="page">
    <p>Problem 1: The summaries sometimes reproduce factual details inaccurately.</p>
    <p>e.g. Germany beat Argentina 3-2</p>
    <p>Two Problems</p>
    <p>Solution: Use a pointer to copy words.</p>
    <p>Problem 2: The summaries sometimes repeat themselves.</p>
    <p>e.g. Germany beat Germany beat Germany beat</p>
  </div>
  <div class="page">
    <p>Problem 1: The summaries sometimes reproduce factual details inaccurately.</p>
    <p>e.g. Germany beat Argentina 3-2</p>
    <p>Two Problems</p>
    <p>Solution: Use a pointer to copy words.</p>
    <p>Problem 2: The summaries sometimes repeat themselves.</p>
    <p>e.g. Germany beat Germany beat Germany beat</p>
    <p>Solution: Penalize repeatedly attending to same parts of the source text.</p>
  </div>
  <div class="page">
    <p>Reducing repetition with coverage</p>
    <p>Coverage = cumulative attention = what has been covered so far</p>
    <p>[4] Modeling coverage for neural machine translation. Tu et al., 2016, [5] Coverage embedding models for neural machine translation. Mi et al., 2016 [6] Distraction-based neural networks for modeling documents. Chen et al., 2016.</p>
  </div>
  <div class="page">
    <p>Reducing repetition with coverage</p>
    <p>Coverage = cumulative attention = what has been covered so far</p>
    <p>[4] Modeling coverage for neural machine translation. Tu et al., 2016, [5] Coverage embedding models for neural machine translation. Mi et al., 2016 [6] Distraction-based neural networks for modeling documents. Chen et al., 2016.</p>
  </div>
  <div class="page">
    <p>Reducing repetition with coverage</p>
    <p>Coverage = cumulative attention = what has been covered so far</p>
    <p>[4] Modeling coverage for neural machine translation. Tu et al., 2016, [5] Coverage embedding models for neural machine translation. Mi et al., 2016 [6] Distraction-based neural networks for modeling documents. Chen et al., 2016.</p>
    <p>Don't attend here</p>
  </div>
  <div class="page">
    <p>Result: repetition rate reduced to level similar to human summaries</p>
    <p>Reducing repetition with coverage</p>
    <p>Coverage = cumulative attention = what has been covered so far</p>
    <p>[4] Modeling coverage for neural machine translation. Tu et al., 2016, [5] Coverage embedding models for neural machine translation. Mi et al., 2016 [6] Distraction-based neural networks for modeling documents. Chen et al., 2016.</p>
    <p>Don't attend here</p>
  </div>
  <div class="page">
    <p>Summaries are still mostly extractive</p>
    <p>Final Coverage</p>
    <p>Source Text</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>ROUGE-1 ROUGE-2 ROUGE-L</p>
    <p>Nallapati et al. 2016 35.5 13.3 32.7 Previous best abstractive result</p>
    <p>ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence of 1-grams, 2-grams, and longest common sequence.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>ROUGE-1 ROUGE-2 ROUGE-L</p>
    <p>Nallapati et al. 2016 35.5 13.3 32.7</p>
    <p>Ours (seq2seq baseline) 31.3 11.8 28.8</p>
    <p>Ours (pointer-generator) 36.4 15.7 33.4</p>
    <p>Ours (pointer-generator + coverage) 39.5 17.3 36.4</p>
    <p>Previous best abstractive result</p>
    <p>Our improvements</p>
    <p>ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence of 1-grams, 2-grams, and longest common sequence.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>ROUGE-1 ROUGE-2 ROUGE-L</p>
    <p>Nallapati et al. 2016 35.5 13.3 32.7</p>
    <p>Ours (seq2seq baseline) 31.3 11.8 28.8</p>
    <p>Ours (pointer-generator) 36.4 15.7 33.4</p>
    <p>Ours (pointer-generator + coverage) 39.5 17.3 36.4</p>
    <p>Paulus et al. 2017 (hybrid RL approach) 39.9 15.8 36.9</p>
    <p>Paulus et al. 2017 (RL-only approach) 41.2 15.8 39.1</p>
    <p>Previous best abstractive result</p>
    <p>Our improvements</p>
    <p>worse ROUGE; better human eval</p>
    <p>better ROUGE; worse human eval</p>
    <p>ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence of 1-grams, 2-grams, and longest common sequence.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>ROUGE-1 ROUGE-2 ROUGE-L</p>
    <p>Nallapati et al. 2016 35.5 13.3 32.7</p>
    <p>Ours (seq2seq baseline) 31.3 11.8 28.8</p>
    <p>Ours (pointer-generator) 36.4 15.7 33.4</p>
    <p>Ours (pointer-generator + coverage) 39.5 17.3 36.4</p>
    <p>Paulus et al. 2017 (hybrid RL approach) 39.9 15.8 36.9</p>
    <p>Paulus et al. 2017 (RL-only approach) 41.2 15.8 39.1</p>
    <p>Previous best abstractive result</p>
    <p>Our improvements</p>
    <p>worse ROUGE; better human eval</p>
    <p>better ROUGE; worse human eval</p>
    <p>?</p>
    <p>ROUGE compares the machine-generated summary to the human-written reference summary and counts co-occurrence of 1-grams, 2-grams, and longest common sequence.</p>
  </div>
  <div class="page">
    <p>Summarization is subjective  There are many correct ways to summarize</p>
    <p>The difficulty of evaluating summarization</p>
  </div>
  <div class="page">
    <p>Summarization is subjective  There are many correct ways to summarize</p>
    <p>ROUGE is based on strict comparison to a reference summary  Intolerant to rephrasing  Rewards extractive strategies</p>
    <p>The difficulty of evaluating summarization</p>
  </div>
  <div class="page">
    <p>Summarization is subjective  There are many correct ways to summarize</p>
    <p>ROUGE is based on strict comparison to a reference summary  Intolerant to rephrasing  Rewards extractive strategies</p>
    <p>Take first 3 sentences as summary  higher ROUGE than (almost) any published system</p>
    <p>Partially due to news article structure</p>
    <p>The difficulty of evaluating summarization</p>
  </div>
  <div class="page">
    <p>First sentences not always a good summary</p>
    <p>Robots tested in Japan companies</p>
    <p>Irrelevant</p>
    <p>Our system starts here</p>
    <p>A crowd gathers near the entrance of Tokyo's upscale Mitsukoshi Department Store, which traces its roots to a kimono shop in the late 17th century.</p>
    <p>Fitting with the store's history, the new greeter wears a traditional Japanese kimono while delivering information to the growing crowd, whose expressions vary from amusement to bewilderment.</p>
    <p>It's hard to imagine the store's founders in the late 1600's could have imagined this kind of employee.</p>
    <p>That's because the greeter is not a human -- it's a robot.</p>
    <p>Aiko Chihira is an android manufactured by Toshiba, designed to look and move like a real person.</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>What next?</p>
  </div>
  <div class="page">
    <p>Extractive methods</p>
    <p>SAFETY</p>
  </div>
  <div class="page">
    <p>Human-level summarization</p>
    <p>lo ng</p>
    <p>te xt</p>
    <p>un</p>
    <p>de rs</p>
    <p>ta nd</p>
    <p>in g</p>
    <p>MOUNT ABSTRACTION</p>
    <p>Extractive methods</p>
    <p>paraphrasing</p>
    <p>SAFETY</p>
  </div>
  <div class="page">
    <p>Human-level summarization</p>
    <p>lo ng</p>
    <p>te xt</p>
    <p>un</p>
    <p>de rs</p>
    <p>ta nd</p>
    <p>in g</p>
    <p>MOUNT ABSTRACTION SWAMP OF BASIC ERRORS</p>
    <p>repetition</p>
    <p>copying errorsnonsense</p>
    <p>Extractive methods</p>
    <p>paraphrasing</p>
    <p>SAFETY</p>
  </div>
  <div class="page">
    <p>Human-level summarization</p>
    <p>MOUNT ABSTRACTION SWAMP OF BASIC ERRORS</p>
    <p>repetition</p>
    <p>copying errorsnonsense</p>
    <p>Extractive methods</p>
    <p>RNNs</p>
    <p>RNNs</p>
    <p>more high-level understanding? more scalability? better metrics?</p>
    <p>SAFETY</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Blog post: www.abigailsee.com</p>
    <p>Code: github.com/abisee/pointer-generator</p>
  </div>
</Presentation>
