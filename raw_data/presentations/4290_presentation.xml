<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Simple and Effective Text Simplification Using Semantic and Neural Methods</p>
    <p>Elior Sulem, Omri Abend and Ari Rappoport The Hebrew University of Jerusalem</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Elior Sulem Omri Abend Ari Rappoport</p>
    <p>Simple and Effective Text Simplification Using Semantic and Neural Methods</p>
  </div>
  <div class="page">
    <p>Text Simplification</p>
    <p>John wrote a book. I read the book.Last year I read the book John authored</p>
    <p>Original sentence One or several simpler sentences</p>
  </div>
  <div class="page">
    <p>Text Simplification</p>
    <p>John wrote a book. I read the book.Last year I read the book John authored Original sentence One or several simpler sentences</p>
    <p>Multiple motivations Preprocessing for Natural Language Processing tasks</p>
    <p>e.g., machine translation, relation extraction, parsing</p>
    <p>Reading aids, Language Comprehension</p>
    <p>e.g., people with aphasia, dyslexia, 2nd language learners</p>
  </div>
  <div class="page">
    <p>Text Simplification</p>
    <p>John wrote a book. I read the book.Last year I read the book John authored Original sentence One or several simpler sentences</p>
    <p>Word or phrase substitution</p>
    <p>Sentence splitting</p>
    <p>Deletion</p>
    <p>Multiple operations</p>
    <p>Structural</p>
    <p>Lexical</p>
  </div>
  <div class="page">
    <p>In this talk</p>
    <p>Both structural and lexical simplification.</p>
    <p>The first simplification system combining structural transformations, using semantic structures, and neural machine translation.</p>
    <p>Compares favorably to the state-of-the-art in combined structural and lexical simplification.</p>
    <p>Alleviates the over-conseratism of MT-based systems.</p>
  </div>
  <div class="page">
    <p>Overview 1. Current approaches and challenges</p>
  </div>
  <div class="page">
    <p>Current Approaches and Challenges</p>
    <p>MT-Based Simplification</p>
    <p>Sentence simplification as monolingual machine translation</p>
    <p>Models</p>
    <p>Phrase-Based SMT (Specia, 2010; Coster and Kauchak, 2011; Wubben et al, 2012; tajner et al., 2015)</p>
    <p>Syntax-Based SMT (Xu et al., 2016)</p>
    <p>Neural Machine Translation (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017)</p>
  </div>
  <div class="page">
    <p>Current Approaches and Challenges</p>
    <p>MT-Based Simplification</p>
    <p>Sentence simplification as monolingual machine translation</p>
    <p>Corpora</p>
    <p>English / Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak., 2011; Hwang et al., 2015)</p>
    <p>Newsela (Xu et al., 2015)</p>
  </div>
  <div class="page">
    <p>Conservatism in MT-Based Simplification</p>
    <p>In both SMT and NMT Text Simplification, a large proportion of the input sentences are not modified. (Alva-Manchego et al., 2017; on the Newsela corpus).</p>
    <p>It is confirmed in the present work (experiments on Wikipedia):</p>
    <p>For the NTS system (Nisioi et al., 2017) / Moses (Koehn et al., 2007)</p>
    <p>- 66% / 80% of the input sentences remain unchanged.</p>
    <p>- None of the references are identical to the source.</p>
    <p>- According to automatic and human evaluation, the references are indeed simpler.</p>
    <p>Conservatism in MT-Based simplification is excessive</p>
  </div>
  <div class="page">
    <p>Sentence Splitting in Text Simplification</p>
    <p>Sentence splitting is not addressed.</p>
    <p>Rareness of splittings in the simplification training corpora.</p>
    <p>(Narayan and Gardent, 2014; Xu et al., 2015).</p>
    <p>Recently, corpus focusing on sentence splitting for the Split-and-Rephrase task</p>
    <p>(Narayan et al., 2017) where the other operations are not addressed.</p>
    <p>Splitting in NMT-Based Simplification</p>
  </div>
  <div class="page">
    <p>Sentence Splitting in Text Simplification</p>
    <p>Directly modeling sentence splitting</p>
    <p>- Compilation and validation can be laborious (Shardlow, 2014)</p>
    <p>- Many rules are often involved (e.g., 111 rules in Siddharthan and Angrosh, 2014) for relative clauses, appositions, subordination and coordination).</p>
    <p>- Usually language specific.</p>
  </div>
  <div class="page">
    <p>Sentence Splitting in Text Simplification</p>
    <p>Example:</p>
    <p>Directly modeling sentence splitting</p>
    <p>One of the two rules for relative clauses in Siddharthan, 2004.</p>
    <p>Relative clause Relative PronounNoun phrase</p>
  </div>
  <div class="page">
    <p>Sentence Splitting in Text Simplification</p>
    <p>Directly modeling sentence splitting</p>
    <p>Narayan and Gardent (2014) - HYBRID</p>
    <p>- Discourse Semantic Representation (DRS) structures for splitting and deletion.</p>
    <p>- Depends on the proportion of splittings in the training corpus.</p>
    <p>We here use an intermediate way:</p>
    <p>Simple algorithm to directly decompose the sentence into its semantic constituents.</p>
  </div>
  <div class="page">
    <p>Direct Semantic Splitting (DSS)</p>
    <p>A simple algorithm that directly decomposes the sentence into its semantic components, using 2 splitting rules.</p>
    <p>The splitting is directed by semantic parsing.</p>
    <p>The semantic annotation directly captures shared arguments.</p>
    <p>It can be used as a preprocessing step for other simplification operations.</p>
    <p>Input sentence Split sentence Output</p>
    <p>DSS NMT-Based Simplification</p>
    <p>Deletions, Word substitutionsSentence Splitting</p>
    <p>Reduces conservatism</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - Based on typological and cognitive theories</p>
    <p>(Dixon, 2010, 2012; Langacker, 2008)</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - Stable across translations (Sulem, Abend and Rappoport, 2015)</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - Used for the evaluation of MT, GEC and Text Simplification</p>
    <p>(Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018)</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - Explicitly annotates semantic distinctions, abstracting away from syntax (like AMR; Banarescu et al., 2013)</p>
    <p>- Unlike AMR, semantic units are directly anchored in the text.</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - UCCA parsing: TUPA parser (Hershcovich et al., 2017, 2018)</p>
    <p>- Shared Task in Sem-Eval 2019!</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - Scenes evoked by a Main Relation (Process or State).</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - A Scene may contain one or several Participants.</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P</p>
    <p>A</p>
    <p>He</p>
    <p>observed</p>
    <p>H</p>
    <p>A</p>
    <p>Parallel Scene (H)</p>
    <p>Participant (A) Process (P) State (S)</p>
    <p>Center (C) Elaborator (E) Relator (R)</p>
    <p>E C E</p>
    <p>the planet</p>
    <p>A</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - A Scene can provide additional information on an established entity:</p>
    <p>it is then an Elaborator Scene.</p>
    <p>R S</p>
    <p>which has</p>
    <p>A</p>
    <p>E C</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>H</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - A Scene may also be a Participant in another Scene:</p>
    <p>It is then a Participant Scene.</p>
    <p>A A</p>
    <p>PA</p>
    <p>P</p>
    <p>His arrival</p>
    <p>surprised Mary</p>
  </div>
  <div class="page">
    <p>The Semantic Structures</p>
    <p>P A</p>
    <p>A</p>
    <p>He came back home</p>
    <p>L and</p>
    <p>H H</p>
    <p>A PA</p>
    <p>played piano</p>
    <p>Semantic Annotation: UCCA (Abend and Rappoport, 2013) - In the other cases, Scenes are annotated as Parallel Scenes.</p>
    <p>A Linker may be included.</p>
    <p>Parallel Scene (H) Linker (L)</p>
    <p>Participant (A) Process (P)</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>Main idea: Placing each Scene in a different sentence.</p>
    <p>Fits with event-wise simplification (Glava and tajner, 2013)</p>
    <p>Here we only use semantic criteria.</p>
    <p>It was also investigated in the context of Text Simplification evaluation:</p>
    <p>SAMSA measure (Sulem, Abend and Rappoport, NAACL 2018)</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>He came back home and played piano.</p>
    <p>He came back home. He played piano.</p>
    <p>He came back home</p>
    <p>and</p>
    <p>H H L</p>
    <p>A P</p>
    <p>A</p>
    <p>played piano</p>
    <p>He came back home</p>
    <p>A P A</p>
    <p>He played piano</p>
    <p>A</p>
    <p>A</p>
    <p>P</p>
    <p>P A</p>
    <p>A</p>
    <p>Rule 1:</p>
    <p>Parallel Scenes</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>He came back home</p>
    <p>and</p>
    <p>H H L</p>
    <p>A P</p>
    <p>A</p>
    <p>played piano</p>
    <p>He came back home</p>
    <p>A P A</p>
    <p>He played piano</p>
    <p>A</p>
    <p>A</p>
    <p>P</p>
    <p>P A</p>
    <p>A</p>
    <p>SSc1Sc2Scn</p>
    <p>Input sentence</p>
    <p>Input Scenes</p>
    <p>Rule 1:</p>
    <p>Parallel Scenes</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>He observed the planet which has 14 satellites.</p>
    <p>He observed the planet. Planet has 14 satellites.</p>
    <p>H</p>
    <p>A A</p>
    <p>E C</p>
    <p>E</p>
    <p>A</p>
    <p>R S</p>
    <p>E C</p>
    <p>A</p>
    <p>He</p>
    <p>observed</p>
    <p>P</p>
    <p>the planet</p>
    <p>which has</p>
    <p>Rule 2:</p>
    <p>He</p>
    <p>P</p>
    <p>A A</p>
    <p>observed C</p>
    <p>E</p>
    <p>the</p>
    <p>the</p>
    <p>planet</p>
    <p>S</p>
    <p>A A</p>
    <p>E Cplanet has</p>
    <p>Elaborator Scenes</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>H</p>
    <p>A A</p>
    <p>E C</p>
    <p>E</p>
    <p>A</p>
    <p>R S</p>
    <p>E C</p>
    <p>A</p>
    <p>He</p>
    <p>observed</p>
    <p>P</p>
    <p>planet</p>
    <p>which has</p>
    <p>Rule 2:</p>
    <p>He</p>
    <p>P</p>
    <p>A A</p>
    <p>observed C</p>
    <p>E</p>
    <p>the</p>
    <p>the</p>
    <p>planet</p>
    <p>S</p>
    <p>A A</p>
    <p>E Cplanet has</p>
    <p>SS(SciCi)Sc1Scn</p>
    <p>Input sentence</p>
    <p>Input sentence without the Elaborator Scenes, preserving the Minimal Center</p>
    <p>Elaborator Scenes</p>
    <p>Elaborator Scenes</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>No regeneration module</p>
    <p>Grammatical errors resulting from the split are not addressed by the rules. e.g., no article regeneration.</p>
    <p>The output is directly fed into the NMT component.</p>
    <p>Example:</p>
    <p>He observed the planet which has 14 satellites</p>
    <p>He observed the planet. Planet has 14 satellites.</p>
  </div>
  <div class="page">
    <p>The Semantic Rules</p>
    <p>Participant Scenes are not separated here to avoid direct splitting in these cases:</p>
    <p>- Nominalizations: His arrival surprised Mary.</p>
    <p>- Indirect speech: He said John went to school.</p>
    <p>More transformations would be required for splitting in these cases.</p>
  </div>
  <div class="page">
    <p>Combining DSS with Neural Text Simplification</p>
    <p>After DSS, the output is fed to an MT-based simplification system.</p>
    <p>We use a state-of-the-art NMT-Based TS system, NTS (Nisioi et al., 2017).</p>
    <p>The combined system is called SENTS.</p>
  </div>
  <div class="page">
    <p>Combining DSS with Neural Text Simplification</p>
    <p>NTS was built using the OpenNMT (Klein et al., 2017) framework.</p>
    <p>We use the NTS-w2v provided model where word2vec embeddings are used for the initialization.</p>
    <p>Beam search is used during decoding. We explore both the highest (h1) and a lower ranked hypothesis (h4), which is less conservative.</p>
    <p>NTS model trained on the corpus of Hwang et al., 2015 (~280K sentence pairs).</p>
    <p>It was tuned on the corpus of Xu et al., 2016 (2000 sentences with 8 references).</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Corpus: Test set of Xu et al., 2016: 359 sentences, each with 8 references</p>
    <p>Automatic evaluation:  BLEU (Panineni et al., 2002)</p>
    <p>SARI (Xu et al., 2016)</p>
    <p>Conservatism statistics:</p>
    <p>e.g., percentage of sentences copied from the input (%Same)</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Human evaluation:  First 70 sentences of the corpus</p>
    <p>3 annotators  native English speakers</p>
    <p>4 questions for each input-output pair</p>
    <p>Is the output fluent and grammatical?</p>
    <p>Does the output preserve the meaning of the input?</p>
    <p>Is the output simpler than the input?</p>
    <p>Is the output simpler than the input, ignoring the complexity of the words?</p>
    <p>Qa</p>
    <p>Qb</p>
    <p>Qc</p>
    <p>Qd</p>
    <p>4 parameters: Grammaticality (G) Meaning Preservation (P) Simplicity (S) Structural Simplicity (StS)</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>BLEU SARI G M S StS</p>
    <p>Identity 94.93 25.44 4.80 5.00 0.00 0.00</p>
    <p>Simple Wikipedia</p>
    <p>G  Grammaticality: 1 to 5 scale S  Simplicity: -2 to +2 scale P  Meaning Preservation: 1 to 5 scale StS  Structural Simplicity: -2 to +2 scale</p>
    <p>Human evaluation (first 70 sentences):</p>
    <p>Automatic evaluation: BLEU, SARI</p>
    <p>Identity gets the highest BLEU score and the lowest SARI scores.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>BLEU SARI G M S StS</p>
    <p>HYBRID 52.82 27.40 2.96 2.46 0.43 0.43</p>
    <p>SENTS-h1 58.94 30.27 3.98 3.33 0.68 0.63</p>
    <p>G  Grammaticality: 1 to 5 scale S  Simplicity: -2 to +2 scale P  Meaning Preservation: 1 to 5 scale StS  Structural Simplicity: -2 to +2 scale</p>
    <p>Human evaluation (first 70 sentences):</p>
    <p>Automatic evaluation: BLEU, SARI</p>
    <p>SENTS-h4 57.71 31.90 3.54 2.98 0.50 0.36</p>
    <p>The two SENTS systems outperform HYBRID in terms of BLEU, SARI, G, M and S.</p>
    <p>SENTS-h1 has the best StS score.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>%Same SARI G M S StS</p>
    <p>NTS-h1 66.02 28.73 4.56 4.48 0.22 0.15</p>
    <p>NTS-h4 2.74 36.55 4.29 3.90 0.31 0.19</p>
    <p>G  Grammaticality: 1 to 5 scale S  Simplicity: -2 to +2 scale P  Meaning Preservation: 1 to 5 scale StS  Structural Simplicity: -2 to +2 scale</p>
    <p>Human evaluation (first 70 sentences):</p>
    <p>Automatic evaluation: %Same, SARI</p>
    <p>SENTS-h1 6.69 30.27 3.98 3.33 0.68 0.63</p>
    <p>SENTS-h4 0.28 31.90 3.54 2.98 0.50 0.36</p>
    <p>Compared to NTS, SENTS reduces conservatism and increases simplicity.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>%Same SARI G M S StS</p>
    <p>DSS 8.64 36.76 3.42 4.15 0.16 0.16</p>
    <p>G  Grammaticality: 1 to 5 scale S  Simplicity: -2 to +2 scale P  Meaning Preservation: 1 to 5 scale StS  Structural Simplicity: -2 to +2 scale</p>
    <p>Human evaluation (first 70 sentences):</p>
    <p>Automatic evaluation: %Same, SARI</p>
    <p>SENTS-h1 6.69 30.27 3.98 3.33 0.68 0.63</p>
    <p>SENTS-h4 0.28 31.90 3.54 2.98 0.50 0.36</p>
    <p>Compared to DSS, SENTS improves grammaticality and increases structural simplicity, since deletions are performed by the NTS component.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Replacing NTS by Statistical MT  Combination of DSS and Moses: SEMoses</p>
    <p>The behavior of SEMoses is similar to that of DSS, confirming the over-conservatism of Moses (Alva-Manchego et al., 2017) for simplification.</p>
    <p>All the splitting points from the DSS phase are preserved.</p>
    <p>Replacing the parser by manual annotation  In the case of SEMoses, meaning preservation is improved.</p>
    <p>Simplicity degrades, possibly due to a larger number of annotated Scenes.</p>
    <p>In the case of SENTS-h1, high simplicity scores are obtained.</p>
  </div>
  <div class="page">
    <p>Human Evaluation Benchmark</p>
    <p>1960 sentence pairs</p>
    <p>70 source sentences</p>
    <p>28 systems</p>
    <p>3 annotators</p>
    <p>4 parameters</p>
    <p>Data: https://github.com/eliorsulem/simplification-acl2018</p>
  </div>
  <div class="page">
    <p>Conclusion (1)</p>
    <p>We presented here the first simplification system combining</p>
    <p>semantic structures and neural machine translation.</p>
    <p>Our system compares favorably to the state-of-the-art in combined structural and lexical simplification.</p>
    <p>This approach addresses the conservatism of MT-based systems.</p>
    <p>Sentence splitting is performed without relying on a specialized corpus.</p>
  </div>
  <div class="page">
    <p>Conclusion (2)</p>
    <p>Sentence splitting is treated as the decomposition of the sentence into its Scenes (as in SAMSA evaluation measure; Sulem, Abend and Rappoport, NAACL 2018)</p>
    <p>Future work will leverage UCCAs cross-linguistic applicability to support multi-lingual text simplification and simplification pre-processing for MT.</p>
  </div>
  <div class="page">
    <p>Thank you</p>
    <p>eliors@cs.huji.ac.il</p>
    <p>Elior Sulem</p>
    <p>www.cs.huji.ac.il/~eliors</p>
    <p>Data: https://github.com/eliorsulem/simplification-acl2018</p>
  </div>
</Presentation>
