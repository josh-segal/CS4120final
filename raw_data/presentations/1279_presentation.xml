<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling</p>
    <p>Long Zheng1, Xianliang Li1, Yaohui Zheng1,Yu Huang1, Xiaofei Liao1, Hai Jin1, Jingling Xue1 Zhiyuan Shao1, and Qiang-Sheng Hua1</p>
    <p>July 15-17, 2020</p>
  </div>
  <div class="page">
    <p>Graph Processing Is Ubiquitous</p>
    <p>Relationship Prediction Recommendation Systems</p>
    <p>Information Tracking Knowledge Mining</p>
  </div>
  <div class="page">
    <p>Graph Processing: CPU vs. GPU</p>
    <p>GPU V100</p>
    <p>Performance Double-Precision: 7.8TFLOPS, Single-Precision: 15.7TFLOPS InterConnect Bandwidth NVLINK 300GB/s</p>
    <p>Memory 32GB HBM2, 1134GB/s</p>
    <p>Data source: V100 Performance, https://developer.nvidia.com/hpc-application-performance</p>
    <p>GPU often offers 10x at least speedup over CPU for graph processing</p>
  </div>
  <div class="page">
    <p>Graph Processing: CPU vs. GPU</p>
    <p>GPU V100</p>
    <p>Performance Double-Precision: 7.8TFLOPS, Single-Precision: 15.7TFLOPS InterConnect Bandwidth NVLINK 300GB/s</p>
    <p>Memory 32GB HBM2, 1134GB/s</p>
    <p>Data source: V100 Performance, https://developer.nvidia.com/hpc-application-performance</p>
    <p>GPU often offers 10x at least speedup over CPU for graph processing</p>
    <p>Many real-world graphs cannot fit into GPU memory to enjoy high-performance in-memory graph processing</p>
  </div>
  <div class="page">
    <p>GPU-Accelerated Heterogeneous Architecture</p>
    <p>The significant performance gap between CPU and GPU may severely limit the performance potential expected on the GPU-accelerated heterogeneous architecture.</p>
  </div>
  <div class="page">
    <p>Existing Solutions on GPU-Accelerated Heterogeneous Architecture  Totem (PACT12)</p>
    <p>Partitioned into two large subgraphs, one for CPU, one for GPU  Significant load unbalance</p>
    <p>Graphie (PACT17)  Subgraphs are partitioned and streamed to GPU  All subgraphs are transferred in their entirety  Bandwidth is wasted</p>
    <p>Garaph (USENIX ATC17)  All the subgraphs are processed on CPU if the active vertices</p>
    <p>in the entire graph have a lot of (50%) outgoing edges  Processed on the host otherwise</p>
  </div>
  <div class="page">
    <p>A Generic Example of Graph Processing Engine</p>
    <p>A graph is partitioned into many slices</p>
    <p>Vertices reside in GPU memory Edges are streamed to GPU on demand</p>
  </div>
  <div class="page">
    <p>A Generic Example of Graph Processing Engine</p>
    <p>A graph is partitioned into many slices</p>
    <p>In an iteration, all active subgraphs will be transferred entirely to GPU and processed there.</p>
    <p>Vertices reside in GPU memory Edges are streamed to GPU on demand</p>
  </div>
  <div class="page">
    <p>A Generic Example of Graph Processing Engine</p>
    <p>A graph is partitioned into many slices</p>
    <p>In an iteration, all active subgraphs will be transferred entirely to GPU and processed there.</p>
    <p>These active subgraphs processed on GPU will activate more destination vertices possibly.</p>
    <p>Vertices reside in GPU memory Edges are streamed to GPU on demand</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>This simple graph engine wastes a considerable amount of limited host-GPU bandwidth, limiting performance and scalability further.</p>
    <p>Algo. Used Unused</p>
    <p>TW CC 12.15GB 21.44GB SSSP 22.74GB 77.42GB MST 25.78GB 106.27GB</p>
    <p>UK CC 43.41GB 688.43GB SSSP 81.64GB 1302.85GB MST 134.93GB 2099.25GB</p>
    <p>Only 6.29%~36.17% transferred data are used</p>
    <p>Perf. can be plateaued quickly at #SMX=4</p>
    <p>Little gains when more powerful GPUs are used</p>
  </div>
  <div class="page">
    <p>Characterization of Subgraph Data The data of a subgraph are changing  Useful Data (UD)</p>
    <p>associated with active vertices  must be transferred to GPU</p>
    <p>Potentially Useful Data (PUD)  associated with all future active vertices  (not) used in future (current) iteration</p>
    <p>Never Used Data (NUD)  Converged  Never be active again</p>
  </div>
  <div class="page">
    <p>Characterization of Subgraph Data The data of a subgraph are changing  Useful Data (UD)</p>
    <p>associated with active vertices  must be transferred to GPU</p>
    <p>Potentially Useful Data (PUD)  associated with all future active vertices  (not) used in future (current) iteration</p>
    <p>Never Used Data (NUD)  Converged  Never be active again</p>
    <p>PUD is substantial in earlier iterations but discarded</p>
    <p>NUD is becoming dominant but streamed redundantly</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Value-Driven differential Scheduling  distinguish high- and</p>
    <p>low-value subgraphs in each iteration adaptively</p>
    <p>Value-Driven Graph Processing Engines  exploit the most value out of high- and</p>
    <p>low-value subgraphs to maximize efficiency</p>
    <p>Scaph A scale-up graph processing for large-scale graph on GPUaccelerated heterogeneous platforms</p>
  </div>
  <div class="page">
    <p>Quantifying the Value of a Subgraph</p>
    <p>Conceptually, the value of a subgraph can be measured by its UD used in the current iteration and its PUD used in future iterations.</p>
    <p>The value of a subgraph from the current iteration and MAX-th iteration can be defined as:</p>
    <p>The value of a subgraph depends upon its active vertices and their degrees</p>
  </div>
  <div class="page">
    <p>Value-Driven Differential Scheduling</p>
    <p>G is partitioned and distributed on NUMA nodes</p>
    <p>Vertices on GPU, edges streamed  Estimate the value of an active</p>
    <p>subgraph</p>
    <p>Differential Scheduling  High-Value Subgraph Engine</p>
    <p>Low-Value Subgraph Engine</p>
    <p>Updated vertices will be transferred from GPU to CPU. Edges, not modified, are not transferred</p>
  </div>
  <div class="page">
    <p>Checking If a Subgraph is High Value  Suppose a subgraph G is a high-value subgraph, its throughput can be measured below:</p>
    <p>Suppose a subgraph G is a low-value subgraph, its throughput can be measured below:</p>
    <p>Now, G is a high-value subgraph if . Thus, we need to analyze:</p>
    <p>This condition is heuristically simplified below:  , which indicates UD is dominant.</p>
    <p>, and . UD remains a medium level but is growing increasingly over iteration.</p>
    <p>a=50%, b=30%</p>
  </div>
  <div class="page">
    <p>High-Value Subgraph Processing</p>
    <p>Inspired from CLIP (ATC17), each high-value subgraph can be scheduled multiple times to exploit intrinsic value of a subgraph</p>
    <p>In a GPU context, subgraph sizes are small.  We propose a delayed scheduling to</p>
    <p>exploit PUD across the subgraphs</p>
    <p>Queue-assisted multi-round processing  k-level priority queue (PQ1, , PQk)</p>
    <p>Subgraph streamed to TransSet asynchronously</p>
    <p>A subgraph in PQ1 is scheduled first. Its priority drops by one once processed</p>
    <p>Subgraph transfer and scheduling are executed concurrently</p>
  </div>
  <div class="page">
    <p>Complexity Analysis</p>
    <p>Time Complexity  The queue depth k is expected to be bounded by BW/BW</p>
    <p>For a typical server (BW=224GB/s and BW=11.4GB/s), k can be less than 20, which is typically small.</p>
    <p>Space Complexity  k-level queue maintains only the indices of the active subgraphs</p>
    <p>The worst complexity is</p>
    <p>For P100 (GPU memory: 16GB, Index size: 4B, subgraph size: 32M), the space overhead of the queue is 2KB, which is small.</p>
  </div>
  <div class="page">
    <p>Low-Value Subgraph Processing</p>
    <p>NUMA-Aware Load Balancing  Intra-node load balancing: The UD extraction for each subgraph is</p>
    <p>done in its own thread.</p>
    <p>Inter-node load balancing: A NUMA node is duplicated an equal number of randomly selected subgraphs from the other nodes</p>
    <p>Bitmap-based UD extraction  All vertices of a subgraph is stored in a bitmap</p>
    <p>1 (0) indicates the corresponding vertex is active (inactive)</p>
    <p>To reduce the fragmentation of the UD-induced subgraphs, we divide each chunk to store a subgraph into smaller tiles.</p>
  </div>
  <div class="page">
    <p>Limitations (More details in the paper)</p>
    <p>Graph Partition  A greedy vertex-cut partition</p>
    <p>Out-of-core solution  Using the disk as secondary storage is promising to support even</p>
    <p>larger graphs</p>
    <p>Performance Profitability</p>
  </div>
  <div class="page">
    <p>Experimental Setup  Baselines</p>
    <p>Totem, Graphie, Garaph</p>
    <p>Graph Size: 32MB  Graph Applications</p>
    <p>Typical algorithms: SSSP/CC/MST</p>
    <p>Actual workloads: Two NNDR/GCS</p>
    <p>Datasets  6 real-world graphs:</p>
    <p>5 large synthesized RMAT graphs</p>
    <p>Platforms  Host: E5-2680v4 (512GB memory, two NUMA nodes)</p>
    <p>GPU: P100 (56 SMXs, 3584 cores, 16GB memory)</p>
  </div>
  <div class="page">
    <p>Efficiency</p>
    <p>Scaph vs. Totem  UD and PUD exploited more fully</p>
    <p>yields 2.23x~7.64x speedups</p>
    <p>Scaph vs. Graphie  Exploit PUD and NUD is discarded</p>
    <p>yields 3.03x~16.41x speedups</p>
    <p>Scaph vs. Garaph  Removing NUD transferred</p>
    <p>yields 1.93x~5.62x speedups</p>
  </div>
  <div class="page">
    <p>Effectiveness</p>
    <p>Scaph-HVSP: All the low-value subgraphs are misidentified as high-value subgraphs  Scaph-LVSP: All the high-value subgraphs are misidentified as low-value subgraphs  Scaph-HBASE: Differential processing is used but queue-based scheduling is not applied  Scaph-LBASE: A variation of Scaph-LVSP except that every subgraph is streamed entirely</p>
    <p>Scaph-HBASE vs. Scaph-HVSP: Significant performance difference shows the effectiveness of our delay-based subgraph scheduling</p>
    <p>Scaph vs. Scaph-LVSP and Scaph-HVSP: Scaph obtains the best of both worlds, showing the effectiveness of differential subgraph scheduling</p>
  </div>
  <div class="page">
    <p>Sensitivity Study</p>
    <p>Varying #SMXs  Significantly more scalable</p>
    <p>Varying Graph Sizes  Slower performance</p>
    <p>reduction rate</p>
    <p>Varying GPU memory  Scaph is nearly insensitive to</p>
    <p>the GPU memory used</p>
    <p>GPU generations  Enables significant speedups</p>
  </div>
  <div class="page">
    <p>Sensitivity Study (con't)</p>
    <p>A1: Scaph-HVSP  A5: Scaph-LVSP  A3 represents a nice point</p>
    <p>for yielding good performance results.</p>
  </div>
  <div class="page">
    <p>Runtime Overhead</p>
    <p>VDDS: The cost of computing the subgraph value is negligible  HVSP: Queue management cost per iteration is as small as 0.79% of total time  LVSP: CPU-GPU bitmap transfer cost per iteration represents 4.3% of total time</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Scaph: Scale up graph processing for large graphs on GPU-accelerated heterogenous architectures.  Subgraph Value Characterization, which quantifies the value of a</p>
    <p>subgraph adaptively and dynamically  Value-Driven Differential Scheduling, which adaptively distinguishes</p>
    <p>high- and low-value subgraphs and dispatches them to an appropriate graph processing engine</p>
    <p>Value-Driven Graph Processing Engines, which squeeze the most value out of high- and low-value subgraphs to maximize efficiency</p>
    <p>It outperforms state-of-the-art heterogeneous graph systems, Totem (4.12), Graphie (8.93), and Garaph (3.71).</p>
  </div>
  <div class="page">
    <p>Thanks! longzh@hust.edu.cn</p>
  </div>
</Presentation>
