<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Using Natural Language Relations between Answer Choices for Machine Comprehension</p>
    <p>Rajkumar Pujari and Dan Goldwasser</p>
    <p>June 5, 2019</p>
  </div>
  <div class="page">
    <p>Intuition</p>
    <p>Intuition</p>
    <p>When humans perform Reading Comprehension, we answer all the given questions consistently.</p>
    <p>But, when we test Machine Comprehension, most computational settings consider each question or each choice in isolation.</p>
    <p>Example</p>
    <p>When they turned on the stove When the pan was the right temperature J</p>
    <p>They didnt use the stove but a microwave Because they needed to heat up the pan J</p>
    <p>Source: SemEval 2018 Task-11 dataset ([Ostermann et al. 2018])</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 2/15</p>
  </div>
  <div class="page">
    <p>Intuition</p>
    <p>Intuition</p>
    <p>When humans perform Reading Comprehension, we answer all the given questions consistently.</p>
    <p>But, when we test Machine Comprehension, most computational settings consider each question or each choice in isolation.</p>
    <p>Example</p>
    <p>When they turned on the stove When the pan was the right temperature J</p>
    <p>They didnt use the stove but a microwave Because they needed to heat up the pan J</p>
    <p>Source: SemEval 2018 Task-11 dataset ([Ostermann et al. 2018])</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 2/15</p>
  </div>
  <div class="page">
    <p>Intuition (contd.)</p>
    <p>Similarly, in settings where multiple choices could be correct, we could use the relationships between choices.</p>
    <p>Example</p>
    <p>How can the military benefit from the existence of the CIA? 1 They can use them as they wish 2 The agency is keenly attentive to the militarys strategic and</p>
    <p>tactical requirements J 3 The CIA knows what intelligence the military requires and has</p>
    <p>the resources to obtain that intelligence J</p>
    <p>c3 entails c2 = flip c2 from wrong to correct. Source: MultiRC dataset ([Khashabi et al. 2018])</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 3/15</p>
  </div>
  <div class="page">
    <p>Intuition (contd.)</p>
    <p>Similarly, in settings where multiple choices could be correct, we could use the relationships between choices.</p>
    <p>Example</p>
    <p>How can the military benefit from the existence of the CIA? 1 They can use them as they wish 2 The agency is keenly attentive to the militarys strategic and</p>
    <p>tactical requirements J 3 The CIA knows what intelligence the military requires and has</p>
    <p>the resources to obtain that intelligence J</p>
    <p>c3 entails c2 = flip c2 from wrong to correct. Source: MultiRC dataset ([Khashabi et al. 2018])</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 3/15</p>
  </div>
  <div class="page">
    <p>Abstract</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 4/15</p>
  </div>
  <div class="page">
    <p>Abstract</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 4/15</p>
  </div>
  <div class="page">
    <p>Abstract</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 4/15</p>
  </div>
  <div class="page">
    <p>Approach</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 5/15</p>
  </div>
  <div class="page">
    <p>Approach</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 5/15</p>
  </div>
  <div class="page">
    <p>Approach</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 5/15</p>
  </div>
  <div class="page">
    <p>Stand-alone QA System</p>
    <p>We use the TriAN-single model proposed by [Wang et al. 2018] for SemEval-2018 task-11 as our stand-alone QA system.</p>
    <p>Figure: TriAN model architecture (figure adopted from [Wang et al. 2018])</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 6/15</p>
  </div>
  <div class="page">
    <p>NLI System</p>
    <p>Our NLI system was inspired from decomposable-attention model proposed by [Parikh et al. 2016]</p>
    <p>Issue: Choices are often short phrases. NLI relations among them exist only in the context of the given question.</p>
    <p>Example</p>
    <p>What do human children learn by playing games and sports?</p>
    <p>Resolution: We modified the architecture proposed in [Parikh et al. 2016] to accommodate the question-choice pairs as opposed to sentence pairs in the original model.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 7/15</p>
  </div>
  <div class="page">
    <p>NLI System</p>
    <p>Our NLI system was inspired from decomposable-attention model proposed by [Parikh et al. 2016]</p>
    <p>Issue: Choices are often short phrases. NLI relations among them exist only in the context of the given question.</p>
    <p>Example</p>
    <p>What do human children learn by playing games and sports?</p>
    <p>Resolution: We modified the architecture proposed in [Parikh et al. 2016] to accommodate the question-choice pairs as opposed to sentence pairs in the original model.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 7/15</p>
  </div>
  <div class="page">
    <p>NLI System</p>
    <p>Our NLI system was inspired from decomposable-attention model proposed by [Parikh et al. 2016]</p>
    <p>Issue: Choices are often short phrases. NLI relations among them exist only in the context of the given question.</p>
    <p>Example</p>
    <p>What do human children learn by playing games and sports?</p>
    <p>Resolution: We modified the architecture proposed in [Parikh et al. 2016] to accommodate the question-choice pairs as opposed to sentence pairs in the original model.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 7/15</p>
  </div>
  <div class="page">
    <p>Inference</p>
    <p>We enforce consistency between the QA answers and the NLI relations at inference time.</p>
    <p>The answers and the relations are scored by the confidence scores from the QA and the NLI systems.</p>
    <p>We used the following rules to enforce consistency: 1 ci is true &amp; ci entails cj = cj is true. 2 ci is true &amp; ci contradicts cj = cj is false.</p>
    <p>We used Deep Relational Learning (DRaiL) framework proposed by [Zhang et al. 2016] for inference</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 8/15</p>
  </div>
  <div class="page">
    <p>Inference</p>
    <p>We enforce consistency between the QA answers and the NLI relations at inference time.</p>
    <p>The answers and the relations are scored by the confidence scores from the QA and the NLI systems.</p>
    <p>We used the following rules to enforce consistency: 1 ci is true &amp; ci entails cj = cj is true. 2 ci is true &amp; ci contradicts cj = cj is false.</p>
    <p>We used Deep Relational Learning (DRaiL) framework proposed by [Zhang et al. 2016] for inference</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 8/15</p>
  </div>
  <div class="page">
    <p>Inference</p>
    <p>We enforce consistency between the QA answers and the NLI relations at inference time.</p>
    <p>The answers and the relations are scored by the confidence scores from the QA and the NLI systems.</p>
    <p>We used the following rules to enforce consistency: 1 ci is true &amp; ci entails cj = cj is true. 2 ci is true &amp; ci contradicts cj = cj is false.</p>
    <p>We used Deep Relational Learning (DRaiL) framework proposed by [Zhang et al. 2016] for inference</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 8/15</p>
  </div>
  <div class="page">
    <p>Inference</p>
    <p>We enforce consistency between the QA answers and the NLI relations at inference time.</p>
    <p>The answers and the relations are scored by the confidence scores from the QA and the NLI systems.</p>
    <p>We used the following rules to enforce consistency: 1 ci is true &amp; ci entails cj = cj is true. 2 ci is true &amp; ci contradicts cj = cj is false.</p>
    <p>We used Deep Relational Learning (DRaiL) framework proposed by [Zhang et al. 2016] for inference</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 8/15</p>
  </div>
  <div class="page">
    <p>Self-Training</p>
    <p>We devised a self-training protocol to adopt the NLI system to the Machine Comprehension datasets (weak-supervision)</p>
    <p>If the SNLI-trained NLI model predicted entailment with a confidence above a threshold and the gold labels of the ordered choice pair were true-true, the relation was labeled entailment, and similarly we generate data for contradiction</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 9/15</p>
  </div>
  <div class="page">
    <p>Self-Training</p>
    <p>We devised a self-training protocol to adopt the NLI system to the Machine Comprehension datasets (weak-supervision)</p>
    <p>If the SNLI-trained NLI model predicted entailment with a confidence above a threshold and the gold labels of the ordered choice pair were true-true, the relation was labeled entailment, and similarly we generate data for contradiction</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 9/15</p>
  </div>
  <div class="page">
    <p>Joint Model</p>
    <p>The design of our joint model is motivated by the two objectives:</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 10/15</p>
  </div>
  <div class="page">
    <p>MultiRC Results</p>
    <p>Method EM0 EM1 Stand-alone QA 18.15 52.99</p>
    <p>QA + NLISNLI 19.41 56.13</p>
    <p>QA + NLIMultiRC 21.62 55.72</p>
    <p>Joint Model 20.36 57.08</p>
    <p>Human 56.56 83.84</p>
    <p>Table: Summary of results on MultiRC dataset. EM0 is the percentage of questions for which all the choices are correct. EM1 is the the percentage of questions for which at most one choice is wrong.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 11/15</p>
  </div>
  <div class="page">
    <p>SemEval 2018 Results</p>
    <p>Model Dev Test</p>
    <p>Stand-alone QA 83.20% 80.80%</p>
    <p>Joint Model 85.40% 82.10%</p>
    <p>Table: Accuracy of various models on SemEval18 task-11 dataset</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 12/15</p>
  </div>
  <div class="page">
    <p>Error Analysis</p>
    <p>Identification of NLI relations is far from perfect.</p>
    <p>NLI system returns entailment when there is a high lexical overlap</p>
    <p>NLI system returns contradiction upon the presence of a strong negation word such as not.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 13/15</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>We proposed a framework to use entailment and contradiction relations to improve Machine Comprehension</p>
    <p>Self-training results suggest the presence of other subtle relationships among choices.</p>
    <p>Consider:</p>
    <p>Text: I snack when I shop</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 14/15</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>We proposed a framework to use entailment and contradiction relations to improve Machine Comprehension</p>
    <p>Self-training results suggest the presence of other subtle relationships among choices.</p>
    <p>Consider:</p>
    <p>Text: I snack when I shop</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 14/15</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>We proposed a framework to use entailment and contradiction relations to improve Machine Comprehension</p>
    <p>Self-training results suggest the presence of other subtle relationships among choices.</p>
    <p>Consider:</p>
    <p>Text: I snack when I shop</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 14/15</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>We proposed a framework to use entailment and contradiction relations to improve Machine Comprehension</p>
    <p>Self-training results suggest the presence of other subtle relationships among choices.</p>
    <p>Consider:</p>
    <p>Text: I snack when I shop</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 14/15</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 15/15</p>
  </div>
  <div class="page">
    <p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.</p>
    <p>Looking beyond the surface: A challenge set for reading comprehension over multiple sentences.</p>
    <p>In NAACL.</p>
    <p>Simon Ostermann, Michael Roth, Ashutosh Modi, Stefan Thater, and Manfred Pinkal.</p>
    <p>Semeval-2018 task 11: Machine comprehension using commonsense knowledge.</p>
    <p>In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 747757. Association for Computational Linguistics.</p>
    <p>Ankur Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit.</p>
    <p>A decomposable attention model for natural language inference.</p>
    <p>In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 22492255, Austin, Texas, November. Association for Computational Linguistics.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 15/15</p>
  </div>
  <div class="page">
    <p>Liang Wang, Meng Sun, Wei Zhao, Kewei Shen, and Liu Jingming.</p>
    <p>Yuanfudao at semeval-2018 task 11: Three-way attention and relational knowledge for commonsense machine comprehension.</p>
    <p>CoRR, abs/1803.00191.</p>
    <p>Xiao Zhang, Maria Leonor Pacheco, Chang Li, and Dan Goldwasser.</p>
    <p>Introducing DRAIL - a step towards declarative deep relational learning.</p>
    <p>In Proceedings of the Workshop on Structured Prediction for NLP@EMNLP 2016, Austin, TX, USA, November 5, 2016, pages 5462.</p>
    <p>Using NLI Relations for Machine Comprehension Rajkumar Pujari and Dan Goldwasser 15/15</p>
  </div>
</Presentation>
