<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>dsync: Efficient Synchronization of Multi</p>
    <p>Gigabyte Binary Data Thomas Knauth, Christof Fetzer</p>
    <p>November 7 @ LISA 2013</p>
  </div>
  <div class="page">
    <p>Whats the problem?</p>
    <p>doing backups is important  backup process should be fast and not</p>
    <p>waste resources</p>
    <p>just reading 4 TB of data (single disk) takes &gt; 6 hours</p>
    <p>periodic, differential, state synchronization with minimal resource consumption</p>
  </div>
  <div class="page">
    <p>How do you do your backups?</p>
  </div>
  <div class="page">
    <p>Picking the right tool</p>
    <p>copy</p>
    <p>rsync</p>
    <p>lightweight rsync</p>
    <p>ZFS</p>
    <p>dsync</p>
    <p>Dropbox</p>
    <p>Time Machine</p>
    <p>dirvish</p>
    <p>rdiff-backup</p>
  </div>
  <div class="page">
    <p>The generalist: rsync</p>
    <p>operates on file system level  goal is to minimize data transfer  has significant computational overhead for</p>
    <p>large (GB) files</p>
    <p>familiar to system administrators</p>
  </div>
  <div class="page">
    <p>The new guy: dsync</p>
    <p>kernel-space modification  supplemented by user-space tools  operates on block device level  independent of file system</p>
  </div>
  <div class="page">
    <p>Where does it fit in the stack?</p>
    <p>virtual machine</p>
    <p>device mapper</p>
    <p>block device block device</p>
    <p>file system</p>
    <p>loopback device</p>
    <p>device mapper</p>
    <p>virtual machine</p>
    <p>Figure 1: Two configurations where the tracked block device is used by a virtual machine (VM). If the VM used a file of the host system as its backing store, the loopback device turns this file into a block device (right).</p>
    <p>to create a tracked device directly on top of the physical block device (Figure 1, left). The tracked block device replaces the physical device as the VMs backing store.</p>
    <p>Often, the backing store of a virtual machine is a file in the hosts filesystem. In these cases, a loopback device is used to convert the file into a block device. Instead of tracking modifications to a physical device, we track modifications to the loopback device (Figure 1, right). The tracked device again functions as the VMs backing store. The tracking functionality is entirely implemented in the host system kernel, i.e., the guests are unaware of the tracking functionality. The guest OS does not need to be modified, and the tracking works with all guest operating systems.</p>
    <p>The total size of the data structure is not the only concern when allocating memory inside the kernel; the size of a single allocation is also constrained. The kernel offers three different mechanisms to allocate memory: (1) kmalloc(), (2) __get_free_pages(), and (3) vmalloc(). However, only vmalloc() allows us to reliably allocate multiple megabytes of memory with a single invocation. The various ways of allocating Linux kernel memory are detailed in Linux Device Drivers [7].</p>
    <p>Total memory consumption of the tracking data structures may still be a concern: even commodity (consumer) machines commonly provide up to 5 SATA ports for attaching disks. Hard disk sizes of 4 TB are standard these days too. To put this in context, the block-wise dirty status for a 10 TiB setup requires 320 MiB of memory. We see two immediate ways to reduce the memory overhead:</p>
    <p>A bloom filter could be configured to work with a fraction of the bit vectors size. The trade-off is potential false positives and a higher (though constant) computational overhead when querying/changing the dirty status. We leave the evaluation of tradeoffs introduced by bloom filters for future work.</p>
    <p>Our prototype currently does not persist the modification status across reboots. Also, the in-memory state is lost, if the server suddenly loses power. One possible solution is to persist the state as part of the servers regular shutdown routine. During startup, the system initializes the tracking bit vector with the state written at shutdown. If the initialization state is corrupt or not existing, each block is marked dirty to force a full synchronization.</p>
    <p>Extracting the modified blocks from a block device is aided by a command line tool called dmextract. The dmextract tool takes as its only parameter the name of the device on which to operate, e.g., # dmextract mydevice. By convention, the block numbers for mydevice are read from /proc/mydevice and the block device is found at /dev/mapper/mydevice. The tool outputs, via standard out, a sequence of (blocknumber,data) pairs. Output can be redirected to a file, for later access, or directly streamed over the network to the backup location. The complementing tool for block integration, dmmerge, reads a stream of information as produced by dmextract from standard input, A single parameter points to the block device into which the changed blocks shall be integrated.</p>
  </div>
  <div class="page">
    <p>How is dsync implemented?</p>
    <p>modification to device mapper module (drivers/md/dm-linear.c)</p>
    <p>one bit per 4 KiB block  for example, 4 TiB disk requires 128 MiB bit</p>
    <p>vector</p>
    <p>in-memory data structure</p>
  </div>
  <div class="page">
    <p>Interfacing with dsync</p>
    <p>virtual file in /proc  user-space tools to extract and merge</p>
    <p>block from/into device</p>
    <p>can build shell pipeline: # dmextract srcdev | ssh remote dmmerge targetdev</p>
  </div>
  <div class="page">
    <p>How was dsync evaluated?</p>
    <p>mix of synthetic and real world workloads  synthetic: random block modifications  real world: virtual machine disks (RUBiS)</p>
    <p>and Microsoft Research traces</p>
    <p>two machines (source and target) connected via switched Gigabit Ethernet</p>
  </div>
  <div class="page">
    <p>Sync times for various tools</p>
    <p>SSD, Figure 3</p>
    <p>state size [GiB]</p>
    <p>sy n</p>
    <p>ch ro</p>
    <p>n iz</p>
    <p>at io</p>
    <p>n ti</p>
    <p>m e</p>
    <p>[m in</p>
    <p>]</p>
    <p>rsync dsync copy blockmd5sync zfs</p>
    <p>Figure 2: Synchronization time for five different syn</p>
    <p>chronization techniques. Lower is better. Data on the</p>
    <p>source and target was stored on HDD.</p>
    <p>Figure 3: Synchronization time for five different syn</p>
    <p>chronization techniques. Lower is better. Data on the</p>
    <p>source and target was stored on SSD.</p>
  </div>
  <div class="page">
    <p>Sync times for various tools</p>
    <p>chronization techniques. Lower is better. Data on the</p>
    <p>source and target was stored on HDD.</p>
    <p>state size [GiB]</p>
    <p>sy n</p>
    <p>ch ro</p>
    <p>n iz</p>
    <p>at io</p>
    <p>n ti</p>
    <p>m e</p>
    <p>[m in</p>
    <p>]</p>
    <p>dsync copy blockmd5sync zfs</p>
    <p>Figure 3: Synchronization time for five different syn</p>
    <p>chronization techniques. Lower is better. Data on the</p>
    <p>source and target was stored on SSD.</p>
    <p>SSD, Figure 3</p>
  </div>
  <div class="page">
    <p>CPU utilization at the source</p>
    <p>time [s]</p>
    <p>C P</p>
    <p>U u</p>
    <p>ti li</p>
    <p>za ti</p>
    <p>on [%</p>
    <p>]</p>
    <p>dsync</p>
    <p>rsync</p>
    <p>copy</p>
    <p>Figure 4: CPU utilization for a sample run of three synchronization tools. 100% means all cores are busy.</p>
    <p>Figure 5: Network transmit traffic on the sender side measured for the entire system. rsync and dsync transmit about the same amount of data in total, although the effective throughput of rsync is much lower.</p>
    <p>about 400 seconds, compared with 400 seconds for dsync and 420 seconds for ZFS.</p>
    <p>We concluded that the random I/O operations were inhibiting dsync performance. Hence, we performed a second set of benchmarks where we used SSDs instead of HDDs. The results are shown in Figure 3. While the increased random I/O performance of SSDs does not matter for rsync, its synchronization time is identical to the HDD benchmark, SSDs enable all other methods to finish faster. dsyncs time to synchronize 32 GiB drops from 400 s on HDD to only 220 s on SSD.</p>
    <p>Intrigued by the trade-off between hard disk and solid state drives, we measured the read and write rate of our drives outside the context of dsync. When extracting or merging modified blocks they are processed in increasing order by their block number. We noticed that the read/write rate increased by up to 10x when processing a sorted randomly generated sequence of block numbers compared to the same unsorted sequence. For a random but sorted sequence of blocks our HDD achieves a read rate of 12 MB/s and a write rate of 7 MB/s. The SSD reads data twice as fast at 25 MB/s and writes data more than 15x as fast at 118 MB/s. This explains why, if HDDs are involved, copy finishes faster than dsync although copys transfer volume is 9x that of dsync: sequentially going through the data on HDD is much faster than selectively reading and writing only changed blocks.</p>
    <p>To better highlight the differences between the methods, we also present CPU and network traffic traces for three of the five methods. Figure 4 shows the CPU utilization while Figure 5 shows the outgoing network traffic at the sender. The trace was collected at the</p>
    <p>sender while synchronizing 32 GiB from/to SSD. The CPU utilization includes the time spent in kernel and user space, as well as waiting for I/O. We observe that rsync is CPU-bound by its single-threaded rolling checksum computation. Up to t = 500 the rsync sender process is idle, while one core on the receiver-side computes checksums (not visible in the graph). During rsyncs second phase, one core, on our 6-core benchmark machine, is busy computing and comparing checksums for the remaining 1400 s (23 min). The network traffic during that time is minimal at less than 5 MB/s. copys execution profile taxes the CPU much less: utilization oscillates between 0% and 15%. On the other hand, it can be visually determined that copy generates much more traffic volume than either rsync or dsync. copy generates about 90 MB/s of network traffic on average. dsyncs execution profile uses double the CPU power of copy, but only incurs a fraction of the network traffic. dsyncs network throughput is limited by the random read-rate at the sender side.</p>
    <p>Even though the SSDs specification promises 22.5k random 4 KiB reads [2], about 90 MiB/s, we are only able to read at a sustained rate of 20 MB/s at the application layer. Adding a loopback device to the configuration, reduces the application layer read throughput by about another 5 MB/s. This explains why dsyncs sender transmits at 17 MB/s. In this particular scenario dsyncs performance is read-limited. Anything that would help with reading the modified blocks from disk faster, would decrease the synchronization time even further.</p>
    <p>Until now we kept the modification ratio fixed at 10%, which seemed like a reasonable change rate. Next we</p>
    <p>Figure 4</p>
  </div>
  <div class="page">
    <p>Network utilization at source</p>
    <p>Figure 4: CPU utilization for a sample run of three synchronization tools. 100% means all cores are busy.</p>
    <p>time [s]</p>
    <p>N et</p>
    <p>w or</p>
    <p>k tr</p>
    <p>an sm</p>
    <p>it tr</p>
    <p>af fi</p>
    <p>c [M</p>
    <p>B /s</p>
    <p>]</p>
    <p>dsync</p>
    <p>rsync</p>
    <p>copy</p>
    <p>Figure 5: Network transmit traffic on the sender side measured for the entire system. rsync and dsync transmit about the same amount of data in total, although the effective throughput of rsync is much lower.</p>
    <p>about 400 seconds, compared with 400 seconds for dsync and 420 seconds for ZFS.</p>
    <p>We concluded that the random I/O operations were inhibiting dsync performance. Hence, we performed a second set of benchmarks where we used SSDs instead of HDDs. The results are shown in Figure 3. While the increased random I/O performance of SSDs does not matter for rsync, its synchronization time is identical to the HDD benchmark, SSDs enable all other methods to finish faster. dsyncs time to synchronize 32 GiB drops from 400 s on HDD to only 220 s on SSD.</p>
    <p>Intrigued by the trade-off between hard disk and solid state drives, we measured the read and write rate of our drives outside the context of dsync. When extracting or merging modified blocks they are processed in increasing order by their block number. We noticed that the read/write rate increased by up to 10x when processing a sorted randomly generated sequence of block numbers compared to the same unsorted sequence. For a random but sorted sequence of blocks our HDD achieves a read rate of 12 MB/s and a write rate of 7 MB/s. The SSD reads data twice as fast at 25 MB/s and writes data more than 15x as fast at 118 MB/s. This explains why, if HDDs are involved, copy finishes faster than dsync although copys transfer volume is 9x that of dsync: sequentially going through the data on HDD is much faster than selectively reading and writing only changed blocks.</p>
    <p>To better highlight the differences between the methods, we also present CPU and network traffic traces for three of the five methods. Figure 4 shows the CPU utilization while Figure 5 shows the outgoing network traffic at the sender. The trace was collected at the</p>
    <p>sender while synchronizing 32 GiB from/to SSD. The CPU utilization includes the time spent in kernel and user space, as well as waiting for I/O. We observe that rsync is CPU-bound by its single-threaded rolling checksum computation. Up to t = 500 the rsync sender process is idle, while one core on the receiver-side computes checksums (not visible in the graph). During rsyncs second phase, one core, on our 6-core benchmark machine, is busy computing and comparing checksums for the remaining 1400 s (23 min). The network traffic during that time is minimal at less than 5 MB/s. copys execution profile taxes the CPU much less: utilization oscillates between 0% and 15%. On the other hand, it can be visually determined that copy generates much more traffic volume than either rsync or dsync. copy generates about 90 MB/s of network traffic on average. dsyncs execution profile uses double the CPU power of copy, but only incurs a fraction of the network traffic. dsyncs network throughput is limited by the random read-rate at the sender side.</p>
    <p>Even though the SSDs specification promises 22.5k random 4 KiB reads [2], about 90 MiB/s, we are only able to read at a sustained rate of 20 MB/s at the application layer. Adding a loopback device to the configuration, reduces the application layer read throughput by about another 5 MB/s. This explains why dsyncs sender transmits at 17 MB/s. In this particular scenario dsyncs performance is read-limited. Anything that would help with reading the modified blocks from disk faster, would decrease the synchronization time even further.</p>
    <p>Until now we kept the modification ratio fixed at 10%, which seemed like a reasonable change rate. Next we</p>
    <p>Figure 5</p>
  </div>
  <div class="page">
    <p>More updates decrease sync time slightly</p>
    <p>percentage randomly updated blocks</p>
    <p>sy n</p>
    <p>ch ro</p>
    <p>n iz</p>
    <p>at io</p>
    <p>n ti</p>
    <p>m e</p>
    <p>[m in</p>
    <p>]</p>
    <p>dsync blockmd5sync zfs</p>
    <p>Figure 6: For comparison, rsync synchronizes the same data set 6, 21, and 41 minutes, respectively. copy took between 1.5 and 2 minutes.</p>
    <p>explored the effect of varying the percentage of modified blocks. The data size was fixed at 8 GiB and we randomly modified 10%, 50%, and 90% percent of the blocks. Figure 6 and 7 shows the timings for spinning and solid-state disks. On HDD, interestingly, even though the amount of data sent across the network increases, the net synchronization time stays almost constant for ZFS and blockmd5sync; it even decreases for dsync. Conversely, on SSD, synchronization takes longer with a larger number of modified blocks across all shown methods; although only minimally so for ZFS. We believe the increase for dsync and blockmd5sync is due to a higher number of block-level re-writes. Updating a block of flash memory is expensive and often done in units larger than 4 KiB [8]. ZFS is not affected by this phenomenon, as ZFS employs a copy-on-write strategy which turns random into sequential writes.</p>
    <p>We argued earlier, that a purely synthetic workload of random block modifications artificially constrains the performance of dsync. Although we already observed a 5x improvement in total synchronization time over rsync, the gain over copy was less impressive. To highlight the difference in spatial locality between the synthetic and RUBiS benchmark, we plotted the number of consecutive modified blocks for each; this is illustrated in Figure 8.</p>
    <p>We observe that 80% of the modifications involve only a single block (36k blocks at x = 1 in Figure 8). In comparison, there are no single blocks for the RUBiS benchmark. Every modification involves at least two consecutive blocks (1k blocks at x = 2). At the other end of</p>
    <p>percentage randomly updated blocks</p>
    <p>sy n</p>
    <p>ch ro</p>
    <p>n iz</p>
    <p>at io</p>
    <p>n ti</p>
    <p>m e</p>
    <p>[m in</p>
    <p>]</p>
    <p>dsync blockmd5sync zfs</p>
    <p>Figure 7: Varying the percentage of modified blocks for an 8 GiB file/device. For comparison, rsync synchronizes the same data set in 5, 21, and 41 minutes, respectively. A plain copy consistently took 1.5 minutes.</p>
    <p>the spectrum, the longest run of consecutively modified blocks is 639 for the RUBiS benchmarks. Randomly updated blocks rarely yield more than 5 consecutively modified blocks. For the RUBiS benchmark, updates of 5 consecutive blocks happen most often: the total number of modified blocks jumps from 2k to 15k moving from 4 to 5 consecutively modified blocks.</p>
    <p>Now that we have highlighted the spatial distribution of updates, Figure 9 illustrates the results for our RUBiS workload. We present numbers for the HDD case only because this workload is less constrained by the number of I/O operations per second. The number of modified blocks was never the same between those 20 runs. Instead, the number varies between 659 and 3813 blocks. This can be explained by the randomness inherent in each RUBiS benchmark invocation. The type and frequency of different actions, e.g., buying an item or browsing the catalog, is determined by chance. Actions that modify the database increase the modified block count.</p>
    <p>The synchronization time shows little variation between runs of the same method. copy transfers the entire 11 GiB of data irrespective of actual modifications. There should, in fact, be no difference based on the number of modifications. rsyncs execution time is dominated by checksum calculations. dsync, however, transfers only modified blocks and should show variations. The relationship between modified block count and synchronization time is just not discernible in Figure 9. Alternatively, we calculated the correlation coefficient for dsync which is 0.54. This suggests a positive correlation between the number of modified blocks and synchronization time. The correlation is not perfect because factors</p>
    <p>Figure 6</p>
  </div>
  <div class="page">
    <p>Sync time on realworld traces</p>
    <p>ti m</p>
    <p>e [s</p>
    <p>]</p>
    <p>rsync dsync</p>
    <p>copy blockmd5sync</p>
    <p>zfs</p>
    <p>Figure 10: Synchronization times for realistic blocklevel update patterns on HDDs. Lower is better. The results for the remaining days 3-6 are identical to the first.</p>
    <p>Regarding the runtime overhead of maintaining the bitmap used to track changed blocks, we do not expect this to noticeably affect performance in typical use cases. Setting a bit in memory is orders of magnitude faster than actually writing a block to disk.</p>
    <p>lvmsync [16] is a tool with identical intentions as dsync, but less generic than dsync. While dsync operates on arbitrary block devices, lvmsync only works for partitions managed by the logical volume manager (LVM). lvmsync extracts the modifications that happened to an LVM partition since the last snapshot. To provide snapshots, LVM has to keep track of the modified blocks, which is stored as meta-data. lvmsync uses this metadata to identify and extract the changed blocks.</p>
    <p>File systems, such as ZFS, and only recently btrfs, also support snapshots and differential backups. In ZFS, differential backups are performed using the ZFS send and receive operations. The delta between two snapshots can be extracted and merged again with another snapshot copy, e.g., at a remote backup machine. Only users of ZFS, however, can enjoy those features. For btrfs, there exists a patch to extract differences between two snapshot states [6]. This feature is, however, still considered experimental. Besides the file system, support for block tracking can be implemented higher up still in the software stack. VMware ESX, since version 4, is one example which supports block tracking at the application layer. In VMware ESX server the feature is called changed block tracking. Implementing support for efficient, differential backups at the block-device level, like</p>
    <p>ti m</p>
    <p>e [s</p>
    <p>]</p>
    <p>rsync dsync</p>
    <p>copy blockmd5sync</p>
    <p>zfs</p>
    <p>Figure 11: Synchronization times for realistic blocklevel update patterns on SSDs. Lower is better. The results for the remaining days 3-6 are identical to the first.</p>
    <p>dsync does, is more general, because it works regardless of the file system and application running on top.</p>
    <p>If updates must be replicated more timely to reduce the inconsistency window, the distributed replicated block device (DRBD) synchronously replicates data at the block level. All writes to the primary block device are mirrored to a second, standby copy. If the primary block device becomes unavailable, the standby copy takes over. In single primary mode, only the primary receives updates which is required by file systems lacking concurrent access semantics. Non-concurrent file systems assume exclusive ownership of the underlying device and single primary mode is the only viable DRBD configuration in this case. However, DRBD also supports dualprimary configurations, where both copies receive updates. A dual-primary setup requires a concurrencyaware file system, such as GFS or OCFS, to maintain consistency. DRBD is part of Linux since kernel version 2.6.33.</p>
    <p>There also exists work to improve the efficiency of synchronization tools. For example, Rasch and Burns [13] proposed for rsync to perform in-place updates. While their intention was to improve rsync performance on resource-constraint mobile devices, their approach also helps with large data sets on regular hardware. Instead of creating an out-of-place copy and atomically swapping this into place at the end of the transfer, the patch performs in-place updates. Since their original patch, in-place updates have been integrated into regular rsync.</p>
    <p>A more recent proposal tackles the problem of page cache pollution [3]. During the backup process many files and related meta-data are read. To improve system</p>
    <p>Figure 10</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>tool to synchronize data at the block device level</p>
    <p>file system agnostic  trades space for CPU and disk I/O</p>
    <p>bandwidth: track modifications instead of computing checksums</p>
  </div>
  <div class="page">
    <p>Open Science</p>
    <p>http://bitbucket.org/tknauth/devicemapper/</p>
    <p>Help! Work for PLX Technology or know anyone who works for them? Please come and talk to me!</p>
  </div>
</Presentation>
