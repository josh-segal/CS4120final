<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Efficient Memory Mapped File I/O for In-Memory File Systems Jungsik Choi, Jiwon Kim, Hwansoo Han</p>
  </div>
  <div class="page">
    <p>Storage Latency Close to DRAM</p>
    <p>Millisec NanosecMicrosec</p>
    <p>O p</p>
    <p>er at</p>
    <p>io n</p>
    <p>s P</p>
    <p>er S</p>
    <p>ec o</p>
    <p>n d</p>
    <p>NVDIMM-N (~10ns)</p>
    <p>PCIe Flash SSD (~60 s)SATA/SAS</p>
    <p>Flash SSD (~100s)</p>
    <p>SATA HDD</p>
  </div>
  <div class="page">
    <p>Large Overhead in Software</p>
    <p>Existing OSes were designed for fast CPUs and slow block devices</p>
    <p>With low-latency storage, SW overhead becomes the largest burden</p>
    <p>Software overhead includes  Complicated I/O stacks  Redundant memory copies  Frequent user/kernel mode switches</p>
    <p>SW overhead must be addressed to fully exploit low-latency NVM storage</p>
    <p>HDD 10Gb NIC</p>
    <p>NVMe 10Gb NIC</p>
    <p>PCM 10Gb NIC</p>
    <p>PCM 100Gb NIC</p>
    <p>La te</p>
    <p>n cy</p>
    <p>D ec</p>
    <p>o m</p>
    <p>p o</p>
    <p>si ti</p>
    <p>o n</p>
    <p>(N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>)</p>
    <p>Software Storage NIC</p>
    <p>(Source: Ahn et al. MICRO-48)</p>
    <p>HDD 10Gb NIC</p>
    <p>PCM 10Gb NIC</p>
    <p>PCM 100Gb NIC</p>
    <p>NVMe SSD 10Gb NIC</p>
  </div>
  <div class="page">
    <p>Eliminate SW Overhead with mmap</p>
    <p>In recent studies, memory mapped I/O has been commonly proposed  Memory mapped I/O can expose the NVM storages raw performance</p>
    <p>Mapping files onto user memory space  Provide memory-like access  Avoid complicated I/O stacks  Minimize user/kernel mode switches  No data copies</p>
    <p>A mmap syscall will be a critical interface 4</p>
    <p>File System</p>
    <p>Device Driver</p>
    <p>r/w syscalls</p>
    <p>Persistent Memory</p>
    <p>App</p>
    <p>load/store instructions</p>
    <p>App</p>
    <p>Disk/Flash</p>
    <p>user</p>
    <p>kernel</p>
    <p>Memory mapped I/OTraditional I/O</p>
  </div>
  <div class="page">
    <p>Microbenchmark: sequential read, a 4GB file (Ext4-DAX on NVDIMM-N)</p>
    <p>El ap</p>
    <p>se d</p>
    <p>T im</p>
    <p>e (s</p>
    <p>ec )</p>
    <p>4</p>
    <p>page fault overhead</p>
    <p>page table entry construction</p>
    <p>file access</p>
    <p>Memory Mapped I/O is Not as Fast as Expected</p>
    <p>default-mmap : mmap without any special flags</p>
    <p>populate-mmap : mmap with a MAP_POPULATE flag</p>
  </div>
  <div class="page">
    <p>Microbenchmark: sequential read, a 4GB file (Ext4-DAX on NVDIMM-N)</p>
    <p>El ap</p>
    <p>se d</p>
    <p>T im</p>
    <p>e (s</p>
    <p>ec )</p>
    <p>4</p>
    <p>page fault overhead</p>
    <p>page table entry construction</p>
    <p>file access</p>
    <p>Memory Mapped I/O is Not as Fast as Expected</p>
  </div>
  <div class="page">
    <p>Overhead in Memory Mapped File I/O</p>
    <p>Memory mapped I/O can avoid the SW overhead of traditional file I/O</p>
    <p>However, Memory mapped I/O causes another SW overhead  Page fault overhead  TLB miss overhead  PTE construction overhead</p>
    <p>It decreases the advantages of memory mapped file I/O</p>
  </div>
  <div class="page">
    <p>Techniques to Alleviate Storage Latency</p>
    <p>Readahead  Preload pages that are expected to be accessed</p>
    <p>Page cache  Cache frequently accessed pages</p>
    <p>fadvise/madvise interfaces  Utilize user hints to manage pages</p>
    <p>However, these cant be used in memory based FSs</p>
    <p>Main Memory</p>
    <p>Secondary Storage</p>
    <p>Page Cache</p>
    <p>Readahead</p>
    <p>App User Hints</p>
    <p>(madvise/fadvise)</p>
  </div>
  <div class="page">
    <p>Techniques to Alleviate Storage Latency</p>
    <p>Readahead  Preload pages that are expected to be accessed</p>
    <p>Page cache  Cache frequently accessed pages</p>
    <p>fadvise/madvise interfaces  Utilize user hints to manage pages</p>
    <p>However, these cant be used in memory based FSs</p>
    <p>Main Memory</p>
    <p>Secondary Storage</p>
    <p>Page Cache</p>
    <p>Readahead</p>
    <p>App User Hints</p>
    <p>(madvise/fadvise)</p>
    <p>Map-ahead</p>
    <p>Mapping cache</p>
    <p>Extended madvise</p>
    <p>New optimization is needed in memory based FSs</p>
  </div>
  <div class="page">
    <p>Map-ahead Constructs PTEs in Advance</p>
    <p>When a page fault occurs, the page fault handler handles  A page that caused the page fault (existing demand paging)  Pages that are expected to be accessed (map-ahead)</p>
    <p>Kernel analyzes page fault patterns to predict pages to be accessed  Sequential fault : map-ahead window size   Random fault : map-ahead window size</p>
    <p>Map-ahead can reduce # of page faults</p>
    <p>NEARLY_SEQ winsize</p>
    <p>PURE_SEQ winsize</p>
    <p>NON_SEQ winsize</p>
    <p>rnd</p>
    <p>rnd</p>
    <p>rnd</p>
    <p>seq</p>
    <p>seq seq</p>
  </div>
  <div class="page">
    <p>Comparison of Demand Paging &amp; Map-ahead</p>
    <p>Existing demand paging aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>Comparison of Demand Paging &amp; Map-ahead</p>
    <p>Existing demand paging aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>time</p>
    <p>overhead in user/kernel mode switch, page fault, TLB miss</p>
  </div>
  <div class="page">
    <p>Comparison of Demand Paging &amp; Map-ahead</p>
    <p>Existing demand paging aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>time</p>
    <p>aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>time</p>
    <p>Map-ahead</p>
    <p>overhead in user/kernel mode switch, page fault, TLB miss</p>
    <p>Map-ahead window size == 5</p>
    <p>Performance gain</p>
  </div>
  <div class="page">
    <p>Async Map-ahead Hides Mapping Overhead</p>
    <p>Map-ahead window size == 5</p>
    <p>aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>time</p>
    <p>Sync map-ahead</p>
  </div>
  <div class="page">
    <p>Async Map-ahead Hides Mapping Overhead</p>
    <p>aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>time</p>
    <p>Sync map-ahead</p>
    <p>time</p>
    <p>aapplication</p>
    <p>page fault handler</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>e</p>
    <p>f</p>
    <p>kernel threads</p>
    <p>Async map-ahead</p>
    <p>Performance gain</p>
  </div>
  <div class="page">
    <p>Extended madvise Makes User Hints Available</p>
    <p>Taking advantage of user hints can maximize the apps performance  However, existing interfaces are to optimize only paging</p>
    <p>Extended madvise makes user hints available in memory based FSs</p>
    <p>Existing madvise Extended madvise MADV_SEQUENTIAL MADV_WILLNEED Run readahead aggressively Run map-ahead aggressively</p>
    <p>MADV_RANDOM Stop readahead Stop map-ahead</p>
    <p>MADV_DONTNEED Release pages in page $ Release mapping in mapping $</p>
  </div>
  <div class="page">
    <p>Mapping Cache Reuses Mapping Data</p>
    <p>When munmap is called, existing kernel releases the VMA and PTEs  In memory based FSs, mapping overhead is very large</p>
    <p>With mapping cache, mapping overhead can be reduced  VMAs and PTEs are cached in mapping cache  When mmap is called, they are reused if possible (cache hit)</p>
    <p>VA | PA VA | PA VA | PA</p>
    <p>Process address space</p>
    <p>Page table Physical memory</p>
    <p>VMA</p>
    <p>VMA</p>
    <p>VMA</p>
    <p>VMA VMA</p>
    <p>VMA</p>
    <p>mm_struct red-black tree</p>
    <p>mmaped region</p>
  </div>
  <div class="page">
    <p>Experimental Environment</p>
    <p>Experimental machine  Intel Xeon E5-2620  32GB DRAM, 32GB NVDIMM-N  Linux kernel 4.4  Ext4-DAX filesystem</p>
    <p>Benchmarks  fio : sequential &amp; random read  YCSB on MongoDB : load workload  Apache HTTP server with httperf</p>
    <p>Netlist DDR4 NVDIMM-N 16GB  2</p>
  </div>
  <div class="page">
    <p>fio : Sequential Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madvise</p>
  </div>
  <div class="page">
    <p>fio : Sequential Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madvise</p>
  </div>
  <div class="page">
    <p>fio : Sequential Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madvise</p>
  </div>
  <div class="page">
    <p>fio : Random Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madvise</p>
  </div>
  <div class="page">
    <p>fio : Random Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madvisepopulate-mmap</p>
  </div>
  <div class="page">
    <p>fio : Random Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madvisemap-ahead</p>
  </div>
  <div class="page">
    <p>fio : Random Read</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Record Size (KB)</p>
    <p>default-mmap populate-mmap map-ahead extended madviseextended madvise</p>
    <p>madvise(MADV_RANDOM) == default-mmap</p>
  </div>
  <div class="page">
    <p>Web Server Experimental Setting</p>
    <p>Apache HTTP server  Memory mapped file I/O  10 thousand 1MB-size HTML files (from Wikipedia)  Total size is about 10GB</p>
    <p>httperf clients  8 additional machines  Zipf-like distribution</p>
    <p>httperf clients</p>
    <p>Apache HTTP server</p>
  </div>
  <div class="page">
    <p>Apache HTTP Server</p>
    <p>C P</p>
    <p>U C</p>
    <p>yc le</p>
    <p>s (t</p>
    <p>ri lli</p>
    <p>o n</p>
    <p>s, 1</p>
    <p>kernel libphp libc others</p>
    <p>Request Rate (reqs/s)</p>
    <p>No mapping cache Mapping cache (2GB)</p>
    <p>Mapping cache (No-Limit)</p>
  </div>
  <div class="page">
    <p>Apache HTTP Server</p>
    <p>C P</p>
    <p>U C</p>
    <p>yc le</p>
    <p>s (t</p>
    <p>ri lli</p>
    <p>o n</p>
    <p>s, 1</p>
    <p>kernel libphp libc others</p>
    <p>Request Rate (reqs/s)</p>
    <p>No mapping cache Mapping cache (2GB)</p>
    <p>Mapping cache (No-Limit)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>SW latency is becoming bigger than the storage latency  Memory mapped file I/O can avoid the SW overhead</p>
    <p>Memory mapped file I/O still incurs expensive additional overhead  Page fault, TLB miss, and PTEs construction overhead</p>
    <p>To exploit the benefits of memory mapped I/O, we propose  Map-ahead, extended madvise, mapping cache</p>
    <p>Our techniques demonstrate good performance by mitigating the mapping overhead</p>
    <p>Map-ahead : 38%   Map-ahead + extended madvise : 23%   Mapping cache : 18%</p>
  </div>
  <div class="page">
    <p>QnA chjs@skku.edu</p>
  </div>
</Presentation>
