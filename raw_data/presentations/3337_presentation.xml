<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Active Sampling for Entity Matching</p>
    <p>Aditya Parameswaran</p>
    <p>Stanford University</p>
    <p>Jointly with: Kedar Bellare, Suresh Iyengar, Vibhor Rastogi</p>
    <p>Yahoo! Research</p>
  </div>
  <div class="page">
    <p>Entity Matching</p>
    <p>Goal: Find duplicate entities in a given data set</p>
    <p>Fundamental data cleaning primitive  decades of prior work</p>
    <p>Especially important at Yahoo! (and other web companies)</p>
    <p>Hommas Brown Rice Sushi California Avenue Palo Alto</p>
    <p>Hommas Brown Rice Sushi California Avenue Palo Alto</p>
    <p>Hommas Sushi Cal Ave Palo Alto</p>
    <p>Hommas Sushi Cal Ave Palo Alto</p>
  </div>
  <div class="page">
    <p>Why is it important?</p>
    <p>Websites</p>
    <p>Databases Content Providers</p>
    <p>Dirty Entitie</p>
    <p>s</p>
    <p>???</p>
    <p>Deduplicate d</p>
    <p>Entities</p>
    <p>Applications: Business Listings in Y! Local Celebrities in Y! Movies Events in Y! Upcoming .</p>
    <p>Applications: Business Listings in Y! Local Celebrities in Y! Movies Events in Y! Upcoming .</p>
    <p>Find Duplicates</p>
    <p>Yelp</p>
    <p>Zagat</p>
    <p>Foursq</p>
  </div>
  <div class="page">
    <p>How?</p>
    <p>Reformulated Goal: Construct a high quality classifier identifying duplicate entity pairs</p>
    <p>Problem: How do we select training data?</p>
    <p>Answer: Active Learning with Human Experts!</p>
  </div>
  <div class="page">
    <p>Reformulated Workflow</p>
    <p>Websites</p>
    <p>Databases Content Providers</p>
    <p>Dirty Entitie s</p>
    <p>Our Techniqu e</p>
    <p>Deduplicate d Entities</p>
  </div>
  <div class="page">
    <p>Active Learning (AL) Primer</p>
    <p>Properties of an AL algorithm: Label Complexity Time Complexity Consistency</p>
    <p>Prior work: Uncertainty Sampling Query by Committee  Importance Weighted Active Learning (IWAL) Online IWAL without Constraints</p>
    <p>Implemented in Vowpal Wabbit (VW)  0-1 Metric  Time and Label efficient  Provably Consistent</p>
    <p>Work even under noisy settings</p>
    <p>Work even under noisy settings}</p>
  </div>
  <div class="page">
    <p>Problem One: Imbalanced Data</p>
    <p>Typical to have 100:1 even after blocking  Solution: Metric from [Arasu11]:</p>
    <p>Maximize Recall Such that Precision &gt;</p>
    <p>Solution: All Nonmatches  Precision 100%  0-1 Error  0</p>
    <p>Correctly identified matches Correctly identified matches</p>
    <p>% of correct matches % of correct matches</p>
  </div>
  <div class="page">
    <p>Problem Two: Guarantees</p>
    <p>Prior work on Entity Matching No guarantees on Recall/Precision Even if they do, they have:</p>
    <p>High time + label complexity</p>
    <p>Can we adapt prior work on AL for the new objective: Maximize recall, such that precision &gt;</p>
    <p>With: Sub-linear label complexity Efficient time complexity</p>
  </div>
  <div class="page">
    <p>Overview of Our Approach</p>
    <p>Recall Optimization with Precision</p>
    <p>Constraint</p>
    <p>Recall Optimization with Precision</p>
    <p>Constraint</p>
    <p>Weighted 0-1 Error</p>
    <p>Weighted 0-1 Error</p>
    <p>Active Learning with 0-1 Error</p>
    <p>Active Learning with 0-1 Error</p>
    <p>Reduction: Convex-hull Search in Relaxed Lagrangian</p>
    <p>Reduction: Rejection Sampling</p>
    <p>This talkThis talk</p>
    <p>PaperPaper</p>
  </div>
  <div class="page">
    <p>Objective</p>
    <p>Given: Hypothesis class H, Threshold  in [0,1]</p>
    <p>Objective: Find h in H that Maximizes recall(h) Such that: precision(h) &gt;=</p>
    <p>Equivalently: Maximize -falseneg(h) Such that:  truepos(h) - falsepos(h) &gt;= 0 Where  = /(1-)</p>
  </div>
  <div class="page">
    <p>Unconstrained Objective</p>
    <p>Current formulation: Maximize -falseneg(h)  truepos(h) - falsepos(h) &gt;= 0</p>
    <p>If we introduce lagrange multiplier : Maximize X(h) +  Y(h), can be rewritten as: Minimize  falseneg (h) + (1  ) falsepos(h)</p>
    <p>X(h ) X(h )</p>
    <p>Y(h ) Y(h )</p>
    <p>Weighted 01 objective Weighted 01 objective</p>
  </div>
  <div class="page">
    <p>Convex Hull of Classifiers</p>
    <p>Y(h )</p>
    <p>X(h )</p>
    <p>We want a classifier here We want a classifier here</p>
    <p>Convex shape formed by joining classifiers strictly dominating others</p>
    <p>Convex shape formed by joining classifiers strictly dominating others</p>
    <p>Maximize X(h) Such that Y(h)</p>
    <p>&gt;= 0</p>
    <p>Maximize X(h) Such that Y(h)</p>
    <p>&gt;= 0</p>
    <p>Can have exponential number of points inside</p>
    <p>Can have exponential number of points inside</p>
  </div>
  <div class="page">
    <p>Convex Hull of Classifiers</p>
    <p>Y(h )</p>
    <p>X(h )</p>
    <p>For any &gt;0, there is a point / line with largest value of X +  Y</p>
    <p>For any &gt;0, there is a point / line with largest value of X +  Y</p>
    <p>If =-1/slope of a line, we get a classifier on the line, else we get a vertex classifier.</p>
    <p>If =-1/slope of a line, we get a classifier on the line, else we get a vertex classifier.</p>
    <p>u</p>
    <p>v</p>
    <p>u-v</p>
    <p>Plug  into weighted objective, get classifier h with highest X(h) +  Y(h)</p>
    <p>Plug  into weighted objective, get classifier h with highest X(h) +  Y(h)</p>
    <p>Maximize X(h) Such that Y(h)</p>
    <p>&gt;= 0</p>
    <p>Maximize X(h) Such that Y(h)</p>
    <p>&gt;= 0</p>
  </div>
  <div class="page">
    <p>Convex Hull of Classifiers</p>
    <p>Y(h )</p>
    <p>X(h )</p>
    <p>Worst case, we get this point Worst case, we get this point</p>
    <p>Nave strategy: try all  Equivalently, try all slopes</p>
    <p>Nave strategy: try all  Equivalently, try all slopes</p>
    <p>Instead, do binary search for</p>
    <p>Problem: When to stop?</p>
    <p>Instead, do binary search for</p>
    <p>Problem: When to stop?</p>
    <p>Too long!Too long!</p>
    <p>Maximize X(h) Such that Y(h)</p>
    <p>&gt;= 0</p>
    <p>Maximize X(h) Such that Y(h)</p>
    <p>&gt;= 0</p>
  </div>
  <div class="page">
    <p>Algorithm I (Ours  Weighted)</p>
    <p>Given: AL black box C for weighted 0-1 error  Goal: Precision constrained objective</p>
    <p>Range of : [min,max] Dont enumerate all candidate   too expensive;</p>
    <p>O(n3) Instead, discretized using factor   see paper!</p>
    <p>Binary search over discretized values  Same complexity as binary search</p>
    <p>O(log n)</p>
  </div>
  <div class="page">
    <p>Algorithm II (Weighted  0-1)</p>
    <p>Given: AL black box B for 0-1 error  Goal: AL Black box C for weighted 0-1 error</p>
    <p>Use trick from Supervised Learning [Zadrozny03] Cost-sensitive objective  Binary Reduction by rejection sampling</p>
  </div>
  <div class="page">
    <p>Overview of Our Approach Recall</p>
    <p>Optimization with Precision</p>
    <p>Constraint</p>
    <p>Recall Optimization with Precision</p>
    <p>Constraint</p>
    <p>Weighted 0-1 Error</p>
    <p>Weighted 0-1 Error</p>
    <p>Active Learning with 0-1 Error</p>
    <p>Active Learning with 0-1 Error</p>
    <p>Reduction: Convex-hull Search in Relaxed Lagrangian</p>
    <p>Reduction: Rejection Sampling</p>
    <p>This talkThis talk</p>
    <p>PaperPaper</p>
    <p>O(log n)O(log n)</p>
    <p>O(log n)O(log n)</p>
    <p>Labels = O(log2 n) L(B) Time = O(log2 n) T(B)</p>
    <p>Labels = O(log2 n) L(B) Time = O(log2 n) T(B)</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Four real-world data sets</p>
    <p>All labels known  Simulate active learning</p>
    <p>Two approaches for AL with Precision Constraint:  Ours</p>
    <p>With Vowpal Wabbit as 0-1 AL Black Box  Monotone [Arasu11]</p>
    <p>Assumes monotonicity of similarity features  High computational + label complexity</p>
    <p>Data Set Size Ratio (+/-) Features</p>
    <p>Y! Local Businesses 3958 0.115 5</p>
    <p>UCI Person Linkage 574913 0.004 9</p>
    <p>DBLP-ACM Bibliography 494437 0.005 7</p>
    <p>Scholar-DBLP Bibliography</p>
  </div>
  <div class="page">
    <p>Results I (Runtime with #Features)</p>
    <p>Computational complexity on UCI Person</p>
    <p>Ours Monotone</p>
    <p>Number of similarity features</p>
    <p>T im</p>
    <p>e (</p>
    <p>in s</p>
    <p>e c o n</p>
    <p>d s )</p>
  </div>
  <div class="page">
    <p>Results II (Quality &amp; #Label Queries)</p>
    <p>Ours Monotone</p>
    <p>threshold</p>
    <p>F -1</p>
    <p>threshold</p>
    <p>L a b</p>
    <p>e l q</p>
    <p>u e ri</p>
    <p>e sBusiness</p>
    <p>Ours Monotone</p>
    <p>threshold</p>
    <p>F -1</p>
    <p>threshold</p>
    <p>L a b</p>
    <p>e l q</p>
    <p>u e ri</p>
    <p>e sPerson</p>
  </div>
  <div class="page">
    <p>Results II (Contd.)</p>
    <p>Ours Monotone</p>
    <p>threshold</p>
    <p>F -1</p>
    <p>threshold</p>
    <p>L a b</p>
    <p>e l q</p>
    <p>u e ri</p>
    <p>e s</p>
    <p>DBLP-ACM</p>
    <p>Ours Monotone</p>
    <p>threshold</p>
    <p>F -1</p>
    <p>threshold</p>
    <p>L a b</p>
    <p>e l q</p>
    <p>u e ri</p>
    <p>e s</p>
    <p>Scholar</p>
  </div>
  <div class="page">
    <p>Results III (0-1 Active Learning)</p>
    <p>Precision Constraint Satisfaction % of 0-1 AL</p>
    <p>business person dblp-acm scholar-dblp 0</p>
    <p>t=0.7 t=0.8 t=0.9 t=0.95</p>
    <p>Datasets</p>
    <p>C o n</p>
    <p>s tr</p>
    <p>a in</p>
    <p>t s a ti</p>
    <p>s fa</p>
    <p>c ti</p>
    <p>o n</p>
    <p>%</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Active learning for Entity Matching  Can use any 0-1 AL as black box  Great real world performance:</p>
    <p>Computationally efficient (600k examples in 25 seconds)</p>
    <p>Label efficient and better F-1 on four real-world tasks</p>
    <p>Guaranteed Precision of matcher Time and label complexity</p>
  </div>
</Presentation>
