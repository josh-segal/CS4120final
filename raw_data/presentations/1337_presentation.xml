<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Design and Implementation of a HighPerformance Distributed Web Crawler</p>
    <p>Vladislav Shkapenyuk* Torsten Suel</p>
    <p>CIS Department Polytechnic University</p>
    <p>Brooklyn, NY 11201</p>
    <p>* Currently at AT&amp;T Research Labs, Florham Park</p>
  </div>
  <div class="page">
    <p>Overview:</p>
  </div>
  <div class="page">
    <p>Web Crawler: (also called spider or robot)</p>
    <p>tool for data acquisition in search engines</p>
    <p>large engines need high-performance crawlers</p>
    <p>need to parallelize crawling task</p>
    <p>PolyBot: a parallel/distributed web crawler</p>
    <p>cluster vs. wide-area distributed</p>
  </div>
  <div class="page">
    <p>Basic structure of a search engine:</p>
    <p>Crawler</p>
    <p>disks</p>
    <p>Index</p>
    <p>indexing</p>
    <p>Search.com Query: computer</p>
    <p>look up</p>
  </div>
  <div class="page">
    <p>Crawler</p>
    <p>Crawler</p>
    <p>disks</p>
    <p>fetches pages from the web  starts at set of seed pages  parses fetched pages for hyperlinks  then follows those links (e.g., BFS)  variations: - recrawling - focused crawling - random walks</p>
  </div>
  <div class="page">
    <p>Breadth-First Crawl:</p>
    <p>Basic idea: - start at a set of known URLs - explore in concentric circles around these URLs</p>
    <p>start pages</p>
    <p>distance-one pages</p>
    <p>distance-two pages</p>
    <p>used by broad web search engines  balances load between servers</p>
  </div>
  <div class="page">
    <p>Crawling Strategy and Download Rate:</p>
    <p>crawling strategy: What page to download next?</p>
    <p>download rate: How many pages per second?</p>
    <p>different scenarios require different strategies</p>
    <p>lots of recent work on crawling strategy</p>
    <p>little published work on optimizing download rate (main exception: Mercator from DEC/Compaq/HP?)</p>
    <p>somewhat separate issues</p>
    <p>building a slow crawler is (fairly) easy ...</p>
  </div>
  <div class="page">
    <p>Application determines crawling strategy</p>
    <p>Basic System Architecture</p>
  </div>
  <div class="page">
    <p>System Requirements:</p>
    <p>flexibility (different crawling strategies)</p>
    <p>scalabilty (sustainable high performance at low cost)</p>
    <p>robustness</p>
    <p>(odd server content/behavior, crashes)</p>
    <p>crawling etiquette and speed control</p>
    <p>(robot exclusion, 30 second intervals, domain level</p>
    <p>throttling, speed control for other users)</p>
    <p>manageable and reconfigurable</p>
    <p>(interface for statistics and control, system setup)</p>
  </div>
  <div class="page">
    <p>Details: (lots of em)</p>
    <p>robot exclusion - robots.txt file and meta tags - robot exclusion adds overhead  handling filetypes (exclude some extensions, and use mime types)</p>
    <p>URL extensions and CGI scripts (to strip or not to strip? Ignore?)</p>
    <p>frames, imagemaps  black holes (robot traps) (limit maximum depth of a site)</p>
    <p>different names for same site? (could check IP address, but no perfect solution)</p>
  </div>
  <div class="page">
    <p>Crawling courtesy</p>
    <p>minimize load on crawled server</p>
    <p>no more than one outstanding request per site</p>
    <p>better: wait 30 seconds between accesses to site (this number is not fixed)</p>
    <p>problems: - one server may have many sites - one site may be on many servers - 3 years to crawl a 3-million page site!</p>
    <p>give contact info for large crawls</p>
  </div>
  <div class="page">
    <p>Contributions:  distributed architecture based on collection of services</p>
    <p>- separation of concerns - efficient interfaces</p>
    <p>I/O efficient techniques for URL handling - lazy URL -seen structure - manager data structures</p>
    <p>scheduling policies</p>
    <p>- manager scheduling and shuffling</p>
    <p>resulting system limited by network and parsing performane</p>
    <p>detailed description and how-to (limited experiments)</p>
  </div>
  <div class="page">
    <p>Structure:</p>
    <p>separation of crawling strategy and basic system</p>
    <p>collection of scalable distributed services</p>
    <p>(DNS, downloading, scheduling, strategy)</p>
    <p>for clusters and wide-area distributed</p>
    <p>optimized per-node performance</p>
    <p>no random disk accesses (no per-page access)</p>
  </div>
  <div class="page">
    <p>Basic Architecture (ctd):</p>
    <p>application issues requests to manager</p>
    <p>manager does DNS and robot exclusion</p>
    <p>manager schedules URL on downloader</p>
    <p>downloader gets file and puts it on disk</p>
    <p>application is notified of new files</p>
    <p>application parses new files for hyperlinks</p>
    <p>application sends data to storage component (indexing done later)</p>
  </div>
  <div class="page">
    <p>System components:  downloader: optimized HTTP client written in Python (everything else in C++)</p>
    <p>DNS resolver: uses asynchronous DNS library</p>
    <p>manager uses Berkeley DB and STL for external and internal data structures</p>
    <p>manager does robot exclusion by generating requests to downloaders and parsing files</p>
    <p>application does parsing and handling of URLs (has this page already been downloaded?)</p>
  </div>
  <div class="page">
    <p>Scaling the system:</p>
    <p>small system on previous pages:</p>
    <p>can scale up by adding downloaders and DNS resolvers</p>
    <p>at 400-600 pages/sec, application becomes bottleneck</p>
    <p>at 8 downloaders manager becomes bottleneck</p>
    <p>need to replicate application and manager</p>
    <p>hash-based technique (Internet Archive crawler)</p>
    <p>partitions URLs and hosts among application parts</p>
    <p>data transfer batched and via file system (NFS)</p>
  </div>
  <div class="page">
    <p>Scaling up:  20 machines</p>
    <p>1500 pages/s?</p>
    <p>depends on crawl strategy</p>
    <p>hash to nodes based on site (b/c robot ex)</p>
  </div>
  <div class="page">
    <p>parsing using pcre library  NFS eventually bottleneck  URL-seen problem: - need to check if file has been parsed or downloaded before</p>
    <p>- after 20 million pages, we have seen over 100 million URLs</p>
    <p>- each URL is 50 to 75 bytes on average</p>
    <p>Options: compress URLs in main memory, or use disk - prefix+huffman coding (DEC, JY01) or Bloom Filter (Archive)</p>
    <p>- disk access with caching (Mercator)</p>
    <p>- we use lazy/bulk operations on disk</p>
    <p>Crawling Application</p>
  </div>
  <div class="page">
    <p>Implementation of URL-seen check:</p>
    <p>- while less than a few million URLs seen, keep in main memory</p>
    <p>- then write URLs to file in alphabetic, prefix-compressed order</p>
    <p>- collect new URLs in memory and periodically reform bulk</p>
    <p>check by merging new URLs into the file on disk</p>
    <p>When is a newly a parsed URL downloaded?</p>
    <p>Reordering request stream</p>
    <p>- want to space ot requests from same subdomain</p>
    <p>- needed due to load on small domains and due to security tools</p>
    <p>- sort URLs with hostname reversed (e.g., com.amazon.www), and then unshuffle the stream provable load balance</p>
  </div>
  <div class="page">
    <p>Crawling Manager</p>
    <p>large stream of incoming URL request files</p>
    <p>goal: schedule URLs roughly in the order that they come, while observing time-out rule (30 seconds) and maintaining high speed</p>
    <p>must do DNS and robot excl. right beforedownload</p>
    <p>keep requests on disk as long as possible!</p>
    <p>- otherwise, structures grow too large after few million pages (performance killer)</p>
  </div>
  <div class="page">
    <p>Manager Data Structures:</p>
    <p>when to insert new URLs into internal structures?</p>
  </div>
  <div class="page">
    <p>URL Loading Policy</p>
    <p>read new request file from disk whenever less</p>
    <p>than x hosts in ready queue</p>
    <p>choose x &gt; speed * timeout (e.g., 100 pages/s * 30s)</p>
    <p># of current host data structures is</p>
    <p>x + speed * timeout + n_down + n_transit</p>
    <p>which is usually &lt; 2x</p>
    <p>nice behavior for BDB caching policy</p>
    <p>performs reordering only when necessary!</p>
  </div>
  <div class="page">
    <p>slow during day, fast at night</p>
    <p>peak about 300 pages/s over T3</p>
    <p>many downtimes due to attacks, crashes, revisions</p>
    <p>slow tail of requests at the end (4 days)</p>
    <p>lots of things happen</p>
  </div>
  <div class="page">
    <p>Experimental Results ctd.</p>
    <p>bytes in bytes out frames out</p>
    <p>Poly T3 connection over 24 hours of 5/28/01 (courtesy of AppliedTheory)</p>
  </div>
  <div class="page">
    <p>Experimental Results ctd.</p>
    <p>sustaining performance: - will find out when data structures hit disk - I/O-efficiency vital</p>
    <p>speed control tricky - vary number of connections based on feedback - also upper bound on connections - complicated interactions in system - not clear what we should want</p>
    <p>other configuration: 140 pages/sec sustained on 2 Ultra10 with 60GB EIDE and 1GB/768MB  similar for Linux on Intel</p>
  </div>
  <div class="page">
    <p>More Detailed Evaluation (to be done)</p>
    <p>Problems - cannot get commercial crawlers - need simulation systen to find system bottlenecks - often not much of a tradeoff (get it right!)</p>
    <p>Example: manager data structures - with our loading policy, manager can feed several downloaders - nave policy: disk access per page</p>
    <p>parallel communication overhead - low for limited number of nodes (URL exchange) - wide-area distributed: where do yu want the data? - more relevant for highly distributed systems</p>
  </div>
  <div class="page">
    <p>Mercator (Heydon/Najork from DEC/Compaq)</p>
    <p>- used in altaVista - centralized system (2-CPU Alpha with RAID disks)</p>
    <p>- URL-seen test by fast disk access and caching</p>
    <p>- one thread per HTTP connection</p>
    <p>- completely in Java, with pluggable components</p>
    <p>Atrax: very recent distributed extension to Mercator</p>
    <p>- combines several Mercators</p>
    <p>- URL hashing, and off-line URL check (as we do)</p>
    <p>Related work</p>
  </div>
  <div class="page">
    <p>early Internet Archive crawler (circa 96)</p>
    <p>- uses hashing to partition URLs between crawlers</p>
    <p>- bloom filter for URL seen structure</p>
    <p>early Google crawler (1998)</p>
    <p>P2P crawlers (grub.org and others)</p>
    <p>Cho/Garcia-Molina (WWW 2002)</p>
    <p>- study of overhead/quality tradeoff in parallel crawlers</p>
    <p>- difference: we scale services separately, and focus on single-node performance</p>
    <p>- in our experience, parallel overhead low</p>
    <p>Related work (ctd.)</p>
  </div>
  <div class="page">
    <p>Open Problems:</p>
    <p>Measuring and tuning peak performance</p>
    <p>- need simulation environment - eventually reduces to parsing and network - to be improved: space, fault-tolerance (Xactions?)  Highly Distributed crawling - highly distributed (e.g., grub.org) ? (maybe) - bybrid? (different services) - few high-performance sites? (several Universities)  Recrawling and focused crawling strategies</p>
    <p>- what strategies? - how to express? - how to implement?</p>
  </div>
</Presentation>
