<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Accurate Latency-based Congestion Feedback for Datacenters</p>
    <p>Changhyun Lee with Chunjong Park, Keon Jang*, Sue Moon, and Dongsu Han</p>
    <p>KAIST *Intel Labs</p>
    <p>USENIX Annual Technical Conference (ATC)</p>
    <p>July 10, 2015</p>
  </div>
  <div class="page">
    <p>Congestion control? Again???</p>
    <p>Numerous congestion control algorithms have been proposed since Jacobsons TCP</p>
    <p>Performance of congestion control fundamentally depends on congestion feedback</p>
    <p>New forms of congestion feedback have enabled innovative congestion control behavior  Packet loss, latency, bandwidth, ECN, in-network (RCP, XCP), etc.</p>
    <p>Congestion feedback</p>
    <p>Reaction</p>
    <p>Control algorithmNetwork</p>
  </div>
  <div class="page">
    <p>Congestion control challenges in DCN</p>
    <p>Datacenters unique environment requires congestion control to be finer-grained than ever  Prevalence of latency sensitive flows (partition/aggregate workload)</p>
    <p>Every 100ms slow down in Amazon = 1% drop in sales*</p>
    <p>Dominance of queuing delay in end-to-end latency</p>
    <p>Accurate and fine-grained congestion feedback is a must!</p>
  </div>
  <div class="page">
    <p>The most popular choice so far: ECN</p>
    <p>ECN (Explicit Congestion Notification) detects congestion earlier than packet loss, but  It still provides very coarse-grained feedback (binary)</p>
    <p>DCTCP puts in more effort to improve granularity  Other ECN-based work also employ the same technique</p>
    <p>Pursuit of better congestion feedback leads to customized in-network feedback  hard to deploy</p>
  </div>
  <div class="page">
    <p>Our proposal: latency feedback</p>
    <p>Network latency is a good indicator of congestion</p>
    <p>Latency congestion feedback has a long history from CARD, DUAL, and TCP Vegas in wide-area networks  Used feedback: RTT measured in TCP stack</p>
    <p>We revisit latency feedback for use in datacenter networks</p>
    <p>Can we reuse the same latency feedback from TCP Vegas?</p>
  </div>
  <div class="page">
    <p>Challenges in latency feedback in DC</p>
    <p>Network latency changes in s time scale in datacenters</p>
    <p>Differentiating network latency change from other noise becomes a challenging task</p>
    <p>Measuring network latency accurately in microsecond scale is crucial</p>
    <p>Datacenter Wide-area</p>
    <p>Link speed 10 Gbps 100 Mbps</p>
    <p>Transmission delay 1.2 s 120 s</p>
    <p>Queueing delay (10 pkts) 12 s 1.2 ms</p>
  </div>
  <div class="page">
    <p>Evaluation of TCP stack measurement</p>
    <p>We test whether RTT measured in TCP stack can indicate network congestion level in datacenters</p>
    <p>We first evaluate the case of no congestion</p>
    <p>Ideally, all the RTT measurements should have the same value</p>
    <p>Sender Receiver</p>
  </div>
  <div class="page">
    <p>Inaccuracy of TCP stack measurement</p>
    <p>Latency feedback from stack cannot indicate network congestion level</p>
  </div>
  <div class="page">
    <p>Why is TCP stack measurement unreliable?</p>
    <p>Sources of errors in RTT measurement  End-host stack delay</p>
    <p>I/O batching</p>
    <p>Reverse path delay</p>
    <p>Clock drift</p>
    <p>Refer to our paper</p>
  </div>
  <div class="page">
    <p>Identifying sources of errors (1)</p>
    <p>End-host stack delay  Packet I/O, stack processing, interrupt handling, CPU scheduling, etc.</p>
    <p>NIC</p>
    <p>Driver</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Timestamping</p>
    <p>NIC</p>
    <p>Driver</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Sender Receiver</p>
    <p>RTT measured from kernel gets affected by host delay jitter</p>
    <p>Measured RTT = ACK TS  Data TS</p>
    <p>Data SENT TS</p>
    <p>ACK RCVD TS</p>
  </div>
  <div class="page">
    <p>Removing stack delay (sender-side)</p>
    <p>Solution #1: Driver-level timestamping (software)  We use SoftNIC*, an Intel DPDK-based packet processing platform</p>
    <p>NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Timestamping</p>
    <p>NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Sender Receiver</p>
    <p>Measured RTT = ACK TS  Data TS</p>
    <p>Data SENT TS</p>
    <p>ACK RCVD TS</p>
    <p>* SoftNIC: A Software NIC to Augment Hardware, Sangjin Han, Keon Jang, Shoumik Palkar, Dongsu Han, and Sylvia Ratnasamy (Technical Report, UCB)</p>
  </div>
  <div class="page">
    <p>Removing stack delay (sender-side)</p>
    <p>Solution #2: NIC-level timestamping (hardware)  We use Mellanox ConnectX-3, a timestamp-capable NIC</p>
    <p>NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Timestamping NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Sender Receiver</p>
    <p>Measured RTT = ACK TS  Data TS</p>
    <p>Data SENT TS</p>
    <p>ACK RCVD TS</p>
  </div>
  <div class="page">
    <p>Removing stack delay (receiver side)</p>
    <p>Solution #3: Timestamping also at the receiver host  We subtract receiver nodes stack delay from RTT</p>
    <p>NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Timestamping</p>
    <p>NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Sender Receiver</p>
    <p>Timestamping</p>
    <p>Measured RTT = (ACK RCVD TS  Data SENT TS)  (ACK SENT TS  Data RCVD TS)</p>
    <p>Data SENT TS</p>
    <p>Data RCVD TS</p>
    <p>ACK SENT TS</p>
    <p>ACK RCVD TS</p>
  </div>
  <div class="page">
    <p>Identifying sources of errors (2)</p>
    <p>Bursty timestamps from I/O batching  Multiple packets acquire the same timestamp in network stack</p>
    <p>NIC</p>
    <p>Driver</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Timestamping</p>
    <p>Sender</p>
    <p>D1 D2 D3</p>
    <p>NIC</p>
    <p>Driver</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>Receiver</p>
    <p>Timestamps do not reflect the actual sending/receiving time</p>
  </div>
  <div class="page">
    <p>Removing bursty timestamps (driver)</p>
    <p>NIC</p>
    <p>SoftNIC</p>
    <p>Network stack</p>
    <p>Application</p>
    <p>D1 D2 D3 D4 D5 D6</p>
    <p>Queue</p>
    <p>Timestamping</p>
    <p>SoftNIC stores bursty packets from upper-layer in a queue and pace before timestamping</p>
  </div>
  <div class="page">
    <p>Removing bursty timestamps (NIC)</p>
    <p>Even NIC-level timestamping generates bursty timestamps  NIC timestamps packets after DMA completion,</p>
    <p>not when sending/receiving packets on the wire</p>
    <p>We calibrate timestamps based on link transmission delay</p>
  </div>
  <div class="page">
    <p>Improved accuracy by our techniques</p>
    <p>Accuracy of HW timestamping is sub-microsecond scale</p>
    <p>SW Best</p>
    <p>HW Best</p>
  </div>
  <div class="page">
    <p>Can we measure accurate queuing delay?</p>
    <p>Using our accurate RTT measurement, we infer queueing delay (queue length) at switch</p>
    <p>Queueing delay is calculated as (Current RTT  Base RTT)  Current RTT: RTT sample from current Data/ACK pair</p>
    <p>Base RTT: RTT measured without congestion (minimum value)</p>
    <p>Switch Queue</p>
    <p>One 1500 byte packet in 1G switch queue = 12us increase in RTT</p>
  </div>
  <div class="page">
    <p>Evaluation of queuing delay measurement</p>
    <p>Traffic  Sender 1 generates 1Gbps full rate TCP traffic</p>
    <p>Sender 2 generates an MTU (1500B) Ping packet every 25ms</p>
    <p>Measurement  Sender 1 measures queueing delay</p>
    <p>Switch measures ground-truth queue length</p>
    <p>Sender 1</p>
    <p>Receiver</p>
    <p>Sender 2</p>
  </div>
  <div class="page">
    <p>Accuracy of queuing delay measurement</p>
    <p>We can measure queueing delay in single packet granularity  Ground truth from switch matches with delay measurement</p>
  </div>
  <div class="page">
    <p>DX: latency-based congestion control</p>
    <p>We propose DX, a new congestion control algorithm based on the accurate latency feedback  Goal: minimizing queueing delay while fully utilizing network links</p>
    <p>DX behavior is straightforward  When queuing delay is zero, DX increases window size</p>
    <p>When queuing delay is positive, DX decreases window size</p>
    <p>How much should we increase or decrease?</p>
  </div>
  <div class="page">
    <p>DX window calculation rule</p>
    <p>Additive Increase: one packet per RTT</p>
    <p>Multiplicative Decrease: proportional to the queuing delay</p>
    <p>Challenge: How can we keep 100% utilization after decrement?</p>
    <p>Q: queueing delay V: normalizer</p>
  </div>
  <div class="page">
    <p>DX example scenario</p>
    <p>Q &gt; 0  Decrease window</p>
  </div>
  <div class="page">
    <p>Challenge: sender #1s view</p>
    <p>CWND=20+1</p>
    <p>CWND=20+1</p>
    <p>CWND=20+1</p>
    <p>How much should I decrease?</p>
    <p>??? Simple assumption: Other senders have the same window size</p>
    <p>How much congestion am I responsible for?</p>
    <p>New window size can be calculated from Link capacity, RTT, and current window size</p>
    <p>*Refer to our paper for detailed derivation</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>We implement timestamping module in SoftNIC  Timestamp collection</p>
    <p>Data and ACK packet match</p>
    <p>RTT and queueing delay calculation</p>
    <p>Bursty timestamp calibration</p>
    <p>We implement DX control algorithm in Linux 3.13 kernel  200+ lines of code addition (mainly in tcp_ack())</p>
    <p>Use of TCP option header for storing timestamps</p>
  </div>
  <div class="page">
    <p>Evaluation methodology</p>
    <p>Testbed experiment (small-scale)  Bottleneck queue length in 2-to-1 topology</p>
    <p>Ns-2 simulation (large-scale)  Flow completion time of datacenter workload in a toy datacenter</p>
    <p>More in our paper</p>
    <p>Queueing delay and utilization with 10/20/30 senders</p>
    <p>Flow throughput convergence</p>
    <p>Impact of measurement noise to headroom</p>
    <p>Fairness and throughput stability</p>
  </div>
  <div class="page">
    <p>Testbed experiment setup</p>
    <p>Two senders share a bottleneck link (1Gbps/10Gbps)</p>
    <p>Senders generate DX/DCTCP traffic to fully utilize the link</p>
    <p>We measure and compare the queue length of DX/DCTCP</p>
    <p>Sender 1</p>
    <p>Receiver</p>
    <p>Sender 2</p>
  </div>
  <div class="page">
    <p>Testbed experiment result at 1Gbps</p>
    <p>DX reduces median queuing delay by 5.33 times from DCTCP</p>
  </div>
  <div class="page">
    <p>Testbed experiment result at 10Gbps</p>
    <p>Hardware timestamping achieves further queueing delay reduction</p>
  </div>
  <div class="page">
    <p>Simulation with datacenter workload</p>
    <p>Topology  A 3-tier fat tree with 192 nodes and 56 switches</p>
    <p>Workload  Empirical web search workload from production datacenter</p>
    <p>C C C C C C C C</p>
    <p>A A</p>
    <p>T T T T</p>
  </div>
  <div class="page">
    <p>FCT of search workload simulation</p>
    <p>DX effectively reduces the completion time of small flows</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>The quality of congestion feedback fundamentally governs the performance of congestion control</p>
    <p>We propose to use latency feedback in datacenters with support from our SW/HW timestamping techniques</p>
    <p>We develop DX, a new latency-based congestion control, which achieves 5.3 times (1Gbps) and 1.6 times (10Gbps) queueing delay reduction than ECN-based DCTCP</p>
  </div>
</Presentation>
