<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>EROFS: A Compression-friendly Readonly File System for Resource-scarce Devices</p>
    <p>Xiang Gao#, Mingkai Dong*, Xie Miao#, Wei Du#, Chao Yu#, Haibo Chen*,#</p>
    <p>#Huawei Technologies Co., Ltd. *IPADS, Shanghai Jiao Tong University</p>
    <p>USENIX ATC 2019, Renton, WA, USA</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>/system Partition Size (MB)</p>
    <p>System resources in Android</p>
    <p>...</p>
    <p>System resources consume significant storage! Read-only system partitions  compressed read-only file systems</p>
    <p>/system</p>
    <p>Read-only /oem</p>
    <p>Read-only</p>
    <p>/odm</p>
    <p>Read-only</p>
    <p>/vendor</p>
    <p>Read-only</p>
  </div>
  <div class="page">
    <p>Existing solution Squashfs is the state-of-the-art compressed read-only file system We tried to use squashfs for system resources in Android</p>
    <p>/system Squashfs</p>
    <p>/oem Squashfs</p>
    <p>/vendor Squashfs</p>
    <p>/odm Squashfs</p>
    <p>Result: The system lagged and even froze for seconds and then rebooted</p>
  </div>
  <div class="page">
    <p>Why does Squashfs fail?</p>
    <p>Fixed-sized input compression</p>
  </div>
  <div class="page">
    <p>Fixed-sized input compression</p>
    <p>Why does Squashfs fail?</p>
  </div>
  <div class="page">
    <p>Read Amplification</p>
    <p>... ...</p>
    <p>decompression amplification</p>
    <p>I/O amplification</p>
    <p>Why does Squashfs fail?</p>
    <p>Fixed-sized input compression</p>
  </div>
  <div class="page">
    <p>Read Amplification</p>
    <p>...</p>
    <p>buffer_head</p>
    <p>...</p>
    <p>decompress</p>
    <p>Page cache</p>
    <p>... copy</p>
    <p>Massive Memory Consumption</p>
    <p>decompression amplification</p>
    <p>I/O amplification</p>
    <p>Why does Squashfs fail?</p>
    <p>allocation</p>
    <p>allocation</p>
    <p>allocation</p>
    <p>copy</p>
  </div>
  <div class="page">
    <p>EROFS</p>
    <p>Fixed-sized output compression</p>
  </div>
  <div class="page">
    <p>EROFS</p>
    <p>Fixed-sized output compression</p>
    <p>Reduce read amplification</p>
    <p>Better compression ratio</p>
    <p>Allows in-place decompression</p>
  </div>
  <div class="page">
    <p>Choosing the page for I/O</p>
    <p>Cached I/O For partially decompressed blocks Allocate a page in dedicated page cache for I/O  Following decompression can reuse the cached page</p>
    <p>In-place I/O For blocks to be fully decompressed Reuse the page allocated by VFS if possible  Memory allocation and consumption are reduced</p>
    <p>Page cache</p>
    <p>Page cache for partially compressed blocks</p>
    <p>Page cache</p>
    <p>Page cache of requested file</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Page cache</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Page cache</p>
    <p>How data is compressed</p>
    <p>Vmap decompression 1. Count data blocks to decompress. 2. Allocate temporary physical pages or choose pages allocated by VFS. 3. Allocate a continuous VM area via vmap(), and map physical pages in the area. 4. For in-place I/O, copy the compressed block to a temporary per-CPU page. 5. Decompress to the VM area.</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Page cache</p>
    <p>How data is compressed</p>
    <p>Vmap decompression 1. Count data blocks to decompress. 2. Allocate temporary physical pages or choose pages allocated by VFS. 3. Allocate a continuous VM area via vmap(), and map physical pages in the area. 4. For in-place I/O, copy the compressed block to a temporary per-CPU page. 5. Decompress to the VM area.</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Page cache</p>
    <p>How data is compressed</p>
    <p>same physical pages</p>
    <p>Vmap decompression 1. Count data blocks to decompress. 2. Allocate temporary physical pages or choose pages allocated by VFS. 3. Allocate a continuous VM area via vmap(), and map physical pages in the area. 4. For in-place I/O, copy the compressed block to a temporary per-CPU page. 5. Decompress to the VM area.</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Page cache</p>
    <p>How data is compressed</p>
    <p>same physical pages</p>
    <p>Vmap decompression 1. Count data blocks to decompress. 2. Allocate temporary physical pages or choose pages allocated by VFS. 3. Allocate a continuous VM area via vmap(), and map physical pages in the area. 4. For in-place I/O, copy the compressed block to a temporary per-CPU page. 5. Decompress to the VM area.</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Vmap decompression 1. Count data blocks to decompress. 2. Allocate temporary physical pages or choose pages allocated by VFS. 3. Allocate a continuous VM area via vmap(), and map physical pages in the area. 4. For in-place I/O, copy the compressed block to a temporary per-CPU page. 5. Decompress to the VM area.</p>
    <p>Page cache</p>
    <p>How data is compressed</p>
    <p>same physical pages</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Vmap decompression</p>
    <p>For all cases  Frequent vmap/vunmap  Unbounded physical page allocations  Data copy for in-place I/O</p>
  </div>
  <div class="page">
    <p>Decompression</p>
    <p>Vmap decompression</p>
    <p>For all cases  Frequent vmap/vunmap  Unbounded physical page allocations  Data copy for in-place I/O</p>
    <p>Buffer decompression Pre-allocate four-page per-CPU buffers  For decompression &lt;4 pages  No vmap/vunmap  No physical page allocation  No data copy for in-place I/O</p>
  </div>
  <div class="page">
    <p>Decompression Vmap decompression</p>
    <p>For all cases  Frequent vmap/vunmap  Unbounded physical page allocations  Data copy for in-place I/O</p>
    <p>Buffer decompression Pre-allocate four-page per-CPU buffers  For decompression &lt;4 pages  No vmap/vunmap  No physical page allocation  No data copy for in-place I/O</p>
    <p>In-place decompression  No data copy for in-place I/O</p>
    <p>Decompression policy</p>
    <p>Optimizations</p>
    <p>Details in the paper Rolling decompression  For decompression &lt; a pre-allocated</p>
    <p>VM area size  No vmap/vunmap  No physical page allocation  Data copy for in-place I/O</p>
  </div>
  <div class="page">
    <p>Evaluation Setup Platform CPU DRAM Storage HiKey 960 Kirin 960 (Cortex-A73 x 4 + Cortex-A53 x 4) 3 GB 32 GB UFS</p>
    <p>Low-end smartphone MT6765 (Cortex-A53 x 8) 2 GB 32 GB eMMC</p>
    <p>High-end smartphone Kirin 980 (Cortex-A76 x 4 + Cortex-A55 x 4) 6 GB 64 GB UFS</p>
    <p>Micro-benchmarks  Platform: HiKey 960  Tool: FIO  Workload: enwik9, silesia.tar</p>
    <p>Application benchmarks  Platform: smartphones  Workload: 13 popular applications</p>
    <p>Evaluated file systems EROFS: LZ4, 4KB-sized output Squashfs: LZ4, {4,8,16,128}KB chunk size Btrfs: LZO, 128KB chunk size</p>
    <p>readonly mode w/o integrity checks Ext4: no compression F2FS: no compression</p>
    <p>More results in the paper</p>
  </div>
  <div class="page">
    <p>Micro-benchmark: FIO Throughput</p>
    <p>Sequential Random Stride</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (M</p>
    <p>B/ s)</p>
    <p>EROFS Squashfs-4K Squashfs-8K Squashfs-16K Squashfs-128K Ext4 F2FS Btrfs</p>
    <p>FIO, enwik9, HiKey 960 A73 2362 MHz</p>
  </div>
  <div class="page">
    <p>Micro-benchmark: FIO Throughput</p>
    <p>Sequential Random Stride</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (M</p>
    <p>B/ s)</p>
    <p>EROFS Squashfs-4K Squashfs-8K Squashfs-16K Squashfs-128K Ext4 F2FS Btrfs</p>
    <p>FIO, enwik9, HiKey 960 A73 2362 MHz</p>
  </div>
  <div class="page">
    <p>Larger chunk size brings better performance for Squashfs, if the cached results can are used.</p>
    <p>Micro-benchmark: FIO Throughput</p>
    <p>Sequential Random Stride</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (M</p>
    <p>B/ s)</p>
    <p>EROFS Squashfs-4K Squashfs-8K Squashfs-16K Squashfs-128K Ext4 F2FS Btrfs</p>
    <p>FIO, enwik9, HiKey 960 A73 2362 MHz</p>
  </div>
  <div class="page">
    <p>Micro-benchmark: FIO Throughput</p>
    <p>Sequential Random Stride</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (M</p>
    <p>B/ s)</p>
    <p>EROFS Squashfs-4K Squashfs-8K Squashfs-16K Squashfs-128K Ext4 F2FS Btrfs</p>
    <p>FIO, enwik9, HiKey 960 A73 2362 MHz</p>
  </div>
  <div class="page">
    <p>Read Amplification and Resource Consumption</p>
    <p>IO (MB) Consumption (MB) Seq. Random Stride Storage Memory</p>
    <p>Requested/Ext4 16 16 16 953.67 988.51 Squashfs-4K 10.65 26.19 26.23 592.43 1597.50 Squashfs-8K 9.82 33.52 34.08 530.43 1534.09 Squashfs-16K 9.05 46.42 48.32 479.38 1481.12 Squashfs-128K 7.25 165.27 203.91 379.76 1379.84 EROFS 10.14 26.12 25.93 533.67 1036.88</p>
  </div>
  <div class="page">
    <p>Read Amplification and Resource Consumption</p>
    <p>IO (MB) Consumption (MB) Seq. Random Stride Storage Memory</p>
    <p>Requested/Ext4 16 16 16 953.67 988.51 Squashfs-4K 10.65 26.19 26.23 592.43 1597.50 Squashfs-8K 9.82 33.52 34.08 530.43 1534.09 Squashfs-16K 9.05 46.42 48.32 479.38 1481.12 Squashfs-128K 7.25 165.27 203.91 379.76 1379.84 EROFS 10.14 26.12 25.93 533.67 1036.88</p>
  </div>
  <div class="page">
    <p>Read Amplification and Resource Consumption</p>
    <p>IO (MB) Consumption (MB) Seq. Random Stride Storage Memory</p>
    <p>Requested/Ext4 16 16 16 953.67 988.51 Squashfs-4K 10.65 26.19 26.23 592.43 1597.50 Squashfs-8K 9.82 33.52 34.08 530.43 1534.09 Squashfs-16K 9.05 46.42 48.32 479.38 1481.12 Squashfs-128K 7.25 165.27 203.91 379.76 1379.84 EROFS 10.14 26.12 25.93 533.67 1036.88</p>
  </div>
  <div class="page">
    <p>Real-world Application Boot Time</p>
    <p>Bo ot</p>
    <p>T im</p>
    <p>e (R</p>
    <p>el at</p>
    <p>iv e</p>
    <p>to E</p>
    <p>xt 4)</p>
    <p>Low-end Smartphone</p>
    <p>High-end Smartphone</p>
  </div>
  <div class="page">
    <p>Deployment</p>
    <p>Deployed in HUAWEI EMUI 9.1 as a top feature Upstreamed to Linux 4.19 System storage consumption decreased &gt;30% Performance comparable or even better than Ext4 Running on 10,000,000+ smartphones</p>
  </div>
  <div class="page">
    <p>Conclusion EROFS: an Enhanced Read-Only File System with compression support Fixed-sized output compression with four decompression approaches:</p>
    <p>Vmap decompression Buffer decompression Rolling decompression In-place decompression</p>
    <p>Running on 10,000,000+ smartphones Reduce system storage consumption &gt;30% Provide comparable or even better performance than Ext4</p>
    <p>Thank you &amp; questions?</p>
    <p>ER OF</p>
    <p>S ER</p>
    <p>OF S</p>
    <p>ER OF</p>
    <p>S</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Throughput and space savings</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (M</p>
    <p>B/ s)</p>
    <p>Space savings (%)</p>
    <p>EROFS-Seq. Ext4-Seq.</p>
    <p>EROFS-Rand. Ext4-Rand.</p>
    <p>Computation &gt; I/O I/O &gt; Computation Less I/O =&gt; Better Throughput</p>
    <p>FIO, customized workload generated from enwik9, HiKey 960 A73 2362 MHz 36</p>
  </div>
  <div class="page">
    <p>Rolling decompression Pre-allocate a large VM area and 17 physical pages per CPU Use 17 physical pages in turn.</p>
    <p>For decompression &lt; the VM area size  No vmap/vunmap  No physical page allocation  Data copy for in-place I/O</p>
    <p>Decompression Observation: In decompression, LZ4 looks backward at most 64KB of decompressed data.</p>
  </div>
  <div class="page">
    <p>In-place decompression If no corruption will happen  For decompression &lt; the VM area size  No vmap/vunmap  No physical page allocation  No data copy for in-place I/O</p>
    <p>Decompression</p>
    <p>Seq. A Seq. corrupted</p>
    <p>decompressed Seq. A</p>
    <p>Before</p>
    <p>decompress in-place</p>
    <p>After</p>
  </div>
</Presentation>
