<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Dynamic Virtual Machine Scheduling in Clouds for Architectural Shared Resources</p>
    <p>Jeongseob Ahn, Changdae Kim, Jaeung Han,</p>
    <p>Young-ri Choi, and Jaehyuk Huh</p>
    <p>KAIST and UNIST</p>
  </div>
  <div class="page">
    <p>Challenges for Cloud Computing</p>
    <p>Virtual machines (VM) share physical resources</p>
    <p>Improve overall utilization of limited resources</p>
    <p>Resource contention: potential performance degradation</p>
    <p>Multi-cores has enabled the sharing of architectural resources</p>
    <p>Shared last level caches (LLCs) and memory controllers</p>
    <p>Contention on such architectural resources</p>
    <p>Major reason for performance variance</p>
    <p>VM performance affected by co-runners</p>
  </div>
  <div class="page">
    <p>Shared Cache Contention</p>
    <p>Shared cache contention</p>
    <p>When a core generates excessive cache misses and evicts the cached data from the other cores, contention occurs</p>
    <p>Way to solve cache contention problem</p>
    <p>Partitioning caches  Qureshi et al. [MICRO06]</p>
    <p>Suh et al. [HPCA02]</p>
    <p>Scheduling threads carefully in multiple LLC environment  Merkel et al. [EuroSys10]</p>
    <p>Zhuravlev et al [ASPLOS10]</p>
  </div>
  <div class="page">
    <p>DRAM MODULE</p>
    <p>Shared Cache Scheduling Opportunities</p>
    <p>Scheduling threads to mitigate interferences in shared caches</p>
    <p>Zhuravlev et al. [ASPLOS10]</p>
    <p>Evenly distribute threads according to LLC miss rate</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>High LLC missrate</p>
    <p>Low LLC missrate</p>
    <p>Threads are grouped to mitigate the interferences for shared cache</p>
    <p>Single Thread</p>
  </div>
  <div class="page">
    <p>Cache Contention + NUMA Affinity</p>
    <p>NUMA affinity complicates such cache aware scheduling</p>
    <p>Blagodurov et al. [USENIX ATC11]  Investigated the impact of NUMA on cache-aware scheduling to reduce</p>
    <p>negative interferences among threads</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M D I M M D I M M</p>
    <p>Remote access occurs !! Local access occurs !! High LLC missrate</p>
    <p>Low LLC missrate</p>
    <p>DRAM MODULE</p>
  </div>
  <div class="page">
    <p>Cache Contention + NUMA Affinity</p>
    <p>NUMA affinity complicates such cache aware partitioning</p>
    <p>Blagodurov et al. [USENIX ATC10]  Investigated the impact of NUMA on cache-aware scheduling to reduce</p>
    <p>negative interferences among threads</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>High load</p>
    <p>Low load D I M M D I M M D I M M D I M M</p>
    <p>However, cloud systems with virtualization open a new opportunity to widen the scope of contention-aware</p>
    <p>scheduling</p>
  </div>
  <div class="page">
    <p>Scheduling Opportunities in Clouds</p>
    <p>Intra-system scheduling limits the opportunity to search the best groups of threads</p>
    <p>Each machines are scheduled efficiently, but it is inefficient in global view</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node A</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node B</p>
    <p>Contention occured Contention not occured</p>
    <p>High LLC missrate Low LLC missrate</p>
  </div>
  <div class="page">
    <p>In virtualized cloud system, we have an increased chance to find a better grouping of VMs</p>
    <p>Scheduling Opportunities in Clouds</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node A</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node B</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node C</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node D</p>
    <p>fUse live migration !!</p>
    <p>Thread  Virtual Machine</p>
  </div>
  <div class="page">
    <p>Performance Implication in Clouds</p>
    <p>Compare six different VM mapping policies for cache &amp; memory</p>
    <p>2 version of cache sharing aspect  Best case (B) : map VMs to cores to minimize the sum of LLC misses from all</p>
    <p>the sockets in the clouds system</p>
    <p>Worst case (W) : map VM with highest difference between the largest and smallest per-socket LLC misses in the clouds system</p>
    <p>3 version of NUMA affinity  Best allocation (B) : all VM memory pages are allocated in their local sockets</p>
    <p>Worst allocation (W) : memory pages of all VMs are allocated in their remote sockets</p>
    <p>Interleaved allocation (I) : assigns the memory pages of a VM to be always in both sockets in an interleaved way</p>
  </div>
  <div class="page">
    <p>Interleaved memory allocation</p>
    <p>Interleaved memory allocation</p>
    <p>Remove NUMA affinity</p>
    <p>Constant memory access latency regardless of the location of VM</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>f</p>
    <p>Access latency : NUMA best &lt; NUMA interleaving &lt; NUMA worst</p>
  </div>
  <div class="page">
    <p>Performance Implication in Clouds</p>
    <p>Normalized with cache worst &amp; NUMA worst case</p>
    <p>W-I : Cache worst &amp; NUMA interleaved</p>
    <p>W-B : Cache worst &amp; NUMA best</p>
    <p>B-W : Cache best &amp; NUMA worst</p>
    <p>B-I : Cache best &amp; NUMA interleaved</p>
    <p>B-B : Cache best &amp; NUMA best</p>
    <p>milc hmmer namd GemsFDTD Total</p>
    <p>P e rf . Im</p>
    <p>p ro</p>
    <p>v e m</p>
    <p>e n t</p>
    <p>(% )</p>
    <p>W-I W-B B-W B-I B-B</p>
    <p>f</p>
    <p>There is significant potential for performance improvements by placing VMs considering architectural shared resources</p>
  </div>
  <div class="page">
    <p>Is Static Placement Possible?</p>
    <p>Maybe possible in single system</p>
    <p>Execute same application mostly</p>
    <p>Impossible in clouds environment</p>
    <p>VM behaviors cannot be predicted</p>
    <p>Dynamic scheduling &amp; placement is needed</p>
  </div>
  <div class="page">
    <p>Memory-Aware Cloud Scheduling</p>
    <p>Computing node  Check LLC misses with PMC, and send LLC miss and NUMA affinity</p>
    <p>information to front-end node</p>
    <p>Front-end node  Based on VM status information from all computing nodes, it makes</p>
    <p>global scheduling decisions</p>
  </div>
  <div class="page">
    <p>Two Scheduling policies</p>
    <p>Cache-aware scheduler</p>
    <p>Only considers the contentions on shared caches</p>
    <p>Composed of two phases  Local phase : VMs are rescheduled on intra-core</p>
    <p>Global phase : VMs are migrated between inter-cores</p>
    <p>NUMA-aware scheduler</p>
    <p>Extends the cache aware scheduler for NUMA affinity of VMs</p>
    <p>Only exists global phase  To keep NUMA affinity</p>
  </div>
  <div class="page">
    <p>Cache-Aware Scheduler</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D D I M I M M M</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M D M I M M</p>
    <p>Local phase</p>
    <p>Unit of scheduling is VM, not thread</p>
    <p>Node A Node B</p>
    <p>VMs in each nodes are grouped to mitigate the interferences for shared resources 2.VMs having most LLC miss rate are assigned to each core 3.VMs having least LLC miss rate are assigned to each core 4.VMs having secondly most LLC miss rate are assigned to each core 5.VMs having secondly least LLC miss rate are assigned to each core 1.VMs are sorted with LLC missrate in each system</p>
  </div>
  <div class="page">
    <p>Global Phase</p>
    <p>Do not aware NUMA affinity</p>
    <p>Cache-Aware Scheduler</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D D I M I M M M</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M D M I M M</p>
    <p>Node A Node B</p>
    <p>Largest number of LLC miss host! Smallest number of LLC miss host!</p>
  </div>
  <div class="page">
    <p>Similar with global phase of cache-aware scheduler, but unit of migration is socket, not host.</p>
    <p>NUMA-Aware Scheduler</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node A Node B</p>
    <p>Largest number of LLC miss</p>
    <p>socket!</p>
    <p>Smallest number of LLC miss</p>
    <p>socket!</p>
  </div>
  <div class="page">
    <p>Intra-host migration is also available.</p>
    <p>NUMA-Aware Scheduler</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>Shared LLC</p>
    <p>Memory Controller</p>
    <p>Network Router</p>
    <p>D I M M D I M M</p>
    <p>Node A Node B</p>
    <p>Largest number of LLC miss</p>
    <p>socket!</p>
    <p>Smallest number of LLC miss</p>
    <p>socket!</p>
  </div>
  <div class="page">
    <p>Experimental Methodology</p>
    <p>Testbed</p>
    <p>Front-end node (1)  Running proposed schedulers</p>
    <p>Storage servers for VM images</p>
    <p>Computing node (4)  Intel Xeon 8 cores on two chips</p>
    <p>Each chip has 12MB LLC shared by 4 cores</p>
    <p>Each VM employs a single core and 1GB memory</p>
    <p>On top of Xen hypervisor, each node runs 8 guest VMs</p>
    <p>VMs use a Ubuntu distribution based on Linux kernel 2.6.18</p>
  </div>
  <div class="page">
    <p>Experimental Methodology</p>
    <p>Benchmark applications</p>
    <p>Selected from SPECcpu 2006</p>
    <p>Each workload has 4 different applications</p>
    <p>8 instances of each application run on the 32VMs in our testbed</p>
  </div>
  <div class="page">
    <p>Performance Improvements</p>
    <p>WL1: 2 Memory bound + 2 CPU bound</p>
    <p>WL3: 3 Memory bound + 1 CPU bound</p>
    <p>milc hmmer namd GemsFDTD Total</p>
    <p>P e rf</p>
    <p>. Im</p>
    <p>p ro</p>
    <p>v e m</p>
    <p>e n t(</p>
    <p>% )</p>
    <p>cache-aware NUMA-aware B-B</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>soplex gcc cactusADM namd Total</p>
    <p>P e rf</p>
    <p>. Im</p>
    <p>p ro</p>
    <p>v e m</p>
    <p>e n t(</p>
    <p>% )</p>
    <p>cache-aware NUMA-aware B-B</p>
  </div>
  <div class="page">
    <p>Performance Improvements</p>
    <p>Show similar trends except for WL6</p>
    <p>WL6 consists of all CPU-bound applications</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>WL1 WL2 WL3 WL4 WL5 WL6</p>
    <p>P e rf</p>
    <p>. Im</p>
    <p>p ro</p>
    <p>v e m</p>
    <p>e n t(</p>
    <p>% )</p>
    <p>cache-aware NUMA-aware B-B</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>WL1 WL2 WL3 WL4 WL5 WL6</p>
    <p>P e rf</p>
    <p>. Im</p>
    <p>p ro</p>
    <p>v e m</p>
    <p>e n t(</p>
    <p>% )</p>
    <p>cache-aware NUMA-aware B-B</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>!!!/</p>
    <p>WL1 WL2 WL3 WL4 WL5 WL6</p>
    <p>P e rf</p>
    <p>. Im</p>
    <p>p ro</p>
    <p>v e m</p>
    <p>e n t(</p>
    <p>% )</p>
    <p>cache-aware NUMA-aware B-B</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>We proposed memory-aware cloud scheduling</p>
    <p>Use live migration</p>
    <p>VM live migration can be used to mitigate architectural resource contentions</p>
    <p>Cloud-level VM scheduler must consider such hidden contentions</p>
    <p>Future work</p>
    <p>Extend our preliminary design of NUMA-aware scheduling</p>
    <p>Investigate a systematic approach based on a cost-benefit analysis for VM migrations and contention reductions</p>
  </div>
  <div class="page">
    <p>Thank you !</p>
    <p>Dynamic Virtual Machine Scheduling in Clouds for Architectural Shared Resources</p>
    <p>Jeongseob Ahn, Changdae Kim, Jaeung Han,</p>
    <p>Young-ri Choi, and Jaehyuk Huh</p>
    <p>KAIST and UNIST</p>
  </div>
</Presentation>
