<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Joseph Gonzalez</p>
    <p>Yucheng Low</p>
    <p>Danny Bickson</p>
    <p>Distributed Graph-Parallel Computa@on on Natural Graphs</p>
    <p>Haijie Gu</p>
    <p>Joint work with:</p>
    <p>Carlos Guestrin</p>
  </div>
  <div class="page">
    <p>Graphs are ubiquitous..</p>
  </div>
  <div class="page">
    <p>Social Media</p>
    <p>Graphs encode rela0onships between:</p>
    <p>Big: billions of ver0ces and edges and rich metadata</p>
    <p>Adver0sing Science Web</p>
    <p>People Facts</p>
    <p>Products Interests</p>
    <p>Ideas</p>
  </div>
  <div class="page">
    <p>Graphs are Essen@al to Data-Mining and Machine Learning</p>
    <p>Iden@fy influen@al people and informa@on  Find communi@es  Target ads and products  Model complex data dependencies</p>
  </div>
  <div class="page">
    <p>Natural Graphs Graphs derived from natural</p>
    <p>phenomena</p>
  </div>
  <div class="page">
    <p>Problem:</p>
    <p>Exis@ng distributed graph computa@on systems perform poorly on Natural Graphs.</p>
  </div>
  <div class="page">
    <p>PageRank on TwiUer Follower Graph Natural Graph with 40M Users, 1.4 Billion Links</p>
    <p>Hadoop results from [Kang et al. '11] Twister (in-memory MapReduce) [Ekanayake et al. 10]</p>
    <p>Hadoop</p>
    <p>GraphLab</p>
    <p>Twister</p>
    <p>Piccolo</p>
    <p>PowerGraph</p>
    <p>Run0me Per Itera0on</p>
    <p>Order of magnitude by exploi3ng proper@es of Natural Graphs</p>
  </div>
  <div class="page">
    <p>Proper@es of Natural Graphs</p>
    <p>Power-Law Degree Distribu@on</p>
  </div>
  <div class="page">
    <p>Power-Law Degree Distribu@on</p>
    <p>degree</p>
    <p>co un t</p>
    <p>Top 1% of ver0ces are adjacent to</p>
    <p>High-Degree Ver@ces</p>
    <p>N um</p>
    <p>be r of V er @c es</p>
    <p>AltaVista WebGraph 1.4B Ver@ces, 6.6B Edges</p>
    <p>Degree</p>
    <p>More than 108 ver0ces have one neighbor.</p>
  </div>
  <div class="page">
    <p>Power-Law Degree Distribu@on</p>
    <p>Star Like Mo@f</p>
    <p>President Obama Followers</p>
  </div>
  <div class="page">
    <p>Power-Law Graphs are Difficult to Par00on</p>
    <p>Power-Law graphs do not have low-cost balanced cuts [Leskovec et al. 08, Lang 04]</p>
    <p>Tradi@onal graph-par@@oning algorithms perform poorly on Power-Law Graphs. [Abou-Rjeili et al. 06]</p>
    <p>CPU 1 CPU 2</p>
  </div>
  <div class="page">
    <p>Proper@es of Natural Graphs</p>
    <p>High-degree Ver@ces</p>
    <p>Low Quality Par@@on</p>
    <p>Power-Law Degree Distribu@on</p>
  </div>
  <div class="page">
    <p>Machine 1 Machine 2</p>
    <p>Split High-Degree ver@ces  New Abstrac0on  Equivalence on Split Ver3ces</p>
    <p>Program For This</p>
    <p>Run on This</p>
  </div>
  <div class="page">
    <p>How do we program graph computa@on?</p>
    <p>Think like a Vertex. -Malewicz et al. [SIGMOD10]</p>
  </div>
  <div class="page">
    <p>The Graph-Parallel Abstrac@on  A user-defined Vertex-Program runs on each vertex  Graph constrains interac0on along edges</p>
    <p>Using messages (e.g. Pregel [PODC09, SIGMOD10])  Through shared state (e.g., GraphLab [UAI10, VLDB12])</p>
    <p>Parallelism: run mul@ple vertex programs simultaneously</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Whats the popularity of this user?</p>
    <p>Popular?</p>
    <p>Depends on popularity of her followers</p>
    <p>Depends on the popularity their followers</p>
  </div>
  <div class="page">
    <p>PageRank Algorithm</p>
    <p>Update ranks in parallel  Iterate un@l convergence</p>
    <p>Rank of user i Weighted sum of</p>
    <p>neighbors ranks</p>
    <p>R[i] = 0.15 + X</p>
    <p>j2Nbrs(i)</p>
    <p>wjiR[j]</p>
  </div>
  <div class="page">
    <p>The Pregel Abstrac@on Vertex-Programs interact by sending messages.</p>
    <p>i Pregel_PageRank(i, messages) : // Receive all the messages total = 0 foreach( msg in messages) : total = total + msg // Update the rank of this vertex R[i] = 0.15 + total // Send new messages to neighbors foreach(j in out_neighbors[i]) : Send msg(R[i] * wij) to vertex j</p>
  </div>
  <div class="page">
    <p>The GraphLab Abstrac@on Vertex-Programs directly read the neighbors state</p>
    <p>i GraphLab_PageRank(i) // Compute sum over neighbors total = 0 foreach( j in in_neighbors(i)): total = total + R[j] * wji // Update the PageRank R[i] = 0.15 + total // Trigger neighbors to run again if R[i] not converged then foreach( j in out_neighbors(i)): signal vertex-program on j</p>
  </div>
  <div class="page">
    <p>Asynchronous Execu@on requires heavy locking (GraphLab)</p>
    <p>Challenges of High-Degree Ver@ces</p>
    <p>Touches a large frac@on of graph</p>
    <p>(GraphLab)</p>
    <p>Sequen@ally process edges</p>
    <p>Sends many messages (Pregel)</p>
    <p>Edge meta-data too large for single</p>
    <p>machine</p>
    <p>Synchronous Execu@on prone to stragglers (Pregel)</p>
  </div>
  <div class="page">
    <p>Communica@on Overhead for High-Degree Ver@ces</p>
    <p>Fan-In vs. Fan-Out</p>
  </div>
  <div class="page">
    <p>Pregel Message Combiners on Fan-In</p>
    <p>Machine 1 Machine 2</p>
    <p>+ B</p>
    <p>A</p>
    <p>C</p>
    <p>D Sum</p>
    <p>User defined commuta0ve associa0ve (+) message opera@on:</p>
  </div>
  <div class="page">
    <p>Pregel Struggles with Fan-Out</p>
    <p>Machine 1 Machine 2</p>
    <p>B</p>
    <p>A</p>
    <p>C</p>
    <p>D</p>
    <p>Broadcast sends many copies of the same message to the same machine!</p>
  </div>
  <div class="page">
    <p>Fan-In and Fan-Out Performance  PageRank on synthe@c Power-Law Graphs  Piccolo was used to simulate Pregel with combiners</p>
    <p>To ta l C om</p>
    <p>m . ( G B)</p>
    <p>Power-Law Constant</p>
    <p>More high-degree ver@ces 24</p>
    <p>High Fan-In Graphs</p>
  </div>
  <div class="page">
    <p>GraphLab Ghos@ng</p>
    <p>Changes to master are synced to ghosts</p>
    <p>Machine 1</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>Machine 2</p>
    <p>DD</p>
    <p>A</p>
    <p>B</p>
    <p>CGhost</p>
  </div>
  <div class="page">
    <p>GraphLab Ghos@ng</p>
    <p>Changes to neighbors of high degree ver0ces creates substan@al network traffic</p>
    <p>Machine 1</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>Machine 2</p>
    <p>DD</p>
    <p>A</p>
    <p>B</p>
    <p>C Ghost</p>
  </div>
  <div class="page">
    <p>Fan-In and Fan-Out Performance</p>
    <p>PageRank on synthe@c Power-Law Graphs  GraphLab is undirected</p>
    <p>To ta l C om</p>
    <p>m . ( G B)</p>
    <p>Power-Law Constant alpha</p>
    <p>More high-degree ver@ces 27</p>
    <p>Pregel Fan-In</p>
    <p>GraphLab Fan-In/Out</p>
  </div>
  <div class="page">
    <p>Graph Par@@oning  Graph parallel abstrac@ons rely on par@@oning:  Minimize communica@on  Balance computa@on and storage</p>
    <p>Y</p>
    <p>Machine 1 Machine 2 28</p>
    <p>Data transmitted across network</p>
    <p>O(# cut edges)</p>
  </div>
  <div class="page">
    <p>Machine 1 Machine 2</p>
    <p>Random Par@@oning</p>
    <p>Both GraphLab and Pregel resort to random (hashed) par@@oning on natural graphs</p>
    <p>D</p>
    <p>A&quot;</p>
    <p>C&quot;</p>
    <p>B&quot; 2&quot; 3&quot;</p>
    <p>C&quot;</p>
    <p>D</p>
    <p>B&quot; A&quot;</p>
    <p>D</p>
    <p>A&quot;</p>
    <p>C&quot;C&quot;</p>
    <p>B&quot;</p>
    <p>(a) Edge-Cut</p>
    <p>B&quot;A&quot; 1&quot;</p>
    <p>C&quot; D3&quot;</p>
    <p>C&quot; B&quot;2&quot;</p>
    <p>C&quot; D</p>
    <p>B&quot;A&quot; 1&quot;</p>
    <p>(b) Vertex-Cut</p>
    <p>Figure 4: (a) An edge-cut and (b) vertex-cut of a graph into three parts. Shaded vertices are ghosts and mirrors respectively.</p>
    <p>The PowerGraph abstraction relies on the distributed datagraph to store the computation state and encode the interaction between vertex programs. The placement of the data-graph structure and data plays a central role in minimizing communication and ensuring work balance.</p>
    <p>A common approach to placing a graph on a cluster of p machines is to construct a balanced p-way edge-cut (e.g., Fig. 4a) in which vertices are evenly assigned to machines and the number of edges spanning machines is minimized. Unfortunately, the tools [21, 31] for constructing balanced edge-cuts perform poorly [1, 26, 23] or even fail on powerlaw graphs. When the graph is difficult to partition, both GraphLab and Pregel resort to hashed (random) vertex placement. While fast and easy to implement, hashed vertex placement cuts most of the edges:</p>
    <p>Theorem 5.1. If vertices are randomly assigned to p machines then the expected fraction of edges cut is:</p>
    <p>E  |Edges Cut|</p>
    <p>|E|</p>
    <p>= 1</p>
    <p>(5.1)</p>
    <p>For example if just two machines are used, half of the of edges will be cut requiring order |E|/2 communication.</p>
    <p>min A</p>
    <p>|A(v)| (5.2)</p>
    <p>s.t. max m</p>
    <p>|{e 2 E | A(e) = m}| &lt; l |E| p</p>
    <p>(5.3)</p>
    <p>where the imbalance factor l  1 is a small constant. We use the term replicas of a vertex v to denote the |A(v)| copies of the vertex v: each machine in A(v) has a replica of v. The objective term (Eq. 5.2) therefore minimizes the</p>
    <p>average number of replicas in the graph and as a consequence the total storage and communication requirements of the PowerGraph engine.</p>
    <p>Vertex-cuts address many of the major issues associated with edge-cuts in power-law graphs. Percolation theory [3] suggests that power-law graphs have good vertex-cuts. Intuitively, by cutting a small fraction of the very high degree vertices we can quickly shatter a graph. Furthermore, because the balance constraint (Eq. 5.3) ensures that edges are uniformly distributed over machines, we naturally achieve improved work balance even in the presence of very high-degree vertices.</p>
    <p>The simplest method to construct a vertex cut is to randomly assign edges to machines. Random (hashed) edge placement is fully data-parallel, achieves nearly perfect balance on large graphs, and can be applied in the streaming setting. In the following we relate the expected normalized replication factor (Eq. 5.2) to the number of machines and the power-law constant a .</p>
    <p>Theorem 5.2 (Randomized Vertex Cuts). Let D[v] denote the degree of vertex v. A uniform random edge placement on p machines has an expected replication factor</p>
    <p>E &quot;</p>
    <p>|A(v)| # =</p>
    <p>p |V | v2V</p>
    <p>D[v]! . (5.4)</p>
    <p>For a graph with power-law constant a we obtain:</p>
    <p>E &quot;</p>
    <p>|A(v)| # = p  p Lia</p>
    <p>p  1</p>
    <p>p</p>
    <p>/z (a) (5.5)</p>
    <p>where Lia (x) is the transcendental polylog function and z (a) is the Riemann Zeta function (plotted in Fig. 5a).</p>
    <p>Higher a values imply a lower replication factor, confirming our earlier intuition. In contrast to a random 2way edge-cut which requires order |E|/2 communication a random 2-way vertex-cut on an a = 2 power-law graph requires only order 0.3|V | communication, a substantial savings on natural graphs where E can be an order of magnitude larger than V (see Tab. 1a).</p>
    <p>arg min k</p>
    <p>E &quot;</p>
    <p>v2V</p>
    <p>|A(v)|</p>
    <p>Ai,A(ei+1) = k</p>
    <p># (5.6)</p>
  </div>
  <div class="page">
    <p>In Summary</p>
    <p>GraphLab and Pregel are not well suited for natural graphs</p>
    <p>Challenges of high-degree ver0ces  Low quality par00oning</p>
  </div>
  <div class="page">
    <p>GAS Decomposi0on: distribute vertex-programs  Move computa@on to data  Parallelize high-degree ver@ces</p>
    <p>Vertex Par00oning:  Effec@vely distribute large power-law graphs</p>
  </div>
  <div class="page">
    <p>Gather Informa0on About Neighborhood</p>
    <p>Update Vertex</p>
    <p>Signal Neighbors &amp; Modify Edge Data</p>
    <p>A Common Pabern for Vertex-Programs</p>
    <p>GraphLab_PageRank(i) // Compute sum over neighbors total = 0 foreach( j in in_neighbors(i)): total = total + R[j] * wji // Update the PageRank R[i] = 0.1 + total // Trigger neighbors to run again if R[i] not converged then foreach( j in out_neighbors(i)) signal vertex-program on j</p>
  </div>
  <div class="page">
    <p>GAS Decomposi@on Y</p>
    <p>+  +</p>
    <p>Y</p>
    <p>Parallel Sum</p>
    <p>User Defined: Gather( )   Y</p>
    <p>1 + 2  3</p>
    <p>Y</p>
    <p>Gather (Reduce) Apply the accumulated value to center vertex</p>
    <p>Apply Update adjacent edges</p>
    <p>and ver@ces.</p>
    <p>ScaUer</p>
    <p>Accumulate informa@on about neighborhood</p>
    <p>Y</p>
    <p>+</p>
    <p>User Defined:</p>
    <p>Apply( , )  Y Y</p>
    <p>Y</p>
    <p>Y</p>
    <p>Update Edge Data &amp; Ac@vate Neighbors</p>
    <p>User Defined: Scaber( )  Y</p>
    <p>Y</p>
  </div>
  <div class="page">
    <p>PowerGraph_PageRank(i)</p>
    <p>Gather( j  i ) : return wji * R[j] sum(a, b) : return a + b;</p>
    <p>Apply(i, ) : R[i] = 0.15 +</p>
    <p>Scaber( i  j ) :  if R[i] changed then trigger j to be recomputed</p>
    <p>PageRank in PowerGraph</p>
    <p>R[i] = 0.15 + X</p>
    <p>j2Nbrs(i)</p>
    <p>wjiR[j]</p>
  </div>
  <div class="page">
    <p>Machine 2 Machine 1</p>
    <p>Machine 4 Machine 3</p>
    <p>Distributed Execu@on of a PowerGraph Vertex-Program</p>
    <p>1 2</p>
    <p>3 4</p>
    <p>+ + +</p>
    <p>Y Y Y Y</p>
    <p>Y</p>
    <p>Y Y Y Gather</p>
    <p>Apply</p>
    <p>ScaUer</p>
    <p>Master</p>
    <p>Mirror</p>
    <p>Mirror Mirror</p>
  </div>
  <div class="page">
    <p>Minimizing Communica@on in PowerGraph</p>
    <p>Y Y Y</p>
    <p>A vertex-cut minimizes machines each vertex spans</p>
    <p>Percola3on theory suggests that power law graphs have good vertex cuts. [Albert et al. 2000]</p>
    <p>Communica@on is linear in the number of machines</p>
    <p>each vertex spans</p>
  </div>
  <div class="page">
    <p>New Approach to Par@@oning</p>
    <p>Rather than cut edges:</p>
    <p>we cut ver@ces: CPU 1 CPU 2</p>
    <p>Y Y Must synchronize</p>
    <p>many edges</p>
    <p>CPU 1 CPU 2</p>
    <p>Y Y Must synchronize a single vertex</p>
    <p>New Theorem: For any edge-cut we can directly construct a vertex-cut which requires strictly less communica3on and storage.</p>
  </div>
  <div class="page">
    <p>Construc@ng Vertex-Cuts</p>
    <p>Evenly assign edges to machines  Minimize machines spanned by each vertex</p>
    <p>Assign each edge as it is loaded  Touch each edge only once</p>
    <p>Propose three distributed approaches:  Random Edge Placement  Coordinated Greedy Edge Placement  Oblivious Greedy Edge Placement</p>
  </div>
  <div class="page">
    <p>Machine 2 Machine 1 Machine 3</p>
    <p>Random Edge-Placement  Randomly assign edges to machines</p>
    <p>Y Y Y Y Z Y Y Y Y Z Y Z Y Spans 3 Machines</p>
    <p>Z Spans 2 Machines</p>
    <p>Balanced Vertex-Cut</p>
    <p>Not cut!</p>
  </div>
  <div class="page">
    <p>Analysis Random Edge-Placement</p>
    <p>Expected number of machines spanned by a vertex:</p>
    <p>ac hi ne</p>
    <p>s Sp an</p>
    <p>ne d</p>
    <p>Number of Machines</p>
    <p>Predicted</p>
    <p>Random</p>
    <p>Twiber Follower Graph 41 Million Ver@ces 1.4 Billion Edges</p>
    <p>Accurately Es@mate Memory and Comm.</p>
    <p>Overhead</p>
  </div>
  <div class="page">
    <p>Random Vertex-Cuts vs. Edge-Cuts</p>
    <p>Expected improvement from vertex-cuts:</p>
    <p>Re du</p>
    <p>c0 on</p>
    <p>in</p>
    <p>Co m m . a nd</p>
    <p>S to ra ge</p>
    <p>Number of Machines 41</p>
    <p>Order of Magnitude Improvement</p>
  </div>
  <div class="page">
    <p>Greedy Vertex-Cuts</p>
    <p>Place edges on machines which already have the ver@ces in that edge.</p>
    <p>Machine1 Machine 2</p>
    <p>B A C B</p>
    <p>D A E B 42</p>
  </div>
  <div class="page">
    <p>Greedy Vertex-Cuts</p>
    <p>De-randomiza0on  greedily minimizes the expected number of machines spanned</p>
    <p>Coordinated Edge Placement  Requires coordina@on to place each edge  Slower: higher quality cuts</p>
    <p>Oblivious Edge Placement  Approx. greedy objec@ve without coordina@on  Faster: lower quality cuts</p>
  </div>
  <div class="page">
    <p>Par@@oning Performance Twiber Graph: 41M ver@ces, 1.4B edges</p>
    <p>Oblivious balances cost and par@@oning @me.</p>
    <p>A vg # o f M</p>
    <p>ac hi ne</p>
    <p>s Sp an</p>
    <p>ne d</p>
    <p>Number of Machines</p>
    <p>in g Ti m e (S ec on</p>
    <p>ds )</p>
    <p>Number of Machines</p>
    <p>Oblivio us</p>
    <p>Coordinated</p>
    <p>Coordinated</p>
    <p>Random</p>
    <p>Cost Construc@on Time</p>
    <p>Be U er</p>
  </div>
  <div class="page">
    <p>Greedy Vertex-Cuts Improve Performance</p>
    <p>PageRank Collabora@ve Filtering</p>
    <p>Shortest Path</p>
    <p>Ru n0</p>
    <p>m e Re</p>
    <p>la 0 ve</p>
    <p>to R an</p>
    <p>do m</p>
    <p>Random Oblivious Coordinated</p>
    <p>Greedy par00oning improves computa0on performance. 45</p>
  </div>
  <div class="page">
    <p>Other Features (See Paper)</p>
    <p>Supports three execu@on modes:  Synchronous: Bulk-Synchronous GAS Phases  Asynchronous: Interleave GAS Phases  Asynchronous + Serializable: Neighboring ver@ces do not run simultaneously</p>
    <p>Delta Caching  Accelerate gather phase by caching par@al sums for each vertex</p>
  </div>
  <div class="page">
    <p>System Evalua@on</p>
  </div>
  <div class="page">
    <p>System Design</p>
    <p>Implemented as C++ API  Uses HDFS for Graph Input and Output  Fault-tolerance is achieved by check-poin@ng  Snapshot @me &lt; 5 seconds for twiUer network</p>
    <p>EC2 HPC Nodes</p>
    <p>MPI/TCP-IP PThreads HDFS</p>
    <p>PowerGraph (GraphLab2) System</p>
  </div>
  <div class="page">
    <p>Implemented Many Algorithms</p>
    <p>Collabora0ve Filtering  Alterna@ng Least Squares  Stochas@c Gradient Descent</p>
    <p>SVD  Non-nega@ve MF</p>
    <p>Sta0s0cal Inference  Loopy Belief Propaga@on  Max-Product Linear Programs</p>
    <p>Gibbs Sampling</p>
    <p>Graph Analy0cs  PageRank  Triangle Coun@ng  Shortest Path  Graph Coloring  K-core Decomposi@on</p>
    <p>Computer Vision  Image s@tching</p>
    <p>Language Modeling  LDA</p>
  </div>
  <div class="page">
    <p>Comparison with GraphLab &amp; Pregel  PageRank on Synthe@c Power-Law Graphs:</p>
    <p>Run@me Communica@on</p>
    <p>To ta l N</p>
    <p>et w or k (G B)</p>
    <p>Power-Law Constant</p>
    <p>s</p>
    <p>Power-Law Constant</p>
    <p>Pregel (Piccolo)</p>
    <p>GraphLab</p>
    <p>Pregel (Piccolo)</p>
    <p>GraphLab</p>
    <p>High-degree ver@ces High-degree ver@ces</p>
    <p>PowerGraph is robust to high-degree ver@ces.</p>
  </div>
  <div class="page">
    <p>PageRank on the TwiUer Follower Graph</p>
    <p>GraphLab Pregel (Piccolo)</p>
    <p>PowerGraph</p>
    <p>GraphLab Pregel (Piccolo)</p>
    <p>PowerGraph</p>
    <p>To ta l N</p>
    <p>et w or k (G B)</p>
    <p>Se co nd</p>
    <p>s</p>
    <p>Communica@on Run@me Natural Graph with 40M Users, 1.4 Billion Links</p>
    <p>Reduces Communica@on Runs Faster 32 Nodes x 8 Cores (EC2 HPC cc1.4x)</p>
  </div>
  <div class="page">
    <p>PowerGraph is Scalable Yahoo Altavista Web Graph (2002):</p>
    <p>One of the largest publicly available web graphs 1.4 Billion Webpages, 6.6 Billion Links</p>
  </div>
  <div class="page">
    <p>Topic Modeling  English language Wikipedia</p>
    <p>2.6M Documents, 8.3M Words, 500M Tokens  Computa@onally intensive algorithm</p>
    <p>Smola et al.</p>
    <p>PowerGraph</p>
    <p>Million Tokens Per Second</p>
  </div>
  <div class="page">
    <p>Counted: 34.8 Billion Triangles</p>
    <p>Triangle Coun0ng on The TwiUer Graph Iden@fy individuals with strong communi0es.</p>
    <p>Hadoop [WWW11]</p>
    <p>S. Suri and S. Vassilvitskii, Coun@ng triangles and the curse of the last reducer, WWW11</p>
    <p>Why? Wrong Abstrac0on  Broadcast O(degree2) messages per Vertex</p>
  </div>
  <div class="page">
    <p>Summary  Problem: Computa@on on Natural Graphs is challenging  High-degree ver@ces  Low-quality edge-cuts</p>
    <p>Solu3on: PowerGraph System  GAS Decomposi0on: split vertex programs  Vertex-par00oning: distribute natural graphs</p>
    <p>PowerGraph theore0cally and experimentally outperforms exis@ng graph-parallel systems.</p>
  </div>
  <div class="page">
    <p>PowerGraph (GraphLab2) System</p>
    <p>Graph Analy@cs</p>
    <p>Graphical Models</p>
    <p>Computer Vision</p>
    <p>Clustering Topic</p>
    <p>Modeling Collabora@ve</p>
    <p>Filtering</p>
    <p>Machine Learning and Data-Mining Toolkits</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Time evolving graphs  Support structural changes during computa@on</p>
    <p>Out-of-core storage (GraphChi)  Support graphs that dont fit in memory</p>
    <p>Improved Fault-Tolerance  Leverage vertex replica0on to reduce snapshots  Asynchronous recovery</p>
  </div>
  <div class="page">
    <p>is GraphLab Version 2.1</p>
    <p>Apache 2 License</p>
    <p>http://graphlab.org! Documentation Code Tutorials (more on the way)</p>
  </div>
</Presentation>
