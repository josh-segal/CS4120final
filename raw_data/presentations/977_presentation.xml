<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Christopher Mitchell, Kate Montgomery, Lamont Nelson, Siddhartha Sen*, Jinyang Li New York University, *Microsoft Research June 23, 2016</p>
    <p>USENIX ATC 2016</p>
  </div>
  <div class="page">
    <p>Server</p>
    <p>DataThread(s) of computation</p>
  </div>
  <div class="page">
    <p>Server</p>
    <p>Client Client</p>
    <p>RequestResponse</p>
    <p>Client</p>
  </div>
  <div class="page">
    <p>Load spikes saturate server CPUs</p>
    <p>Options 1. Over-provision server CPUs (wasteful) 2. Spin up extra servers during spike (slow)</p>
    <p>Solution: Relax locality by processing requests at client?</p>
    <p>Clients fetch the required server state</p>
  </div>
  <div class="page">
    <p>Server</p>
    <p>ReadData</p>
    <p>Client Client Client</p>
  </div>
  <div class="page">
    <p>Server CPU bottleneck -&gt; use client-side</p>
    <p>NIC bottleneck -&gt; use server-side  (If you have excess server CPU, just use it)</p>
  </div>
  <div class="page">
    <p>Combining client-side and server-side operations</p>
    <p>Insight: Selectively relaxing locality improves load balancing</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Motivation: Selectively Relaxed Locality  Cell Distributed B-tree  Evaluation  Related Work</p>
  </div>
  <div class="page">
    <p>Motivation: Selectively Relaxed Locality  Cell: Balancing Server-Side &amp; Client-Side Search</p>
    <p>Evaluation  Related Work</p>
  </div>
  <div class="page">
    <p>Client</p>
    <p>Server 1</p>
    <p>Server 2</p>
    <p>&quot;FOO&quot;?</p>
    <p>Continue at server 2</p>
    <p>&quot;FOO&quot;?</p>
    <p>&quot;FOO&quot;:&quot;BAR&quot;</p>
  </div>
  <div class="page">
    <p>Optimized for Ethernet  Data-computation locality heavily emphasized  BigTable: 128MB blocks, 3 RTs per operation  Large, opaque B-trees inside each B-tree node</p>
    <p>Great for server-side operations, bad for client-side operations  Bounded by server CPUs  Shouldnt ship large nodes via RDMA</p>
  </div>
  <div class="page">
    <p>Selectively relax data-computation locality</p>
    <p>B-link tree of (accessible) B-link trees  Traverse tree by 1KB lean nodes</p>
    <p>Client-side processing  Traverse tree by 64MB fat nodes</p>
    <p>Server-side processing</p>
  </div>
  <div class="page">
    <p>ROOT MEGANODE R</p>
    <p>MEGANODE</p>
    <p>L L L L L L</p>
    <p>MEGANODE</p>
    <p>node-to-node link level link</p>
  </div>
  <div class="page">
    <p>Client-Side and Server-Side Server-Side Only  Search</p>
    <p>Server-side: traverse fat nodes (meganodes) when server CPU is plentiful</p>
    <p>Client-side: traverse slim nodes when server CPU is bottleneck</p>
    <p>Scan</p>
    <p>Insert  Node splits  Meganode splits</p>
    <p>Delete  No rebalancing  No distributed locks</p>
  </div>
  <div class="page">
    <p>root</p>
    <p>meganodeR</p>
    <p>L</p>
    <p>meganode meganode</p>
    <p>Client</p>
    <p>Server 1</p>
    <p>Server 2</p>
    <p>RDMA read R for &quot;FOO&quot;?</p>
    <p>Node R returned. Continue at node 1</p>
    <p>RDMA read 1 for &quot;FOO&quot;?</p>
    <p>Node 1 returned. Continue at node 2</p>
    <p>RPC TRAVERSE for &quot;FOO, start at node 2?</p>
    <p>&quot;FOO&quot; found in leaf L!</p>
    <p>High Load!</p>
    <p>R</p>
    <p>Server 1</p>
    <p>Server 2</p>
  </div>
  <div class="page">
    <p>Writes: server-side only  Reads: client-side or server-side</p>
    <p>Server side: B-Link tree offers lock-free reads  Client side: lock-free reads if theyre atomic</p>
  </div>
  <div class="page">
    <p>Node Body</p>
    <p>Version V0</p>
    <p>Version V0</p>
    <p>Node Body</p>
    <p>Version V1</p>
    <p>Version V1</p>
    <p>Node Body</p>
    <p>Version V2</p>
    <p>Version V2 Correctly read as unlocked</p>
    <p>Correctly read as locked</p>
    <p>Correctly read as unlocked</p>
    <p>Partially-modified node contents</p>
  </div>
  <div class="page">
    <p>Nave: pick lowest latency  Suboptimal! Keep NIC and CPUs occupied.  Potential pitfalls</p>
    <p>Properly weighting operations  Extremely short transient conditions, outliers -&gt;</p>
    <p>moving average  Stale measurements -&gt; exploration</p>
  </div>
  <div class="page">
    <p>Clients select client- or server-side search  Queuing theory model</p>
    <p>Select server queue currently least full</p>
    <p>Server Client</p>
    <p>Request</p>
    <p>Network Response</p>
    <p>NIC</p>
    <p>CPU</p>
    <p>Message Passing Queue</p>
    <p>RDMA Queue</p>
  </div>
  <div class="page">
    <p>Motivation: Selectively Relaxed Locality  Cell: Balancing Server-Side &amp; Client-Side Search  Evaluation  Related Work</p>
  </div>
  <div class="page">
    <p>C++, 16K LOC  Infiniband with TCP-like connection mode  Cell clients: Connection-sharing</p>
  </div>
  <div class="page">
    <p>spikes?</p>
  </div>
  <div class="page">
    <p>CPU Savings</p>
    <p>&gt;170% VS. STRICT</p>
    <p>Advantage over Server-Side</p>
  </div>
  <div class="page">
    <p>Cell: 2 core/NIC advantage</p>
  </div>
  <div class="page">
    <p>Hybrid client-side and server-side processing</p>
    <p>&lt;40s/meganode &gt;400K ops/sec/core</p>
  </div>
  <div class="page">
    <p>Log scale</p>
    <p>Cell: 170% to 222% throughput of</p>
    <p>server-side only searches</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Cell: ~200ms reaction time</p>
    <p>Server side: Unbounded queue growth</p>
  </div>
  <div class="page">
    <p>Motivation: Selectively Relaxed Locality  Cell: Balancing Server-Side &amp; Client-Side Search  Evaluation  Related Work</p>
  </div>
  <div class="page">
    <p>RDMA for faster message passing:  MPI, Memcached, Hbase, Hadoop, PVFS, NFS  Recent: HERD, FaRM</p>
    <p>In-memory K-V and sorted stores  FaRM: Similar to DSM, includes K-V store app  H-Store, VoltDB, Masstree, Silo</p>
    <p>Distributed B-trees  Sagivs B-link tree: Johnson &amp; Colbrook, Boxwood</p>
  </div>
  <div class="page">
    <p>Tomorrows datacenters will include RDMAcapable, ultra-low latency networks</p>
    <p>New system architectures: 1. Selectively-relaxed locality for load balancing and</p>
    <p>CPU efficiency 2. Self-verifying data structures make this practical 3. Locality-relaxation techniques work at scale</p>
    <p>Thank you! Any questions?</p>
  </div>
  <div class="page">
    <p>MPI: Liu 2003, Liu 2004, Shipman 2006,  Memcached: Stuedi 2012, Nishtala 2013, Jose 2011, Jose 2012  Hbase: Huang 2012  Hadoop: Lu 2013  PVFS: Wu 2003  NFS: Gibson 2008  HERD: Kalia 2014  FaRM: Dragojevic 2014, Dragojevic 2015  H-Store: Kallman 2008  VoltDB: Unknown, 2010  Masstree: Mao 2012  Silo: Tu 2013  Sagivs B-link tree: Lehman 1981, Sagiv 1986  Johnson &amp; Colbrook: Johnson 1992  Boxwood: MacCormick 2004</p>
  </div>
  <div class="page">
    <p>Potentially-useful extra slides</p>
  </div>
  <div class="page">
    <p>CPU</p>
    <p>Read</p>
    <p>Write</p>
    <p>Memory</p>
    <p>In fi</p>
    <p>n ib</p>
    <p>a n d</p>
    <p>N IC</p>
    <p>In fi</p>
    <p>n ib</p>
    <p>a n d</p>
    <p>N IC</p>
    <p>RDMA Read: SEARCH, GET</p>
    <p>RPC: SEARCH, GET</p>
    <p>PUT, DELETE</p>
    <p>RPC to other</p>
    <p>servers:</p>
    <p>SPLIT, SEARCH</p>
    <p>Client Server</p>
    <p>CPU</p>
  </div>
  <div class="page">
    <p>Internal Node</p>
    <p>Version 1</p>
    <p>Valid Min Key Max Key</p>
    <p>Version 2</p>
    <p>Key Region ID Offset</p>
    <p>Key Region ID Offset</p>
    <p>Key Region ID Offset</p>
    <p>Key Region ID Offset</p>
    <p>Leaf Node</p>
    <p>Version 1</p>
    <p>Valid Min Key Max Key</p>
    <p>Key Virt Addr CRCSize</p>
    <p>Key Virt Addr CRCSize</p>
    <p>Key Virt Addr CRCSize</p>
    <p>Key Virt Addr CRCSize</p>
    <p>Version 2</p>
  </div>
  <div class="page">
    <p>&lt;</p>
    <p>qs = Server-side search queue length  Ts = Server-side service capacity  qr = RDMA search queue length  Tr = RDMA service capacity  m = RDMA traversals per meganode</p>
  </div>
</Presentation>
