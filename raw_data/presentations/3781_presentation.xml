<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Speeding Up Algorithms for Hidden Markov Models by Exploiting</p>
    <p>Repetitions</p>
    <p>Shay Mozes</p>
    <p>Oren Weimann (MIT)</p>
    <p>Michal Ziv-Ukelson (Tel-Aviv U.)</p>
  </div>
  <div class="page">
    <p>Shortly:</p>
    <p>Hidden Markov Models are extensively used to model processes in many fields</p>
    <p>The runtime of HMM algorithms is usually linear in the length of the input</p>
    <p>We show how to exploit repetitions to obtain speedup</p>
    <p>First provable speedup of Viterbis algorithm</p>
    <p>Can use different compression schemes</p>
    <p>Applies to several decoding and training algorithms</p>
  </div>
  <div class="page">
    <p>Markov Models</p>
    <p>q1 q2 states q1 ,  , qk</p>
    <p>transition probabilities Pij</p>
    <p>emission probabilities ei()</p>
    <p>time independent, discrete, finite</p>
    <p>e1(A) = 0.3</p>
    <p>e1(C) = 0.2</p>
    <p>e1(G) = 0.2</p>
    <p>e1(T) = 0.3</p>
    <p>e2(A) = 0.2</p>
    <p>e2(C) = 0.3</p>
    <p>e2(G) = 0.3</p>
    <p>e2(T) = 0.2</p>
    <p>P11 = 0.9 P21 = 0.1 P22 = 0.8</p>
    <p>P12 = 0.2</p>
  </div>
  <div class="page">
    <p>Hidden Markov Models</p>
    <p>k</p>
    <p>k</p>
    <p>k</p>
    <p>time</p>
    <p>sta te</p>
    <p>s</p>
    <p>observed string</p>
    <p>k</p>
    <p>x1 x2 xnx3</p>
    <p>Markov Models</p>
    <p>We are only given the description of the model and the observed string</p>
    <p>Decoding: find the hidden sequence of states that is most likely to have generated the observed string</p>
  </div>
  <div class="page">
    <p>probability of best sequence of states that emits first 5 chars and ends in state 2</p>
    <p>v6[4]= e4(c)P42v5[2]</p>
    <p>probability of best sequence of states that emits first 5 chars and ends in state j</p>
    <p>v6[4]= P42v5[2]v6[4]= v5[2]v6[4]=maxj{e4(c)P4jv5[j]}v5[2]</p>
    <p>Decoding  Viterbis Algorithm 1 2 3 4 5 6 7 8 9 n</p>
    <p>a a c g a c g g t</p>
    <p>sta te</p>
    <p>s</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Overview</p>
    <p>Exploiting repetitions</p>
    <p>Using LZ78</p>
    <p>Using Run-Length Encoding</p>
    <p>Summary of results</p>
  </div>
  <div class="page">
    <p>vn=M(xn)  M(xn-1)    M(x1)  v0</p>
    <p>v2 = M(x2)  M(x1)  v0</p>
    <p>VA in Matrix Notation</p>
    <p>Viterbis algorithm:</p>
    <p>v1[i]=maxj{ei(x1)Pij  v0[j]}</p>
    <p>v1[i]=maxj{ Mij(x1)  v0[j]}</p>
    <p>Mij() = ei ()Pij</p>
    <p>v1 = M(x1)  v0</p>
    <p>(AB)ij= maxk{Aik Bkj }</p>
    <p>O(k2n)</p>
    <p>O(k3n)</p>
  </div>
  <div class="page">
    <p>use it twice!</p>
    <p>vn=M(W)M(t)M(W)M(t)M(a)M(c) v0</p>
    <p>Exploiting Repetitions c a t g a a c t g a a c</p>
    <p>vn=M(c)M(a)M(a)M(g)M(t)M(c)M(a)M(a)M(g)M(t)M(a)M(c)v0</p>
    <p>compute M(W) = M(c)M(a)M(a)M(g) once</p>
  </div>
  <div class="page">
    <p>- length of repetition W</p>
    <p>number of times W repeats in string</p>
    <p>computing M(W) costs ( -1)k3</p>
    <p>each time W appears we save ( -1)k2</p>
    <p>W is good if ( -1)k2 &gt; ( -1)k3</p>
    <p>number of repeats  &gt; k number of states</p>
    <p>Exploiting repetitions</p>
    <p>&gt;</p>
    <p>matrix-matrix multiplication</p>
    <p>matrix-vector multiplication</p>
  </div>
  <div class="page">
    <p>I. dictionary selection: choose the set D={Wi } of good substrings</p>
    <p>II. encoding: compute M(Wi ) for every Wi in D</p>
    <p>III. parsing: partition the input X into good substrings X = Wi</p>
    <p>n</p>
    <p>X = i1,i2,  ,in</p>
    <p>IV. propagation: run Viterbis Algorithm on X using M(Wi)</p>
    <p>General Scheme</p>
    <p>Offline</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Overview</p>
    <p>Exploiting repetitions</p>
    <p>Using LZ78</p>
    <p>Using Run-Length Encoding</p>
    <p>Summary of results</p>
  </div>
  <div class="page">
    <p>LZ78</p>
    <p>The next LZ-word is the longest LZ-word previously seen plus one character</p>
    <p>Use a trie a</p>
    <p>c</p>
    <p>g</p>
    <p>g</p>
    <p>aacgacg</p>
    <p>Number of LZ-words is asymptotically &lt; n  log n</p>
  </div>
  <div class="page">
    <p>I. O(n)</p>
    <p>II. O(k3n  log n)</p>
    <p>III. O(n)</p>
    <p>IV. O(k2n  log n)</p>
    <p>Using LZ78 Cost</p>
    <p>I. dictionary selection: D = words in LZ parse of X</p>
    <p>II. encoding: use incremental nature of LZ M(W)= M(W)  M()</p>
    <p>III. parsing: X = LZ parse of X</p>
    <p>IV. propagation: run VA on X using M(Wi )</p>
    <p>Speedup: k2n log n</p>
    <p>k3n  log n k</p>
  </div>
  <div class="page">
    <p>Remember speedup condition:  &gt; k</p>
    <p>Use just LZ-words that appear more than k times</p>
    <p>These words are represented by trie nodes with more than k descendants</p>
    <p>Now must parse X (step III) differently</p>
    <p>Ensures graceful degradation with increasing k:</p>
    <p>Speedup: min(1,log n  k)</p>
    <p>Improvement a</p>
    <p>c</p>
    <p>g</p>
    <p>g</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>Short - 1.5Mbp chromosome 4 of S. Cerevisiae (yeast)  Long - 22Mbp human Y-chromosome</p>
    <p>~x5 faster:</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Overview</p>
    <p>Exploiting repetitions</p>
    <p>Using LZ78</p>
    <p>Using Run-Length Encoding</p>
    <p>Summary of results</p>
  </div>
  <div class="page">
    <p>Run Length Encoding aaaccggggg  a3c2g5</p>
    <p>aaaccggggg  a2a1c2g4g1</p>
  </div>
  <div class="page">
    <p>Summary of results</p>
    <p>General framework  LZ78 log(n)  k  RLE r  log(r)  Byte-Pair Encoding r  Path reconstruction O(n)  F/B algorithms (standard matrix multiplication)  Viterbi training same speedups apply  Baum-Welch training speedup, many details  Parallelization</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Any questions?</p>
  </div>
  <div class="page">
    <p>Path traceback</p>
    <p>In VA, easy to do in O(n) time by keeping track of maximizing states during computation</p>
    <p>The problem: we run VA on X, so we get the sequence of states for X, not for X. we only get the states on the boundaries of good substrings of X</p>
    <p>Solution: keep track of maximizing states when computing the matrices M(w). Takes O(n) time and O(nk2) space</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>Estimate unknown parameters Pij , ei()</p>
    <p>Use Expectation Maximization: 1. Decoding</p>
    <p>Viterbi Training: each iteration costs O( VA + n + k2)</p>
    <p>Decoding (bottleneck) speedup!</p>
    <p>path traceback +</p>
    <p>update Pij , ei()</p>
  </div>
  <div class="page">
    <p>Baum Welch Training</p>
    <p>each iteration costs: O( FB + nk2)</p>
    <p>If substring w has length l and repeats  times satisfies:</p>
    <p>then can speed up the entire process by precalculation</p>
    <p>kl</p>
    <p>lk</p>
    <p>path traceback +</p>
    <p>update Pij , ei()</p>
    <p>Decoding O(nk2)</p>
  </div>
</Presentation>
