<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>EyeQ: Prac%cal Network Performance Isola%on</p>
    <p>at the Edge Vimalkumar Jeyakumar</p>
    <p>Mohammad Alizadeh Balaji Prabhakar David Mazires</p>
    <p>Changhoon Kim Albert Greenberg</p>
  </div>
  <div class="page">
    <p>Once upon a 5me</p>
  </div>
  <div class="page">
    <p>Once upon a 5me</p>
  </div>
  <div class="page">
    <p>Once upon a 5me</p>
  </div>
  <div class="page">
    <p>Once upon a 5me</p>
  </div>
  <div class="page">
    <p>Performance Unpredictability</p>
    <p>http://amistrongeryet.com/op_detail.jsp? op=gae_db_readCachedHandles_1&amp;hoursAgo=24</p>
  </div>
  <div class="page">
    <p>Conges5on Kills Predictability</p>
  </div>
  <div class="page">
    <p>Conges5on Kills Predictability</p>
    <p>Key Issue Todays transport (TCP/UDP) lacks predictability in sharing bandwidth</p>
  </div>
  <div class="page">
    <p>Status Quo is Insufficient</p>
  </div>
  <div class="page">
    <p>Status Quo is Insufficient  TCP  Cannot force all to use TCP or agree on one TCP version!</p>
    <p>Sharing is per-flow: not built for predictability  Performance Isola%on with Per-tenant Queues  State management complexity: &gt;10k tenants, configuring queues on all links is an opera%onal nightmare</p>
    <p>WFQ/DRR does not ensure admissibility</p>
  </div>
  <div class="page">
    <p>Status Quo is Insufficient  TCP  Cannot force all to use TCP or agree on one TCP version!</p>
    <p>Sharing is per-flow: not built for predictability  Performance Isola%on with Per-tenant Queues  State management complexity: &gt;10k tenants, configuring queues on all links is an opera%onal nightmare</p>
    <p>WFQ/DRR does not ensure admissibility</p>
  </div>
  <div class="page">
    <p>Where does Conges5on Happen?</p>
    <p>Ideal network fabric (one switch)</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>Server Shared</p>
    <p>Tenant 1 Tenant 2</p>
  </div>
  <div class="page">
    <p>Where does Conges5on Happen?</p>
    <p>Ideal network fabric (one switch)</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>Server Shared</p>
    <p>Tenant 1 Tenant 2</p>
  </div>
  <div class="page">
    <p>Conges5on Study on Windows Azure</p>
    <p>Spine Layer</p>
    <p>Leaf layer &lt; 3:1 oversub</p>
  </div>
  <div class="page">
    <p>Core Edge</p>
    <p>Timescales: over 2 weeks, 99.9th pcile = several minutes</p>
    <p>Core Edge</p>
  </div>
  <div class="page">
    <p>EyeQ: Predictable Bandwidth Par55oning at the Edge</p>
    <p>Alices Switch</p>
    <p>VM1 VM2 VMn VM3</p>
    <p>Bobs Switch</p>
    <p>VM1 VM2 VMi VM3</p>
    <p>Customer specifies capacity of the virtual NIC. No traffic matrix. (Hose Model)</p>
    <p>Provider: assures near dedicated performance. EyeQ is deployable today at the Edge.</p>
  </div>
  <div class="page">
    <p>EyeQs Key Contribu5on: Simplicity</p>
    <p>Observa%on  Network Conges%on predominantly occurs at the Edge (Hypervisor / Top of Rack)</p>
    <p>Consequences: Simplicity  Distributed, end-to-end bandwidth alloca%on</p>
    <p>Amenable to NIC-based implementa%on  Network need not be tenant aware</p>
    <p>Implementa%on  High speed in sonware at 10Gb/s</p>
  </div>
  <div class="page">
    <p>Shim</p>
    <p>Shim</p>
    <p>Shim</p>
    <p>Shim</p>
    <p>Decentralized Scheduling</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>EyeQ Shim Layer In the trusted</p>
    <p>Domain (Hypervisor/NIC)</p>
  </div>
  <div class="page">
    <p>Decentralized Scheduling</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
  </div>
  <div class="page">
    <p>RX Module</p>
    <p>Decentralized Scheduling</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
  </div>
  <div class="page">
    <p>Decentralized Scheduling</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
  </div>
  <div class="page">
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>Decentralized Scheduling</p>
  </div>
  <div class="page">
    <p>RX Module</p>
    <p>Work Conserving Alloca5ons</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>Spare capacity</p>
  </div>
  <div class="page">
    <p>Work Conserving Alloca5ons</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
  </div>
  <div class="page">
    <p>Transmit/Receive Modules</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>Conges%on detectors Rate limit.</p>
    <p>Rate limit.</p>
    <p>Rate limit.</p>
    <p>Per-des%na%on rate limiters: only if dest. is congested bypass otherwise</p>
  </div>
  <div class="page">
    <p>Transmit/Receive Modules</p>
    <p>VM</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>VM</p>
    <p>Conges%on detectors Rate limit.</p>
    <p>Rate limit.</p>
    <p>Rate limit.</p>
    <p>RCP: Rate feedback (R) every 10kB (no per-source state needed)</p>
    <p>Per-des%na%on rate limiters: only if dest. is congested bypass otherwise</p>
    <p>Feedback pkt Rate: 1Gb/s 2Gb/s</p>
  </div>
  <div class="page">
    <p>Timescales MaUer</p>
    <p>Fast convergence important  Switches only have few MB (milliseconds) worth of buffering before they drop packets</p>
    <p>RCPs worst-case convergence %me  N long lived flows compe%ng for a single boqleneck: few milliseconds.</p>
    <p>Usually few 100 microseconds.</p>
  </div>
  <div class="page">
    <p>But what if the Core gets congested? How?  Transient failures or ECMP collisions Case 1: Mild network conges%on  Use ECN for graceful fallback  Per receiver-VM max-min sharing  Conges%on detector: mul%plica%ve decrease on adver%sed rate on receiving ECN</p>
    <p>Case 2: Severe network conges%on (unlikely!)  Mul%plica%ve decrease (rate limiter %meout)</p>
  </div>
  <div class="page">
    <p>Sobware Prototype Linux Kernel Module (qdisc) Windows Filter Driver (in VMSwitch)  Non-intrusive: no changes to applica%ons or exis%ng network stack. Works even with UDP.</p>
    <p>~1700 lines of code Linux Kernel Module is Open-Source  Full system and documenta%on at http://jvimal.github.com/eyeq</p>
    <p>Fully func%onal version in Mininet to play with J 4 Apr 2013 29 NSDI 2013</p>
  </div>
  <div class="page">
    <p>High speed sobware rate limiters</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>Single shared queue increases lock conten%on  High CPU overhead  High packet latency  Controlled burst</p>
    <p>Packets on the wire %me</p>
    <p>NSDI 2013</p>
  </div>
  <div class="page">
    <p>Parallel transmit path</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>Packets on the wire %me</p>
    <p>Split queue to per-cpu queues  Lower CPU overhead  Lower packet latency  Fairness across CPU queues  Higher, but bounded burst</p>
    <p>NSDI 2013</p>
    <p>Lazily Grab tokens</p>
    <p>Token filling clocked by packets</p>
  </div>
  <div class="page">
    <p>Rate Limiter Efficiency</p>
    <p>(a) CPU overhead at high load. (b) CDF of latency at low load.</p>
    <p>Figure 8: EyeQs rate limiter is multi-core aware, and is more efficient both in terms of CPU usage and packet latency, than todays Linuxs Hierarchical Token Bucket htb.</p>
    <p>one would expect, for a given rate, larger packet sizes implies smaller number of packets/second in software, and therefore the overhead is comparable for 32kB packets.</p>
    <p>To measure latency overhead of htb due to locking, we ran 512 netperf processes, each generating back-toback 1-byte requests and waiting for 1-byte responses. On a bare-metal system, this test generated about 10Mb/s of traffic, and thus the rate limiter configured at 5Gb/s should not have any effect. EyeQs rate limiters work well, but with htb, we observe a 2.2x increase in the median pertransaction latency, as shown in Figure 8(b).</p>
    <p>Recall that EyeQ requires a number of rate limiters that varies depending on the number of flows. In practice, the number of active flows (flows that have outstanding data) is typically less than a few 100 on a machine [12]. Nevertheless, we evaluated EyeQs rate limiters by creating 20000 long lived flows that are assigned to a number of rate limiters in a round robin fashion. As we increased the number of rate limiters connected to the root (which is limited to 5Gb/s) from 1 to 1000 to 10000, we found that the CPU usage stays the same. This is due to the fact that the net work output (packets per second) is the same in all cases, except for the (small) overhead involved in book keeping rate limiters.</p>
    <p>To mimic a Hadoop jobs network traffic component, we generate an all to all traffic pattern of a sort job using a traffic generator. Hadoops reduce phase is bandwidth intensive and job completion times depend on the availabil</p>
    <p>ity of bandwidth [37]. We pick the sort workload; a sort of S TB of data using a cluster of N nodes involves roughly an equal amount of data shuffle between all pairs; in effect, S</p>
    <p>N(N1) TB of data is shuffled between every pair of nodes. We use a TCP traffic generator to create long lived flows according to the above traffic pattern, and record the flow completion times of all the N(N 1) flows. We then plot the CDF of flow completion times for every job to visualize its progress over time; the job is complete when the last flow completes.</p>
    <p>Multiple all-to-all shuffles. In this test, we run three collocated all-to-all shuffle jobs. Each job has 16 workers, one on each server in the cluster; and each server has three hadoop workers, one of each job. Each job has a varying degree of aggressiveness when consuming network bandwidth; job Pi (i 2 0, 1, 2) creates 2i parallel TCP connections between each pair of its nodes, and each TCP connection an transfers equal amount of data. The jobs are all temporally and spatially collocated with each other and run a common 1 TB sort workload.</p>
    <p>Figure 9(a) shows that jobs that open more TCP connections complete faster. However, EyeQ provides flexibility to explicitly configure job priorities, irrespective of the traffic, or protocol behavior. Figure 9(b) shows the job completion times if the lesser aggressive jobs are given higher priority; the priority can be inverted, and the job completion times reflect the change of priorities. The job priority is inverted by assigning minimum bandwidth guarantees Bi to job Pi that is inversely proportional to their aggressiveness; i.e., B0 : B1 : B2 = 4 : 2 : 1. The final completion time in EyeQ increases from 180s to 210s, because the REM (3.3) does not share bandwidth in a fine-grained, per-packet fashion, but over a 200s time window. This leads to a small loss of utilization, when (say) P0 is allocated some bandwidth but does not use it completely.</p>
    <p>Our final macro-evaluation is a scenario where a memcached tenant is collocated alongside an adversarial UDP tenant. The memcached tenant is a cluster consists of 16 processes: 4 memcached instances and 12 clients. Each process is located on a different host. At the start of the experiment, each cache instance allocates 8GB of memory, each client starts one thread per cache instance, and each thread opens 10 permanent TCP connections to its designated cache instance.</p>
    <p>Throughput test. We generate an external load of about 288k requests/sec load balanced equally across all clients. Hence, at each client, the mean load is 6000 requests/sec</p>
    <p>Single rate limiter at 5Gb/s. HTB succumbs at 9Gb/s.</p>
    <p>Throughput Latency</p>
    <p>NSDI 2013</p>
    <p>Low lock conten%on due to fewer packets/sec and %mer interrupts/sec.</p>
    <p>Traffic Generator</p>
  </div>
  <div class="page">
    <p>Rate Limiter Efficiency</p>
    <p>(a) CPU overhead at high load. (b) CDF of latency at low load.</p>
    <p>Figure 8: EyeQs rate limiter is multi-core aware, and is more efficient both in terms of CPU usage and packet latency, than todays Linuxs Hierarchical Token Bucket htb.</p>
    <p>one would expect, for a given rate, larger packet sizes implies smaller number of packets/second in software, and therefore the overhead is comparable for 32kB packets.</p>
    <p>To measure latency overhead of htb due to locking, we ran 512 netperf processes, each generating back-toback 1-byte requests and waiting for 1-byte responses. On a bare-metal system, this test generated about 10Mb/s of traffic, and thus the rate limiter configured at 5Gb/s should not have any effect. EyeQs rate limiters work well, but with htb, we observe a 2.2x increase in the median pertransaction latency, as shown in Figure 8(b).</p>
    <p>Recall that EyeQ requires a number of rate limiters that varies depending on the number of flows. In practice, the number of active flows (flows that have outstanding data) is typically less than a few 100 on a machine [12]. Nevertheless, we evaluated EyeQs rate limiters by creating 20000 long lived flows that are assigned to a number of rate limiters in a round robin fashion. As we increased the number of rate limiters connected to the root (which is limited to 5Gb/s) from 1 to 1000 to 10000, we found that the CPU usage stays the same. This is due to the fact that the net work output (packets per second) is the same in all cases, except for the (small) overhead involved in book keeping rate limiters.</p>
    <p>To mimic a Hadoop jobs network traffic component, we generate an all to all traffic pattern of a sort job using a traffic generator. Hadoops reduce phase is bandwidth intensive and job completion times depend on the availabil</p>
    <p>ity of bandwidth [37]. We pick the sort workload; a sort of S TB of data using a cluster of N nodes involves roughly an equal amount of data shuffle between all pairs; in effect, S</p>
    <p>N(N1) TB of data is shuffled between every pair of nodes. We use a TCP traffic generator to create long lived flows according to the above traffic pattern, and record the flow completion times of all the N(N 1) flows. We then plot the CDF of flow completion times for every job to visualize its progress over time; the job is complete when the last flow completes.</p>
    <p>Multiple all-to-all shuffles. In this test, we run three collocated all-to-all shuffle jobs. Each job has 16 workers, one on each server in the cluster; and each server has three hadoop workers, one of each job. Each job has a varying degree of aggressiveness when consuming network bandwidth; job Pi (i 2 0, 1, 2) creates 2i parallel TCP connections between each pair of its nodes, and each TCP connection an transfers equal amount of data. The jobs are all temporally and spatially collocated with each other and run a common 1 TB sort workload.</p>
    <p>Figure 9(a) shows that jobs that open more TCP connections complete faster. However, EyeQ provides flexibility to explicitly configure job priorities, irrespective of the traffic, or protocol behavior. Figure 9(b) shows the job completion times if the lesser aggressive jobs are given higher priority; the priority can be inverted, and the job completion times reflect the change of priorities. The job priority is inverted by assigning minimum bandwidth guarantees Bi to job Pi that is inversely proportional to their aggressiveness; i.e., B0 : B1 : B2 = 4 : 2 : 1. The final completion time in EyeQ increases from 180s to 210s, because the REM (3.3) does not share bandwidth in a fine-grained, per-packet fashion, but over a 200s time window. This leads to a small loss of utilization, when (say) P0 is allocated some bandwidth but does not use it completely.</p>
    <p>Our final macro-evaluation is a scenario where a memcached tenant is collocated alongside an adversarial UDP tenant. The memcached tenant is a cluster consists of 16 processes: 4 memcached instances and 12 clients. Each process is located on a different host. At the start of the experiment, each cache instance allocates 8GB of memory, each client starts one thread per cache instance, and each thread opens 10 permanent TCP connections to its designated cache instance.</p>
    <p>Throughput test. We generate an external load of about 288k requests/sec load balanced equally across all clients. Hence, at each client, the mean load is 6000 requests/sec</p>
    <p>Single rate limiter at 5Gb/s. HTB doesnt work at 9Gb/s.</p>
    <p>Throughput Latency</p>
    <p>NSDI 2013</p>
    <p>Input rate to rate limiter limited by end-to- end latency.</p>
    <p>Request (on mul%ple</p>
    <p>CPUs)</p>
    <p>Response Median latency reduced by 2x</p>
  </div>
  <div class="page">
    <p>Macro Evalua5on: Memcached Latency</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>Each server has 10Gb/s link</p>
  </div>
  <div class="page">
    <p>Macro Evalua5on: Memcached Latency</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>Set 6kB objects Load: 2.3Gb/s/server MS</p>
    <p>UDP</p>
    <p>External Load: 144k SET req/sec</p>
    <p>Each server has 10Gb/s link</p>
  </div>
  <div class="page">
    <p>Macro Evalua5on: Memcached Latency</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>Set 6kB objects Load: 2.3Gb/s/server MS</p>
    <p>UDP</p>
    <p>External Load: 144k SET req/sec</p>
    <p>UDP bursty 5Gb/s 0.5s to 1 server, chosen round robin. 0.5s sleep between bursts.</p>
    <p>Each server has 10Gb/s link</p>
  </div>
  <div class="page">
    <p>Macro Evalua5on: Memcached Latency</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MS</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>MC</p>
    <p>UDP</p>
    <p>Set 6kB objects Load: 2.3Gb/s/server MS</p>
    <p>UDP</p>
    <p>External Load: 144k SET req/sec</p>
    <p>UDP bursty 5Gb/s 0.5s to 1 server, chosen round robin. 0.5s sleep between bursts.</p>
    <p>Each server has 10Gb/s link</p>
    <p>Scenario 50th 99.9th Throughput</p>
    <p>Baseline (Linux 3.4) 98us 666us 144kreq/s Without Interference + EyeQ 100us 630us 144kreq/s With Interference 4127us &gt;106us 144kreq/s With Interference + EyeQ 102us 750us 144kreq/s</p>
  </div>
  <div class="page">
    <p>Thank you! EyeQ: An edge-based flow scheduler for the data center to par%%on bandwidth in a simple and predictable way.</p>
    <p>NSDI 2013</p>
    <p>http://jvimal.github.com/eyeq jvimal@stanford.edu</p>
  </div>
  <div class="page"/>
</Presentation>
