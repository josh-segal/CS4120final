<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Libnvmmio: Reconstructing SW IO Path with Failure-Atomic Memory-Mapped Interface</p>
    <p>Jungsik Choi1, Jaewan Hong2, Youngjin Kwon2, Hwansoo Han1</p>
    <p>USENIX ATC 20</p>
    <p>fi</p>
    <p>Signature `</p>
    <p>University Identity System   '               ,  ` '     ' .    , '   fl CD-Rom      .</p>
    <p>fi</p>
    <p>fi   B</p>
    <p>fi   A</p>
    <p>fi</p>
    <p>fi</p>
    <p>BS_06</p>
  </div>
  <div class="page">
    <p>SW Overhead Greater than Storage Latency La</p>
    <p>te nc</p>
    <p>y</p>
    <p>S W</p>
    <p>O ve</p>
    <p>rh ea</p>
    <p>d</p>
    <p>ns</p>
    <p>s</p>
    <p>ms</p>
    <p>TLC 3D NAND SSD</p>
    <p>Optane SSD</p>
    <p>NVDIMM-N PM</p>
    <p>XL-Flash SSD</p>
    <p>DCPMM PM</p>
    <p>HDD</p>
    <p>SSD</p>
    <p>Time 2</p>
  </div>
  <div class="page">
    <p>Reconstruct SW IO Path with Libnvmmio</p>
    <p>Libnvmmio - Library - Run on any POSIX FS (DAX-mmap) - Transparent MMIO with logging - Make common IO path efficient</p>
    <p>Handle data ops at user-level  Route metadata ops to kernel FS</p>
    <p>- Low-latency &amp; scalable IO - Data-atomicity</p>
    <p>Atomic Write</p>
    <p>open write</p>
    <p>read fsync</p>
    <p>close</p>
    <p>Application</p>
    <p>NVM-aware FSKernel</p>
    <p>FilesNVMM</p>
    <p>Libnvmmio Logs</p>
    <p>Memory Mapped Files</p>
    <p>munmap /close</p>
    <p>open /mmap MMIOa</p>
  </div>
  <div class="page">
    <p>User-Level IO is Suitable in NVMM system</p>
    <p>Kernels IO stacks introduce SW overhead</p>
    <p>User-level IO with mmap - Access files directly with load/store - Reduce user/kernel mode switches - Avoid complex IO stacks - No indexing, no permission checks</p>
    <p>MMIO is the fastest way to access files</p>
    <p>VFS File System</p>
    <p>Device Driver</p>
    <p>NVMM</p>
    <p>read/write load/store</p>
    <p>OS Kernel</p>
    <p>Application</p>
  </div>
  <div class="page">
    <p>Logging is more Efficeint than CoW</p>
    <p>CoW (or shadow paging) - High write amplification - Hugepages make CoW more expensive - Frequent TLB-shutdown  Logging (or journaling) - Writing data twice: logs and files - Differential logging - Checkpointing can be postponed</p>
  </div>
  <div class="page">
    <p>Redo vs. Undo</p>
    <p>Most logging systems use only one policy (redo or undo)  They have different pros &amp; cons depending on access type</p>
    <p>App</p>
    <p>UNDO Log</p>
    <p>File</p>
    <p>App</p>
    <p>REDO Log</p>
    <p>File</p>
    <p>Write Async Write Read</p>
    <p>- REDO is better for writing , UNDO is better for reading</p>
  </div>
  <div class="page">
    <p>Hybrid Logging</p>
    <p>Uses adaptive policy depending on the access type of a file - Read-intensive file  Undo logging - Write-intensive file  Redo logging  Maintains per-file read/write counters  Determines logging policy on each fsync  Achieves the best case performance of two logging policies - Reduce SW overhead and improve logging efficiency</p>
  </div>
  <div class="page">
    <p>Centralized Logging with Fine-Grained Locks</p>
    <p>Decentralized logging was designed for transactions - e.g., per-thread logging, per-transaction logging  Centralized logging is appropriate for file IO, but not scalable - Requires fine-grained locks for scalable file IO</p>
    <p>File</p>
    <p>Log Log Log</p>
    <p>File</p>
    <p>Log</p>
    <p>Decentralized LoggingCentralized Logging 8</p>
  </div>
  <div class="page">
    <p>Per-Block Logging</p>
    <p>File</p>
    <p>Per-Block Log</p>
    <p>Radix Tree Multi-Level</p>
    <p>Tree</p>
  </div>
  <div class="page">
    <p>Lock-Free Radix Tree</p>
    <p>Global Upper Middle Table Offset</p>
    <p>entry rwlock offset len dest policy epoch</p>
    <p>File Offset</p>
    <p>lgd skip</p>
    <p>radix_root</p>
    <p>LGD LUD</p>
    <p>LMD Table</p>
    <p>Index Entry</p>
    <p>Delta</p>
    <p>Log Entry (4KB)</p>
    <p>Per-Block Log</p>
    <p>size</p>
  </div>
  <div class="page">
    <p>Commit &amp; Checkpoint based on Epoch</p>
    <p>Per-block logs are atomically committed on fsync  Libnvmmio commits by increasing the global epoch value - Committed logs have an epoch smaller than the global epoch  Background ckeckpointing</p>
    <p>ad ix</p>
    <p>T re</p>
    <p>e</p>
    <p>M em</p>
    <p>or y</p>
    <p>M ap</p>
    <p>pe d</p>
    <p>Fi le</p>
    <p>Per-File Metadata</p>
    <p>Background Threads</p>
    <p>Per-Block Logs</p>
  </div>
  <div class="page">
    <p>Design Summary</p>
    <p>Low-latency IO  User-level IO with mmap  Differential logging  Hybrid logging  Various log sizes  Epoch-based committing  Background checkpointing</p>
    <p>Scalable IO  Per-block logging  Lock-free index data structure</p>
    <p>Libnvmmio provides low-latency and scalable IO while guaranteeing data-atomicity</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>Experimental Machines - 32GB NVDIMM-N, 20 cores and 32GB DRAM - 256GB Optane DC, 16 cores and 128GB DRAM (in our paper)  Comparison systems</p>
    <p>Filesystem File IO Data-Atomicity Kernel Ext4-DAX Kernel X 5.1 PMFS Kernel X 4.13 NOVA Kernel O 5.1 SplitFS User O 4.13 Libnvmmio* User O 5.1</p>
  </div>
  <div class="page">
    <p>Hybrid Logging</p>
    <p>:9 0 20</p>
    <p>:8 0 30</p>
    <p>:7 0 40</p>
    <p>:6 0 50</p>
    <p>:5 0 60</p>
    <p>:4 0 70</p>
    <p>:3 0 80</p>
    <p>:2 0 90</p>
    <p>:1 0 10</p>
    <p>(l ap</p>
    <p>sH d</p>
    <p>H (s</p>
    <p>Hc )</p>
  </div>
  <div class="page">
    <p>FIO: Different Access Patterns</p>
    <p>A single thread, file size=4GB, block size=4KB, time=60s</p>
    <p>Dn Gw</p>
    <p>LG Wh</p>
    <p>( G</p>
    <p>LB /V</p>
    <p>) (xW4-DAX P0)6 N2VA /LEnvPPLR</p>
  </div>
  <div class="page">
    <p>FIO: Different Write Sizes</p>
    <p>B Dn</p>
    <p>Gw LG</p>
    <p>WK (</p>
    <p>G LB</p>
    <p>/V )</p>
    <p>(xW4-DAX 30)6 12VA /LEnvPPLo</p>
    <p>WrLWe 6Lze</p>
  </div>
  <div class="page">
    <p>FIO: Random Write with Multithreads</p>
    <p>B Dn</p>
    <p>Gw LG</p>
    <p>th (</p>
    <p>G LB</p>
    <p>/V )</p>
    <p>PrLvDte fLOe (xt4-DAX P0)6 12VA /LEnvPPLo</p>
  </div>
  <div class="page">
    <p>TPC-C on SQLite</p>
    <p>Underlying FS with WAL, and Libnvmmio without WAL</p>
    <p>Ext4-DAX P0FS 1OVA SSOLtFS 0.0</p>
    <p>or P</p>
    <p>DO Lz</p>
    <p>ed t</p>
    <p>SP C</p>
    <p>OnOy XnderOyLng FS LLEnvPPLo on FS</p>
  </div>
  <div class="page">
    <p>SQLite WAL vs. Libnvmmio</p>
    <p>SQLite WAL  Design for block devices  Similar to REDO logging  Read both WAL and DB file  Only one writer at a time  Synchronous checkpointing</p>
    <p>Libnvmmio  Design for NVMM  Hybrid Logging  Read DB file (UNDO)  Concurrent writes  Background checkpointing</p>
    <p>Easily improve performance with Libnvmmio  Support any FS, Even FS that does not provide data-atomicity</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>It is important to minimize SW overhead in NVMM systems  Libnvmmio is a simple and practical solution - Reconstruct SW IO path - Run on any filesystem that provide DAX-mmap  Low-latency, scalable IO while guaranteeing data-atomicity - 2.2x better throughput - 13x better scalability  https://github.com/chjs/libnvmmio</p>
  </div>
  <div class="page">
    <p>QnA</p>
    <p>chjs@skku.edu</p>
  </div>
</Presentation>
