<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit</p>
    <p>Feedback</p>
    <p>Carolin Lawrence, Stefan Riezler</p>
    <p>Heidelberg University Institute for Computational Linguistics</p>
    <p>July 17, 2018</p>
  </div>
  <div class="page">
    <p>Situation Overview</p>
    <p>I Situation: deployed system (e.g. QA, MT ...)</p>
    <p>I Goal: improve system using human feedback</p>
    <p>I Plan: create a log Dlog of user-system interactions &amp; improve system offline (safety)</p>
    <p>Here: Improve a Neural Semantic Parser</p>
  </div>
  <div class="page">
    <p>Contrast to Previous Approaches</p>
    <p>question x</p>
    <p>Parser</p>
    <p>Database</p>
    <p>Comparison</p>
    <p>gold answer</p>
    <p>train</p>
    <p>predict</p>
    <p>for 1...n</p>
    <p>required data</p>
    <p>parses y1, ..., ys</p>
    <p>Rewards r1, ..., rs</p>
    <p>Answers a1, ..., as</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>question x</p>
    <p>Parser log</p>
    <p>predict</p>
    <p>User Feedback</p>
    <p>r</p>
    <p>parse y</p>
    <p>Parser(x, y, r)</p>
    <p>Database</p>
    <p>for 1...n</p>
    <p>train</p>
    <p>required data</p>
    <p>answer a</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>question x</p>
    <p>Parser</p>
    <p>Database</p>
    <p>Comparison</p>
    <p>gold answer</p>
    <p>train</p>
    <p>predict</p>
    <p>question x</p>
    <p>Parser log</p>
    <p>predict</p>
    <p>User Feedback</p>
    <p>r</p>
    <p>parse y</p>
    <p>Parser(x, y, r)</p>
    <p>for 1...n</p>
    <p>Database</p>
    <p>for 1...n</p>
    <p>train</p>
    <p>required data required data</p>
    <p>answer a parses y1, ..., ys</p>
    <p>Rewards r1, ..., rs</p>
    <p>Answers a1, ..., as</p>
    <p>I No supervision: given an input, the gold output is unknown</p>
    <p>I Bandit: feedback is given for only one system output</p>
    <p>I Bias: log D is biased to the decisions of the deployed system</p>
    <p>Solution: Counterfactual / Off-policy Reinforcement Learning</p>
  </div>
  <div class="page">
    <p>Task</p>
  </div>
  <div class="page">
    <p>A natural language interface to OpenStreetMap</p>
    <p>I OpenStreetMap (OSM): geographical database I NLmaps v2: extension of the previous corpus, now totalling</p>
  </div>
  <div class="page">
    <p>A natural language interface to OpenStreetMap</p>
    <p>I example question: How many hotels are there in Paris?  Answer: 951</p>
    <p>I correctness of answers are difficult to judge  judge parses by making them human-understandable</p>
    <p>I feedback collection setup: 1. automatically convert a parse to a set of statements 2. humans judge the statements</p>
  </div>
  <div class="page">
    <p>Example: Feedback Formula</p>
    <p>query(around(center(area(keyval('name','Paris')), nwr(keyval('name','Place de la Rpublique'))), search(nwr(keyval('amenity','parking'))), maxdist(WALKING_DIST)),qtype(findkey('name')))</p>
  </div>
  <div class="page">
    <p>Objectives</p>
  </div>
  <div class="page">
    <p>Counterfactual Learning</p>
    <p>Resources</p>
    <p>collected log Dlog = {(xt,yt,t )}nt=1 with I xt : input</p>
    <p>I yt : most likely output of deployed system 0 I t  [1, 0]: loss (i.e. negative reward) received from user</p>
    <p>Deterministic Propensity Matching (DPM)</p>
    <p>I minimize the expected risk for a target policy w</p>
    <p>RDPM(w ) = 1</p>
    <p>n</p>
    <p>n t=1</p>
    <p>tw (yt|xt )</p>
    <p>I improve w using (stochastic) gradient descent</p>
    <p>I high variance  use multiplicative control variate</p>
  </div>
  <div class="page">
    <p>Multiplicative Control Variate</p>
    <p>I for random variables X and Y , with Y the expectation of Y :</p>
    <p>E[X ] = E[ X</p>
    <p>Y ]  Y</p>
    <p>RHS has lower variance if Y positively correlates with X</p>
    <p>DPM with Reweighting (DPM+R)</p>
    <p>RDPM+R(w ) = 1 n</p>
    <p>n t=1 tw (yt|xt )</p>
    <p>n t=1 w (yt|xt )</p>
    <p>1 Reweight Sum R</p>
    <p>I reduces variance but introduces a bias of order O( 1 n</p>
    <p>) that decreases as n increases  n should be as large as possible</p>
    <p>I Problem: in stochastic minibatch learning, n is too small</p>
  </div>
  <div class="page">
    <p>One-Step Late (OSL) Reweighting</p>
    <p>Perform gradient descent updates &amp; reweighting asynchronously</p>
    <p>I evaluate reweight sum R on the entire log of size n using parameters w</p>
    <p>I update using minibatches of size m, m  n I periodically update R</p>
    <p>retains all desirable properties</p>
    <p>DPM+OSL</p>
    <p>RDPM+OSL(w ) = 1 m</p>
    <p>m t=1 tw (yt|xt )</p>
    <p>n t=1 w (yt|xt )</p>
  </div>
  <div class="page">
    <p>Token-Level Feedback</p>
    <p>DPM+T</p>
    <p>RDPM+T(w ) = 1</p>
    <p>n</p>
    <p>n t=1</p>
    <p>|y|</p>
    <p>j=1</p>
    <p>jw (yj|xt )</p>
    <p>DPM+T+OSL</p>
    <p>RDPM+T+OSL(w ) =</p>
    <p>m t=1</p>
    <p>(|y| j=1 jw (yj|xt )</p>
    <p>) 1 n</p>
    <p>n t=1 w (yt|xt )</p>
  </div>
  <div class="page">
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>I sequence-to-sequence neural network Nematus</p>
    <p>I deployed system: pre-trained on 2k question-parse pairs</p>
    <p>I feedback collection: 1. humans judged 1k system outputs</p>
    <p>I average time to judge a parse: 16.4s I most parses (&gt;70%) judged in &lt;10s</p>
    <p>I token-wise comparison to gold parse</p>
    <p>I bandit-to-supervised conversion (B2S): all instances in log with reward 1 are used as supervised training</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>Human Feedback (1k) Large-Scale Simulated Feedback (23k)</p>
    <p>F1 Sc or e</p>
    <p>B2S DPM+T+OSL</p>
  </div>
  <div class="page">
    <p>Take Away</p>
    <p>Counterfactual Learning I safely improve a system by collecting interaction logs</p>
    <p>I applicable to any task if the underlying model is differentiable</p>
    <p>I DPM+OSL: new objective for stochastic minibatch learning</p>
    <p>Improving a Semantic Parser I collect feedback by making parses human-understandable</p>
    <p>I judging a parse is often easier &amp; faster than formulating a parse or answer</p>
    <p>NLmaps v2 I large question-parse corpus for QA in the geographical domain</p>
    <p>Future Work I integrate feedback form in the online NL interface to OSM</p>
  </div>
</Presentation>
