<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hybrid Batch Attacks</p>
    <p>Finding Black-box Adversarial Examples with Limited</p>
    <p>Queries</p>
    <p>Fnu Suya Jianfeng Chi David Evans Yuan Tian</p>
    <p>University of Virginia</p>
    <p>USENIX Security 2020</p>
    <p>evadeML.org</p>
  </div>
  <div class="page">
    <p>Two Types of Black-box Attacks</p>
    <p>Query Cost Success Rate Optimization Attacks High High</p>
    <p>Transfer Attacks Low Low</p>
    <p>Can we combine the attack strategies to get high success and low cost?</p>
  </div>
  <div class="page">
    <p>Result Summary: Hybrid Attack</p>
    <p>Model</p>
    <p>Attack Success Rate (%) Query Cost Baseline</p>
    <p>Optimization Attack</p>
    <p>Hybrid Attack</p>
    <p>Baseline Optimization</p>
    <p>Attack</p>
    <p>Hybrid Attack</p>
    <p>Standard CIFAR10</p>
    <p>Robust CIFAR10</p>
    <p>Standard ImageNet</p>
  </div>
  <div class="page">
    <p>Attacks Evaluated by Average Cost</p>
    <p>Top 10 mean: 3,402 Overall mean: 13,722 Potential Savings: 75%</p>
  </div>
  <div class="page">
    <p>Batch Attack</p>
    <p>Without Prioritization</p>
    <p>Result Highlight: Batch Attack</p>
    <p>Total queries to find 10 adversarial examples</p>
    <p>(given 1000 candidate seeds)</p>
    <p>Batch Attack</p>
    <p>Without Prioritization</p>
  </div>
  <div class="page">
    <p>Rest of Talk</p>
    <p>Batch Attacks</p>
    <p>How to efficiently find low-cost seeds?</p>
    <p>Hybrid Attacks</p>
    <p>How to combine transfer and optimization attacks?</p>
    <p>Attackers can exploit all known attacks</p>
    <p>Attackers can choose best candidate seeds to attack</p>
    <p>Relax assumptions to better estimate attack cost for realistic adversaries</p>
  </div>
  <div class="page">
    <p>Our Threat Model</p>
    <p>Attacks in our experiments would average $42-$120 per adversarial example found</p>
    <p>Black-box: only query access to model without internal information</p>
    <p>pig: 0.84 dog: 0.02</p>
    <p>Model API</p>
    <p>Queries are expensive (financial cost or detection risk)</p>
    <p>Google Vision API: first 1000 queries (free), $1.5/1000 queries</p>
    <p>Amazon Face Recognition: $1.0/1000 queries</p>
    <p>Clarifai (NSFW): first 5000 queries (free), $3.2/1000 queries for custom model</p>
  </div>
  <div class="page">
    <p>Transfer Attacks</p>
    <p>Local Models</p>
    <p>Input Seed</p>
    <p>Perturbation is generated with local model information (e.g., gradient)</p>
    <p>Local AE</p>
    <p>Attack Search Space</p>
    <p>Low success (transfer) rate for harder attack settings</p>
    <p>Goodfellow et al. (2014)</p>
    <p>Papernot et al. (2017)</p>
    <p>Liu et al. (2017)</p>
    <p>Dong et al. (2018)</p>
    <p>Xie et al. (2019)</p>
    <p>...</p>
    <p>Li et al. (2020)</p>
    <p>Target Model</p>
    <p>API Query</p>
  </div>
  <div class="page">
    <p>Target Region</p>
    <p>Optimization Attacks</p>
    <p>Input Seed</p>
    <p>Perturbation is given by zeroth-order optimization with</p>
    <p>target model information</p>
    <p>Zhang et al. (2017)</p>
    <p>Ilyas et al. (2018)</p>
    <p>Bhagoji et. al (2019)</p>
    <p>Tu et al. (2019)</p>
    <p>Moon et al. (2019)</p>
    <p>...</p>
    <p>Andriushchenko et al. (2020)</p>
    <p>Attack Search Space</p>
  </div>
  <div class="page">
    <p>Combine Transfer and Optimization Attacks</p>
    <p>Local Models</p>
    <p>Input Seed</p>
    <p>Target Region</p>
    <p>Search gradient calculated with respect to local models to find local AE</p>
    <p>Local AE</p>
    <p>Search direction given by zeroth-order optimization with</p>
    <p>target model</p>
    <p>Use (test, label) byproducts to tune local models</p>
    <p>(See paper for byproduct direction, which is usually not successful)</p>
  </div>
  <div class="page">
    <p>Standard Target Model</p>
    <p>Success Rate (%) Query Cost Fraction Better</p>
    <p>(%)Optimization Hybrid Optimization Hybrid</p>
    <p>MNIST [1] 90.9 98.8 1,645 298 99.8</p>
    <p>CIFAR10 [1] 92.2 98.1 1,227 277 98.7</p>
    <p>ImageNet [1] 93.6 97.2 42,417 24,104 91.8</p>
    <p>ImageNet [2] 73.0 98.0 31,849 6,840 100.0</p>
    <p>Optimization Attack: AutoZOOM [1], SimBA [2]; Transfer Attack: PGD on ensemble [3]; Local Models: Standard; Target Class: Least Likely Class</p>
    <p>Local Adversarial Example Generally Helps for Standard Target Models</p>
    <p>[1] Tu, Chun-Chen, et al. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. (AAAI 2019).</p>
    <p>[3] Liu, Yanpei, et al. &quot;Delving into transferable adversarial examples and black-box attacks.&quot; (ICLR 2017).</p>
    <p>[2] Guo, Chuan, et al. &quot;Simple black-box adversarial attacks.&quot; (ICML 2019).</p>
  </div>
  <div class="page">
    <p>Except Against Robust Target Model</p>
    <p>Robust Target Model</p>
    <p>Success Rate (%) Query Cost Fraction</p>
    <p>Better (%)Optimization Hybrid Optimization Hybrid</p>
    <p>CIFAR10 [4] (Untargeted)</p>
    <p>Optimization Attack: AutoZOOM; Transfer Attack: PGD on ensemble; Local Models: Standard;</p>
    <p>[4] Madry, Aleksander, et al. Towards deep learning models resistant to adversarial attacks. (ICLR 2018).</p>
  </div>
  <div class="page">
    <p>Failure on Robust Target Model Hypothesis: different vulnerability space of standard and robust models</p>
    <p>Local and target models of same type match best Failed to find universal local models (see paper for details)</p>
    <p>Transfer Rate (%) Success Rate (%) Cost Reduction (%)</p>
    <p>Standard Local</p>
    <p>Robust Local</p>
    <p>Standard Local</p>
    <p>Robust Local</p>
    <p>Standard Local</p>
    <p>Robust Local</p>
    <p>Standard Target</p>
    <p>Robust Target</p>
  </div>
  <div class="page">
    <p>Main Takeaway from Hybrid Attack</p>
    <p>Starting from local failed transfers reduces cost of optimization attacks</p>
    <p>So far: reducing average cost</p>
  </div>
  <div class="page">
    <p>Low Query Cost</p>
  </div>
  <div class="page">
    <p>How can we efficiently find those lowcost seeds?</p>
  </div>
  <div class="page">
    <p>Phase 1: Transfer</p>
    <p>Information available: Results of attempted attack using local models</p>
    <p>Hypothesis: If local models find successful adversarial example easily, more likely to transfer.</p>
    <p>Strategy: Prioritize seeds with fewer attack iterations</p>
    <p>Phase 2: Optimization</p>
    <p>Information available: Results of previous attempts for transferability</p>
    <p>Hypothesis: Lower loss values are closer to the target region and easier to attack.</p>
    <p>Strategy: Prioritize seeds with lower loss function values</p>
  </div>
  <div class="page">
    <p>Phase 1: Direct Transfers</p>
  </div>
  <div class="page">
    <p>Phase 1Phase 1Phase 1</p>
    <p>Phase 2: Optimization Attack</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Performance of Two-Phase Strategy</p>
    <p>Prioritization Method</p>
    <p>Retroactive Optimal</p>
    <p>Two-Phase 20 218 826</p>
    <p>Random 24,054 125,327 251,917</p>
    <p>Number of queries needed to get different fraction of 1000 images Target: Robust CIFAR10, Local: Standard CIFAR10, Attack: AutoZOOM, Averaged 5 Times</p>
    <p>(see paper for standard errors)</p>
  </div>
  <div class="page">
    <p>Open Source Implementation</p>
    <p>https://github.com/suyeecav/Hybrid-Attack</p>
    <p>Tutorials of incorporating new attacks</p>
    <p>Supports both TensorFlow and PyTorch</p>
    <p>Can be applied to decision-based attacks</p>
  </div>
  <div class="page">
    <p>Main Takeaway</p>
    <p>Understanding cost of an attack, requires considering realistic adversaries</p>
    <p>who can pick and choose what and how to attack to achieve their goals</p>
    <p>Hybrid Attack: combine attacks Batch Attack: seek easy images</p>
    <p>David Evans Yuan Tian</p>
    <p>Jianfeng ChiFnu Suya</p>
    <p>Contributors: Emily Buerk, Jessie Li, Konrad Siebor, Brian Tran</p>
    <p>updated paper: https://arxiv.org/abs/1908.07000</p>
    <p>code: https://github.com/suyeecav/Hybrid-Attack</p>
    <p>contact: Fnu Suya suya@virginia.edu</p>
  </div>
</Presentation>
