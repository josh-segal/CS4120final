<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Selective Attention for Context-aware Neural Machine Translation</p>
    <p>Sameen Maruf, Andre F. T. Martins, Gholamreza Haffari</p>
    <p>Faculty of Information Technology, Monash University, Australia</p>
    <p>Unbabel &amp; Instituto de Telecomunicacoes, Lisbon, Portugal</p>
    <p>NAACL-HLT, Minneapolis, June, 2019</p>
  </div>
  <div class="page">
    <p>Overview</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Overview</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why document-level machine translation?</p>
    <p>Most state-of-the-art NMT models translate sentences independently</p>
    <p>Discourse phenomena are ignored, e.g., pronominal anaphora and coherence, which may have long-range dependency</p>
    <p>Most of the works in document NMT focus on using a few previous sentences as context ignoring the rest of the document</p>
    <p>[Jean et al., 2017, Wang et al., 2017, Bawden et al., 2018, Voita et al., 2018, Tu et al., 2018, Zhang et al., 2018, Miculicich et al., 2018]</p>
    <p>The global document context for MT [Maruf and Haffari, 2018]</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why document-level machine translation?</p>
    <p>Most state-of-the-art NMT models translate sentences independently</p>
    <p>Discourse phenomena are ignored, e.g., pronominal anaphora and coherence, which may have long-range dependency</p>
    <p>Most of the works in document NMT focus on using a few previous sentences as context ignoring the rest of the document</p>
    <p>[Jean et al., 2017, Wang et al., 2017, Bawden et al., 2018, Voita et al., 2018, Tu et al., 2018, Zhang et al., 2018, Miculicich et al., 2018]</p>
    <p>The global document context for MT [Maruf and Haffari, 2018]</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why document-level machine translation?</p>
    <p>Most state-of-the-art NMT models translate sentences independently</p>
    <p>Discourse phenomena are ignored, e.g., pronominal anaphora and coherence, which may have long-range dependency</p>
    <p>Most of the works in document NMT focus on using a few previous sentences as context ignoring the rest of the document</p>
    <p>[Jean et al., 2017, Wang et al., 2017, Bawden et al., 2018, Voita et al., 2018, Tu et al., 2018, Zhang et al., 2018, Miculicich et al., 2018]</p>
    <p>The global document context for MT [Maruf and Haffari, 2018]</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why document-level machine translation?</p>
    <p>Most state-of-the-art NMT models translate sentences independently</p>
    <p>Discourse phenomena are ignored, e.g., pronominal anaphora and coherence, which may have long-range dependency</p>
    <p>Most of the works in document NMT focus on using a few previous sentences as context ignoring the rest of the document</p>
    <p>[Jean et al., 2017, Wang et al., 2017, Bawden et al., 2018, Voita et al., 2018, Tu et al., 2018, Zhang et al., 2018, Miculicich et al., 2018]</p>
    <p>The global document context for MT [Maruf and Haffari, 2018]</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why document-level machine translation?</p>
    <p>Most state-of-the-art NMT models translate sentences independently</p>
    <p>Discourse phenomena are ignored, e.g., pronominal anaphora and coherence, which may have long-range dependency</p>
    <p>Most of the works in document NMT focus on using a few previous sentences as context ignoring the rest of the document</p>
    <p>[Jean et al., 2017, Wang et al., 2017, Bawden et al., 2018, Voita et al., 2018, Tu et al., 2018, Zhang et al., 2018, Miculicich et al., 2018]</p>
    <p>The global document context for MT [Maruf and Haffari, 2018]</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why selective attention for document MT?</p>
    <p>Soft attention over words in the document context</p>
    <p>Forms a long-tail absorbing significant probability mass</p>
    <p>Incapable of ignoring irrelevant words</p>
    <p>Not scalable to long documents</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why selective attention for document MT?</p>
    <p>Soft attention over words in the document context</p>
    <p>Forms a long-tail absorbing significant probability mass</p>
    <p>Incapable of ignoring irrelevant words</p>
    <p>Not scalable to long documents</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why selective attention for document MT?</p>
    <p>Soft attention over words in the document context</p>
    <p>Forms a long-tail absorbing significant probability mass</p>
    <p>Incapable of ignoring irrelevant words</p>
    <p>Not scalable to long documents</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why selective attention for document MT?</p>
    <p>Soft attention over words in the document context</p>
    <p>Forms a long-tail absorbing significant probability mass</p>
    <p>Incapable of ignoring irrelevant words</p>
    <p>Not scalable to long documents</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why selective attention for document MT?</p>
    <p>Soft attention over words in the document context</p>
    <p>Forms a long-tail absorbing significant probability mass</p>
    <p>Incapable of ignoring irrelevant words</p>
    <p>Not scalable to long documents</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>Why selective attention for document MT?</p>
    <p>Soft attention over words in the document context</p>
    <p>Forms a long-tail absorbing significant probability mass</p>
    <p>Incapable of ignoring irrelevant words</p>
    <p>Not scalable to long documents</p>
  </div>
  <div class="page">
    <p>The Whys?</p>
    <p>This Work</p>
    <p>We propose a sparse and hierarchical attention approach for document NMT which:</p>
    <p>identifies the key sentences in the global document context, and</p>
    <p>attends to the key words within those sentences</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Overview</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Context Attention</p>
    <p>For each query word:</p>
    <p>s: attention weights given to sentences in context</p>
    <p>w : attention weights given to words in context</p>
    <p>hier : re-scaled attention weights of words in context</p>
    <p>Vw : from words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Context Attention</p>
    <p>For each query word:</p>
    <p>s: attention weights given to sentences in context</p>
    <p>w : attention weights given to words in context</p>
    <p>hier : re-scaled attention weights of words in context</p>
    <p>Vw : from words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Context Attention</p>
    <p>For each query word:</p>
    <p>s: attention weights given to sentences in context</p>
    <p>w : attention weights given to words in context</p>
    <p>hier : re-scaled attention weights of words in context</p>
    <p>Vw : from words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Context Attention</p>
    <p>For each query word:</p>
    <p>s: attention weights given to sentences in context</p>
    <p>w : attention weights given to words in context</p>
    <p>hier : re-scaled attention weights of words in context</p>
    <p>Vw : from words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Context Attention</p>
    <p>For each query word:</p>
    <p>s: attention weights given to sentences in context</p>
    <p>w : attention weights given to words in context</p>
    <p>hier : re-scaled attention weights of words in context</p>
    <p>Vw : from words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qs: representation of words in current sentence Ks: representation of sentences in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qs: representation of words in current sentence Ks: representation of sentences in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qs: representation of words in current sentence Ks: representation of sentences in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qs: representation of words in current sentence Ks: representation of sentences in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qw : representation of words in current sentence Kw : representation of words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qw : representation of words in current sentence Kw : representation of words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Qw : representation of words in current sentence Kw : representation of words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Our sparse hierarchical attention module is able to selectively focus on relevant sentences in the document</p>
    <p>context and then attends to key words in those sentences</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Our sparse hierarchical attention module is able to selectively focus on relevant sentences in the document</p>
    <p>context and then attends to key words in those sentences</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Hierarchical Selective Attention over Source Document</p>
    <p>Our sparse hierarchical attention module is able to selectively focus on relevant sentences in the document</p>
    <p>context and then attends to key words in those sentences</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:</p>
    <p>multi-head attention  dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:</p>
    <p>multi-head attention  dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:</p>
    <p>multi-head attention  dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:</p>
    <p>multi-head attention  dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:</p>
    <p>multi-head attention  dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:  multi-head attention</p>
    <p>dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft sentence-level attention over all sentences in the document context</p>
    <p>K, V : representation of sentences in context</p>
    <p>Comparison to [Maruf and Haffari, 2018]:  multi-head attention  dynamic</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Flat Attention over Source Document</p>
    <p>Soft word-level attention over all words in the document context</p>
    <p>K, V : representation of words in context</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Document-level Context Layer</p>
    <p>Hierarchical selective or Flat</p>
    <p>Monolingual context (source) integrated in encoder</p>
    <p>Bilingual context (source &amp; target) integrated in decoder</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Document-level Context Layer</p>
    <p>Hierarchical selective or Flat</p>
    <p>Monolingual context (source) integrated in encoder</p>
    <p>Bilingual context (source &amp; target) integrated in decoder</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Document-level Context Layer</p>
    <p>Hierarchical selective or Flat</p>
    <p>Monolingual context (source) integrated in encoder</p>
    <p>Bilingual context (source &amp; target) integrated in decoder</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Document-level Context Layer</p>
    <p>Hierarchical selective or Flat</p>
    <p>Monolingual context (source) integrated in encoder</p>
    <p>Bilingual context (source &amp; target) integrated in decoder</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Our Models and Settings</p>
    <p>Our Models:</p>
    <p>Hierarchical Attention over context  sparse at sentence-level, soft at word-level  sparse at both sentence and word-level</p>
    <p>Flat Attention over context  soft at sentence-level  soft at word-level</p>
    <p>Our Settings:</p>
    <p>Offline document MT</p>
    <p>Online document MT</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Our Models and Settings</p>
    <p>Our Models:</p>
    <p>Hierarchical Attention over context  sparse at sentence-level, soft at word-level  sparse at both sentence and word-level</p>
    <p>Flat Attention over context  soft at sentence-level  soft at word-level</p>
    <p>Our Settings:</p>
    <p>Offline document MT</p>
    <p>Online document MT</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Our Models and Settings</p>
    <p>Our Models:</p>
    <p>Hierarchical Attention over context  sparse at sentence-level, soft at word-level  sparse at both sentence and word-level</p>
    <p>Flat Attention over context  soft at sentence-level  soft at word-level</p>
    <p>Our Settings:</p>
    <p>Offline document MT</p>
    <p>Online document MT</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Our Models and Settings</p>
    <p>Our Models:</p>
    <p>Hierarchical Attention over context  sparse at sentence-level, soft at word-level  sparse at both sentence and word-level</p>
    <p>Flat Attention over context  soft at sentence-level  soft at word-level</p>
    <p>Our Settings:</p>
    <p>Offline document MT</p>
    <p>Online document MT</p>
  </div>
  <div class="page">
    <p>Proposed Approach</p>
    <p>Our Models and Settings</p>
    <p>Our Models:</p>
    <p>Hierarchical Attention over context  sparse at sentence-level, soft at word-level  sparse at both sentence and word-level</p>
    <p>Flat Attention over context  soft at sentence-level  soft at word-level</p>
    <p>Our Settings:</p>
    <p>Offline document MT</p>
    <p>Online document MT</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Overview</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Experimental Setup</p>
    <p>Training/dev/test corpora statistics for En-De:</p>
    <p>Domain #Sentences Document length</p>
    <p>TED 0.21M/9K/2.3K 120.89/96.42/98.74 News 0.24M/2K/3K 38.93/26.78/19.35 Europarl 1.67M/3.6K/5.1K 14.14/14.95/14.06</p>
    <p>Baselines:</p>
    <p>Context-agnostic baselines (RNNSearch, Transformer)</p>
    <p>Local source context baselines for online document MT:  [Zhang et al., 2018] &amp; [Miculicich et al., 2018]</p>
    <p>Evaluation Metrics: BLEU, METEOR</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>B L</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer [Miculicich et al., 2018]</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>B L</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word)</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>B L</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>B L</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>B L</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Bilingual Context integration in Decoder (Online Setting)</p>
    <p>TED 22</p>
    <p>B L</p>
    <p>E U</p>
    <p>News 22</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
    <p>Europarl 27.5</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Analyses</p>
    <p>Automatic evaluation metrics for translation do not assess how well models translate inter-sentential phenomena</p>
    <p>Measure accuracy of translating English pronoun it to its German counterparts es, er and sie using a contrastive test set [Muller et al., 2018]</p>
    <p>Perform subjective evaluation in terms of adequacy and fluency [Laubli et al., 2018]</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Analyses</p>
    <p>Automatic evaluation metrics for translation do not assess how well models translate inter-sentential phenomena</p>
    <p>Measure accuracy of translating English pronoun it to its German counterparts es, er and sie using a contrastive test set [Muller et al., 2018]</p>
    <p>Perform subjective evaluation in terms of adequacy and fluency [Laubli et al., 2018]</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Analyses</p>
    <p>Automatic evaluation metrics for translation do not assess how well models translate inter-sentential phenomena</p>
    <p>Measure accuracy of translating English pronoun it to its German counterparts es, er and sie using a contrastive test set [Muller et al., 2018]</p>
    <p>Perform subjective evaluation in terms of adequacy and fluency [Laubli et al., 2018]</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Accuracy of pronoun translation vs. antecedent distance</p>
    <p>antecedent distance</p>
    <p>A cc</p>
    <p>u ra</p>
    <p>cy</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Accuracy of pronoun translation vs. antecedent distance</p>
    <p>antecedent distance</p>
    <p>A cc</p>
    <p>u ra</p>
    <p>cy Transformer</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Accuracy of pronoun translation vs. antecedent distance</p>
    <p>antecedent distance</p>
    <p>A cc</p>
    <p>u ra</p>
    <p>cy</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Accuracy of pronoun translation vs. antecedent distance</p>
    <p>antecedent distance</p>
    <p>A cc</p>
    <p>u ra</p>
    <p>cy</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Accuracy of pronoun translation vs. antecedent distance</p>
    <p>antecedent distance</p>
    <p>A cc</p>
    <p>u ra</p>
    <p>cy</p>
    <p>Transformer [Miculicich et al., 2018] Attention(sent) Attention(word) H-Attention(sp-soft) H-Attention(sp-sp)</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Model Complexity</p>
    <p>Model #Params Speed (words/sec.) Training Decoding</p>
    <p>Transformer 50M 5100 86.33</p>
    <p>+Attention, sentence 53.7M 3750 83.84 word 53.7M 3100 81.38</p>
    <p>+H-Attention 54.2M 2600 74.11</p>
    <p>[Miculicich et al., 2018] 54.8M 1650 76.90</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Model Complexity</p>
    <p>Model #Params Speed (words/sec.) Training Decoding</p>
    <p>Transformer 50M 5100 86.33</p>
    <p>+Attention, sentence 53.7M 3750 83.84 word 53.7M 3100 81.38</p>
    <p>+H-Attention 54.2M 2600 74.11</p>
    <p>[Miculicich et al., 2018] 54.8M 1650 76.90</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Model Complexity</p>
    <p>Model #Params Speed (words/sec.) Training Decoding</p>
    <p>Transformer 50M 5100 86.33</p>
    <p>+Attention, sentence 53.7M 3750 83.84 word 53.7M 3100 81.38</p>
    <p>+H-Attention 54.2M 2600 74.11</p>
    <p>[Miculicich et al., 2018] 54.8M 1650 76.90</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Model Complexity</p>
    <p>Model #Params Speed (words/sec.) Training Decoding</p>
    <p>Transformer 50M 5100 86.33</p>
    <p>+Attention, sentence 53.7M 3750 83.84 word 53.7M 3100 81.38</p>
    <p>+H-Attention 54.2M 2600 74.11</p>
    <p>[Miculicich et al., 2018] 54.8M 1650 76.90</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Model Complexity</p>
    <p>Model #Params Speed (words/sec.) Training Decoding</p>
    <p>Transformer 50M 5100 86.33</p>
    <p>+Attention, sentence 53.7M 3750 83.84 word 53.7M 3100 81.38</p>
    <p>+H-Attention 54.2M 2600 74.11</p>
    <p>[Miculicich et al., 2018] 54.8M 1650 76.90</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Qualitative Analysis</p>
    <p>Src: Croatia is their homeland , too . Tgt: Kroatien ist auch ihre Heimat . Transformer: Kroatien ist auch seine Heimat . Our Model: Kroatien ist auch ihr Heimatland .</p>
    <p>Head 8: Top sentences with attention to words related to the antecedent</p>
    <p>sj1: to name but a few , these include cooperation with the Hague Tribunal , efforts</p>
    <p>made so far in prosecuting corruption , restructuring the economy and finances</p>
    <p>and greater commitment and sincerity in eliminating the obstacles to the return</p>
    <p>of Croatia s Serbian population .</p>
    <p>sj4: by signing a border arbitration agreement with its neighbour Slovenia ,</p>
    <p>the new Croatian Government has not only eliminated an obstacle to the</p>
    <p>negotiating process , but has also paved the way for the resolution of other</p>
    <p>issues .</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Qualitative Analysis</p>
    <p>Src: Croatia is their homeland , too . Tgt: Kroatien ist auch ihre Heimat . Transformer: Kroatien ist auch seine Heimat . Our Model: Kroatien ist auch ihr Heimatland .</p>
    <p>Head 8: Top sentences with attention to words related to the antecedent</p>
    <p>sj1: to name but a few , these include cooperation with the Hague Tribunal , efforts</p>
    <p>made so far in prosecuting corruption , restructuring the economy and finances</p>
    <p>and greater commitment and sincerity in eliminating the obstacles to the return</p>
    <p>of Croatia s Serbian population .</p>
    <p>sj4: by signing a border arbitration agreement with its neighbour Slovenia ,</p>
    <p>the new Croatian Government has not only eliminated an obstacle to the</p>
    <p>negotiating process , but has also paved the way for the resolution of other</p>
    <p>issues .</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Qualitative Analysis</p>
    <p>Src: Croatia is their homeland , too . Tgt: Kroatien ist auch ihre Heimat . Transformer: Kroatien ist auch seine Heimat . Our Model: Kroatien ist auch ihr Heimatland .</p>
    <p>Head 8: Top sentences with attention to words related to the antecedent</p>
    <p>sj1: to name but a few , these include cooperation with the Hague Tribunal , efforts</p>
    <p>made so far in prosecuting corruption , restructuring the economy and finances</p>
    <p>and greater commitment and sincerity in eliminating the obstacles to the return</p>
    <p>of Croatia s Serbian population .</p>
    <p>sj4: by signing a border arbitration agreement with its neighbour Slovenia ,</p>
    <p>the new Croatian Government has not only eliminated an obstacle to the</p>
    <p>negotiating process , but has also paved the way for the resolution of other</p>
    <p>issues .</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Qualitative Analysis</p>
    <p>Src: Croatia is their homeland , too . Tgt: Kroatien ist auch ihre Heimat . Transformer: Kroatien ist auch seine Heimat . Our Model: Kroatien ist auch ihr Heimatland .</p>
    <p>Head 8: Top sentences with attention to words related to the antecedent</p>
    <p>sj1: to name but a few , these include cooperation with the Hague Tribunal , efforts</p>
    <p>made so far in prosecuting corruption , restructuring the economy and finances</p>
    <p>and greater commitment and sincerity in eliminating the obstacles to the return</p>
    <p>of Croatia s Serbian population .</p>
    <p>sj4: by signing a border arbitration agreement with its neighbour Slovenia ,</p>
    <p>the new Croatian Government has not only eliminated an obstacle to the</p>
    <p>negotiating process , but has also paved the way for the resolution of other</p>
    <p>issues .</p>
  </div>
  <div class="page">
    <p>Experiments and Analyses</p>
    <p>Qualitative Analysis</p>
    <p>Src: Croatia is their homeland , too . Tgt: Kroatien ist auch ihre Heimat . Transformer: Kroatien ist auch seine Heimat . Our Model: Kroatien ist auch ihr Heimatland .</p>
    <p>Head 8: Top sentences with attention to words related to the antecedent</p>
    <p>sj1: to name but a few , these include cooperation with the Hague Tribunal , efforts</p>
    <p>made so far in prosecuting corruption , restructuring the economy and finances</p>
    <p>and greater commitment and sincerity in eliminating the obstacles to the return</p>
    <p>of Croatia s Serbian population .</p>
    <p>sj4: by signing a border arbitration agreement with its neighbour Slovenia ,</p>
    <p>the new Croatian Government has not only eliminated an obstacle to the</p>
    <p>negotiating process , but has also paved the way for the resolution of other</p>
    <p>issues .</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Overview</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Summary</p>
    <p>Proposed a novel and scalable top-down approach to hierarchical attention for document NMT</p>
    <p>Our experiments in two document MT settings show that our approach surpasses context-agnostic and context-aware baselines in majority cases</p>
    <p>Future Work: Investigate benefits of sparse attention in terms of better interpretability of context-aware NMT models</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Summary</p>
    <p>Proposed a novel and scalable top-down approach to hierarchical attention for document NMT</p>
    <p>Our experiments in two document MT settings show that our approach surpasses context-agnostic and context-aware baselines in majority cases</p>
    <p>Future Work: Investigate benefits of sparse attention in terms of better interpretability of context-aware NMT models</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Summary</p>
    <p>Proposed a novel and scalable top-down approach to hierarchical attention for document NMT</p>
    <p>Our experiments in two document MT settings show that our approach surpasses context-agnostic and context-aware baselines in majority cases</p>
    <p>Future Work: Investigate benefits of sparse attention in terms of better interpretability of context-aware NMT models</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References I</p>
    <p>Jean, S. and Lauly, L. and Firat, O. and Cho, K. (2017).</p>
    <p>Does Neural Machine Translation Benefit from Larger Context? arXiv:1704.05135.</p>
    <p>Wang, L. and Tu, Z. and Way, A. and Liu, Q. (2017).</p>
    <p>Exploiting Cross-Sentence Context for Neural Machine Translation. Proceedings of the Conference on Empirical Methods in Natural Language Processing.</p>
    <p>Bawden, R. and Sennrich, R. and Birch, A. and Haddow, B. (2018).</p>
    <p>Evaluating Discourse Phenomena in Neural Machine Translation. Proceedings of the NAACL-HLT 2018.</p>
    <p>Voita, E. and Serdyukov, P. and Sennrich, R. and Titov, I. (2018).</p>
    <p>Context-aware neural machine translation learns anaphora resolution. Proceedings of ACL 2018.</p>
    <p>Tu, Z. and Liu, Y. and Shi, S. and Zhang, T. (2018).</p>
    <p>Learning to Remember Translation History with a Continuous Cache. Proceedings of TACL 2018.</p>
    <p>Zhang, J., Luan, H., Sun, M., Zhai, F., Xu, J., Zhang, M., and Liu, Y. (2018).</p>
    <p>Improving the transformer translation model with document-level context. Proceedings of EMNLP 2018.</p>
    <p>Miculicich, L., Ram, D., Pappas, N., and Henderson, J. (2018).</p>
    <p>Document-level neural machine translation with hierarchical attention networks. Proceedings of EMNLP 2018.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References II</p>
    <p>Maruf, S. and Haffari, G. (2018).</p>
    <p>Document Context Neural Machine Translation with Memory Networks. Proceedings of ACL 2018.</p>
    <p>Neubig, G. and Dyer, C. and Goldberg, Y. and Matthews, A. and Ammar, W. and Anastasopoulos, A. and Ballesteros,</p>
    <p>M. and Chiang, D. and Clothiaux, D. and Cohn, T. and Duh, K. and Faruqui, M. and Gan, C. and Garrette, D. and Ji, Y. and Kong, L. and Kuncoro, A. and Kumar, G. and Malaviya, C. and Michel, P. and Oda, Y. and Richardson, M. and Saphra, N. and Swayamdipta, S. and Yin, P. (2017). DyNet: The Dynamic Neural Network Toolkit.</p>
    <p>Muller, M. and Rios, A. and Voita, E. and Sennrich, R. (2018).</p>
    <p>A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation. Proceedings of WMT 2018.</p>
    <p>Laubli, S. and Sennrich, R. and Volk, M. (2018).</p>
    <p>Has machine translation achieved human parity? A case for document-level evaluation. Proceedings of EMNLP 2018.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Implementation and Hyperparameters</p>
    <p>Implementation: DyNet C++ interface [Neubig et al., 2017], using Transformer-DyNet (https://github.com/duyvuleo/Transformer-DyNet)</p>
    <p>Parameters Details #Layers 4 #Heads 8</p>
    <p>Hidden dimensions 512 Feed-forward layer size 2048</p>
    <p>Optimizer Adam (lr=0.0001) Dropout (Base model) 0.1</p>
    <p>Dropout (Document-level model) 0.2 Label smoothing 0.1</p>
    <p>Src/Tgt vocab sizes: TED 17.1k/23.2k, News 16.9k/23.3k, Europarl 16.6k/25.4k (Joint BPE vocab size 30k)</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Monolingual Context Integration in Encoder</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Bilingual Context Integration in Decoder</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Qualitative Analysis</p>
    <p>Src: my thoughts are also with the victims . Ref: meine Gedanken sind auch bei den Opfern . Transformer: ich denke auch an die Opfer . Our Model: meine Gedanken sind auch bei den Opfern .</p>
    <p>Head 2: Top sentences with attention to related words</p>
    <p>sj2: ( FR ) Madam President , many things have already been said , but I would</p>
    <p>like to echo all the words of sympathy and support that have already been</p>
    <p>addressed to the peoples of Tunisia and Egypt .</p>
    <p>sj+4: it must implement a strong strategy towards these countries .</p>
    <p>sj1: they are a symbol of hope for all those who defend freedom .</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Qualitative Analysis</p>
    <p>Src: my thoughts are also with the victims . Ref: meine Gedanken sind auch bei den Opfern . Transformer: ich denke auch an die Opfer . Our Model: meine Gedanken sind auch bei den Opfern .</p>
    <p>Head 2: Top sentences with attention to related words</p>
    <p>sj2: ( FR ) Madam President , many things have already been said , but I would</p>
    <p>like to echo all the words of sympathy and support that have already been</p>
    <p>addressed to the peoples of Tunisia and Egypt .</p>
    <p>sj+4: it must implement a strong strategy towards these countries .</p>
    <p>sj1: they are a symbol of hope for all those who defend freedom .</p>
  </div>
</Presentation>
