<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Octopus: an RDMA-enabled Distributed Persistent Memory File System</p>
    <p>Youyou Lu1, Jiwu Shu1, Youmin Chen1, Tao Li2</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and Motivation</p>
    <p>Octopus Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>NVMM &amp; RDMA</p>
    <p>NVMM (PCM, ReRAM, etc)  Data persistency</p>
    <p>Byte-addressable</p>
    <p>Low latency</p>
    <p>RDMA  Remote direct access  Bypass remote kernel  Low latency and high throughput</p>
    <p>Client Server Registered Memory Registered Memory</p>
    <p>HCA HCACPU CPU</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>E</p>
  </div>
  <div class="page">
    <p>Modular-Designed Distributed File System</p>
    <p>DiskGluster  Disk for data storage</p>
    <p>GigE for communication</p>
    <p>MemGluster  Memory for data storage</p>
    <p>RDMA for communication</p>
    <p>Latency (1KB write+sync)</p>
    <p>Overall</p>
    <p>HDD</p>
    <p>Software</p>
    <p>Network</p>
    <p>Overall</p>
    <p>MEM</p>
    <p>Software</p>
    <p>RDMA</p>
  </div>
  <div class="page">
    <p>Modular-Designed Distributed File System</p>
    <p>DiskGluster  Disk for data storage</p>
    <p>GigE for communication</p>
    <p>MemGluster  Memory for data storage</p>
    <p>RDMA for communication</p>
    <p>Bandwidth (1MB write)</p>
    <p>HDD</p>
    <p>File System</p>
    <p>MEM</p>
    <p>File System</p>
  </div>
  <div class="page">
    <p>RDMA-enabled Distributed File System  More than fast hardware</p>
    <p>It is suboptimal to simply replace the network/storage module</p>
    <p>Opportunities and Challenges  NVM</p>
    <p>Byte-addressability</p>
    <p>Significant overhead of data copies</p>
    <p>RDMA  Flexible programming verbs (message/memory semantics)</p>
    <p>Imbalanced CPU processing capacity vs. network I/Os</p>
  </div>
  <div class="page">
    <p>RDMA-enabled Distributed File System</p>
    <p>It is necessary to rethink the design of DFS over NVM &amp; RDMA 7</p>
    <p>Byte-addressability of NVM</p>
    <p>One-sided RDMA verbs Shared data managements</p>
    <p>CPU is the new bottleneck New data flow strategies</p>
    <p>Flexible RDMA verbs Efficient RPC design</p>
    <p>RDMA Atomics Concurrent control</p>
    <p>Opportunity Approaches</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and Motivation  Octopus Design  Shared Persistent Memory Pool  Self-Identified Metadata RPC  Client-Active Data I/O  Collect-Dispatch Transaction</p>
    <p>Evaluation  Conclusion</p>
    <p>-&gt; Reduce data copies -&gt; Reduce response latency -&gt; Rebalance CPU/network overhead -&gt; Efficient concurrent control</p>
  </div>
  <div class="page">
    <p>Octopus Architecture</p>
    <p>N2</p>
    <p>NVMM</p>
    <p>N2</p>
    <p>NVMM</p>
    <p>N3</p>
    <p>NVMM</p>
    <p>HCA HCA HCA</p>
    <p>...</p>
    <p>Shared Persistent Memory Pool</p>
    <p>Client 1 Client 2 Self-Identified RPC</p>
    <p>RDMA-based Data IO</p>
    <p>create(/home/cym) Read(/home/lyy)</p>
    <p>It performs remote direct data access just like an Octopus uses its eight legs</p>
  </div>
  <div class="page">
    <p>Existing DFSs  Redundant data copy</p>
    <p>Client Server</p>
    <p>FS Image</p>
    <p>User Space Buffer</p>
    <p>GlusterFS</p>
    <p>Page Cache</p>
    <p>User Space Buffer</p>
    <p>mbuf</p>
    <p>NICNIC</p>
    <p>mbuf</p>
  </div>
  <div class="page">
    <p>Existing DFSs  Redundant data copy</p>
    <p>Client Server</p>
    <p>FS Image</p>
    <p>User Space Buffer</p>
    <p>GlusterFS + DAX</p>
    <p>Page Cache</p>
    <p>User Space Buffer</p>
    <p>mbuf</p>
    <p>NICNIC</p>
    <p>mbuf</p>
  </div>
  <div class="page">
    <p>Octopus with SPMP  Introduces the shared persistent memory</p>
    <p>pool  Global view of data layout</p>
    <p>Existing DFSs  Redundant data copy</p>
    <p>Client Server</p>
    <p>FS Image</p>
    <p>User Space Buffer User Space Buffer</p>
    <p>mbuf</p>
    <p>NICNIC</p>
    <p>mbuf</p>
    <p>Message Pool</p>
  </div>
  <div class="page">
    <p>Server-Active  Server threads process the</p>
    <p>data I/O</p>
    <p>Works well for slow Ethernet</p>
    <p>CPUs can easily become the bottleneck with fast hardware</p>
    <p>Client-Active  Let clients read/write data</p>
    <p>directly from/to the SPMP</p>
    <p>C1 timeNIC MEMCPUC2</p>
    <p>Lookup file data Send data</p>
    <p>Lookup file data Send address 13</p>
    <p>SERVER</p>
  </div>
  <div class="page">
    <p>Message-based RPC  easy to implement, lower throughput</p>
    <p>DaRPC[SoCC14], FaSST[OSDI16]</p>
    <p>Memory-based RPC  CPU cores scan the message buffer</p>
    <p>FaRM[NSDI14]</p>
    <p>Using rdma_write_with_imm?  Scan by polling</p>
    <p>Imm data for self-identification</p>
    <p>Message PoolHCA</p>
    <p>Thead1 Theadn</p>
    <p>HCA DATAID</p>
    <p>Message PoolHCA</p>
    <p>Thead1 Theadn</p>
    <p>HCA DATA</p>
  </div>
  <div class="page">
    <p>Two-Phase Locking &amp; Commit  Distributed logging</p>
    <p>Two-phase coordination</p>
    <p>Coordinator</p>
    <p>Participant</p>
    <p>Begin Log</p>
    <p>Begin Local Lock wait</p>
    <p>Log Commit/</p>
    <p>Abort</p>
    <p>Write Data</p>
    <p>End</p>
    <p>EndBegin</p>
    <p>Log Begin</p>
    <p>Tx Exec</p>
    <p>Log Context</p>
    <p>Log Commit /Abort</p>
    <p>Local Lock</p>
    <p>Log Begin</p>
    <p>Tx Exec</p>
    <p>Log Context</p>
    <p>Log Commit /Abort</p>
    <p>wait</p>
    <p>Local Unlock</p>
    <p>Write Data</p>
    <p>Local Unlock</p>
    <p>wait Log End</p>
    <p>prepare commit</p>
    <p>mkdir, mknod operations need distributed transactions</p>
    <p>Collect-Dispatch Transaction  Local logging with remote in-place update</p>
    <p>Distributed lock service</p>
    <p>Coordinator</p>
    <p>Participant</p>
    <p>Begin Log</p>
    <p>Begin Local Lock</p>
    <p>wait Local Exec Log &amp;</p>
    <p>Update Local</p>
    <p>Unlock</p>
    <p>wait Local Lock</p>
    <p>Collect</p>
    <p>Commit /Abort</p>
    <p>End</p>
    <p>Update</p>
    <p>Unlock</p>
    <p>End</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and Motivation</p>
    <p>Octopus Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Evaluation Setup  Evaluation Platform</p>
    <p>Connected with Mellanox SX1012 switch</p>
    <p>Evaluated Distributed File Systems  memGluster, runs on memory, with RDMA connection  NVFS[OSU], Crail[IBM], optimized to run on RDMA  memHDFS, Alluxio, for big data comparison</p>
    <p>Cluster CPU Memory ConnectX-3 FDR Number</p>
    <p>A E5-2680 * 2 384 GB Yes * 5</p>
    <p>B E5-2620 16 GB Yes * 7</p>
  </div>
  <div class="page">
    <p>Overall Efficiency</p>
    <p>getattr readdir</p>
    <p>Latency Breakdown</p>
    <p>software mem network</p>
    <p>Write Read</p>
    <p>Bandwidth Utilization</p>
    <p>software mem network</p>
    <p>Software latency is reduced rom 326 us to 6 us</p>
    <p>Achieves read/write bandwidth that approaches the raw storage and network bandwidth</p>
  </div>
  <div class="page">
    <p>Metadata Operation Performance</p>
    <p>Octopus provides metadata IOPS in the order of 105~106</p>
    <p>Octopus can scales linearly</p>
    <p>MKNOD</p>
    <p>glusterfs nvfs crail dmfs crail-poll</p>
    <p>GETATTR</p>
    <p>glusterfs nvfs crail dmfs crail-poll</p>
    <p>RMNOD</p>
    <p>glusterfs nvfs crail dmfs crail-poll</p>
  </div>
  <div class="page">
    <p>Read/Write Performance</p>
    <p>Octopus can easily reach the maximum bandwidth of hardware with a single client</p>
    <p>Octopus can achieve the same bandwidth as Crail even add an extra data copy [not shown]</p>
    <p>WRITE</p>
    <p>glusterfs</p>
    <p>nvfs</p>
    <p>crail-np</p>
    <p>crail-poll</p>
    <p>dmfs</p>
    <p>READ</p>
  </div>
  <div class="page">
    <p>Big Data Evaluation</p>
    <p>Octopus can also provide better performance for big data applications than existing file systems.</p>
    <p>write read</p>
    <p>TestDFSIO (MB/s)</p>
    <p>memHDFS Alluxio NVFS Crail Octopus</p>
    <p>Teragen Wordcount</p>
    <p>Normalized Execution Time</p>
    <p>memHDFS Alluxio NVFS Crail Octopus</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and Motivation</p>
    <p>Octopus Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion  It is necessary to rethink the DFS designs over emerging H/Ws</p>
    <p>Octopuss internal mechanisms  Simplifies data management layer by reducing data copies  Rebalances network and server loads with Client-Active I/O  Redesigns the metadata RPC and distributed transaction with</p>
    <p>RDMA primitives</p>
    <p>Evaluations show that Octopus significantly outperforms existing file systems</p>
  </div>
  <div class="page">
    <p>Q&amp;A</p>
    <p>Thanks</p>
  </div>
</Presentation>
