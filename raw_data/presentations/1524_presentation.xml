<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>FlashGraph: Processing Billion-Node Graphs on an Array of Commodity SSDs</p>
    <p>Da Zheng, Disa Mhembere, Randal Burns, Joshua Vogelstein, Carey E. Priebe, Alexander S. Szalay</p>
    <p>Johns Hopkins University</p>
  </div>
  <div class="page">
    <p>Overview  Large-scale graph analysis using a commodity SSD</p>
    <p>array.  We built a semi-external memory [1] graph</p>
    <p>processing framework called FlashGraph.  Achieve performance comparable to in-memory graph engines.  Scale to graphs with billions of verTces and hundreds of billions</p>
    <p>of edges.</p>
    <p>[1] J. Abello, A.L. Buchsbaum, J.R. Westbrook, A functional approach to external graph algorithms. In Algorithmica (1998), Springer-Verlag, pp. 332343.</p>
  </div>
  <div class="page">
    <p>MoTvaTon  Graph analysis is ubiquitous:</p>
    <p>Many real-world problems are modeled with graphs.</p>
  </div>
  <div class="page">
    <p>Challenges in Graph analysis  Many real-world graphs are massive.</p>
    <p>Facebooks social graph has billions of verTces.  Web page graph has tens of billions of verTces.</p>
    <p>Seemingly random vertex connecTon.  Small random memory access.  Hard to explore data locality.  Efficient graph analysis requires RAM.</p>
    <p>Power-law distribuTon in vertex degree.  Load balance.</p>
  </div>
  <div class="page">
    <p>AlternaTves for scaling  OpTon 1: Buy a machine with large RAM.</p>
    <p>Examples: SNAP, Galois, Ligra  Problems: Price &amp; Scalability.</p>
    <p>OpTon 2: Scale out in a cluster.  Examples: Pregel, GraphLab/PowerGraph, GraphX  Problem: Network is the bo\leneck.</p>
    <p>OpTon 3: Scale with hard drives.  Examples: GraphChi, X-Streams  Problem: Slow.</p>
  </div>
  <div class="page">
    <p>Why Scale with SSDs?  SSD properTes:</p>
    <p>Throughput: over 1M random IOPS.  Latency: 20s.  Much cheaper and much larger than RAM.</p>
    <p>QuesTons:  How much can we approach to the performance of in-memory</p>
    <p>graph analysis?  How do we maximize the benefits SSDs bring for graph</p>
    <p>analysis?</p>
  </div>
  <div class="page">
    <p>Challenges in using SSDs  High OS overhead in fast I/O.</p>
    <p>Heavy locking overhead in the OS block subsystem.  We tackle it with SAFS, a user-space filesystem opTmized for a</p>
    <p>large SSD array [1].</p>
    <p>High latency and low throughput compared with RAM.  Latency: three orders of magnitude higher.  Throughput: almost an order of magnitude lower.</p>
    <p>[1] D. Zheng, R. Burns, A. S. Szalay, Toward Millions of File System IOPS on Low-Cost, Commodity Hardware, in Supercomputing 2013</p>
    <p>Design principles for I/O optimization:  Reduce I/O.  Overlap I/O and computation.  Perform sequential I/O.</p>
  </div>
  <div class="page">
    <p>Architecture  Three layers for graph</p>
    <p>analysis using a large SSD array.  SAFS: a user-space file system</p>
    <p>for delivering maximal I/O performance.</p>
    <p>FlashGraph: schedules vertex programs to opTmize I/O.</p>
    <p>Graph algorithms: part of vertex computaTon in the page cache.</p>
    <p>SAFS</p>
    <p>Asynchronous user-task I/O interface</p>
    <p>Page cache</p>
    <p>SSD SSD SSD SSD</p>
    <p>Vertex tasks</p>
    <p>FlashGraph</p>
    <p>Graph algorithms</p>
    <p>Vertex-centric interface</p>
    <p>Vertex scheduler</p>
    <p>Vertex programs</p>
  </div>
  <div class="page">
    <p>Semi-external memory  Algorithmic vertex state in memory.  Edge lists of verTces on SSDs.  Scalability:</p>
    <p>In proporTon to the raTo of edges to verTces in a graph.</p>
    <p>Advantage:  vs. distributed memory:</p>
    <p>Enable in-memory vertex communicaTon.</p>
    <p>vs. external memory:  Reduce reads and avoid writes to SSDs.</p>
  </div>
  <div class="page">
    <p>I/O access in FlashGraph  Access edge lists only required by an applicaTon.</p>
    <p>Reduce I/O.</p>
    <p>ConservaTvely merge I/O requests.  Increase sequenTal I/O.  Merge criteria:</p>
    <p>Two edge lists are on the same SSD page.  Two edge lists are on adjacent SSD pages.</p>
    <p>Never increase the amount of data read from SSD.</p>
  </div>
  <div class="page">
    <p>I/O merging in FlashGraph  The most common case:</p>
    <p>A vertex only accesses its own edge list.</p>
    <p>v1 v3 v2 v6 v8 v4 v5 v7 v9 v10 Vertices</p>
    <p>in memory v1 v2 v6 v8</p>
    <p>v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 Edge lists on SSDs</p>
    <p>Page 1 Page 2 Page 3 Page 4 Page 5</p>
    <p>FlashGraph</p>
    <p>v1 v2 v6 v7 v8</p>
    <p>I/O request 1 I/O request 2</p>
    <p>v1 v2 v6 v7 v8</p>
    <p>v1 v2 v6 v8 Active vertices</p>
  </div>
  <div class="page">
    <p>ApplicaTons  Class 1: subset of verTces access their own edge</p>
    <p>lists.  Breadth-first search (BFS).  Betweenness centrality (BC).</p>
    <p>Class 2: all verTces access their own edge lists.  PageRank (PR).  Weakly connected components (WCC).</p>
    <p>Class 3: a vertex accesses mulTple edges lists.  Triangle counTng (TC).  Scan staTsTcs (SS).</p>
  </div>
  <div class="page">
    <p>FlashGraph Performance  FlashGraph has performance comparable to Galois.  Ext-mem FlashGraph has performance comparable to in-mem</p>
    <p>FlashGraph.  Ext-mem FlashGraph significantly outperforms PowerGraph.</p>
    <p>BFS BC WCC PR TC SS</p>
    <p>Ru n+</p>
    <p>m e (s )</p>
    <p>FG in-mem FG ext-1G PG Galois</p>
  </div>
  <div class="page">
    <p>FlashGraph Scalability  Largest graph publicly available: 3.5B verTces and</p>
    <p>Microsoo Trinity used 14 12-core machines for BFS in +10 min on 1B vertex and 13B edge graph.</p>
    <p>Algorithm RunTme (min) Memory (GB)</p>
    <p>BFS 5 22</p>
    <p>Betweenness 10 81</p>
    <p>Triangle 130 55</p>
    <p>WCC 8 47</p>
    <p>PageRank 34 46</p>
    <p>Scan staTsTcs 6 83</p>
    <p>FlashGraph uses one machine with 32 CPU cores.</p>
    <p>We can scale to a much larger graph!</p>
  </div>
  <div class="page">
    <p>Conclusion  SSD-based graph analysis has performance</p>
    <p>comparable to in-memory counterparts.  FlashGraph offers unprecedented opportuniTes to</p>
    <p>perform massive graph analysis efficiently with commodity hardware.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Da Zheng: dzheng5@jhu.edu  FlashGraph: h\ps://github.com/icoming/FlashGraph</p>
  </div>
  <div class="page">
    <p>Scale out  Graph analysis in distributed memory.  PageRank on EC2 m2.4xlarge.  Problems:</p>
    <p>Network is the bo\leneck.</p>
  </div>
  <div class="page">
    <p>ExecuTon model</p>
    <p>A graph is parTToned for parallel processing.</p>
    <p>Each thread schedules and executes vertex programs independently.</p>
    <p>Vertex status:  Running.  AcTve.  InacTve.</p>
    <p>Inactive region</p>
    <p>Active region</p>
    <p>Running region</p>
    <p>Vertex scheduler</p>
    <p>Thread 1</p>
  </div>
  <div class="page">
    <p>FlashGraph programming interface</p>
    <p>Vertex centric  Users write programs from the perspecTve of verTces</p>
    <p>class vertex { void run(graph_engine &amp;g); void run_on_vertex(graph_engine &amp;g, page_vertex &amp;v); void run_on_message(graph_engine &amp;g, vertex_message &amp;msg); void run_on_iteration_end(graph_engine &amp;g); };</p>
    <p>Asynchronous Run in the page cache asynchronously</p>
  </div>
</Presentation>
