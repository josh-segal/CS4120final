<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Utilitarian Performance Isolation in Shared SSDs</p>
    <p>Bryan S. Kim</p>
    <p>(Seoul National University, Korea)</p>
  </div>
  <div class="page">
    <p>Flash storage landscape</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>Flash device</p>
    <p>Flash</p>
    <p>chip</p>
    <p>Flash</p>
    <p>chip</p>
    <p>Flash</p>
    <p>chip</p>
    <p>Flash</p>
    <p>chip</p>
    <p>Host system</p>
    <p>NVMe</p>
    <p>MQ blk IO (SYSTOR13)</p>
    <p>NVMeDirect (HotStorage16)</p>
    <p>SPDK (CloudCom17)</p>
    <p>and more</p>
    <p>Ozone (TC11)</p>
    <p>SSDsim (ICS11)</p>
    <p>Sprinkler (HPCA14)</p>
    <p>and more</p>
    <p>Increase</p>
    <p>parallelism!</p>
    <p>(2013)Expose</p>
    <p>parallelism!</p>
    <p>Exploit</p>
    <p>parallelism!</p>
  </div>
  <div class="page">
    <p>Noisy neighbors</p>
    <p>C. Petersen and A. Huffman, Solving Latency Challenges with NVM Express SSDs at Scale,</p>
    <p>Flash memory summit 2017,</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Unified sharing of resources (free-for-all)</p>
    <p>Blue tenant : large capacity; infrequent access Red tenant : Write-intensive Green tenant : QoS-sensitive</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>flash chip 0 flash chip 1 flash chip 2 flash chip 3</p>
    <p>flash chip 4 flash chip 5 flash chip 6 flash chip 7</p>
  </div>
  <div class="page">
    <p>Partitioning of resources (egalitarian)</p>
    <p>Under-utilized parallelism High GC overhead</p>
    <p>Lack of read parallelism</p>
    <p>flash chip 0 flash chip 1 flash chip 2 flash chip 3</p>
    <p>flash chip 4 flash chip 5 flash chip 6 flash chip 7</p>
    <p>Blue tenant : large capacity; infrequent access Red tenant : Write-intensive Green tenant : QoS-sensitive</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Slashing parallelism for isolation</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>DAS-AS DAS-PS DTRS LM-TBE MSN-CFS RAD-AS RAD-BE</p>
    <p>N o r m</p>
    <p>. a v g . R</p>
    <p>T 4x4 2x4 1x4 1x2</p>
    <p>Performance suffers from reduced parallelism!</p>
  </div>
  <div class="page">
    <p>Dynamic allocation of resources (utilitarian)</p>
    <p>Reduced GC overhead</p>
    <p>Altruistic sharing of parallelism High-degree of read parallelism</p>
    <p>flash chip 0 flash chip 1 flash chip 2 flash chip 3</p>
    <p>flash chip 4 flash chip 5 flash chip 6 flash chip 7</p>
    <p>Blue tenant : large capacity; infrequent access Red tenant : Write-intensive Green tenant : QoS-sensitive</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Utilitarian performance isolation</p>
    <p>Lessons from storage arrays</p>
    <p>Monitor each tenants fair share of I/O</p>
    <p>Determine optimal data placement</p>
    <p>To balance the load across multiple storage devices</p>
    <p>while considering data relocation overheads</p>
    <p>Key insight</p>
    <p>Flash memorys challenges  Flash memorys opportunities</p>
    <p>Need to maintain mapping  Easy to balance load</p>
    <p>Need to garbage collect  Easy to relocate data</p>
    <p>The utilitarian approach</p>
    <p>Compute tenants utility (measure of received service)</p>
    <p>Determine the allocation set (a set of chips for writing data) for each tenant</p>
    <p>Allocation sets are mutually exclusive and collectively exhaustive</p>
    <p>Allow data relocation among sets if needed</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Utility of tenants</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>flash chip 0 flash chip 1 flash chip 2 flash chip 3</p>
    <p>flash chip 4 flash chip 5 flash chip 6 flash chip 7</p>
    <p>Blue tenants utility : 0.9 Red tenants utility : 0.7 Green tenants utility : 0.8</p>
  </div>
  <div class="page">
    <p>Load balancing</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>flash chip 0 flash chip 1 flash chip 2 flash chip 3</p>
    <p>flash chip 4 flash chip 5 flash chip 6 flash chip 7</p>
    <p>Red tenants writes are striped across a larger set of flash memory chips. Blues performance loss is minor.</p>
  </div>
  <div class="page">
    <p>Data relocation</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>flash chip 0 flash chip 1 flash chip 2 flash chip 3</p>
    <p>flash chip 4 flash chip 5 flash chip 6 flash chip 7</p>
    <p>Garbage collection in chip 1 isolates some of Blue tenants data by relocating them to its set. Red tenants GC efficiency improves.</p>
  </div>
  <div class="page">
    <p>Utility function</p>
    <p>Utility</p>
    <p>Utility of a tenant is high when its reads experience less traffic</p>
    <p>Traffic</p>
    <p>Traffic of a chip indicates the overall busyness of the chip</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Set allocation &amp; data relocation</p>
    <p>Set allocation</p>
    <p>Objective</p>
    <p>Find allocation set that minimizes max-min ratio of utility across all tenants</p>
    <p>Approximation</p>
    <p>Transfer one chip from max utility tenant to min utility tenant</p>
    <p>Avoid thrashing by transferring only if it balances the utility</p>
    <p>Select a chip that experienced least number of reads</p>
    <p>Data relocation</p>
    <p>Considering the number of reads of a foreign block during garbage collection</p>
    <p>Foreign blocks that high number of reads are incentivized to relocate to its own set</p>
    <p>Infrequently accessed cold data may remain in another set</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Evaluation environment &amp; methodology</p>
    <p>Storage system configuration</p>
    <p>150GB storage with 28% over-provisioning</p>
    <p>3 channels x 4 chips/chan</p>
    <p>Garbage collection: reclaims space for writes + considers foreign block reads</p>
    <p>Workload configuration</p>
    <p>3 real-world I/O traces collected from MS production servers</p>
    <p>DAS-AS: lowest throughput, highest read-to-write ratio</p>
    <p>DTRS: relatively random workload with bursts of writes</p>
    <p>LM-TBE: large sequential reads and writes</p>
    <p>ConclusionDesignBackground Evaluation</p>
  </div>
  <div class="page">
    <p>Average performance</p>
    <p>Partitioned : dedicates channel to each tenant Unified : shares all resources among tenants UPI : dynamically allocates based on utility</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>DAS-AS DTRS LM-TBE</p>
    <p>N o r m</p>
    <p>. a v .g</p>
    <p>r e a d</p>
    <p>R T</p>
    <p>Partitioned Unified UPI</p>
    <p>DAS-AS DTRS LM-TBE</p>
    <p>N o</p>
    <p>r m</p>
    <p>. 9</p>
    <p>r e a</p>
    <p>d R</p>
    <p>T</p>
    <p>Partitioned Unified UPI</p>
    <p>for latency-critical</p>
    <p>application</p>
    <p>for throughput</p>
    <p>oriented application</p>
  </div>
  <div class="page">
    <p>QoS performance</p>
    <p>Partitioned : dedicates channel to each tenant Unified : shares all resources among tenants UPI : dynamically allocates based on utility</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>m u</p>
    <p>la ti</p>
    <p>v e p</p>
    <p>r o</p>
    <p>b a</p>
    <p>b il</p>
    <p>it y</p>
    <p>Read response time (ms)</p>
    <p>Unified Partitioned UPI</p>
    <p>m u</p>
    <p>la ti</p>
    <p>v e p</p>
    <p>r o</p>
    <p>b a</p>
    <p>b il</p>
    <p>it y</p>
    <p>Read response time (ms)</p>
    <p>Unified Partitioned UPI</p>
    <p>DAS-AS LM-TBE</p>
  </div>
  <div class="page">
    <p>Microscopic view</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>W r.</p>
    <p>B W</p>
    <p>( M</p>
    <p>B /s</p>
    <p>)</p>
    <p>Time (seconds)</p>
    <p>DAS-AS DTRS LM-TBE</p>
    <p>A v</p>
    <p>g .</p>
    <p>r e a</p>
    <p>d R</p>
    <p>T (</p>
    <p>s)</p>
    <p>Time (seconds)</p>
    <p>DAS-AS DTRS LM-TBE</p>
    <p>A v</p>
    <p>g .</p>
    <p>r e a</p>
    <p>d R</p>
    <p>T (</p>
    <p>s)</p>
    <p>Time (seconds)</p>
    <p>DAS-AS DTRS LM-TBE</p>
    <p>Partitioned UPI</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Dynamic allocation of resources based on utility</p>
    <p>Decouple parallelism, isolation, and capacity</p>
    <p>Balancing the load by distributing write traffic</p>
    <p>Relocate data through existing SSD management mechanisms</p>
    <p>ConclusionDesignBackground Evaluation</p>
    <p>Utilitarian Performance Isolation</p>
    <p>reduces average response time</p>
    <p>by 16.1% for high-throughput workload</p>
    <p>reduces 99% QoS</p>
    <p>by 38.5% for latency-critical workload</p>
  </div>
</Presentation>
