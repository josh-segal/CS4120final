<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>GASPP: A GPU-Accelerated Stateful Packet Processing Framework</p>
    <p>Giorgos Vasiliadis, FORTH-ICS, Greece Lazaros Koromilas, FORTH-ICS, Greece Michalis Polychronakis, Columbia University, USA So5ris Ioannidis, FORTH-ICS, Greece</p>
  </div>
  <div class="page">
    <p>Network Packet Processing</p>
    <p>Computa5onally and memory-intensive</p>
    <p>High levels of data parallelism  Each packet can be processed in parallel</p>
    <p>Poor temporal locality for data  Typically, each packet is processed only once</p>
  </div>
  <div class="page">
    <p>GPU = Graphics Processing Units</p>
    <p>Highly parallel manycore devices  Hundreds of cores  High memory bandwidth  Up to 6GB of memory</p>
  </div>
  <div class="page">
    <p>GPUs for Network Packet Processing</p>
    <p>Gnort [RAID08]  PacketShader [SIGCOMM10]  SSLShader [NSDI11]  MIDeA [CCS11], Kargus [CCS12]</p>
  </div>
  <div class="page">
    <p>GPUs for Network Packet Processing</p>
    <p>Gnort [RAID08]  PacketShader [SIGCOMM10]  SSLShader [NSDI11]  MIDeA [CCS11], Kargus [CCS12]</p>
    <p>Independent/Monolithic Designs</p>
  </div>
  <div class="page">
    <p>Need a framework for developing GPU accelerated packet processing applica5ons</p>
  </div>
  <div class="page">
    <p>TCP Flow State Management</p>
    <p>Packet Scheduling</p>
    <p>Consolida/on'Enables'Extensibility'</p>
    <p>Session'Management'</p>
    <p>Protocol'Parsers'</p>
    <p>VPN'''Web'''Mail'''IDS'''Proxy''</p>
    <p>Firewall'</p>
    <p>Contribu/on'of'reusable'modules:''30''80'%'</p>
    <p>e.g.,'xOMB'(UCSD)' Bro''</p>
    <p>Packet Decoding</p>
    <p>GASPP Framework</p>
    <p>Packet Reordering</p>
    <p>AES Regex Match String Match Firewall</p>
  </div>
  <div class="page">
    <p>GASPP Framework</p>
    <p>Fast user-space packet capturing</p>
    <p>Modular and flexible</p>
    <p>Efficient packet scheduling mechanisms</p>
    <p>TCP processing and flow management support</p>
  </div>
  <div class="page">
    <p>GASPP Framework</p>
    <p>Fast user-space packet capturing</p>
    <p>Modular and flexible</p>
    <p>Efficient packet scheduling mechanisms</p>
    <p>TCP processing and flow management support</p>
  </div>
  <div class="page">
    <p>Fast user-space packet capturing</p>
    <p>Use a single user-space buffer between the NIC and the GPU</p>
    <p>Stage packets back-to-back to a separate buffer</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
  </div>
  <div class="page">
    <p>Fast user-space packet capturing</p>
    <p>Packets size (#bytes)</p>
    <p>Gbit/s</p>
    <p>Packets size (#bytes)</p>
    <p>Gbit/s</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
  </div>
  <div class="page">
    <p>Fast user-space packet capturing</p>
    <p>Packets size (#bytes)</p>
    <p>Gbit/s</p>
    <p>Packets size (#bytes)</p>
    <p>Gbit/s</p>
    <p>&lt; !</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
  </div>
  <div class="page">
    <p>Why staging is be_er than zero-copy (for small packets)</p>
    <p>NICs Packet Buffer:</p>
    <p>Staging buffer:</p>
    <p>Be1er space u6liza6on =&gt; No redundant transfers</p>
  </div>
  <div class="page">
    <p>Selec5ve scheme</p>
    <p>Packets are are copied back-to-back to a separate buffer, if the buffer occupancy is sparse</p>
    <p>Otherwise, they are transferred directly to the GPU</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
    <p>a) b)</p>
    <p>e)d)</p>
    <p>c)</p>
    <p>f)</p>
    <p>Received and forwarded packet New packet</p>
    <p>Figure 4: Subsequent packets (dashed line) may arrive in-sequence ((a)(d)) or out of order, creating holes in the reconstructed TCP stream ((e)(f)).</p>
    <p>ets would be actively dropped until the missing packet arrives. Although this approach would ensure an in-order packet flow, it has several disadvantages. First, in situations where the percentage of out-of-order packets is high, performance will degrade. Second, if the endpoints are using selective retransmission and there is a high rate of data loss in the network, connections would be rendered unusable due to excessive packet drops.</p>
    <p>To deal with TCP sequence hole scenarios, GASPP only processes packets with sequence numbers less than or equal to the connections current sequence number (Figure 4(a)(d)). Received packets with no preceding packets in the current batch and with sequence numbers larger than the ones stored in the connection table imply sequence holes (Figure 4(e)(f)), and are copied in a separate buffer in global device memory. If a thread encounters an out-of-order packet (i.e., a packet with a sequence number larger than the sequence number stored in the connection table, with no preceding packet in the current batch after the hashing calculations of 4.2), it traverses the next packet array and marks as out-oforder all subsequent packets of the same flow contained in the current batch (if any). This allows the system to identify sequences of out-of-order packets, as the ones shown in the examples of Figure 4(e)(f). The buffer size is configurable and can be up to several hundred MBs, depending on the network needs. If the buffer contains any out-of-order packets, these are processed right after a new batch of incoming packets is processed.</p>
    <p>Although packets are copied using the very fast device-to-device copy mechanism, with a memory bandwidth of about 145 GB/s, an increased number of out-oforder packets can have a major effect on overall performance. For this reason, by default we limit the number of out-of-order packets that can be buffered to be equal to the available slots in a batch of packets. This size is enough under normal conditions, where out-oforder packets are quite rare [9], and it can be configured as needed for other environments. If the percentage of out-of-order packets exceeds this limit, our system starts to drop out-of-order packets, causing the corresponding host to retransmit them.</p>
    <p>GPU</p>
    <p>NIC</p>
    <p>DMA Buffer</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>DMA Buffer</p>
    <p>Main Memory</p>
    <p>NIC GPUCPU</p>
    <p>(a) (b)</p>
    <p>Figure 5: Normal (a) and zero-copy (b) data transfer between the NIC and the GPU.</p>
    <p>The problem of data transfers between the CPU and the GPU is well-known in the GPGPU community, as it results in redundant cross-device communication. The traditional approach is to exchange data using DMA between the memory regions assigned by the OS to each device. As shown in Figure 5(a), network packets are transferred to the page-locked memory of the NIC, then copied to the page-locked memory of the GPU, and from there, they are finally transferred to the GPU.</p>
    <p>To avoid costly packet copies and context switches, GASPP uses a single buffer for efficient data sharing between the NIC and the GPU, as shown in Figure 5(b), by adjusting the netmap module [20]. The shared buffer is added to the internal tracking mechanism of the CUDA driver to automatically accelerate calls to functions, as it can be accessed directly by the GPU. The buffer is managed by GASPP through the specification of a policy based on time and size constraints. This enables realtime applications to process incoming packets whenever a timeout is triggered, instead of waiting for buffers to fill up over a specified threshold. Per-packet buffer allocation overheads are reduced by transferring several packets at a time. Buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. Slots are reused whenever the circular hardware queue wraps around. The size of each slot is 1,536 bytes, which is consistent with the NICs alignment requirements, and enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copies, this does not always lead to better performance. As previous studies have shown [12, 26] (we verify their results in 7.1), contrary to NICs, current GPU implementations suffer from poor performance for small data transfers. To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64</p>
  </div>
  <div class="page">
    <p>GASPP Framework</p>
    <p>Fast user-space packet capturing</p>
    <p>Modular and flexible</p>
    <p>Efficient packet scheduling mechanisms</p>
    <p>TCP processing and flow management support</p>
  </div>
  <div class="page">
    <p>Modular and Flexible</p>
    <p>Basic abstrac5on of processing: ``modules  processPacket(packet){ ... }</p>
    <p>Modules are executed sequen5ally or in parallel</p>
    <p>Module 1: IP-learn Module 3:</p>
    <p>Encryption</p>
    <p>RX processPacket()</p>
    <p>processPacket()</p>
    <p>TX</p>
    <p>processPacket()</p>
    <p>Module 2: Content</p>
    <p>Inspection</p>
  </div>
  <div class="page">
    <p>Batch Processing Pipeline</p>
    <p>RX Module1 TX Module2 [me</p>
  </div>
  <div class="page">
    <p>Batch Processing Pipeline</p>
    <p>RX Module1 TX Module2</p>
    <p>RX batch</p>
    <p>[me</p>
  </div>
  <div class="page">
    <p>Batch Processing Pipeline</p>
    <p>RX Module1 TX Module2</p>
    <p>RX batch Batch</p>
    <p>processing</p>
    <p>[me</p>
    <p>copy to GPU</p>
  </div>
  <div class="page">
    <p>Batch Processing Pipeline</p>
    <p>RX Module1 TX Module2</p>
    <p>RX batch Batch</p>
    <p>processing</p>
    <p>[me</p>
    <p>copy to GPU</p>
  </div>
  <div class="page">
    <p>Batch Processing Pipeline</p>
    <p>RX Module1 TX Module2</p>
    <p>RX batch</p>
    <p>TX batch</p>
    <p>Batch processing</p>
    <p>[me</p>
    <p>copy to GPU</p>
    <p>copy to CPU</p>
  </div>
  <div class="page">
    <p>GASPP Framework</p>
    <p>Fast user-space packet capturing</p>
    <p>Modular and flexible</p>
    <p>Efficient packet scheduling mechanisms</p>
    <p>TCP processing and flow management support</p>
  </div>
  <div class="page">
    <p>Single Instruc5on, Mul5ple Threads</p>
    <p>Threads within the same warp have to execute the same instruc5ons</p>
    <p>Great for regular computaEons!</p>
    <p>SIMT group (warp)</p>
    <p>The College of William and Mary eddy@cs.wm.edu 3</p>
    <p>a SIMD group (warp)</p>
    <p>Graphic Processing Unit (GPU)</p>
    <p>Massive parallelism  Favorable</p>
    <p>computing power  cost effectiveness  energy efficiency</p>
  </div>
  <div class="page">
    <p>Parallelism in packet processing</p>
    <p>Network packets are processed in batches  More packets =&gt; more parallelism</p>
    <p>Batch Size (#packets)</p>
    <p>Network traffic</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>Received network packets mix is very dynamic</p>
    <p>Batch Size (#packets)</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>Received network packets mix is very dynamic  Different packet lengths</p>
    <p>Batch Size (#packets)</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>Received network packets mix is very dynamic  Different packet lengths  Divergent parallel module processing</p>
    <p>Batch Size (#packets)</p>
    <p>module 1 module 2 module 3</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>[me</p>
    <p>module 1 module 2 module 3</p>
    <p>warp 1 warp 2 warp 3 warp 4 warp 5 warp 6 warp 7</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>[me</p>
    <p>module 1 module 2 module 3</p>
    <p>warp 1 warp 2 warp 3 warp 4 warp 5 warp 6 warp 7</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>[me</p>
    <p>module 1 module 2 module 3</p>
    <p>warp 1 warp 2 warp 3 warp 4 warp 5 warp 6 warp 7</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>[me</p>
    <p>module 1 module 2 module 3</p>
    <p>warp 1 warp 2 warp 3 warp 4 warp 5 warp 6 warp 7</p>
  </div>
  <div class="page">
    <p>Dynamic Irregulari5es</p>
    <p>[me</p>
    <p>module 1 module 2 module 3</p>
    <p>warp 1 warp 2 warp 3 warp 4 warp 5 warp 6 warp 7</p>
    <p>Low warp occupancy</p>
  </div>
  <div class="page">
    <p>Packet grouping</p>
    <p>Batch Size</p>
    <p>Batch Size</p>
  </div>
  <div class="page">
    <p>Packet grouping</p>
    <p>Harmonized execu5on  Symmetric processing</p>
    <p>warp 1 warp 2 warp 3 warp 4 warp 5 warp 6 warp 7 [me</p>
  </div>
  <div class="page">
    <p>GASPP Framework</p>
    <p>Fast user-space packet capturing</p>
    <p>Modular and flexible</p>
    <p>Efficient packet scheduling mechanisms</p>
    <p>TCP processing and flow management support</p>
  </div>
  <div class="page">
    <p>TCP Flow State Management</p>
    <p>Maintain the state of TCP connec5ons</p>
    <p>Rx</p>
    <p>HtoD</p>
    <p>Tx</p>
    <p>GPU DtoH</p>
    <p>Rx</p>
    <p>HtoD</p>
    <p>Tx</p>
    <p>GPU DtoH</p>
    <p>Rx</p>
    <p>HtoD</p>
    <p>Tx</p>
    <p>GPU DtoH</p>
    <p>Figure 3: The I/O and processing pipeline.</p>
    <p>by transferring several packets at a time to and from the NIC. The buffers consist of fixed-size slots, with each slot corresponding to one packet in the hardware queue. The slots are reused whenever the circular hardware queue wraps-up. Each slot of the packet buffer is manually configured to be 1,536-byte long, which is consistent with the NICs alignment requirements, and is enough for the typical 1,518-byte maximum Ethernet frame size.</p>
    <p>Although making the NICs packet queue directly accessible to the GPU eliminates redundant copy operations, this does not always lead to better performance. As previous studies have shown [15, 45], contrary to NICs, current GPU implementations suffer from poor performance for small data transfers (we verify their results in 5). To improve PCIe throughput, we batch several packets and transfer them at once. However, the fixed-size partitioning of the NICs queue leads to redundant data transfers for traffic with many small packets. For example, a 64b packet consumes only the 1/24 of the available space in its slot. This introduces an interesting trade-off, and as we show in 5, there are cases where it is better to use a second buffer and store the packets back-to-back. GASPP dynamically switches to the optimal approach by monitoring the actual utilization of the slots.</p>
    <p>The forwarding path requires the transmission of network packets after processing is completed, and this is achieved using a triple-pipeline solution, as shown in Figure 3. Both packet reception and transmission, as well as GPU data transfers and execution, are executed asynchronously in a multiplexed manner.</p>
    <p>Memory alignment is a major factor that affects the packet decoding process. GPU execution constrains memory accesses to be memory aligned for certain data types. For instance, int variables should be stored to addresses that are multiple of sizeof(int). However, due to the layered nature of network protocols, it is not feasible to align the fields of all encapsulated protocol headers in a packet. Instead, GASPP reads them from the global memory and stores them in GPU registers. Modern GPU architectures contain a large number of 32-bit registersthe GTX480 we used for this work contains about 480K such registers. Moreover, to utilize the memory more efficiently, we redesigned the input reading pro</p>
    <p>Hash key : 4 bytes</p>
    <p>Connection Table</p>
    <p>State : 1 byte</p>
    <p>Seq CLIENT : 4 bytes</p>
    <p>Seq SERVER : 4 bytes</p>
    <p>Next : 4 bytes</p>
    <p>Connection Record</p>
    <p>Connection Records</p>
    <p>Figure 4: Each connection record holds the minimum information for each connection, and is stored in a locking chained hash table.</p>
    <p>cess to fetch multiple bytes at time. The minimum size of device memory translations is 32 bytes, but the largest data type available is 16 bytes, i.e., an int4 variable. As such, input data are accessed in units of 16 bytes, decoded to their original data types, and stored to the appropriate registers.</p>
    <p>The stateful protocol analysis component of GASPP is designed with minimal complexity so as to maximize processing speed. This component is responsible for maintaining the state of TCP connections, and reconstructing the application-level byte stream by merging packet payloads and reordering out-of-order packets.</p>
    <p>GASPP uses an array stored in the global device memory of the GPU for keeping the state of TCP connections. Each record is 17-bytes long, as shown in Figure 4. A 4byte hash of the source and destination IP addresses and TCP ports is used to handle collisions in the flow classifier. Connection state is stored in a 1-byte variable. The sequence numbers of the most recently received client and server segments are stored in two 4-byte fields, and are updated every time the next in-order segment arrives. Hash table collisions are handled using a locking chained hash table with linked lists (described in detail in 3.5).</p>
    <p>Note that the connection table can easily fill up with adversarial partially-established connections, benign connections that stay idle for a long time, or connections that failed to terminate properly. For this reason, we periodically remove connection records that have been idle for more than a certain timeout, set to 60 seconds by default (configurable). Unfortunately, current GPU devices do not provide support for measuring real-world time, so we use a separate GPU kernel that is initiated by the host periodically according to the timeout value. Its task is to simply mark each connection record by setting the first bit of the state variable. If a connection record is already marked, it is removed from the table. A marked</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>Batch Size</p>
    <p>connec5on1 connec5on2 connec5on3</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>Batch Size</p>
    <p>Sequen6al processing</p>
    <p>connec5on1 connec5on2 connec5on3</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>Batch Size</p>
    <p>Sequen6al processing</p>
    <p>connec5on1 connec5on2 connec5on3</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>Sequen6al processing Packet-level</p>
    <p>parallel processing</p>
    <p>Batch Size</p>
    <p>connec5on1 connec5on2 connec5on3</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>Key insight  Packets &lt;A, B&gt; are consecu5ve if SeqB = (SeqA+lenA)</p>
    <p>Batch Size</p>
    <p>A B C</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>A A</p>
    <p>Batch Size</p>
    <p>A B C</p>
    <p>H(Seq) H(seq+len)</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>A A B B</p>
    <p>Batch Size</p>
    <p>A B C</p>
    <p>H(Seq) H(seq+len)</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>A A B B C C</p>
    <p>Batch Size</p>
    <p>A B C</p>
    <p>H(Seq) H(seq+len)</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>A A B B C C</p>
    <p>Batch Size</p>
    <p>A B C</p>
    <p>Parallel Processing</p>
  </div>
  <div class="page">
    <p>TCP Stream Reassembly</p>
    <p>A A B B C C</p>
    <p>next_packet:</p>
    <p>index: A B</p>
    <p>B C A</p>
    <p>Batch Size</p>
    <p>A B C</p>
    <p>C</p>
    <p>-</p>
  </div>
  <div class="page">
    <p>Other TCP corner cases</p>
    <p>TCP sequence holes</p>
    <p>Out-of-order packets</p>
  </div>
  <div class="page">
    <p>Other TCP corner cases</p>
    <p>TCP sequence holes</p>
    <p>Out-of-order packets</p>
  </div>
  <div class="page">
    <p>Evalua5on</p>
    <p>Forwarding  Latency  Individual Applica5ons  Consolidated applica5ons</p>
  </div>
  <div class="page">
    <p>Evalua5on Setup</p>
    <p>GASPP machine has:  2x NUMA nodes (Intel Xeon E5520 2.27GHz quad-core CPUs)  2x banks of 6GB of DDR3 1066MHz RAM  2x Intel 82599EB network adapters (with dual 10GbE ports)  2x NVIDIA GTX480 graphics cards</p>
    <p>Packet generator (4x 10GbE ports)</p>
    <p>GASPP machine (4x 10GbE ports)</p>
    <p>generated traffic</p>
    <p>forwarded traffic</p>
  </div>
  <div class="page">
    <p>Basic Forwarding</p>
    <p>Effective</p>
    <p>Packet size (bytes)</p>
    <p>T h ro</p>
    <p>u g h p</p>
    <p>u t (G</p>
    <p>b it /s</p>
    <p>)</p>
    <p>Figure 8: Data transfer throughput for different packet sizes when using two dual-port 10GbE NICs.</p>
    <p>packet buffer. However, small data transfers to the GPU incur significant penalties. Table 1 shows that for transfers of less than 4KB, the PCIe throughput falls below 7 Gbit/s. With a large buffer though, the transfer rate to the GPU exceeds 45 Gbit/s, while the transfer rate from the GPU to the host decreases to about 25 Gbit/s.1</p>
    <p>To overcome the low PCIe throughput, GASPP transfers batches of network packets to the GPU, instead of one at a time. However, as packets are placed in fixedsized slots, transferring many slots at once results in redundant data transfers when the slots are not fully occupied. As we can see in Table 2, when traffic consists of small packets, the actual PCIe throughput drops drastically. Thus, it is better to copy small network packets sequentially into another buffer, rather than transfer the corresponding slots directly. Direct transfer pays off only for packet sizes over 512 bytes (when buffer occupancy is over 512/1536 = 33.3%), achieving 47.8 Gbit/s for 1518-byte packets (a 2.3 speedup).</p>
    <p>Consequently, we adopted a simple selective offloading scheme, whereby packets in the shared buffer are copied to another buffer sequentially (in 16-byte aligned boundaries) if the overall occupancy of the shared buffer is sparse. Otherwise, the shared buffer is transferred directly to the GPU. Occupancy is computedwithout any additional overheadby simply counting the number of bytes of the newly arrived packets every time a new interrupt is generated by the NIC.</p>
    <p>Figure 8 shows the throughput for forwarding packets with all data transfers included, but without any GPU computations. We observe that the forwarding performance for 64-byte packets reaches 21 Gbit/s, out of the maximum 29.09 Gbit/s, while for large packets it reaches the maximum full line rate. We also observe that the GPU transfers of large packets are completely hidden on the Rx+GPU+Tx path, as they are performed in parallel using the pipeline shown in Figure 6, and thus they do not affect overall performance. Unfortunately, this is not the case for small packets (less than 128-bytes), which suffer an additional 29% hit due to memory contention.</p>
    <p>Having examined data transfer costs, we now evaluate the computational performance of a single GPU exluding all network I/O transfersfor packet decoding, connection state management, TCP stream reassembly, and some representative traffic processing applications.</p>
    <p>Packet Decoding. Decoding a packet according to its protocols is one of the most basic packet processing operations, and thus we use it as a base cost of our framework. Figure 9(a) shows the GPU performance for fully decoding incoming UDP packets into appropriately aligned structures, as described in 5.2 (throughput is very similar for TCP). As expected, the throughput increases as the number of packets processed in parallel increases. When decoding 64-byte packets, the GPU performance with PCIe transfers included, reaches 48 Mpps, which is about 4.5 times faster than the computational throughput of the tcpdump decoding process sustained by a single CPU core, when packets are read from memory. For 1518-byte packets, the GPU sustains about 3.8 Mpps and matches the performance of 1.92 CPU cores.</p>
    <p>Connection State Management and TCP Stream Re</p>
    <p>assembly. In this experiment we measure the performance of maintaining connection state on the GPU, and the performance of reassembling the packets of TCP flows into application-level streams. Figure 9(b) shows the packets processed per second for both operations. Test traffic consists of real HTTP connections with random IP addresses and TCP ports. Each connection fetches about 800KB from a server, and comprises about 870 packets (320 minimum-size ACKs, and 550 fullsize data packets). We also use a trace-driven workload (Equinix) based on a trace captured by CAIDAs equinix-sanjose monitor [3], in which the average and median packet length is 606.2 and 81 bytes respectively.</p>
    <p>Keeping state and reassembling streams requires several hashtable lookups and updates, which result to marginal overhead for a sufficient number of simultaneous TCP connections and the Equinix trace; about 20 25% on the raw GPU performance sustained for packet decoding, that increases to 4550% when the number of concurrent connections is low. The reason is that smaller numbers of concurrent connections result to lower parallelism. To compare with a CPU implementation, we measure the equivalent functionality of the Libnids TCP reassembly library [6], when packets are read from memory. Although Libnids implements more specific cases of the TCP stack processing, compared to GASPP, the network traces that we used for the evaluation enforce exactly the same functionality to be exercised. We can see that the throughput of a single CPU core is 0.55 Mpps, about 10 lower than the GPU version with all PCIe data transfers included.</p>
    <p>Effective</p>
    <p>Packet size (bytes)</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u t (G</p>
    <p>b it /s</p>
    <p>)</p>
    <p>Figure 8: Data transfer throughput for different packet sizes when using two dual-port 10GbE NICs.</p>
    <p>packet buffer. However, small data transfers to the GPU incur significant penalties. Table 1 shows that for transfers of less than 4KB, the PCIe throughput falls below 7 Gbit/s. With a large buffer though, the transfer rate to the GPU exceeds 45 Gbit/s, while the transfer rate from the GPU to the host decreases to about 25 Gbit/s.1</p>
    <p>To overcome the low PCIe throughput, GASPP transfers batches of network packets to the GPU, instead of one at a time. However, as packets are placed in fixedsized slots, transferring many slots at once results in redundant data transfers when the slots are not fully occupied. As we can see in Table 2, when traffic consists of small packets, the actual PCIe throughput drops drastically. Thus, it is better to copy small network packets sequentially into another buffer, rather than transfer the corresponding slots directly. Direct transfer pays off only for packet sizes over 512 bytes (when buffer occupancy is over 512/1536 = 33.3%), achieving 47.8 Gbit/s for 1518-byte packets (a 2.3 speedup).</p>
    <p>Consequently, we adopted a simple selective offloading scheme, whereby packets in the shared buffer are copied to another buffer sequentially (in 16-byte aligned boundaries) if the overall occupancy of the shared buffer is sparse. Otherwise, the shared buffer is transferred directly to the GPU. Occupancy is computedwithout any additional overheadby simply counting the number of bytes of the newly arrived packets every time a new interrupt is generated by the NIC.</p>
    <p>Figure 8 shows the throughput for forwarding packets with all data transfers included, but without any GPU computations. We observe that the forwarding performance for 64-byte packets reaches 21 Gbit/s, out of the maximum 29.09 Gbit/s, while for large packets it reaches the maximum full line rate. We also observe that the GPU transfers of large packets are completely hidden on the Rx+GPU+Tx path, as they are performed in parallel using the pipeline shown in Figure 6, and thus they do not affect overall performance. Unfortunately, this is not the case for small packets (less than 128-bytes), which suffer an additional 29% hit due to memory contention.</p>
    <p>Having examined data transfer costs, we now evaluate the computational performance of a single GPU exluding all network I/O transfersfor packet decoding, connection state management, TCP stream reassembly, and some representative traffic processing applications.</p>
    <p>Packet Decoding. Decoding a packet according to its protocols is one of the most basic packet processing operations, and thus we use it as a base cost of our framework. Figure 9(a) shows the GPU performance for fully decoding incoming UDP packets into appropriately aligned structures, as described in 5.2 (throughput is very similar for TCP). As expected, the throughput increases as the number of packets processed in parallel increases. When decoding 64-byte packets, the GPU performance with PCIe transfers included, reaches 48 Mpps, which is about 4.5 times faster than the computational throughput of the tcpdump decoding process sustained by a single CPU core, when packets are read from memory. For 1518-byte packets, the GPU sustains about 3.8 Mpps and matches the performance of 1.92 CPU cores.</p>
    <p>Connection State Management and TCP Stream Re</p>
    <p>assembly. In this experiment we measure the performance of maintaining connection state on the GPU, and the performance of reassembling the packets of TCP flows into application-level streams. Figure 9(b) shows the packets processed per second for both operations. Test traffic consists of real HTTP connections with random IP addresses and TCP ports. Each connection fetches about 800KB from a server, and comprises about 870 packets (320 minimum-size ACKs, and 550 fullsize data packets). We also use a trace-driven workload (Equinix) based on a trace captured by CAIDAs equinix-sanjose monitor [3], in which the average and median packet length is 606.2 and 81 bytes respectively.</p>
    <p>Keeping state and reassembling streams requires several hashtable lookups and updates, which result to marginal overhead for a sufficient number of simultaneous TCP connections and the Equinix trace; about 20 25% on the raw GPU performance sustained for packet decoding, that increases to 4550% when the number of concurrent connections is low. The reason is that smaller numbers of concurrent connections result to lower parallelism. To compare with a CPU implementation, we measure the equivalent functionality of the Libnids TCP reassembly library [6], when packets are read from memory. Although Libnids implements more specific cases of the TCP stack processing, compared to GASPP, the network traces that we used for the evaluation enforce exactly the same functionality to be exercised. We can see that the throughput of a single CPU core is 0.55 Mpps, about 10 lower than the GPU version with all PCIe data transfers included.</p>
    <p>Effective</p>
    <p>Packet size (bytes)</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u t (G</p>
    <p>b it /s</p>
    <p>)</p>
    <p>Figure 8: Data transfer throughput for different packet sizes when using two dual-port 10GbE NICs.</p>
    <p>packet buffer. However, small data transfers to the GPU incur significant penalties. Table 1 shows that for transfers of less than 4KB, the PCIe throughput falls below 7 Gbit/s. With a large buffer though, the transfer rate to the GPU exceeds 45 Gbit/s, while the transfer rate from the GPU to the host decreases to about 25 Gbit/s.1</p>
    <p>To overcome the low PCIe throughput, GASPP transfers batches of network packets to the GPU, instead of one at a time. However, as packets are placed in fixedsized slots, transferring many slots at once results in redundant data transfers when the slots are not fully occupied. As we can see in Table 2, when traffic consists of small packets, the actual PCIe throughput drops drastically. Thus, it is better to copy small network packets sequentially into another buffer, rather than transfer the corresponding slots directly. Direct transfer pays off only for packet sizes over 512 bytes (when buffer occupancy is over 512/1536 = 33.3%), achieving 47.8 Gbit/s for 1518-byte packets (a 2.3 speedup).</p>
    <p>Consequently, we adopted a simple selective offloading scheme, whereby packets in the shared buffer are copied to another buffer sequentially (in 16-byte aligned boundaries) if the overall occupancy of the shared buffer is sparse. Otherwise, the shared buffer is transferred directly to the GPU. Occupancy is computedwithout any additional overheadby simply counting the number of bytes of the newly arrived packets every time a new interrupt is generated by the NIC.</p>
    <p>Figure 8 shows the throughput for forwarding packets with all data transfers included, but without any GPU computations. We observe that the forwarding performance for 64-byte packets reaches 21 Gbit/s, out of the maximum 29.09 Gbit/s, while for large packets it reaches the maximum full line rate. We also observe that the GPU transfers of large packets are completely hidden on the Rx+GPU+Tx path, as they are performed in parallel using the pipeline shown in Figure 6, and thus they do not affect overall performance. Unfortunately, this is not the case for small packets (less than 128-bytes), which suffer an additional 29% hit due to memory contention.</p>
    <p>Having examined data transfer costs, we now evaluate the computational performance of a single GPU exluding all network I/O transfersfor packet decoding, connection state management, TCP stream reassembly, and some representative traffic processing applications.</p>
    <p>Packet Decoding. Decoding a packet according to its protocols is one of the most basic packet processing operations, and thus we use it as a base cost of our framework. Figure 9(a) shows the GPU performance for fully decoding incoming UDP packets into appropriately aligned structures, as described in 5.2 (throughput is very similar for TCP). As expected, the throughput increases as the number of packets processed in parallel increases. When decoding 64-byte packets, the GPU performance with PCIe transfers included, reaches 48 Mpps, which is about 4.5 times faster than the computational throughput of the tcpdump decoding process sustained by a single CPU core, when packets are read from memory. For 1518-byte packets, the GPU sustains about 3.8 Mpps and matches the performance of 1.92 CPU cores.</p>
    <p>Connection State Management and TCP Stream Re</p>
    <p>assembly. In this experiment we measure the performance of maintaining connection state on the GPU, and the performance of reassembling the packets of TCP flows into application-level streams. Figure 9(b) shows the packets processed per second for both operations. Test traffic consists of real HTTP connections with random IP addresses and TCP ports. Each connection fetches about 800KB from a server, and comprises about 870 packets (320 minimum-size ACKs, and 550 fullsize data packets). We also use a trace-driven workload (Equinix) based on a trace captured by CAIDAs equinix-sanjose monitor [3], in which the average and median packet length is 606.2 and 81 bytes respectively.</p>
    <p>Keeping state and reassembling streams requires several hashtable lookups and updates, which result to marginal overhead for a sufficient number of simultaneous TCP connections and the Equinix trace; about 20 25% on the raw GPU performance sustained for packet decoding, that increases to 4550% when the number of concurrent connections is low. The reason is that smaller numbers of concurrent connections result to lower parallelism. To compare with a CPU implementation, we measure the equivalent functionality of the Libnids TCP reassembly library [6], when packets are read from memory. Although Libnids implements more specific cases of the TCP stack processing, compared to GASPP, the network traces that we used for the evaluation enforce exactly the same functionality to be exercised. We can see that the throughput of a single CPU core is 0.55 Mpps, about 10 lower than the GPU version with all PCIe data transfers included.</p>
    <p>Effective</p>
    <p>Packet size (bytes)</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u t (G</p>
    <p>b it /s</p>
    <p>)</p>
    <p>Figure 8: Data transfer throughput for different packet sizes when using two dual-port 10GbE NICs.</p>
    <p>packet buffer. However, small data transfers to the GPU incur significant penalties. Table 1 shows that for transfers of less than 4KB, the PCIe throughput falls below 7 Gbit/s. With a large buffer though, the transfer rate to the GPU exceeds 45 Gbit/s, while the transfer rate from the GPU to the host decreases to about 25 Gbit/s.1</p>
    <p>To overcome the low PCIe throughput, GASPP transfers batches of network packets to the GPU, instead of one at a time. However, as packets are placed in fixedsized slots, transferring many slots at once results in redundant data transfers when the slots are not fully occupied. As we can see in Table 2, when traffic consists of small packets, the actual PCIe throughput drops drastically. Thus, it is better to copy small network packets sequentially into another buffer, rather than transfer the corresponding slots directly. Direct transfer pays off only for packet sizes over 512 bytes (when buffer occupancy is over 512/1536 = 33.3%), achieving 47.8 Gbit/s for 1518-byte packets (a 2.3 speedup).</p>
    <p>Consequently, we adopted a simple selective offloading scheme, whereby packets in the shared buffer are copied to another buffer sequentially (in 16-byte aligned boundaries) if the overall occupancy of the shared buffer is sparse. Otherwise, the shared buffer is transferred directly to the GPU. Occupancy is computedwithout any additional overheadby simply counting the number of bytes of the newly arrived packets every time a new interrupt is generated by the NIC.</p>
    <p>Figure 8 shows the throughput for forwarding packets with all data transfers included, but without any GPU computations. We observe that the forwarding performance for 64-byte packets reaches 21 Gbit/s, out of the maximum 29.09 Gbit/s, while for large packets it reaches the maximum full line rate. We also observe that the GPU transfers of large packets are completely hidden on the Rx+GPU+Tx path, as they are performed in parallel using the pipeline shown in Figure 6, and thus they do not affect overall performance. Unfortunately, this is not the case for small packets (less than 128-bytes), which suffer an additional 29% hit due to memory contention.</p>
    <p>Having examined data transfer costs, we now evaluate the computational performance of a single GPU exluding all network I/O transfersfor packet decoding, connection state management, TCP stream reassembly, and some representative traffic processing applications.</p>
    <p>Packet Decoding. Decoding a packet according to its protocols is one of the most basic packet processing operations, and thus we use it as a base cost of our framework. Figure 9(a) shows the GPU performance for fully decoding incoming UDP packets into appropriately aligned structures, as described in 5.2 (throughput is very similar for TCP). As expected, the throughput increases as the number of packets processed in parallel increases. When decoding 64-byte packets, the GPU performance with PCIe transfers included, reaches 48 Mpps, which is about 4.5 times faster than the computational throughput of the tcpdump decoding process sustained by a single CPU core, when packets are read from memory. For 1518-byte packets, the GPU sustains about 3.8 Mpps and matches the performance of 1.92 CPU cores.</p>
    <p>Connection State Management and TCP Stream Re</p>
    <p>assembly. In this experiment we measure the performance of maintaining connection state on the GPU, and the performance of reassembling the packets of TCP flows into application-level streams. Figure 9(b) shows the packets processed per second for both operations. Test traffic consists of real HTTP connections with random IP addresses and TCP ports. Each connection fetches about 800KB from a server, and comprises about 870 packets (320 minimum-size ACKs, and 550 fullsize data packets). We also use a trace-driven workload (Equinix) based on a trace captured by CAIDAs equinix-sanjose monitor [3], in which the average and median packet length is 606.2 and 81 bytes respectively.</p>
    <p>Keeping state and reassembling streams requires several hashtable lookups and updates, which result to marginal overhead for a sufficient number of simultaneous TCP connections and the Equinix trace; about 20 25% on the raw GPU performance sustained for packet decoding, that increases to 4550% when the number of concurrent connections is low. The reason is that smaller numbers of concurrent connections result to lower parallelism. To compare with a CPU implementation, we measure the equivalent functionality of the Libnids TCP reassembly library [6], when packets are read from memory. Although Libnids implements more specific cases of the TCP stack processing, compared to GASPP, the network traces that we used for the evaluation enforce exactly the same functionality to be exercised. We can see that the throughput of a single CPU core is 0.55 Mpps, about 10 lower than the GPU version with all PCIe data transfers included.</p>
    <p>CPU (8x cores) GASPP</p>
  </div>
  <div class="page">
    <p>Latency</p>
    <p>8192 batch:  CPU: 0.48 us  GASPP: 3.87 ms</p>
    <p>1024 batch: 0.49 ms  Same performance for basic forwarding  but 2x-4x throughput slowdown for heavyweight processing applica5ons</p>
  </div>
  <div class="page">
    <p>Individual Applica5ons</p>
    <p>Each applica5on is wri_en as a GPU kernel  No CPU-side development</p>
    <p>Speedup over a single CPU-core</p>
    <p>Applica[ons GASPP (8192 batch)</p>
    <p>GASPP (1024 batch)</p>
    <p>Firewall 3.6x 3.6x StringMatch 28.4x 9.3x RegExMatch 173.1x 36.9x AES 14.6x 6.5x</p>
  </div>
  <div class="page">
    <p>Consolida5ng Applica5ons</p>
    <p>Firewall StringMatch RegExMatch AES</p>
    <p>Firewall StringMatch RegExMatch</p>
    <p>Firewall StringMatch Firewall</p>
    <p>GASPP reduces irregular execu5on by 1.19-2.12X</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>What we offer:  Fast inter-device data transfers  GPU-based flow state management and stream reconstruc5on</p>
    <p>Efficient packet scheduling mechanisms</p>
    <p>Limita5ons  High packet processing latency</p>
  </div>
  <div class="page">
    <p>GASPP: A GPU-Accelerated Stateful Packet Processing Framework</p>
    <p>Giorgos Vasiliadis, FORTH-ICS, Greece Lazaros Koromilas, FORTH-ICS, Greece Michalis Polychronakis, Columbia University, USA So5ris Ioannidis, FORTH-ICS, Greece</p>
  </div>
</Presentation>
