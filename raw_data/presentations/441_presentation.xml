<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Mesos A Platform for Fine-Grained Resource Sharing in the Data Center</p>
    <p>Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony Joseph, Randy Katz, Scott Shenker, Ion Stoica University of California, Berkeley</p>
  </div>
  <div class="page">
    <p>Pig</p>
    <p>Background Rapid innovation in cluster computing frameworks</p>
    <p>Dryad</p>
    <p>Pregel</p>
    <p>Percolator</p>
    <p>CIEL!</p>
  </div>
  <div class="page">
    <p>Problem Rapid innovation in cluster computing frameworks</p>
    <p>No single framework optimal for all applications</p>
    <p>Want to run multiple frameworks in a single cluster  to maximize utilization  to share data between frameworks</p>
  </div>
  <div class="page">
    <p>Where We Want to Go</p>
    <p>Pregel</p>
    <p>MPI Shared cluster</p>
    <p>Today: static partitioning Mesos: dynamic sharing</p>
  </div>
  <div class="page">
    <p>Solution Mesos is a common resource sharing layer over which diverse frameworks can run</p>
    <p>Mesos</p>
    <p>Node Node Node Node</p>
    <p>Hadoop Pregel</p>
    <p>Node Node</p>
    <p>Hadoop</p>
    <p>Node Node</p>
    <p>Pregel</p>
  </div>
  <div class="page">
    <p>Other Benefits of Mesos</p>
    <p>Run multiple instances of the same framework  Isolate production and experimental jobs  Run multiple versions of the framework concurrently</p>
    <p>Build specialized frameworks targeting particular problem domains  Better performance than general-purpose abstractions</p>
  </div>
  <div class="page">
    <p>Outline Mesos Goals and Architecture</p>
    <p>Implementation</p>
    <p>Results</p>
    <p>Related Work</p>
  </div>
  <div class="page">
    <p>Mesos Goals High utilization of resources</p>
    <p>Support diverse frameworks (current &amp; future)</p>
    <p>Scalability to 10,000s of nodes</p>
    <p>Reliability in face of failures</p>
    <p>Resulting design: Small microkernel-like core that pushes scheduling logic to frameworks</p>
  </div>
  <div class="page">
    <p>Design Elements Fine-grained sharing:  Allocation at the level of tasks within a job  Improves utilization, latency, and data locality</p>
    <p>Resource offers:  Simple, scalable application-controlled scheduling mechanism</p>
  </div>
  <div class="page">
    <p>Element 1: Fine-Grained Sharing</p>
    <p>Framework 1</p>
    <p>Framework 2</p>
    <p>Framework 3</p>
    <p>Coarse-Grained Sharing (HPC): Fine-Grained Sharing (Mesos):</p>
    <p>+ Improved utilization, responsiveness, data locality</p>
    <p>Storage System (e.g. HDFS) Storage System (e.g. HDFS)</p>
    <p>Fw. 1</p>
    <p>Fw. 1 Fw. 3</p>
    <p>Fw. 3 Fw. 2 Fw. 2</p>
    <p>Fw. 2</p>
    <p>Fw. 1</p>
    <p>Fw. 3</p>
    <p>Fw. 2 Fw. 3</p>
    <p>Fw. 1</p>
    <p>Fw. 1 Fw. 2 Fw. 2</p>
    <p>Fw. 1</p>
    <p>Fw. 3 Fw. 3</p>
    <p>Fw. 3</p>
    <p>Fw. 2</p>
    <p>Fw. 2</p>
  </div>
  <div class="page">
    <p>Element 2: Resource Offers Option: Global scheduler  Frameworks express needs in a specification language, global scheduler matches them to resources</p>
    <p>+ Can make optimal decisions  Complex: language must support all framework needs  Difficult to scale and to make robust  Future frameworks may have unanticipated needs</p>
  </div>
  <div class="page">
    <p>Element 2: Resource Offers Mesos: Resource offers  Offer available resources to frameworks, let them pick which resources to use and which tasks to launch</p>
    <p>+ Keeps Mesos simple, lets it support future frameworks - Decentralized decisions might not be optimal</p>
  </div>
  <div class="page">
    <p>Mesos Architecture MPI job</p>
    <p>MPI scheduler</p>
    <p>Hadoop job</p>
    <p>Hadoop scheduler</p>
    <p>Allocation module</p>
    <p>Mesos master</p>
    <p>Mesos slave MPI</p>
    <p>executor</p>
    <p>Mesos slave MPI</p>
    <p>executor</p>
    <p>task task</p>
    <p>Resource offer</p>
    <p>Pick framework to offer resources to</p>
  </div>
  <div class="page">
    <p>Mesos Architecture MPI job</p>
    <p>MPI scheduler</p>
    <p>Hadoop job</p>
    <p>Hadoop scheduler</p>
    <p>Allocation module</p>
    <p>Mesos master</p>
    <p>Mesos slave MPI</p>
    <p>executor</p>
    <p>Mesos slave MPI</p>
    <p>executor</p>
    <p>task task</p>
    <p>Pick framework to offer resources to Resource</p>
    <p>offer</p>
    <p>Resource offer = list of (node, availableResources) E.g. { (node1, &lt;2 CPUs, 4 GB&gt;), (node2, &lt;3 CPUs, 2 GB&gt;) }</p>
  </div>
  <div class="page">
    <p>Mesos Architecture MPI job</p>
    <p>MPI scheduler</p>
    <p>Hadoop job</p>
    <p>Hadoop scheduler</p>
    <p>Allocation module</p>
    <p>Mesos master</p>
    <p>Mesos slave MPI</p>
    <p>executor Hadoop executor</p>
    <p>Mesos slave MPI</p>
    <p>executor</p>
    <p>task task</p>
    <p>Pick framework to offer resources to</p>
    <p>task Framework-specific</p>
    <p>scheduling</p>
    <p>Resource offer</p>
    <p>Launches and isolates executors</p>
  </div>
  <div class="page">
    <p>Optimization: Filters</p>
    <p>Let frameworks short-circuit rejection by providing a predicate on resources to be offered  E.g. nodes from list L or nodes with &gt; 8 GB RAM  Could generalize to other hints as well</p>
    <p>Ability to reject still ensures correctness when needs cannot be expressed using filters</p>
  </div>
  <div class="page">
    <p>Implementation</p>
  </div>
  <div class="page">
    <p>Implementation Stats 20,000 lines of C++</p>
    <p>Master failover using ZooKeeper</p>
    <p>Frameworks ported: Hadoop, MPI, Torque</p>
    <p>New specialized framework: Spark, for iterative jobs (up to 20 faster than Hadoop)</p>
    <p>Open source in Apache Incubator</p>
  </div>
  <div class="page">
    <p>Users Twitter uses Mesos on &gt; 100 nodes to run ~12 production services (mostly stream processing)</p>
    <p>Berkeley machine learning researchers are running several algorithms at scale on Spark</p>
    <p>Conviva is using Spark for data analytics</p>
    <p>UCSF medical researchers are using Mesos to run Hadoop and eventually non-Hadoop apps</p>
  </div>
  <div class="page">
    <p>Results  Utilization and performance vs static partitioning  Framework placement goals: data locality  Scalability  Fault recovery</p>
  </div>
  <div class="page">
    <p>Dynamic Resource Sharing</p>
  </div>
  <div class="page">
    <p>Mesos vs Static Partitioning</p>
    <p>Compared performance with statically partitioned cluster where each framework gets 25% of nodes</p>
    <p>Framework Speedup on Mesos Facebook Hadoop Mix 1.14 Large Hadoop Mix 2.10 Spark 1.26 Torque / MPI 0.96</p>
  </div>
  <div class="page">
    <p>Ran 16 instances of Hadoop on a shared HDFS cluster</p>
    <p>Used delay scheduling [EuroSys 10] in Hadoop to get locality (wait a short time to acquire data-local nodes)</p>
    <p>Data Locality with Resource Offers</p>
    <p>Static Partitioning</p>
    <p>Mesos</p>
    <p>Local Map Tasks (%)</p>
    <p>Static Partitioning</p>
    <p>Mesos</p>
    <p>Job Duration (s)</p>
  </div>
  <div class="page">
    <p>Scalability Mesos only performs inter-framework scheduling (e.g. fair sharing), which is easier than intra-framework scheduling</p>
    <p>Ta sk</p>
    <p>S ta rt O ve</p>
    <p>rh ea</p>
    <p>d (s</p>
    <p>)</p>
    <p>Number of Slaves</p>
    <p>Result: Scaled to 50,000 emulated slaves, 200 frameworks, 100K tasks (30s len)</p>
  </div>
  <div class="page">
    <p>Fault Tolerance Mesos master has only soft state: list of currently running frameworks and tasks</p>
    <p>Rebuild when frameworks and slaves re-register with new master after a failure</p>
    <p>Result: fault detection and recovery in ~10 sec</p>
  </div>
  <div class="page">
    <p>Related Work HPC schedulers (e.g. Torque, LSF, Sun Grid Engine)  Coarse-grained sharing for inelastic jobs (e.g. MPI)</p>
    <p>Virtual machine clouds  Coarse-grained sharing similar to HPC</p>
    <p>Condor  Centralized scheduler based on matchmaking</p>
    <p>Parallel work: Next-Generation Hadoop  Redesign of Hadoop to have per-application masters  Also aims to support non-MapReduce jobs  Based on resource request language with locality prefs</p>
  </div>
  <div class="page">
    <p>Conclusion Mesos shares clusters efficiently among diverse frameworks thanks to two design elements:  Fine-grained sharing at the level of tasks  Resource offers, a scalable mechanism for application-controlled scheduling</p>
    <p>Enables co-existence of current frameworks and development of new specialized ones</p>
    <p>In use at Twitter, UC Berkeley, Conviva and UCSF</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Framework Isolation Mesos uses OS isolation mechanisms, such as Linux containers and Solaris projects</p>
    <p>Containers currently support CPU, memory, IO and network bandwidth isolation</p>
    <p>Not perfect, but much better than no isolation</p>
  </div>
  <div class="page">
    <p>Analysis Resource offers work well when:  Frameworks can scale up and down elastically  Task durations are homogeneous  Frameworks have many preferred nodes</p>
    <p>These conditions hold in current data analytics frameworks (MapReduce, Dryad, )  Work divided into short tasks to facilitate load balancing and fault recovery  Data replicated across multiple nodes</p>
  </div>
  <div class="page">
    <p>Revocation Mesos allocation modules can revoke (kill) tasks to meet organizational SLOs</p>
    <p>Framework given a grace period to clean up</p>
    <p>Guaranteed share API lets frameworks avoid revocation by staying below a certain share</p>
  </div>
  <div class="page">
    <p>Mesos API Scheduler Callbacks</p>
    <p>resourceOffer(offerId, offers) offerRescinded(offerId) statusUpdate(taskId, status) slaveLost(slaveId)</p>
    <p>Executor Callbacks</p>
    <p>launchTask(taskDescriptor) killTask(taskId)</p>
    <p>Executor Actions</p>
    <p>sendStatus(taskId, status)</p>
    <p>Scheduler Actions</p>
    <p>replyToOffer(offerId, tasks) setNeedsOffers(bool) setFilters(filters) getGuaranteedShare() killTask(taskId)</p>
  </div>
</Presentation>
