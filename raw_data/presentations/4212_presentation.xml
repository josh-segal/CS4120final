<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Guiding Extractive Summarization with Question-Answering Rewards</p>
    <p>Kristjan Arumae and Fei Liu Computer Science Department</p>
    <p>University of Central Florida, Orlando, FL 32816</p>
    <p>June 4, 2019</p>
  </div>
  <div class="page">
    <p>Summary Usefulness</p>
    <p>Republicans run down Obama's clock</p>
    <p>GOP hogs the spotlight with funding deadlines like the battle over money for the ______________________________.</p>
    <p>He says the continual crises deprive _______ of the chance to move his agenda forward even slightly.</p>
    <p>(CNN) It looks like the Republicans in Congress have failed again. House Republicans defeated a plan pushed by Senate Majority Leader Mitch McConnell to fund the Department of Homeland Security, money that congressional Republicans have been holding hostage in their effort to overturn President Obama's executive order on immigration.</p>
    <p>McConnell proposed that there would be a separate vote on the immigration issue. When Speaker John Boehner proposed an even narrower compromise, funding the Department for only three more weeks, his caucus said no. The final bill provides funding for one more week, at which point Congress needs to take up the issue again. []</p>
  </div>
  <div class="page">
    <p>Summary Usefulness</p>
    <p>Republicans run down Obama's clock</p>
    <p>(CNN) It looks like the Republicans in Congress have failed again. House Republicans defeated a plan pushed by Senate Majority Leader Mitch McConnell to fund the Department of Homeland Security, money that congressional Republicans have been holding hostage in their effort to overturn President Obama's executive order on immigration.</p>
    <p>McConnell proposed that there would be a separate vote on the immigration issue. When Speaker John Boehner proposed an even narrower compromise, funding the Department for only three more weeks, his caucus said no. The final bill provides funding for one more week, at which point Congress needs to take up the issue again. []</p>
    <p>GOP hogs the spotlight with funding deadlines like the battle over money for the Department of Homeland Security.</p>
    <p>He says the continual crises deprive Obama of the chance to move his agenda forward even slightly.</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Our system seeks to identify salient and consecutive sequences of words from the source document to assist users in comprehending lengthy documents.</p>
    <p>We hypothesize that quality extractive summaries should contain informative content so that they can be used as document surrogates.</p>
    <p>We investigate a new strategy that seeks to better utilize human abstracts to guide the extraction of summary text units.</p>
    <p>To accomplish this we utilize a reinforcement learning framework to explore the space of possible extractive summaries to answer important questions.</p>
  </div>
  <div class="page">
    <p>Overview 1. Representing an extraction unit.</p>
  </div>
  <div class="page">
    <p>Representing an Extraction Unit We obtain text chunks by breaking down constituent parse tree until each fragment governs at most 5 words.</p>
  </div>
  <div class="page">
    <p>Bidirectional Recurrent Encoder</p>
  </div>
  <div class="page">
    <p>Constructing an Extractive Summary It is desirable to first develop a supervised framework for identifying summaryworthy text segments from a source article.</p>
    <p>The task can be formulated as a sequence labeling problem.</p>
    <p>We build a framework to extract summary units where the importance of the t-th source unit is characterized by</p>
    <p>informativeness</p>
    <p>position in the document</p>
    <p>relationship with the partial summary</p>
  </div>
  <div class="page">
    <p>Summary Encoding</p>
    <p>Given</p>
    <p>informativeness:</p>
    <p>position in the document:</p>
    <p>relationship with the partial summary:</p>
    <p>We employ a multilayer perceptron to predict how likely the unit is to be included in the summary.</p>
  </div>
  <div class="page">
    <p>Question Answering Question-answer (QA) pairs can be conveniently developed from human abstracts.</p>
    <p>CNN/Daily Mail.</p>
    <p>For any sentence in the human abstract, we identify an answer token from it, then replace the answer token with a blank to create a cloze-style QA pair.</p>
    <p>We set an answer token to be either a salient word or a named entity to limit the space of potential answers.</p>
  </div>
  <div class="page">
    <p>Question Answering Model Given an extractive summary containing a set of source text units, and a collection of question-answer pairs we develop a mechanism leveraging the summary to answer these questions (Chen et al. 2016).</p>
    <p>With an attention driven system, an extractive summary can be used to answer multiple questions related to the document.</p>
  </div>
  <div class="page">
    <p>Question Answering Model</p>
    <p>Given an extractive summary containing a set of source text units, and a collection of question-answer pairs we develop a mechanism leveraging the summary to answer these questions (Chen et al. 2016).</p>
    <p>With an attention driven system, an extractive summary can be used to answer multiple questions related to the document.</p>
  </div>
  <div class="page">
    <p>Question Answering Model Given an extractive summary containing a set of source text units, and a collection of question-answer pairs we develop a mechanism leveraging the summary to answer these questions (Chen et al. 2016).</p>
    <p>With an attention driven system, an extractive summary can be used to answer multiple questions related to the document.</p>
  </div>
  <div class="page">
    <p>A Reinforcement Learning Framework We introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function.</p>
    <p>The reward promotes summaries that are adequate, fluent, restricted in length, and competent in question answering.</p>
  </div>
  <div class="page">
    <p>A Reinforcement Learning Framework We introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function.</p>
    <p>The reward promotes summaries that are adequate, fluent, restricted in length, and competent in question answering.</p>
  </div>
  <div class="page">
    <p>A Reinforcement Learning Framework We introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function.</p>
    <p>The reward promotes summaries that are adequate, fluent, restricted in length, and competent in question answering.</p>
  </div>
  <div class="page">
    <p>A Reinforcement Learning Framework We introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function.</p>
    <p>The reward promotes summaries that are adequate, fluent, restricted in length, and competent in question answering.</p>
  </div>
  <div class="page">
    <p>A Reinforcement Learning Framework We introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function.</p>
    <p>The reward promotes summaries that are adequate, fluent, restricted in length, and competent in question answering.</p>
  </div>
  <div class="page">
    <p>A Reinforcement Learning Framework Training the system with policy gradient involves repeatedly sampling an extractive summary from the source document (Lei et al. 2016).</p>
    <p>At time t, the agent takes an action by sampling a decision based on indicating whether the t-th source text unit is to be included in the summary.</p>
  </div>
  <div class="page">
    <p>Recap 1. Representing an extraction unit.</p>
  </div>
  <div class="page">
    <p>Experimental Results (CNN)</p>
    <p>System R-1 R-2 R-L #Ans</p>
    <p>QASumm (No QA) 16.38 7.25 11.30</p>
    <p>QASumm + SUBJ/OBJ 26.16 8.97 18.24 9,893</p>
    <p>QASumm + ROOT 26.67 9.19 18.76 3,678</p>
    <p>QASumm + NER 27.38 9.38 19.02 6,167</p>
    <p>Models outperform the counterpart QASumm (No QA) that makes no use of the QA pairs by a substantial margin.</p>
  </div>
  <div class="page">
    <p>Experimental Results (Daily Mail)</p>
    <p>System R-1 R-2 R-L #Ans</p>
    <p>QASumm (No QA) 22.26 9.16 19.78</p>
    <p>QASumm + SUBJ/OBJ 23.38 9.54 20.14 19,151</p>
    <p>QASumm + ROOT 26.87 11.97 23.07 5,498</p>
    <p>QASumm + NER 25.74 11.98 22.38 15,342</p>
    <p>We conjecture that maintaining a moderate number of answers is important to maximize performance.</p>
  </div>
  <div class="page">
    <p>Question Answering Results</p>
    <p>No Text QASumm (no QA) Gold Summ Full Text</p>
    <p>Answer Type Train Dev Train Dev Train Dev Train Dev</p>
    <p>SUBJ/OBJ 49.7 24.4 55.9 31.2 69.3 48.6 67.6 43.3</p>
    <p>ROOT 68.1 34.9 71.6 36.3 76.9 44.9 76.0 35.7</p>
    <p>NER 61.0 15.8 66.0 32.7 85.2 54.0 82.4 46.3</p>
    <p>We observe that question-answering with Gold Summ performs the best for all QA types.</p>
    <p>The results suggest that extractive summaries with even modest ROUGE scores can prove useful for question-answering.</p>
  </div>
  <div class="page">
    <p>Human Evaluation We conducted a human evaluation to assess whether the highlighted summaries contribute to document understanding. (Amazon Mechanical Turk)</p>
    <p>We presented each participant with the document and fill-in-the-blank questions created from the human abstracts.</p>
    <p>VS.</p>
  </div>
  <div class="page">
    <p>Human Evaluation We compare our reinforced extracted summary (presented as a bold overlay to the document), against our supervised method, abstractive summaries generated by See et al. (2017), and the human abstracts in full.</p>
    <p>Additionally we asked the participants to rate the quality of the summary presented (1-5, with 5 being most informative).</p>
    <p>VS.</p>
  </div>
  <div class="page">
    <p>Human Evaluation</p>
    <p>Summary Type Time Acc. Inform.</p>
    <p>Human Abstract 69.5s 87.3 4.23</p>
    <p>QASumm (No QA) 87.9s 53.7 1.76</p>
    <p>Pointer + Cov (See et al. 2017) 100.9s 52.3 2.14</p>
    <p>QASumm +NER 95.0s 62.3 2.14</p>
    <p>Although participants rated the informativeness of the summaries to be the same our systems yielded a higher performance.</p>
  </div>
  <div class="page">
    <p>Conclusion We exploited an extractive summarization framework using deep reinforcement learning to identify word sequences from a document to form a summary.</p>
    <p>Our reward function promotes fluent summaries that can serve as document surrogates to answer important questions.</p>
    <p>Experimental results on benchmark data demonstrated the efficacy of our proposed method, assessed by both automatic metrics and human evaluators.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>github.com/ucfnlp/summ_qa_rewards</p>
    <p>Thank you!</p>
  </div>
</Presentation>
