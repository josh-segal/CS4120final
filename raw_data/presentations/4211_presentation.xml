<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Adversarial Category Alignment Network for Cross-domain Sentiment Classification</p>
    <p>Xiaoye Qu*, Zhikang Zou*, Yu Cheng, Yang Yang, Pan Zhou</p>
    <p>NAACL 2019</p>
    <p>Sentiment polarity</p>
    <p>* equal contributions</p>
    <p>xiaoye@hust.edu.cn</p>
    <p>Huazhong University of Science and Technology</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 2/20</p>
    <p>Problem</p>
    <p>Setting: Cross-domain</p>
    <p>Source domain (Electronics)  Target domain (Books)</p>
    <p>Goal: Train a task-specific classifier using both labeled source samples and unlabeled target</p>
    <p>samples that generalizes well to the target domain.</p>
    <p>Training</p>
    <p>Source (labeled)</p>
    <p>Target (unlabeled )</p>
    <p>high quality fifth the blades are built into the stainless steel container</p>
    <p>a lot of other big idea books this one is very literate and readable</p>
    <p>Classifier</p>
    <p>Testing</p>
    <p>Target</p>
    <p>I fail to understand the complaints of two previous reviewers</p>
    <p>Classifier Positive/ Negative ?</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 3/20</p>
    <p>Key Challenges</p>
    <p>Words express emotional tendency vary across domains</p>
    <p>Electronics: incompatible, blurry</p>
    <p>Kitchen: delicious, tasty How to bridge the divergence? generalization</p>
    <p>Use unlabeled data to improve the generalization of supervised learning task and better align the features.</p>
    <p>How to align the source and target features?</p>
  </div>
  <div class="page">
    <p>Previous methods</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 4/20</p>
    <p>Pivot-based works</p>
    <p>Infer the correlation between pivot words and non-pivot words by utilizing multiple pivot prediction tasks.</p>
    <p>domain-shared sentiment words e.g. interesting</p>
    <p>domain-specific sentiment words</p>
    <p>Learning domain invariant features should be similar in source and target domain</p>
    <p>Minimize the discrepancy between domain-specific latent feature representations</p>
    <p>design different distance functions</p>
    <p>KL-divergence, MMD, CMD,</p>
    <p>capture the emotional tendency of non-pivot words</p>
  </div>
  <div class="page">
    <p>Previous methods</p>
    <p>Adversarial learning methods</p>
    <p>Using domain classifier to align distributions between two feature domains</p>
    <p>Domain adversarial training of neural networks Ganin et al., 2017</p>
    <p>Training</p>
    <p>Source (labeled)</p>
    <p>Target (unlabeled )</p>
    <p>high quality fifth the blades are built into the stainless steel container</p>
    <p>a lot of other big idea books this one is very literate and readable</p>
    <p>Classifier</p>
    <p>Domain Classifier</p>
    <p>Positive or Negative?</p>
    <p>Source or Target?</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 5/20</p>
  </div>
  <div class="page">
    <p>Previous methods</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 6/20</p>
    <p>Problem exist with the domain classifier  Not consider task-specific decision boundary between classes</p>
    <p>Simply distinguish the features as a source or target (Global marginal alignment)</p>
    <p>Generate ambiguous features near class boundary</p>
    <p>decision boundary</p>
    <p>ambiguous features</p>
    <p>After adaptation</p>
    <p>hinder the performance</p>
  </div>
  <div class="page">
    <p>Our motivation</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 7/20</p>
    <p>Locate ambiguous features and boost the adaptation performance</p>
    <p>Train two task-specific classifiers as diverse views</p>
    <p>Locate ambiguous features</p>
    <p>create more discriminative features away from the decision boundary</p>
    <p>More discriminative features</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 8/20</p>
    <p>Goal and assumptions</p>
    <p>Train two sentiment classifiers  Global marginal alignment  Locate ambiguous features  Create more discriminative features</p>
    <p>Category alignment</p>
    <p>Our goals:</p>
    <p>Semi-supervised assumptions:  Cluster assumption: The optimal predictor is constant or smooth on the connected high-density regions. The decision boundary tends to cross low-density region instead of high-density region.  Manifolds assumption: Supporting set data lies on low-dimensional manifolds. Features in a local neighborhood is similar. It focuses more on local smoothness.</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 9/20</p>
    <p>Notations</p>
    <p>: labeled source examples</p>
    <p>: unlabeled target examples</p>
    <p>K : number of different polarities</p>
    <p>G : feature generator</p>
    <p>F1, F2 : two different task-specific classifiers</p>
    <p>p1(y|x), p2(y|x) : probabilistic outputs for input sample</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Networks</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 10/20</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 11/20</p>
    <p>Training Steps</p>
    <p>Step 1: Marginal Distribution Alignment</p>
    <p>Train generator and two classifier to classify the source samples correctly, at the same time, minimize the divergence between the source samples and target samples.</p>
    <p>Total loss  L1 = Lcls 1Lkl</p>
    <p>KL-divergence</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 12/20</p>
    <p>Training Steps</p>
    <p>Step 2: Locate ambiguous features</p>
    <p>Fix generator, update classifier by maximizing the discrepancy between two classifiers.</p>
    <p>Maximize Discrepancy</p>
    <p>Features in high-density region have same sentiment under two classifiers</p>
    <p>Locate more ambiguous features in low-density region</p>
    <p>L2 = Lcls -2Ldis</p>
  </div>
  <div class="page">
    <p>Training Steps</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 13/20</p>
    <p>Step 3: Generate more discriminative features</p>
    <p>Fix classifier, update generator by minimizing the discrepancy between two classifiers. Generate features into high-density region where two classifiers have same sentiment prediction.</p>
    <p>Minimize Discrepancy</p>
    <p>Generated into h igh-density regio</p>
    <p>n</p>
    <p>L3 = Lcls + 3Ldis</p>
  </div>
  <div class="page">
    <p>Training Steps</p>
    <p>Step 3: Generate more discriminative features</p>
    <p>Further enhance the feature generator, regularize generator with information of unlabeled target data. We consider the similarity and connections between samples.</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 14/20</p>
    <p>regularize generator</p>
    <p>Considering the local smoothness</p>
    <p>Pseudo label correlation</p>
    <p>L3 = Lcls+3Ldis+</p>
    <p>Pseudo label is not accurate at the beginning</p>
  </div>
  <div class="page">
    <p>Adversarial learning</p>
    <p>Maximize discrepancy and minimize discrepancy between two classifiers</p>
    <p>The training process of the whole adversarial learning</p>
    <p>Locate features in low-density region and generate to high-density region</p>
    <p>Training by batch, conducting step1, step2, step3 iteratively</p>
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 15/20</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 16/20</p>
    <p>Experiment on classification</p>
    <p>Experiments on the Amazon benchmark by 5-fold cross-validation</p>
    <p>ACAN obtains a state-of-the-art result  Rising trend from Baseline to ACAN</p>
    <p>Progressive rising</p>
    <p>KL-divergenceCategory alignmentGenerator regularization</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 17/20</p>
    <p>Case study</p>
    <p>Comparison of the top trigrams chosen from 10 most related CNN filters learned on the task BE. * denotes a padding. The domain-specific words are in bold.</p>
    <p>ACAN is able to extract the domain-specific words</p>
    <p>Baseline tends to extract the domain-independent words</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 18/20</p>
    <p>PCA Visualization Visualize the representation of source training data and target testing data for KE task.</p>
    <p>The red, blue, green, and black points denote the source positive, source negative, target positive, and target negative examples correspondingly.</p>
    <p>ACAN-KL ACAN global marginal alignment</p>
    <p>Low-density region Category alignment</p>
    <p>Significantly suppress points in low-density region</p>
  </div>
  <div class="page">
    <p>Xiaoye Qu (Huazhong University of Science and Technology ) June 4, 2019 19/20</p>
    <p>Conclusion</p>
    <p>We propose a new approach for category-level feature alignment. This adversarial learning method is based on two semi-supervised assumptions.</p>
    <p>We obtain a state-of-the-art result on cross-domain sentiment classification.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>xiaoye@hust.edu.cn</p>
  </div>
</Presentation>
