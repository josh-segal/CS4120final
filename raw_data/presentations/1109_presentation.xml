<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MDev-NVMe: A NVMe Storage Virtualization Solution with Mediated Pass-Through</p>
    <p>Bo Peng1,2,Haozhong Zhang2, Jianguo Yao1, Yaozu Dong2, Yu Xu1, Haibing Guan1</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background Motivation Design Implementation Experiments Discussion Conclusion</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>NVMe Protocol  Design a logical device interface via a PCIe bus;  Deliver I/O operations with high throughput and low latency;  Admin or I/O Commandsand Queues, Doorbells;  Submission Queue (SQ) and Completion Queue (CQ);</p>
    <p>Usage of NVMe SSDs  Storage for data-intensive datacenters;  accelerate I/O between system storage and other PCIe devices, such as GPUs (BERGMAN, S., Spin, ATC17)</p>
    <p>Admin Submission Queue</p>
    <p>Admin Completion Queue</p>
    <p>I/O Submission Queue</p>
    <p>I/O Completion Queue</p>
    <p>Controller Management Core 0 Core 1 Core n</p>
    <p>NVMe Device</p>
    <p>IRQIRQIRQIRQ</p>
    <p>Host Kernel</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>I/O Virtualization  Optimize utilization of physical storage resource  Simplify storage management  Provide a simple and consistent interface</p>
    <p>NVMe Virtualization  Blind Mode  Virtual Mode  Physical Mode</p>
    <p>NVMe MainstreamVirtualization  VirtIO, Userspacedriver inQEMU(Fam zheng), SPDK</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Requirement of High-performance Virtualization for NVMe  High Throughput</p>
    <p>Quick data access and parallel processing  Low Latency</p>
    <p>Virtualization brings latency overhead when context switches happen  Device Sharing</p>
    <p>Separate VMs on one device with good scalability  Device Feature</p>
    <p>Guest uses original device drivers.  Migration Support</p>
    <p>live migration relies on hypervisor management</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Different NVMe Comparison</p>
    <p>Virtio Front-end drivers</p>
    <p>DMA Buffer</p>
    <p>Virtio Back-end drivers QEMU &amp; KVM</p>
    <p>Data Buffer</p>
    <p>VFS/File System</p>
    <p>Block Layer</p>
    <p>I/O Scheduler</p>
    <p>NVMe Driver</p>
    <p>NVMe Storage Device</p>
    <p>VM</p>
    <p>Host</p>
    <p>NVMe Driver</p>
    <p>DMA Buffer</p>
    <p>VFIO-pci</p>
    <p>NVMe Storage Device</p>
    <p>VM</p>
    <p>Host</p>
    <p>NVMe Driver DMA Buffer</p>
    <p>VFIO-MDev</p>
    <p>NVMe Storage Device</p>
    <p>VM</p>
    <p>Host</p>
    <p>MDev-NVMe driver</p>
    <p>VirtIO VFIO MDev-NVMe</p>
    <p>Virtio suffers readily apparent overhead from software layers.</p>
    <p>VFIO can not meet the requirements of device sharing.</p>
    <p>MDev-NVMe module manage all the statuses of queues with VFIOMDev interface.</p>
  </div>
  <div class="page">
    <p>Design</p>
    <p>Mediated Pass-through (MPT)  An intermediate between direct pass-through and full emulated virtual device  Pass through performance-critical resource but traps and emulates privileged operations and resource</p>
    <p>Famous MPT research (MPT on GPU)  Kun Tian, et al: A Full GPU Virtualization Solution with Mediated Pass-Through. (ATC 2014)</p>
    <p>Yaozu Dong, et al :Boosting GPU Virtualization Performance with Hybrid Shadow Page Tables. (ATC 2015)</p>
    <p>Mochi Xue, et al: gScale: Scaling up GPU Virtualization with Dynamic Sharing of Graphics Memory Space. (ATC 2016)</p>
  </div>
  <div class="page">
    <p>Design</p>
    <p>Architecture  A Full Virtualization  MDev-NVMe Module  Native Drivers in Guests</p>
    <p>TechnicalPoints:  Device Emulation for PCIe</p>
    <p>PCI registers &amp; BAR  NVMe registers &amp; logics  IRQ: INTx, MSI, MSI-X</p>
    <p>Admin Queue Emulation  Resource emulation</p>
    <p>I/O Queue Shadow  Resource pass-through</p>
    <p>DMA Buffer Access</p>
    <p>Admin Submission Queue</p>
    <p>Admin Completion Queue</p>
    <p>I/O Submission Queue</p>
    <p>I/O Completion Queue</p>
    <p>NVMe Device</p>
    <p>IRQIRQIRQ</p>
    <p>Group 0 Group 1 Group n</p>
    <p>DMA Buffer</p>
    <p>Physical I/O SQ/CQ</p>
    <p>VM0</p>
    <p>Host I/O Queue DMA &amp; LBA translation</p>
    <p>DMA Buffer</p>
    <p>MDev-NVMe Module</p>
    <p>Direct Access</p>
    <p>IRQ</p>
    <p>IRQIRQIRQMMIO MMIO MMIO</p>
    <p>Virtual SQ/CQ Virtual SQ/CQVM1</p>
    <p>Device Emualation</p>
    <p>Admin Queue Emulation</p>
    <p>Physical Admin SQ/CQ</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Queue Handling  Emulation for Admin Queues  Queue Shadow from guest to host queues</p>
    <p>Q</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>DMA &amp; LBA translation  Data pointer (DPTR) in IO commands points to a DMA buffer</p>
    <p>The address is Guest Physical Address (GPA)  vfio_pin_pagestranslates GPA to HPA and pin the DMA buffer in host memory</p>
    <p>Start LBA is specified by SLBA in IO commands</p>
  </div>
  <div class="page">
    <p>implementation</p>
    <p>Each IO transaction requires an MMIO write to the doorbell and VM exit  IRQ becomes a big delay when IO is intensive</p>
    <p>Host</p>
    <p>Host</p>
    <p>Host</p>
    <p>Host</p>
    <p>N VM</p>
    <p>e D evice</p>
    <p>Guest SQ</p>
    <p>Guest CQ</p>
    <p>MMIO VM Exit</p>
    <p>Poll vSQ Doorbell</p>
    <p>IRQ</p>
    <p>Poll CQ DoorbellPolling mode</p>
    <p>Trap &amp; Interrupt MMIO</p>
    <p>MMIO</p>
    <p>vIRQ</p>
    <p>vIRQ</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Hardware Configuration  A server with 2 Intel Xeon CPU E5-2699 v3, 64GB system memory  Intel OPTANE DC P4800X SSD (375G)  INTEL SSD DC P3600 (400G)</p>
    <p>Application benchmark  Flexible I/O tester (FIO), 4K random read or write.  libaio as the fio engine, O_DIRECT for Direct I/O experiments</p>
    <p>Comparison Virtualization mechanism  Virtio, FamzUserspace Driver in QEMU, SPDK (vhost-scsi, vhost-blk)</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Throughput (IOPS)</p>
    <p>OptaneP4800 P3600</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Average Latency</p>
    <p>OptaneP4800 P3600</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>QoS</p>
    <p>Optane P4800 P3600</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Device Sharing with good scalability</p>
  </div>
  <div class="page">
    <p>Experiments Influence of I/O Blocksize</p>
    <p>Optane P4800</p>
    <p>P3600</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Importance of polling (polling VS. interrupts)  The main agreement of MDev-NVMe and SPDK is taking active polling  The trap of the vm-exit needs additional context switches  polling is necessary when the I/O workloads are aggressive  To support adaptive polling with a mild policy for I/O acceleration</p>
    <p>We expect that high-performance I/O device will be designed with components to actively support or cooperate with the polling.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>MDev-NVMe is a full NVMe storage virtualization mechanism which takes a mediated pass-through as the main implementation. MDev-NVMe presents Admin queue emulations and I/O queues shadowing, achieving high throughput, low latency performance and a reliable scalability with device sharing feature. We offer large numbers of Fio benchmarks to provides evidence for MDev-NVMe, and give research conclusion about the all mainstream NVMe virtualization mechnisms. We raise focus and consideration into the influence of blocksize. We raise a discussion about polling vs. interrupts.</p>
  </div>
  <div class="page">
    <p>Thank youQuestions?</p>
  </div>
</Presentation>
