<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Incremental workflow improvement through analysis of its data provenance</p>
    <p>Paolo Missier School of Computing,</p>
    <p>Newcastle University, UK</p>
    <p>In collaboration with the eScience Central group at Newcastle: Prof. Paul Watson, Dr. Simon Woodman, Dr Hugo Hiden, Dr. Jacek Cala</p>
    <p>TAPP11 workshop Heraklion, Crete June 20-21, 2011</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Despite the growing momentum around provenance as a premiere form of metadata,</p>
    <p>success exploitation stories trail models and technology advances</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>We are getting very good at recording provenance of data:  multiple data models (OPM, Provenir, Janus, Karma, PML,...)  provenance-aware system / service architectures ...</p>
    <p>PASS, Karma  ... and workflows</p>
    <p>Kepler, Taverna, Galaxy, VisTrails,...</p>
    <p>But, what are systems/applications really doing with it?  deliver value to users? i.e., in e-science, in the Web</p>
    <p>scientific reproducibility, quality, trust  optimize system analysis, performance?</p>
    <p>enable partial re-run of resource-intensive processes 2</p>
    <p>Despite the growing momentum around provenance as a premiere form of metadata,</p>
    <p>success exploitation stories trail models and technology advances</p>
  </div>
  <div class="page">
    <p>Broad goal and specific research context</p>
    <p>An opportunistic starting point: optimization of resource-intensive, repetitive, provenance-aware escience workflows</p>
    <p>[1] IBM. An architectural blueprint for autonomic computing. Tech. rep., IBM, 2011 [2] Huebscher, M. C., and McCann, J. A. A survey of autonomic computing - degrees, models, and applications. ACM Computing Surveys CSUR 40, 3 (2008), 128.</p>
    <p>A systematic study on methods and applications of mining / learning techniques applied to large corpora of provenance metadata</p>
    <p>Reference framework: adaptive, self-managing software systems  systems that can dynamically adapt their behaviour in response to changing conditions in</p>
    <p>the inputs or in their environment [1,2]</p>
    <p>sequent states of the managed element, and might also include forecasts of any external processes that generate workflow or traffic processed by the managed element.</p>
    <p>A key factor limiting rapid adoption and wide usage of self-* managing systems such as the system in Figure 1 is the difficulty of engineering a sufficiently accurate knowledge module that can achieve acceptable performance in deployed systems. Because todays computing systems are highly complex and distributed, developing accurate models of them is a potentially complex and timeconsuming task. Moreover, developing such models might require original research. For example, queuing network models of multitier Internet services have only recently appeared in the literature.4</p>
    <p>In particular, researchers are only beginning to approach proper treatment of the full range of computing systems complex dynamic behaviors within standard model-building frameworks. Standard control models, for example, model such effects only approximately, and standard queuing models ignore them entirely given that they assume steady-state system behavior.</p>
    <p>A further challenge for model-based approaches is that todays computing systems, as well as the workflow characteristics and business processes that they support, are continually evolving, so periodic redesign of the knowledge modules of self-* systems will be necessary.</p>
    <p>Defeating the Knowledge Bottleneck with Machine Learning Machine learning (ML) might hold great promise in overcoming the knowledge bottleneck just described. ML is a subfield of artificial intelligence that aims to develop methods for automatically acquiring knowledge from data. For example, the broad paradigm of supervised learning uses a data set of (input, output) pairs to learn a classification or regression model exhibiting a deeper understanding of the relation between inputs and outputs beyond the specific training exemplars. Ideally, the learning algorithm will correctly generalize to novel exemplars not seen during training. Likewise, unsupervised learning is another broad paradigm that uses input exemplars only (no target outputs), with the goal of discovering previously unknown structures or relationships between exemplars or their components. Such knowledge can be useful in data mining or for grouping exemplars into closely related clusters.</p>
    <p>To overcome the knowledge bottleneck in developing self-* systems, ML approaches would ideally use so-called tabula rasa learning methods  that is, methods that learn with little or no built-in domain knowledge. Having to build significant knowledge into the learning algorithms architecture would defeat the purpose. However, given that some level of domain knowledge is usually available for most systems, and that completely knowledge-free learning might be impractical for various reasons, another desideratum of machinelearning methods is that they can also gracefully incorporate any available initial domain knowledge.</p>
    <p>Consider the ML paradigms that would be appropriate for learning self-* management policies  that is, mappings from system states to selected management actions. (Note that this definition of policy differs from other more general meanings used in autonomic computing.) Adaptive control,5 one such paradigm already used in systems management, fits readily into the modelbased control theory framework. Adaptive control methods aim to automatically and continuously adapt model parameters as system characteristics change, and to automatically perform system identification  that is, develop a model of how control actions influence the system states evolution. To the extent that such efforts require little engineering of domain-specific knowledge, they can contribute to our overall goal of avoiding the knowledge bottleneck.</p>
    <p>JANUARY  FEBRUARY 2007 23</p>
    <p>Reinforcement Learning</p>
    <p>Figure 1. A standard autonomic computing monitor-analyze-planexecute (MAPE) loop. The autonomic manager continually monitors sensor readings, analyzes and plans management decisions, and executes the decisions via effectors. The MAPE components have access to a central knowledge base containing information pertaining to the likely effectiveness of different management decisions in achieving policy objectives.</p>
    <p>Sensor readings Effectors</p>
    <p>Autonomic manager</p>
    <p>Analyze Plan</p>
    <p>Monitor ExecuteKnowledge</p>
    <p>Managed element</p>
    <p>Sensor readings Effectors</p>
    <p>valuable testbed readily available:  real e-science applications  large provenance graphs</p>
    <p>dynamic optimization requires many runs</p>
    <p>Cloud-based workflows come with a clear cost model</p>
  </div>
  <div class="page">
    <p>Research</p>
    <p>Approach: Add adaptive control to an existing workflow, with provenance analysis at its core  new recommender task</p>
    <p>Hypothesis: Provenance traces recorded for past runs of a workflow can be used to make future runs more efficient</p>
  </div>
  <div class="page">
    <p>Research</p>
    <p>Approach: Add adaptive control to an existing workflow, with provenance analysis at its core  new recommender task</p>
    <p>Hypothesis: Provenance traces recorded for past runs of a workflow can be used to make future runs more efficient</p>
    <p>Applicable for instance to a Panel of Experts (Poe) pattern:</p>
    <p>N experts are activated on same inputs, best outputs are selected</p>
    <p>Panel of Experts</p>
    <p>step</p>
    <p>step</p>
    <p>output queue</p>
    <p>Expert 1 Expert n</p>
    <p>input queue</p>
    <p>provenance case base</p>
    <p>online provenance</p>
    <p>analysis</p>
    <p>expert select</p>
    <p>results merge</p>
    <p>recommender</p>
    <p>Provenance used for incremental correlation of the inputs to the experts success rate</p>
    <p>- Provenance of run i indicates which experts performed well on their input</p>
    <p>- Adaptively pruning the process space: on run i+1, use provenance of output computed by runs 1..n to select/prioritize invocation of experts</p>
  </div>
  <div class="page">
    <p>Feature Selection</p>
    <p>Model Discovery</p>
    <p>MB1 MBh</p>
    <p>M(i)11 M (i) 1h M</p>
    <p>(i) k1 M</p>
    <p>(i) kh</p>
    <p>FS(i)1 FS (i) k</p>
    <p>DS(i)</p>
    <p>Case study: DiscoveryBus workflow  QSAR: Quantitative structure-activity relationships</p>
    <p>at forefront of Chemical Engineering research</p>
    <p>OpenQsar project (http://www.openqsar.com):  Establish correlations between the structure of molecular compounds and some of</p>
    <p>their associated activities (toxicity, solubility, etc)</p>
    <p>DiscoveryBus: a workflow implementation of OpenQSAR  eScience Central cloud-based</p>
    <p>workflow system  datasets DS(i) are a family of</p>
    <p>structurally homogeneous molecules  Feature Selection extracts few</p>
    <p>relevant features from DS(i)</p>
    <p>Each learning scheme MB1...MBh generates a different predictive models for molecular activity</p>
    <p>Repetitive and resource-intensive: Workflow execution repeated over about 10K different input datasets</p>
  </div>
  <div class="page">
    <p>Role of provenance in DiscoveryBus</p>
    <p>Feature Selection</p>
    <p>Model Discovery</p>
    <p>MB1 MBh</p>
    <p>M(i)11 M (i) 1h M</p>
    <p>(i) k1 M</p>
    <p>(i) kh</p>
    <p>FS(i)1 FS (i) k</p>
    <p>DS(i)</p>
    <p>M (i)jh W asGeneratedBy MB h</p>
    <p>used FS (i)j W asDerivedF rom DS (i)</p>
    <p>Provenance correlates quality of output models M(i)jk to intermediate feature sets FS(i)j:</p>
    <p>Connection to Panel of Experts:  Experts  model builders MBi  Experts outcome  quality of generated</p>
    <p>model (predictive power, stability)</p>
    <p>Optimization goal:  to prioritize invocation of the MBi based on</p>
    <p>their past performance on inputs similar to FS(i)j</p>
  </div>
  <div class="page">
    <p>The recommender  One Quality Matrix QMFS is associated to each Feature Set FS  QMFS[MBi] encodes the success history of model builder MBi in the workflow</p>
    <p>every time FS is used as input:</p>
    <p>QM FS [MB h] = G, B</p>
    <p>G (resp B): number of times MBh has been observed to produce a good (resp. bad) model when applied to input FS</p>
    <p>QMFS is updated every time FS appears in the provenance graph</p>
    <p>The builders historical success rate G induces a dynamic partial order on the MBi</p>
    <p>For each run, the Recommender:  intercepts FS in the flow  returns partial order from QMFS</p>
    <p>Panel of Experts</p>
    <p>step</p>
    <p>step</p>
    <p>output queue</p>
    <p>Expert 1 Expert n</p>
    <p>input queue</p>
    <p>provenance case base</p>
    <p>online provenance</p>
    <p>analysis</p>
    <p>expert select</p>
    <p>results merge</p>
    <p>recommender</p>
  </div>
  <div class="page">
    <p>Early experimental results</p>
    <p>max_attempts is the accuracy/resources trade-off parameter  max_attempts = n: only first n out of H model builders are invoked</p>
    <p>Chart shows net accuracy over the entire available history of runs  success rate / number of recommendations given</p>
    <p>!&quot; #</p>
    <p>#$ %!</p>
    <p>$# $&amp; &amp;</p>
    <p>'&quot; #&amp; '</p>
    <p>%( #&quot; %</p>
    <p>&amp;! &quot;$ )</p>
    <p>!% !% &amp;</p>
    <p>*$ '&quot; #</p>
    <p>() () $</p>
    <p>&quot;) $% )</p>
    <p>&quot;# *' &amp;</p>
    <p>#&quot; %) &amp;</p>
    <p>$) *&quot; %)</p>
    <p>$$ '&quot; #'</p>
    <p>$' $# )&amp;</p>
    <p>$' #&amp; %%</p>
    <p>$% &quot;$ &quot;#</p>
    <p>$&amp; ** (%</p>
    <p>$! &amp;( #&quot;</p>
    <p>$* '# ('</p>
    <p>$( '* !&quot;</p>
    <p>$&quot; '( %#</p>
    <p>$# )( %%</p>
    <p>$# &quot;) $(</p>
    <p>)+)),</p>
    <p>$)+)),</p>
    <p>')+)),</p>
    <p>%)+)),</p>
    <p>&amp;)+)),</p>
    <p>!)+)),</p>
    <p>*)+)),</p>
    <p>()+)),</p>
    <p>&quot;)+)),</p>
    <p>#)+)),</p>
    <p>$))+)),</p>
    <p>)+)),</p>
    <p>$)+)),</p>
    <p>')+)),</p>
    <p>%)+)),</p>
    <p>&amp;)+)),</p>
    <p>!)+)),</p>
    <p>*)+)),</p>
    <p>()+)),</p>
    <p>&quot;)+)),</p>
    <p>#)+)),</p>
    <p>$))+)),</p>
    <p>-./0.112-314050$</p>
    <p>-./0.112-314050'</p>
    <p>-./0.112-314050%</p>
    <p>-./0.112-314050&amp;</p>
  </div>
  <div class="page">
    <p>Limitations and ongoing work</p>
    <p>Approach suffers when FS space is sparse  most FS seen only once</p>
    <p>Recommender abstains when QMFS not sufficiently populated</p>
    <p>This is the main hurdle to successful optimization</p>
    <p>! &quot; !# !&quot; $# $&quot; %# %&quot; &amp;# &amp;&quot; '&amp;&quot;</p>
    <p>#</p>
    <p>&quot;###</p>
    <p>!####</p>
    <p>!&quot;###</p>
    <p>$####</p>
    <p>$&quot;###</p>
    <p>%####</p>
    <p>%&quot;###</p>
    <p>()*+,-./0.-,0,-,(1,2.3/.4(5.26(78,.9:</p>
    <p>Strategy: increase space density by clustering the FS  needs a distance metric over the set of FS  hierarchical clustering should provide a way to experiment with</p>
    <p>accuracy/efficiency trade-offs</p>
  </div>
  <div class="page">
    <p>Short summary  What happens when you try to apply mining/learning techniques to</p>
    <p>large corpora of provenance metadata?  any interesting applications / use cases?  which techniques apply?  are there significant research challenges, or an orchard of low-hanging fruits?</p>
    <p>privacy in provenance mining  what provenance models lend themselves well to mining  ...</p>
  </div>
</Presentation>
