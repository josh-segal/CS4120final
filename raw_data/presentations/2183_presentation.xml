<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deduplicating Compressed Contents in Cloud Storage Environment</p>
    <p>Zhichao Yan1, Hong Jiang1, Yujuan Tan2 and Hao Luo3 University of Texas Arlington1</p>
    <p>Chongqing University2 University of Nebraska Lincoln3</p>
  </div>
  <div class="page">
    <p>Reality and Trend</p>
    <p>Information explosion</p>
  </div>
  <div class="page">
    <p>Reality and Trend</p>
    <p>Pain point: how to manage storage growth</p>
    <p>Table Source: IDC</p>
  </div>
  <div class="page">
    <p>Reality and Trend</p>
    <p>Data Migrate to Cloud</p>
    <p>Figure Source: IDC</p>
  </div>
  <div class="page">
    <p>Reality and Trend  Network bandwidth (Low and Asymmetric)</p>
    <p>Broadband Speed Greater Than 10 Mbps(2014-2019) from Cisco</p>
    <p>Summary of Existing Internet Plans of Time Warner Cable</p>
  </div>
  <div class="page">
    <p>Why data reduction</p>
    <p>Information explosion  Huge amount of digital contents  How to store these data? Cloud storage  Lower storage cost  Data reduction technology</p>
    <p>Network bandwidth Low and Asymmetric  How to transfer a large amount of data to cloud?  Data reduction technology</p>
  </div>
  <div class="page">
    <p>Two common data reduction technologies</p>
    <p>Data lossless compression finds repeated strings within the specific range of the individual files and replaces them with a more compact coding scheme (Compression dictionary)</p>
    <p>Data deduplication identifies and removes the redundant files/chunks across all the files (maintaining pointers information to assemble the data from files/chunks for future access)</p>
  </div>
  <div class="page">
    <p>What will happen</p>
    <p>Both end users and cloud service providers have performance (data transferring time) and economic (data storage cost) incentives to deploy data reduction technologies</p>
    <p>Cloud will become the digital content aggregating point in the digital universe, containing a lot of compressed packages from different end users</p>
  </div>
  <div class="page">
    <p>A common scenario: Compression at the client side Dedup at the cloud side</p>
    <p>Different users will use various compression tools to compress their data before sending to the cloud</p>
  </div>
  <div class="page">
    <p>Problem</p>
    <p>Old dedup works well with plain data, but not with shuffled data in compressed packages</p>
    <p>Redundancy hidden within compressed contents might widely exist in cloud storage environment and will increase with the time</p>
    <p>Efficient cloud requires an approach to dedup such kind of redundant data within the compressed contents</p>
  </div>
  <div class="page">
    <p>X-Ray Dedup</p>
    <p>Compression tools usually have some data integrity mechanism to avoid compressed data corruption</p>
    <p>Same checksum algorithm will generate the same checksum value for the same file no matter which compress algorithm it works with</p>
    <p>One most popular checksum mechanism is CRC32 (collision can be studied in future)</p>
    <p>Combined with original file length as ID for dedup</p>
  </div>
  <div class="page">
    <p>X-Ray Notion</p>
    <p>Checksums used in different compression tools: CRC32 , MD5, SHA1, RIPEMD-160, SHA256, SHA512, BLAKE2</p>
  </div>
  <div class="page">
    <p>System Overview</p>
  </div>
  <div class="page">
    <p>System Workflow A file metadata extractor module on the client side extracts the metadata of compressed package by parsing through the compressed package (i.e., name, uncompressed length and checksum).</p>
    <p>A file signatures store is used to help the file signature identification module identify and remove file-level data redundancy by its recorded files' metadata entries. The unique (non-redundant) files are chunked to generate data chunks and their individual fingerprints. The conventional chunk-level</p>
    <p>deduplication will be executed to generate file recipes and unique chunks. Finally, the previously generated package recipes, file recipes and unique chunks are stored to the storage servers.</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Based on chunk level dedup system (destor)  Add an extra file level dedup for compressed</p>
    <p>contents  Tar only checksum the header, we need to</p>
    <p>extend it to whole file content checksum for such kind of compressed tools like tar.gz and tar.xz, it can be translated at the sever side by adding some extra checksum information</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Some Results</p>
    <p>What if we only deploy chunk level dedup on compressed packages (of the same content)</p>
  </div>
  <div class="page">
    <p>Some Results</p>
    <p>What about the hidden data redundancy? (local and global)</p>
  </div>
  <div class="page">
    <p>Some Results</p>
    <p>How much redundant data X-Ray dedup can reduce</p>
    <p>Compressed redundancy = compressed intact files size / size of compressed package</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Find new ID (checksum + file length)to detect redundant file across the compressed packages</p>
    <p>An extra file level dedup designed for compressed files</p>
    <p>Significant reduce the capacity requirement for an efficient cloud storage environment</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
