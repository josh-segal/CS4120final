<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Tutorial on</p>
    <p>Graph-based Semi-Supervised Learning Algorithms for NLP</p>
    <p>Partha Pratim Talukdar (Carnegie Mellon University)</p>
    <p>Amarnag Subramanya (Google Research)</p>
    <p>http://graph-ssl.wikidot.com/ACL 2012, South Korea</p>
  </div>
  <div class="page">
    <p>Supervised Learning</p>
    <p>ModelLabeled Data Learning</p>
    <p>Algorithm</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Learning (SSL)</p>
    <p>ModelLabeled Data</p>
    <p>Learning Algorithm</p>
    <p>A Lot of Unlabeled Data</p>
  </div>
  <div class="page">
    <p>With Unlabeled DataWithout Unlabeled Data</p>
    <p>Why SSL?</p>
    <p>How can unlabeled data be helpful?</p>
    <p>Example from [Belkin et al., JMLR 2006]</p>
    <p>Labeled Instances</p>
    <p>Decision Boundary</p>
    <p>More accurate decision boundary in the presence of unlabeled instances</p>
    <p>Unlabeled Instances</p>
  </div>
  <div class="page">
    <p>Inductive vs Transductive</p>
    <p>SVM, Maximum Entropy</p>
    <p>X</p>
    <p>Manifold Regularization</p>
    <p>Graph SSL algorithms</p>
    <p>Supervised (Labeled)</p>
    <p>Semi-supervised (Labeled + Unlabeled)</p>
    <p>Inductive (Generalize to Unseen Data)</p>
    <p>Transductive (Doesnt Generalize to</p>
    <p>Unseen Data)</p>
    <p>See Chapter 25 of SSL Book: http://olivier.chapelle.cc/ssl-book/discussion.pdf</p>
    <p>Most Graph SSL algorithms are non-parametric</p>
  </div>
  <div class="page">
    <p>Why Graph-based SSL?</p>
    <p>Some datasets are naturally represented by a graph  web, citation network, social network, ...</p>
    <p>Uniform representation for heterogeneous data  Easily parallelizable, scalable to large data  Effective in practice</p>
    <p>Graph SSL</p>
    <p>Supervised</p>
    <p>Non-Graph SSL</p>
    <p>Text Classification</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
    <p>Similarity</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
    <p>Similarity</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
    <p>politicsbusiness</p>
    <p>Similarity</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
    <p>politicsbusiness</p>
    <p>business politics</p>
    <p>Similarity</p>
  </div>
  <div class="page">
    <p>Graph-based SSL</p>
    <p>Effective for both relational and IID data  Two stages  Graph construction (if not already present)  Label Inference</p>
    <p>Smoothness Assumption If two instances are similar</p>
    <p>according to the graph, then output labels should be similar</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Neighborhood Methods  k-NN Graph Construction  e-Neighborhood Method</p>
    <p>Metric Learning  Other approaches</p>
  </div>
  <div class="page">
    <p>Neighborhood Methods</p>
    <p>k-Nearest Neighbor (k-NN)  add edges between an instance and its</p>
    <p>k-nearest neighbors</p>
    <p>e-Neighborhood  add edges to all instances inside a ball of</p>
    <p>radius e</p>
    <p>e</p>
    <p>k = 3</p>
  </div>
  <div class="page">
    <p>Issues with k-NN graphs</p>
    <p>Not scalable (quadratic)  Results in an asymmetric graph  Results in irregular graphs</p>
    <p>some nodes may end up with higher degree than other nodes</p>
    <p>Node of degree 4 in the 1-NN graph</p>
  </div>
  <div class="page">
    <p>Issues with e-Neighborhood</p>
    <p>Fragmented Graph: disconnected components</p>
    <p>Sensitive to value of e : not invariant to scaling</p>
    <p>Not scalable</p>
    <p>Figure from [Jebara et al., ICML 2009]</p>
    <p>e-Neighborhood k-NNData</p>
    <p>Disconnected</p>
  </div>
  <div class="page">
    <p>Graph Construction using Metric Learning</p>
    <p>Supervised Metric Learning  ITML [Kulis et al., ICML 2007]  LMNN [Weinberger and Saul, JMLR 2009]</p>
    <p>Semi-supervised Metric Learning  IDML [Dhillon et al., UPenn TR 2010]</p>
    <p>xi xj wij  exp(DA(xi, xj))</p>
    <p>Estimated using Mahalanobis metric learning algorithms</p>
    <p>DA(xi, xj) = (xi  xj)T A(xi  xj)</p>
  </div>
  <div class="page">
    <p>Benefits of Metric Learning for Graph Construction</p>
    <p>Graph constructed using semisupervised metric learning [Dhillon et al, 2010]</p>
    <p>Graph constructed using supervised metric learning</p>
    <p>Careful graph construction is critical!</p>
    <p>[Dhillon et al., UPenn TR 2010]</p>
  </div>
  <div class="page">
    <p>Other Graph Construction Approaches</p>
    <p>Local Reconstruction  Linear Neighborhood [Wang and Zhang, ICML 2005]  Regular Graph: b-matching [Jebara et al., ICML 2008]  Fitting Graph to Vector Data [Daitch et al., ICML 2009]</p>
    <p>Graph Kernels  [Zhu et al., NIPS 2005]</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Label Propagation - Modified Adsorption - Manifold Regularization - Spectral Graph Transduction - Measure Propagation - Sparse Label Propagation</p>
  </div>
  <div class="page">
    <p>Graph Laplacian</p>
    <p>Laplacian (un-normalized) of a graph:</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>d</p>
    <p>a b c d</p>
    <p>a b c d</p>
    <p>L = D  W, where Dii =</p>
    <p>j</p>
    <p>Wij, Dij( =i) = 0</p>
  </div>
  <div class="page">
    <p>fT Lf =</p>
    <p>i,j</p>
    <p>Wij(fi  fj)2</p>
    <p>Graph Laplacian (contd.)</p>
    <p>L is positive semi-definite (with non-negative weights)  Smoothness of function f over the graph in terms</p>
    <p>of the Laplacian:</p>
    <p>Measure of Smoothness</p>
  </div>
  <div class="page">
    <p>Spectrum of the Laplacian</p>
    <p>Figure from [Zhu et al., 2005]</p>
    <p>Number of connected</p>
    <p>components = Number of 0 eigenvalues</p>
    <p>Constant within component</p>
    <p>Higher Eigenvalue, Irregular Eigenvector,</p>
    <p>Less smoothness</p>
  </div>
  <div class="page">
    <p>Notations Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priors</p>
    <p>Yv,l : score of estimated label l on node v</p>
    <p>Yv,l : score of seed label l on node v</p>
    <p>Rv,l : regularization target for label l on node v</p>
    <p>S : seed node indicator (diagonal matrix)</p>
    <p>v</p>
    <p>Wuv : weight of edge (u, v) in the graph</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Label Propagation - Modified Adsorption - Manifold Regularization - Spectral Graph Transduction - Measure Propagation - Sparse Label Propagation</p>
  </div>
  <div class="page">
    <p>LP-ZGL [Zhu et al., ICML 2003]</p>
    <p>arg min Y</p>
    <p>m!</p>
    <p>l=1</p>
    <p>Wuv(Yul ! Yvl) 2</p>
    <p>Yul = Yul, !Suu = 1such that</p>
    <p>Smooth</p>
    <p>Match Seeds (hard)</p>
    <p>Smoothness  two nodes connected by</p>
    <p>an edge with high weight should be assigned similar labels</p>
    <p>Solution satisfies harmonic property</p>
    <p>=</p>
    <p>m!</p>
    <p>l=1</p>
    <p>Y T l LYl</p>
    <p>Graph Laplacian</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Label Propagation - Modified Adsorption - Manifold Regularization - Spectral Graph Transduction - Measure Propagation - Sparse Label Propagation</p>
  </div>
  <div class="page">
    <p>Two Related Views</p>
    <p>UL</p>
    <p>Label Diffusion</p>
    <p>L</p>
    <p>Random Walk</p>
    <p>U</p>
  </div>
  <div class="page">
    <p>Continue walk with probability</p>
    <p>Assign Vs seed label to U with probability</p>
    <p>Abandon random walk with probability  assign U a dummy label</p>
    <p>p cont</p>
    <p>v</p>
    <p>p inj v</p>
    <p>p abnd v</p>
    <p>Random Walk View</p>
    <p>UV</p>
    <p>what next? Starting node</p>
  </div>
  <div class="page">
    <p>Discounting Nodes</p>
    <p>Certain nodes can be unreliable (e.g., high degree nodes)  do not allow propagation/walk through them</p>
    <p>Solution: increase abandon probability on such nodes:</p>
    <p>p abnd v</p>
    <p>! degree(v)</p>
  </div>
  <div class="page">
    <p>Redefining Matrices</p>
    <p>W !</p>
    <p>uv = pcont</p>
    <p>u ! Wuv</p>
    <p>Suu =</p>
    <p>!</p>
    <p>p inj u</p>
    <p>Ru! = p abnd u</p>
    <p>, and 0 for non-dummy labels</p>
    <p>Dummy Label</p>
    <p>New Edge Weight</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>!!</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Match Seeds (soft)</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>!!</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Match Seeds (soft) Smooth</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>!!</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Match Seeds (soft) Smooth Match Priors (Regularizer)</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>!!</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Match Seeds (soft) Smooth Match Priors (Regularizer)</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>!!for none-of-the-above label</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Match Seeds (soft) Smooth Match Priors (Regularizer)</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>MAD has extra regularization compared to LP-ZGL [Zhu et al, ICML 03]; similar to QC [Bengio et al, 2006]</p>
    <p>!!for none-of-the-above label</p>
  </div>
  <div class="page">
    <p>Modified Adsorption (MAD) [Talukdar and Crammer, ECML 2009]</p>
    <p>arg min Y</p>
    <p>m+1</p>
    <p>l=1</p>
    <p>SY l  SY l2 + 1</p>
    <p>u,v</p>
    <p>Muv(Y ul  Y vl)2 + 2Y l  Rl2</p>
    <p>m labels, +1 dummy label</p>
    <p>M = W  + W is the symmetrized weight matrix</p>
    <p>Y vl: weight of label l on node v</p>
    <p>Y vl: seed weight for label l on node v</p>
    <p>S: diagonal matrix, nonzero for seed nodes</p>
    <p>Rvl: regularization target for label l on node v</p>
    <p>Match Seeds (soft) Smooth Match Priors (Regularizer)</p>
    <p>Seed Scores</p>
    <p>Estimated Scores</p>
    <p>Label Priorsv</p>
    <p>MAD has extra regularization compared to LP-ZGL [Zhu et al, ICML 03]; similar to QC [Bengio et al, 2006]</p>
    <p>!!for none-of-the-above label</p>
    <p>MADs Objective is Convex</p>
  </div>
  <div class="page">
    <p>Solving MAD Objective</p>
    <p>Can be solved using matrix inversion (like in LP)  but matrix inversion is expensive (cubic)</p>
    <p>Instead solved exactly using a system of linear equations</p>
    <p>solved using Jacobi iterations  results in iterative updates  guaranteed convergence  see [Bengio et al., 2006] and</p>
    <p>[Talukdar and Crammer, ECML 2009] for details</p>
  </div>
  <div class="page">
    <p>Solving MAD using Iterative Updates</p>
    <p>Inputs Y , R : |V |  (|L| + 1), W : |V |  |V |, S : |V |  |V | diagonal Y  Y M = W + W</p>
    <p>Zv  Svv + 1</p>
    <p>u =v Mvu + 2 v  V repeat</p>
    <p>for all v  V do Y v  1Zv</p>
    <p>(SY )v + 1MvY + 2Rv</p>
    <p>end for until convergence</p>
    <p>!!</p>
    <p>Seed Prior</p>
    <p>Current label estimate on ba b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>Solving MAD using Iterative Updates</p>
    <p>Inputs Y , R : |V |  (|L| + 1), W : |V |  |V |, S : |V |  |V | diagonal Y  Y M = W + W</p>
    <p>Zv  Svv + 1</p>
    <p>u =v Mvu + 2 v  V repeat</p>
    <p>for all v  V do Y v  1Zv</p>
    <p>(SY )v + 1MvY + 2Rv</p>
    <p>end for until convergence</p>
    <p>!!</p>
    <p>Seed Prior</p>
    <p>New label estimate on v</p>
    <p>a b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>Solving MAD using Iterative Updates</p>
    <p>Inputs Y , R : |V |  (|L| + 1), W : |V |  |V |, S : |V |  |V | diagonal Y  Y M = W + W</p>
    <p>Zv  Svv + 1</p>
    <p>u =v Mvu + 2 v  V repeat</p>
    <p>for all v  V do Y v  1Zv</p>
    <p>(SY )v + 1MvY + 2Rv</p>
    <p>end for until convergence</p>
    <p>!!</p>
    <p>Seed Prior</p>
    <p>New label estimate on v</p>
    <p>a b</p>
    <p>c</p>
    <p>v</p>
    <p>Importance of a node can be discounted  Easily Parallelizable: Scalable (more later)</p>
  </div>
  <div class="page">
    <p>When is MAD most effective?</p>
    <p>R e</p>
    <p>la ti</p>
    <p>v e</p>
    <p>I n</p>
    <p>cr e</p>
    <p>a se</p>
    <p>i n</p>
    <p>M R</p>
    <p>R b</p>
    <p>y M</p>
    <p>A D</p>
    <p>o v e</p>
    <p>r L</p>
    <p>P -Z</p>
    <p>G L</p>
    <p>Average Degree</p>
    <p>MAD is particularly effective in denser graphs, where there is greater need for regularization.</p>
  </div>
  <div class="page">
    <p>Extension to Dependent Labels</p>
    <p>Label Graph (over Beer Categories)</p>
    <p>BrownAle 1.0</p>
    <p>Ale</p>
    <p>PaleAle</p>
    <p>ScotchAle</p>
    <p>TopFormentedBeer0.95</p>
    <p>White</p>
    <p>Porter</p>
    <p>Labels are not always mutually exclusive</p>
    <p>Labels Label Similarity</p>
  </div>
  <div class="page">
    <p>MAD with Dependent Labels (MADDL) [Talukdar and Crammer, ECML 2009]</p>
    <p>MADDL Objective</p>
    <p>MADDL objective results in a scalable iterative update, with convergence guarantee.</p>
    <p>min Edge</p>
    <p>Smoothness Loss</p>
    <p>Label Prior Loss (e.g. prior on dummy label)</p>
    <p>+ + Seed Label</p>
    <p>Loss (if any)</p>
    <p>+ Dependent Label Loss</p>
    <p>Penalize if similar labels are assigned different scores on a node</p>
    <p>BrownAle 1.0</p>
    <p>Ale</p>
  </div>
  <div class="page">
    <p>Smooth Sentiment Ranking</p>
    <p>smooth predictions</p>
    <p>rank 1</p>
    <p>rank 4</p>
  </div>
  <div class="page">
    <p>Smooth Sentiment Ranking</p>
    <p>non-smooth predictions</p>
    <p>smooth predictions</p>
    <p>rank 1</p>
    <p>rank 4</p>
  </div>
  <div class="page">
    <p>Smooth Sentiment Ranking</p>
    <p>overPrefer</p>
    <p>non-smooth predictions</p>
    <p>smooth predictions</p>
    <p>rank 1</p>
    <p>rank 4</p>
  </div>
  <div class="page">
    <p>Smooth Sentiment Ranking</p>
    <p>overPrefer</p>
    <p>non-smooth predictions</p>
    <p>smooth predictions</p>
    <p>rank 1</p>
    <p>rank 4</p>
  </div>
  <div class="page">
    <p>Smooth Sentiment Ranking</p>
    <p>Label 1</p>
    <p>Count of Top Predicted Pair in MAD Output</p>
    <p>Label 2</p>
    <p>MADDL generates smoother ranking, while preserving accuracy of prediction.</p>
    <p>Label 1</p>
    <p>Count of Top Predicted Pair in MADDL Output</p>
    <p>Label 2</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Label Propagation - Modified Adsorption - Manifold Regularization - Spectral Graph Transduction - Measure Propagation - Sparse Label Propagation</p>
  </div>
  <div class="page">
    <p>Manifold Regularization [Belkin et al., JMLR 2006]</p>
    <p>f = arg min f</p>
    <p>l</p>
    <p>l</p>
    <p>i=1</p>
    <p>V (yi, f(xi)) + A||f||2K +  f T Lf</p>
    <p>Loss Function (e.g., soft margin)</p>
    <p>Laplacian of graph over labeled and unlabeled data</p>
    <p>Trains an inductive classifier (e.g., SVM) which can generalize to unseen instances</p>
  </div>
  <div class="page">
    <p>Manifold Regularization [Belkin et al., JMLR 2006]</p>
    <p>f = arg min f</p>
    <p>l</p>
    <p>l</p>
    <p>i=1</p>
    <p>V (yi, f(xi)) + A||f||2K +  f T Lf</p>
    <p>Loss Function (e.g., soft margin)</p>
    <p>Laplacian of graph over labeled and unlabeled data</p>
    <p>Trains an inductive classifier (e.g., SVM) which can generalize to unseen instances</p>
    <p>Training Data Loss</p>
  </div>
  <div class="page">
    <p>Manifold Regularization [Belkin et al., JMLR 2006]</p>
    <p>f = arg min f</p>
    <p>l</p>
    <p>l</p>
    <p>i=1</p>
    <p>V (yi, f(xi)) + A||f||2K +  f T Lf</p>
    <p>Loss Function (e.g., soft margin)</p>
    <p>Laplacian of graph over labeled and unlabeled data</p>
    <p>Trains an inductive classifier (e.g., SVM) which can generalize to unseen instances</p>
    <p>Training Data Loss</p>
    <p>Regulairzer (e.g., L2)</p>
  </div>
  <div class="page">
    <p>Manifold Regularization [Belkin et al., JMLR 2006]</p>
    <p>f = arg min f</p>
    <p>l</p>
    <p>l</p>
    <p>i=1</p>
    <p>V (yi, f(xi)) + A||f||2K +  f T Lf</p>
    <p>Loss Function (e.g., soft margin)</p>
    <p>Laplacian of graph over labeled and unlabeled data</p>
    <p>Trains an inductive classifier (e.g., SVM) which can generalize to unseen instances</p>
    <p>Training Data Loss</p>
    <p>Regulairzer (e.g., L2)</p>
    <p>Smoothness Regularizer</p>
  </div>
  <div class="page">
    <p>Spectral Graph Transduction [Joachims, ICML 2003]</p>
    <p>Approximation to normalized graph cut with constraints</p>
    <p>Performs spectral analysis (finds eigenvalues and eigenfunctions) of the normalized Laplacian</p>
    <p>Code: http://sgt.joachims.org/</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Label Propagation - Modified Adsorption - Manifold Regularization - Spectral Graph Transduction - Measure Propagation - Sparse Label Propagation</p>
  </div>
  <div class="page">
    <p>arg min {pi}</p>
    <p>l</p>
    <p>i=1</p>
    <p>DKL(ri||pi) +</p>
    <p>i,j</p>
    <p>wijDKL(pi||pj)   n</p>
    <p>i=1</p>
    <p>H(pi)</p>
    <p>Measure Propagation (MP) [Subramanya and Bilmes, EMNLP 2008, NIPS 2009, JMLR 2010]</p>
    <p>KL Divergence</p>
    <p>DKL(pi||pj) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>pj(y)</p>
    <p>Entropy H(pi) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>Seed and estimated label distributions (normalized)</p>
    <p>on node i</p>
    <p>CKL is convex (with non-negative edge weights and hyper-parameters) MP is related to Information Regularization [Corduneanu and Jaakkola, 2003]</p>
    <p>Normalization Constraint</p>
    <p>CKL</p>
    <p>s.t.</p>
    <p>y</p>
    <p>pi(y) = 1, pi(y)  0, y, i</p>
  </div>
  <div class="page">
    <p>arg min {pi}</p>
    <p>l</p>
    <p>i=1</p>
    <p>DKL(ri||pi) +</p>
    <p>i,j</p>
    <p>wijDKL(pi||pj)   n</p>
    <p>i=1</p>
    <p>H(pi)</p>
    <p>Measure Propagation (MP) [Subramanya and Bilmes, EMNLP 2008, NIPS 2009, JMLR 2010]</p>
    <p>Divergence on seed nodes</p>
    <p>KL Divergence</p>
    <p>DKL(pi||pj) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>pj(y)</p>
    <p>Entropy H(pi) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>Seed and estimated label distributions (normalized)</p>
    <p>on node i</p>
    <p>CKL is convex (with non-negative edge weights and hyper-parameters) MP is related to Information Regularization [Corduneanu and Jaakkola, 2003]</p>
    <p>Normalization Constraint</p>
    <p>CKL</p>
    <p>s.t.</p>
    <p>y</p>
    <p>pi(y) = 1, pi(y)  0, y, i</p>
  </div>
  <div class="page">
    <p>arg min {pi}</p>
    <p>l</p>
    <p>i=1</p>
    <p>DKL(ri||pi) +</p>
    <p>i,j</p>
    <p>wijDKL(pi||pj)   n</p>
    <p>i=1</p>
    <p>H(pi)</p>
    <p>Measure Propagation (MP) [Subramanya and Bilmes, EMNLP 2008, NIPS 2009, JMLR 2010]</p>
    <p>Divergence on seed nodes</p>
    <p>Smoothness (divergence across edge)</p>
    <p>KL Divergence</p>
    <p>DKL(pi||pj) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>pj(y)</p>
    <p>Entropy H(pi) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>Seed and estimated label distributions (normalized)</p>
    <p>on node i</p>
    <p>CKL is convex (with non-negative edge weights and hyper-parameters) MP is related to Information Regularization [Corduneanu and Jaakkola, 2003]</p>
    <p>Normalization Constraint</p>
    <p>CKL</p>
    <p>s.t.</p>
    <p>y</p>
    <p>pi(y) = 1, pi(y)  0, y, i</p>
  </div>
  <div class="page">
    <p>arg min {pi}</p>
    <p>l</p>
    <p>i=1</p>
    <p>DKL(ri||pi) +</p>
    <p>i,j</p>
    <p>wijDKL(pi||pj)   n</p>
    <p>i=1</p>
    <p>H(pi)</p>
    <p>Measure Propagation (MP) [Subramanya and Bilmes, EMNLP 2008, NIPS 2009, JMLR 2010]</p>
    <p>Divergence on seed nodes</p>
    <p>Smoothness (divergence across edge)</p>
    <p>Entropic Regularizer</p>
    <p>KL Divergence</p>
    <p>DKL(pi||pj) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>pj(y)</p>
    <p>Entropy H(pi) =</p>
    <p>y</p>
    <p>pi(y) log pi(y)</p>
    <p>Seed and estimated label distributions (normalized)</p>
    <p>on node i</p>
    <p>CKL is convex (with non-negative edge weights and hyper-parameters) MP is related to Information Regularization [Corduneanu and Jaakkola, 2003]</p>
    <p>Normalization Constraint</p>
    <p>CKL</p>
    <p>s.t.</p>
    <p>y</p>
    <p>pi(y) = 1, pi(y)  0, y, i</p>
  </div>
  <div class="page">
    <p>Solving MP Objective</p>
    <p>For ease of optimization, reformulate MP objective:</p>
    <p>arg min {pi,qi}</p>
    <p>l</p>
    <p>i=1</p>
    <p>DKL(ri||qi) +</p>
    <p>i,j</p>
    <p>w</p>
    <p>ijDKL(pi||qj)   n</p>
    <p>i=1</p>
    <p>H(pi)</p>
    <p>w</p>
    <p>ij = wij +   (i, j)New probability measure, one for each</p>
    <p>vertex, similar to pi</p>
    <p>CMP</p>
    <p>Encourages agreement between pi and qi</p>
    <p>CMP is also convex (with non-negative edge weights and hyper-parameters)</p>
    <p>CMP can be solved using Alternating Minimization (AM)</p>
  </div>
  <div class="page">
    <p>Alternating Minimization</p>
  </div>
  <div class="page">
    <p>Alternating Minimization</p>
    <p>Q0</p>
  </div>
  <div class="page">
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>P1</p>
  </div>
  <div class="page">
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1</p>
  </div>
  <div class="page">
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1 P2</p>
  </div>
  <div class="page">
    <p>Q2</p>
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1 P2</p>
  </div>
  <div class="page">
    <p>Q2</p>
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1 P2P3</p>
  </div>
  <div class="page">
    <p>Q2</p>
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1 P2P3</p>
  </div>
  <div class="page">
    <p>Q2</p>
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1 P2P3</p>
  </div>
  <div class="page">
    <p>Q2</p>
    <p>Alternating Minimization</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>P1 P2P3</p>
    <p>CMP satisfies the necessary conditions for AM to converge [Subramanya and Bilmes, JMLR 2010]</p>
  </div>
  <div class="page">
    <p>Why AM?</p>
  </div>
  <div class="page">
    <p>Performance of SSL Algorithms</p>
    <p>Comparison of accuracies for different number of labeled samples across COIL (6 classes) and OPT (10 classes) datasets</p>
    <p>Graph SSL can be effective when the data satisfies manifold assumption. More results and discussion in Chapter 21 of</p>
    <p>the SSL Book (Chapelle et al.)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Label Propagation - Modified Adsorption - Manifold Regularization - Spectral Graph Transduction - Measure Propagation - Sparse Label Propagation</p>
  </div>
  <div class="page">
    <p>Background: Factor Graphs [Kschischang et al., 2001]</p>
    <p>Factor Graph  bipartite graph  variable nodes (e.g., label distribution on a node)  factor nodes: fitness function over variable assignment</p>
    <p>Distribution over all variables values</p>
    <p>Variable Nodes (V)</p>
    <p>Factor Nodes (F)</p>
    <p>variables connected to factor f</p>
  </div>
  <div class="page">
    <p>Factor Graph Interpretation of Graph SSL [Zhu et al., ICML 2003] [Das and Smith, NAACL 2012]</p>
    <p>q1 q2</p>
    <p>min Edge Smoothness Loss Regularization</p>
    <p>Loss + + Seed Matching Loss (if any)</p>
    <p>w1,2 ||q1  q2||2</p>
    <p>q1 q2(q1, q2)</p>
    <p>Regularization factor (unary)</p>
    <p>(q1, q2)  w1,2 ||q1  q2||2 Smoothness</p>
    <p>Factor</p>
    <p>Seed Matching Factor (unary)</p>
  </div>
  <div class="page">
    <p>Factor Graph Interpretation [Zhu et al., ICML 2003][Das and Smith, NAACL 2012]</p>
    <p>r1 r2</p>
    <p>r3 r4</p>
    <p>q1 q2</p>
    <p>q4 q3</p>
    <p>q9264</p>
    <p>q9265</p>
    <p>q9266</p>
    <p>q9267 q9268 q9269 q9270 1. Factor encouraging agreement on seed</p>
    <p>labels</p>
  </div>
  <div class="page">
    <p>Label Propagation with Sparsity</p>
    <p>Enforce through sparsity inducing unary factor</p>
    <p>log t(qt) =</p>
    <p>log t(qt) =</p>
    <p>Lasso (Tibshirani, 1996)</p>
    <p>Elitist Lasso (Kowalski and Torrsani, 2009)</p>
    <p>For more details, see [Das and Smith, NAACL 2012]</p>
  </div>
  <div class="page">
    <p>Other Graph-SSL Methods</p>
    <p>SSL on Directed Graphs  [Zhou et al, NIPS 2005], [Zhou et al., ICML 2005]</p>
    <p>Learning with dissimilarity edges  [Goldberg et al., AISTATS 2007]</p>
    <p>Graph Transduction using Alternating Minimization  [Wang et al., ICML 2008]</p>
    <p>Graph as regularizer for Multi-Layered Perceptron  [Karlen et al., ICML 2008], [Malkin et al., Interspeech 2009]</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Scalability Issues - Node reordering - MapReduce Parallelization</p>
  </div>
  <div class="page">
    <p>More (Unlabeled) Data is Better Data</p>
    <p>Inference over graph with 120m</p>
    <p>vertices</p>
    <p>Supervised baseline</p>
    <p>Challenges with large unlabeled data:</p>
    <p>Constructing graph from large data  Scalable inference over large graphs</p>
    <p>[Subramanya &amp; Bilmes, JMLR 2011] 54</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Scalability Issues - Node reordering - MapReduce Parallelization</p>
  </div>
  <div class="page">
    <p>Brute force (exact) k-NN too expensive (quadratic)</p>
    <p>Approximate nearest neighbor using kd-tree [Friedman et al., 1977]</p>
    <p>Approximate Nearest Neighbor library (http://www.cs.umd.edu/mount/)</p>
    <p>Scalability Issues (I) Graph Construction</p>
  </div>
  <div class="page">
    <p>Sub-sample the data</p>
    <p>Construct graph over a subset of a unlabeled data [Delalleau et al., AISTATS 2005]</p>
    <p>Sparse Grids [Garcke &amp; Griebel, KDD 2001]</p>
    <p>Scalability Issues (II) Label Inference</p>
  </div>
  <div class="page">
    <p>Sub-sample the data</p>
    <p>Construct graph over a subset of a unlabeled data [Delalleau et al., AISTATS 2005]</p>
    <p>Sparse Grids [Garcke &amp; Griebel, KDD 2001]</p>
    <p>How about using more compute? (next section)  Symmetric multi-processor (SMP)  Distributed Computer</p>
    <p>Scalability Issues (II) Label Inference</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Scalability Issues - Node reordering [Subramanya &amp; Bilmes, JMLR 2011; Bilmes &amp; Subramanya, 2011]</p>
    <p>- MapReduce Parallelization</p>
  </div>
  <div class="page">
    <p>Label Update using Message Passing</p>
    <p>Seed Prior</p>
    <p>a b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>Label Update using Message Passing</p>
    <p>Seed Prior</p>
    <p>New label estimate on v</p>
    <p>a b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>Speed-up on SMP</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011]</p>
    <p>Graph with 1.4M nodes  SMP with 16 cores and 128GB of RAM</p>
  </div>
  <div class="page">
    <p>Speed-up on SMP</p>
    <p>Ratio of time spent when using 1 processor to</p>
    <p>time spent using n processors</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011]</p>
    <p>Graph with 1.4M nodes  SMP with 16 cores and 128GB of RAM</p>
  </div>
  <div class="page">
    <p>Speed-up on SMP</p>
    <p>Ratio of time spent when using 1 processor to</p>
    <p>time spent using n processors</p>
    <p>Cache miss?</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011]</p>
    <p>Graph with 1.4M nodes  SMP with 16 cores and 128GB of RAM</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm</p>
    <p>Input: Graph G = (V, E)</p>
    <p>Result: Node ordered graph</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011] 61</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm</p>
    <p>Input: Graph G = (V, E)</p>
    <p>Result: Node ordered graph</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011] 61</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm</p>
    <p>Input: Graph G = (V, E)</p>
    <p>Result: Node ordered graph</p>
    <p>Exhaustive for sparse</p>
    <p>(e.g., k-NN) graphs</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011] 61</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm : Intuition</p>
    <p>k</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>a</p>
    <p>e</p>
    <p>j</p>
    <p>c</p>
    <p>b</p>
    <p>a</p>
    <p>d</p>
    <p>c</p>
    <p>c</p>
    <p>l</p>
    <p>m</p>
    <p>n 62</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm : Intuition</p>
    <p>k</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>a</p>
    <p>e</p>
    <p>j</p>
    <p>c</p>
    <p>b</p>
    <p>a</p>
    <p>d</p>
    <p>c</p>
    <p>c</p>
    <p>l</p>
    <p>m</p>
    <p>n</p>
    <p>|N(k)  N(a)| = 1</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm : Intuition</p>
    <p>k</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>a</p>
    <p>e</p>
    <p>j</p>
    <p>c</p>
    <p>b</p>
    <p>a</p>
    <p>d</p>
    <p>c</p>
    <p>c</p>
    <p>l</p>
    <p>m</p>
    <p>n</p>
    <p>|N(k)  N(a)| = 1</p>
    <p>|N(k)  N(b)| = 2</p>
    <p>|N(k)  N(c)| = 0</p>
  </div>
  <div class="page">
    <p>Node Reordering Algorithm : Intuition</p>
    <p>k</p>
    <p>a</p>
    <p>b</p>
    <p>c</p>
    <p>a</p>
    <p>e</p>
    <p>j</p>
    <p>c</p>
    <p>b</p>
    <p>a</p>
    <p>d</p>
    <p>c</p>
    <p>c</p>
    <p>l</p>
    <p>m</p>
    <p>n</p>
    <p>|N(k)  N(a)| = 1</p>
    <p>|N(k)  N(b)| = 2</p>
    <p>|N(k)  N(c)| = 0</p>
    <p>Best Node</p>
  </div>
  <div class="page">
    <p>Speed-up on SMP after Node Ordering</p>
    <p>[Subramanya &amp; Bilmes, JMLR, 2011] 63</p>
  </div>
  <div class="page">
    <p>Distributed Processing</p>
    <p>Maximize overlap between consecutive nodes within the same machine</p>
    <p>Minimize overlap across machines (reduce inter machine communication)</p>
  </div>
  <div class="page">
    <p>Distributed Processing</p>
    <p>[Bilmes &amp; Subramanya, 2011] 65</p>
  </div>
  <div class="page">
    <p>Distributed Processing Maximize</p>
    <p>intra-processor overlap</p>
    <p>Minimize inter-processor</p>
    <p>overlap</p>
    <p>[Bilmes &amp; Subramanya, 2011] 65</p>
  </div>
  <div class="page">
    <p>Node reordering for Distributed Computer</p>
    <p>...... ............ ......</p>
    <p>Processor #i Processor #j</p>
  </div>
  <div class="page">
    <p>Node reordering for Distributed Computer</p>
    <p>...... ............ ......</p>
    <p>Processor #i Processor #j</p>
    <p>Minimize Overlap</p>
  </div>
  <div class="page">
    <p>Node reordering for Distributed Computer</p>
    <p>...... ............ ......</p>
    <p>Processor #i Processor #j</p>
    <p>Minimize Overlap</p>
    <p>Maximize Overlap</p>
  </div>
  <div class="page">
    <p>Distributed Processing Results</p>
    <p>[Bilmes &amp; Subramanya, 2011] 67</p>
  </div>
  <div class="page">
    <p>Distributed Processing Results</p>
    <p>Graph-based algorithms are amenable to distributed processing</p>
    <p>[Bilmes &amp; Subramanya, 2011] 67</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Scalability Issues - Node reordering - MapReduce Parallelization</p>
  </div>
  <div class="page">
    <p>MapReduce Implementation of MAD</p>
    <p>Seed Prior</p>
    <p>Current label estimate on ba b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>MapReduce Implementation of MAD  Map</p>
    <p>Each node send its current label assignments to its neighbors</p>
    <p>Seed Prior</p>
    <p>Current label estimate on ba b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>MapReduce Implementation of MAD  Map</p>
    <p>Each node send its current label assignments to its neighbors</p>
    <p>Seed Prior</p>
    <p>a b</p>
    <p>c</p>
    <p>v</p>
  </div>
  <div class="page">
    <p>MapReduce Implementation of MAD  Map</p>
    <p>Each node send its current label assignments to its neighbors</p>
    <p>Seed Prior</p>
    <p>a b</p>
    <p>c</p>
    <p>v  Reduce  Each node updates its own label</p>
    <p>assignment using messages received from neighbors, and its own information (e.g., seed labels, reg. penalties etc.)</p>
    <p>Repeat until convergence</p>
    <p>Reduce</p>
  </div>
  <div class="page">
    <p>MapReduce Implementation of MAD  Map</p>
    <p>Each node send its current label assignments to its neighbors</p>
    <p>Seed Prior</p>
    <p>New label estimate on v</p>
    <p>a b</p>
    <p>c</p>
    <p>v  Reduce  Each node updates its own label</p>
    <p>assignment using messages received from neighbors, and its own information (e.g., seed labels, reg. penalties etc.)</p>
    <p>Repeat until convergence</p>
  </div>
  <div class="page">
    <p>MapReduce Implementation of MAD  Map</p>
    <p>Each node send its current label assignments to its neighbors</p>
    <p>Seed Prior</p>
    <p>New label estimate on v</p>
    <p>a b</p>
    <p>c</p>
    <p>v  Reduce  Each node updates its own label</p>
    <p>assignment using messages received from neighbors, and its own information (e.g., seed labels, reg. penalties etc.)</p>
    <p>Repeat until convergence Code in Junto Label Propagation Toolkit</p>
    <p>(includes Hadoop-based implementation)</p>
    <p>http://code.google.com/p/junto/</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Text Categorization - Sentiment Analysis - Class Instance Acquisition - POS Tagging - MultiLingual POS Tagging - Semantic Parsing</p>
  </div>
  <div class="page">
    <p>Problem Description &amp; Motivation</p>
    <p>Given a document (e.g., web page, news article), assign it to a fixed number of semantic categories (e.g., sports, politics, entertainment)</p>
    <p>Multi-label problem  Training supervised models requires large</p>
    <p>amounts of labeled data [Dumais et al., 1998]</p>
  </div>
  <div class="page">
    <p>Corpora</p>
    <p>Reuters [Lewis, et al., 1978]  Newswire  About 20K document with 135 categories. Use</p>
    <p>top 10 categories (e.g., earnings, acquistions, wheat, interest) and label the remaining as other</p>
  </div>
  <div class="page">
    <p>Corpora</p>
    <p>Reuters [Lewis, et al., 1978]  Newswire  About 20K document with 135 categories. Use</p>
    <p>top 10 categories (e.g., earnings, acquistions, wheat, interest) and label the remaining as other</p>
    <p>WebKB [Bekkerman, et al., 2003]  8K webpages from 4 academic domains  Categories include course, department,</p>
    <p>faculty and project 72</p>
  </div>
  <div class="page">
    <p>Feature Extraction</p>
    <p>Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, ...</p>
    <p>Document [Lewis, et al., 1978]</p>
  </div>
  <div class="page">
    <p>Feature Extraction</p>
    <p>Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, ...</p>
    <p>Document</p>
    <p>Showers continued week Bahia cocoa zone alleviating drought early January improving prospects coming temporao, ...</p>
    <p>Stop-word Removal[Lewis, et al., 1978]</p>
  </div>
  <div class="page">
    <p>Feature Extraction</p>
    <p>Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, ...</p>
    <p>Document</p>
    <p>Showers continued week Bahia cocoa zone alleviating drought early January improving prospects coming temporao, ...</p>
    <p>shower continu week bahia cocoa zone allevi drought earli januari improv prospect come temporao, ...</p>
    <p>Stop-word Removal</p>
    <p>Stemming [Lewis, et al., 1978]</p>
  </div>
  <div class="page">
    <p>Feature Extraction</p>
    <p>Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, ...</p>
    <p>Document</p>
    <p>Showers continued week Bahia cocoa zone alleviating drought early January improving prospects coming temporao, ...</p>
    <p>shower continu week bahia cocoa zone allevi drought earli januari improv prospect come temporao, ...</p>
    <p>Stop-word Removal</p>
    <p>Stemming</p>
    <p>shower bahia cocoa</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>TFIDF weighted Bag-of-words[Lewis, et al., 1978]</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Average PRBEP</p>
    <p>SVM TSVM SGT LP MP MAD</p>
    <p>Reuters 48.9 59.3 60.3 59.7 66.3</p>
    <p>WebKB 23.0 29.2 36.8 41.2 51.9 53.7</p>
    <p>Precision-recall break even point (PRBEP)</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Average PRBEP</p>
    <p>SVM TSVM SGT LP MP MAD</p>
    <p>Reuters 48.9 59.3 60.3 59.7 66.3</p>
    <p>WebKB 23.0 29.2 36.8 41.2 51.9 53.7</p>
    <p>Precision-recall break even point (PRBEP)</p>
    <p>Support Vector</p>
    <p>Machine (Supervised)</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Average PRBEP</p>
    <p>SVM TSVM SGT LP MP MAD</p>
    <p>Reuters 48.9 59.3 60.3 59.7 66.3</p>
    <p>WebKB 23.0 29.2 36.8 41.2 51.9 53.7</p>
    <p>Precision-recall break even point (PRBEP)</p>
    <p>Support Vector</p>
    <p>Machine (Supervised)</p>
    <p>Transductive SVM</p>
    <p>[Joachims 1999]</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Average PRBEP</p>
    <p>SVM TSVM SGT LP MP MAD</p>
    <p>Reuters 48.9 59.3 60.3 59.7 66.3</p>
    <p>WebKB 23.0 29.2 36.8 41.2 51.9 53.7</p>
    <p>Precision-recall break even point (PRBEP)</p>
    <p>Support Vector</p>
    <p>Machine (Supervised)</p>
    <p>Transductive SVM</p>
    <p>[Joachims 1999]</p>
    <p>Spectral Graph</p>
    <p>Transduction (SGT)</p>
    <p>[Joachims 2003]</p>
    <p>Label Propagation [Zhu &amp; Ghahramani 2002]</p>
    <p>Measure Propagation [Subramanya &amp; Bilmes 2008]</p>
    <p>Modified Adsorption [Talukdar &amp; Crammer 2009]</p>
  </div>
  <div class="page">
    <p>Results on WebKB</p>
    <p>[Subramanya &amp; Bilmes, EMNLP 2008] 75</p>
  </div>
  <div class="page">
    <p>Results on WebKB Modified Adsorption (MAD)</p>
    <p>[Talukdar &amp; Crammer 2009]</p>
    <p>[Subramanya &amp; Bilmes, EMNLP 2008] 75</p>
  </div>
  <div class="page">
    <p>Results on WebKB Modified Adsorption (MAD)</p>
    <p>[Talukdar &amp; Crammer 2009]</p>
    <p>More labeled data =&gt; Better Performance  Unnormalized distributions (scores) more suitable for multi-label problems (MAD outperforms other approaches)</p>
    <p>[Subramanya &amp; Bilmes, EMNLP 2008] 75</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Text Categorization - Sentiment Analysis - Class Instance Acquisition - POS Tagging - MultiLingual POS Tagging - Semantic Parsing</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>fortunately, they managed to do it in an interesting and funny way.</p>
    <p>he is one of the most exciting martial artists on the big screen.</p>
    <p>the romance was enchanting.</p>
    <p>A woman in peril. A confrontation. An explosion. The end. Yawn. Yawn. Yawn.</p>
    <p>dont go see this movie</p>
    <p>Movie review dataset [Pang et al. EMNLP 2002]</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Given a document either  classify it as expressing a positive or</p>
    <p>negative sentiment or</p>
    <p>assign a star rating  Similar to text categorization  Can be solved using standard machine</p>
    <p>learning approaches [Pang, Lee &amp; Vaidyanathan, EMNLP 2002]</p>
  </div>
  <div class="page">
    <p>Polarity Lexicons (I)</p>
    <p>Large lists of phrases that encode the polarity (positive or negative) of each phrase</p>
    <p>Positive polarity: enjoyable, breathtakingly, once in a life time</p>
    <p>Negative polarity: bad, humorless, unbearable, out of touch, bumps in the road</p>
    <p>Best results obtained by combining with machine learning approaches [Wilson et al., HLT-EMNLP 05; BlairGoldensohn et al. 08; Choi &amp; Cardie EMNLP 09]</p>
  </div>
  <div class="page">
    <p>Polarity Lexicons (II)</p>
    <p>Common strategy: start with two small seed sets  P: positive phrases, e.g., great fantastic  N: negative phrases, e.g., awful, dreadful</p>
    <p>Grow lexicons with graph propagation algorithms</p>
  </div>
  <div class="page">
    <p>Polarity Lexicons (II)</p>
    <p>Common strategy: start with two small seed sets  P: positive phrases, e.g., great fantastic  N: negative phrases, e.g., awful, dreadful</p>
    <p>Grow lexicons with graph propagation algorithms</p>
    <p>great</p>
    <p>love it</p>
    <p>good</p>
    <p>nice</p>
    <p>excellent</p>
    <p>awful</p>
    <p>fantastic</p>
    <p>painful</p>
    <p>bad</p>
    <p>irritatingugly</p>
    <p>dreadful</p>
    <p>pretty slightly off</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Polarity Lexicons (II)</p>
    <p>Common strategy: start with two small seed sets  P: positive phrases, e.g., great fantastic  N: negative phrases, e.g., awful, dreadful</p>
    <p>Grow lexicons with graph propagation algorithms</p>
    <p>great</p>
    <p>love it</p>
    <p>good</p>
    <p>nice</p>
    <p>excellent</p>
    <p>awful</p>
    <p>fantastic</p>
    <p>painful</p>
    <p>bad</p>
    <p>irritatingugly</p>
    <p>dreadful</p>
    <p>pretty slightly off</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Polarity Lexicons (II)</p>
    <p>Common strategy: start with two small seed sets  P: positive phrases, e.g., great fantastic  N: negative phrases, e.g., awful, dreadful</p>
    <p>Grow lexicons with graph propagation algorithms</p>
    <p>great</p>
    <p>love it</p>
    <p>good</p>
    <p>nice</p>
    <p>excellent</p>
    <p>awful</p>
    <p>fantastic</p>
    <p>painful</p>
    <p>bad</p>
    <p>irritatingugly</p>
    <p>dreadful</p>
    <p>pretty slightly off</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Polarity Lexicons (II)</p>
    <p>Common strategy: start with two small seed sets  P: positive phrases, e.g., great fantastic  N: negative phrases, e.g., awful, dreadful</p>
    <p>Grow lexicons with graph propagation algorithms</p>
    <p>great</p>
    <p>love it</p>
    <p>good</p>
    <p>nice</p>
    <p>excellent</p>
    <p>awful</p>
    <p>fantastic</p>
    <p>painful</p>
    <p>bad</p>
    <p>irritatingugly</p>
    <p>dreadful</p>
    <p>pretty slightly off</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Graph Construction (I)</p>
    <p>WordNet [Hu &amp; Liu, KDD 04; Kim &amp; Hovy, ICCL 04; BlairGoldensohn 08; Rao &amp; Ravichandran EACL 09]</p>
    <p>Defines synonyms, antonyms, hypernyms, etc.  Make edges between synonyms  Enforce constraints between antonyms  Issues  coverage  hard to find resources for all languages</p>
  </div>
  <div class="page">
    <p>Graph Construction (II)</p>
    <p>Use web data!  All n-grams (phrases) up to length 10 from 4</p>
    <p>billion web pages</p>
    <p>Pruned down to 20 million candidate phrases</p>
    <p>Feature vector obtained by aggregating words that occurred in local context</p>
    <p>Graph is more syntactic than semantic [Velikovich, et al., NAACL 2010]</p>
  </div>
  <div class="page">
    <p>Graph Propagation (I)</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>Equal?</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>excellent appealing</p>
    <p>satisfying</p>
    <p>so-so -0.1</p>
  </div>
  <div class="page">
    <p>Graph Propagation (I)</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>Equal?</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>excellent appealing</p>
    <p>satisfying</p>
    <p>so-so -0.1</p>
    <p>No benefit from multiple</p>
    <p>sources 83</p>
  </div>
  <div class="page">
    <p>Graph Propagation (II)</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>Worse?</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>excellent appealing</p>
    <p>satisfying</p>
    <p>so-so -0.1</p>
  </div>
  <div class="page">
    <p>Graph Propagation (III)</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>so-so -0.1</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>so-so -0.1</p>
  </div>
  <div class="page">
    <p>Graph Propagation (III)</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>so-so -0.1</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0</p>
    <p>so-so -0.1</p>
    <p>Single edge</p>
    <p>difference</p>
  </div>
  <div class="page">
    <p>Best Path to Seed Propagation</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0 0.5</p>
    <p>so-so -0.1</p>
    <p>[Velikovich, et al., NAACL 2010] 86</p>
  </div>
  <div class="page">
    <p>Best Path to Seed Propagation</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0 0.5</p>
    <p>so-so -0.1</p>
    <p>[Velikovich, et al., NAACL 2010] 86</p>
  </div>
  <div class="page">
    <p>Best Path to Seed Propagation</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0 0.5</p>
    <p>so-so -0.1</p>
    <p>[Velikovich, et al., NAACL 2010] 86</p>
  </div>
  <div class="page">
    <p>Best Path to Seed Propagation</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0 0.5</p>
    <p>so-so -0.1</p>
    <p>[Velikovich, et al., NAACL 2010] 86</p>
  </div>
  <div class="page">
    <p>Best Path to Seed Propagation</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0 0.5</p>
    <p>so-so -0.1</p>
    <p>Best Path</p>
    <p>[Velikovich, et al., NAACL 2010] 86</p>
  </div>
  <div class="page">
    <p>Best Path to Seed Propagation</p>
    <p>great</p>
    <p>good ok</p>
    <p>nice 1.0 0.5</p>
    <p>so-so -0.1</p>
    <p>Best Path</p>
    <p>Key observation: sentiment phrases are those that have short highly weighted paths to multiple seeds</p>
    <p>[Velikovich, et al., NAACL 2010] 86</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Lexicon Phrases Positive Negative</p>
    <p>Wilson et al. 2005 7,618 2,718 4,900</p>
    <p>WordNet LP [Blair-Goldensohn et al. 07]</p>
    <p>Web GP [Velikovich et al. 2010]</p>
    <p>Size of the output lexicon</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Multi-word expressions once in a life time</p>
    <p>state - of - the - art fail - safe operation just what you need</p>
    <p>just what the doctor ordered out of this world top of the line</p>
    <p>melt in your mouth snug as a bug up to the job</p>
    <p>out of the box more good than bad</p>
    <p>Vulgarity, ??? $#%! face $#%!ed up</p>
    <p>shut your $#%!ing mouth complete bull$#%!</p>
    <p>bladder spasms green slime</p>
    <p>vacuum of leadership electro - static discharge</p>
    <p>muttered under his breath harm to the environment</p>
    <p>Positive</p>
    <p>What youd expect excellent fabulous beautiful inspiring awesome</p>
    <p>plucky ravishing brilliant</p>
    <p>nice delightful splendid</p>
    <p>incredible stupendous comfortable</p>
    <p>Spelling variations loveable</p>
    <p>nicee niice</p>
    <p>cooool coooool koool kewl cozy cosy sikk</p>
    <p>What youd expect bad</p>
    <p>awful terrible dirty</p>
    <p>repulsive crappy sucky subpar</p>
    <p>horrendous miserable</p>
    <p>lousy abysmal stupid</p>
    <p>wretched</p>
    <p>Negative</p>
    <p>Multi-word expressions run of the mill out of touch over the hill</p>
    <p>flash in the pan bumps in the road</p>
    <p>hit or miss foaming at the mouth</p>
    <p>dime a dozen pie - in - the - sky</p>
    <p>cast a pall over sick to my stomach</p>
    <p>pain in my ass</p>
    <p>[Velikovich, et al., NAACL 2010] 88</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Multi-word expressions once in a life time</p>
    <p>state - of - the - art fail - safe operation just what you need</p>
    <p>just what the doctor ordered out of this world top of the line</p>
    <p>melt in your mouth snug as a bug up to the job</p>
    <p>out of the box more good than bad</p>
    <p>Vulgarity, ??? $#%! face $#%!ed up</p>
    <p>shut your $#%!ing mouth complete bull$#%!</p>
    <p>bladder spasms green slime</p>
    <p>vacuum of leadership electro - static discharge</p>
    <p>muttered under his breath harm to the environment</p>
    <p>Positive</p>
    <p>What youd expect excellent fabulous beautiful inspiring awesome</p>
    <p>plucky ravishing brilliant</p>
    <p>nice delightful splendid</p>
    <p>incredible stupendous comfortable</p>
    <p>Spelling variations loveable</p>
    <p>nicee niice</p>
    <p>cooool coooool koool kewl cozy cosy sikk</p>
    <p>What youd expect bad</p>
    <p>awful terrible dirty</p>
    <p>repulsive crappy sucky subpar</p>
    <p>horrendous miserable</p>
    <p>lousy abysmal stupid</p>
    <p>wretched</p>
    <p>Negative</p>
    <p>Multi-word expressions run of the mill out of touch over the hill</p>
    <p>flash in the pan bumps in the road</p>
    <p>hit or miss foaming at the mouth</p>
    <p>dime a dozen pie - in - the - sky</p>
    <p>cast a pall over sick to my stomach</p>
    <p>pain in my ass</p>
    <p>Ability to learn spelling</p>
    <p>variations and mistakes</p>
    <p>[Velikovich, et al., NAACL 2010] 88</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Recall</p>
    <p>P re</p>
    <p>c is</p>
    <p>io n</p>
    <p>Wilson et al.</p>
    <p>WordNet LP</p>
    <p>Web GPPositive</p>
    <p>[Velikovich, et al., NAACL 2010] 89</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Recall</p>
    <p>P re</p>
    <p>c is</p>
    <p>io n</p>
    <p>Wilson et al.</p>
    <p>WordNet LP</p>
    <p>Web GPPositive</p>
    <p>Recall</p>
    <p>P re</p>
    <p>c is</p>
    <p>io n</p>
    <p>Wilson et al.</p>
    <p>WordNet LP</p>
    <p>Web GP Negative</p>
    <p>[Velikovich, et al., NAACL 2010] 89</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Recall</p>
    <p>P re</p>
    <p>c is</p>
    <p>io n</p>
    <p>Wilson et al.</p>
    <p>WordNet LP</p>
    <p>Web GPPositive</p>
    <p>Recall</p>
    <p>P re</p>
    <p>c is</p>
    <p>io n</p>
    <p>Wilson et al.</p>
    <p>WordNet LP</p>
    <p>Web GP Negative</p>
    <p>Resulting lexicon is larger in size and has much better precision</p>
    <p>[Velikovich, et al., NAACL 2010] 89</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
    <p>- Text Categorization - Sentiment Analysis - Class Instance Acquisition</p>
    <p>[Talukdar et al., EMNLP 2008]</p>
    <p>- POS Tagging - MultiLingual POS Tagging - Semantic Parsing</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Given an entity, assign human readable descriptors to it  Toyota is a car manufacturer, japanese company, multinational company</p>
    <p>African countries such as Uganda and Angola</p>
    <p>Large scale, open domain (&gt; 100 classes)</p>
    <p>Applications  web search, advertising, etc.</p>
  </div>
  <div class="page">
    <p>Extraction Techniques</p>
  </div>
  <div class="page">
    <p>Extraction Techniques .... What Other Musicians Would Fans of the Album Listen to: Storytelling musicians come to mind. Musicians such as Johnny Cash, and Woodie Guthrie. What is Distinctive About this Release?: Every song on the album has its own unique sound. From the fast paced That Texas Girl to the acoustic ....</p>
    <p>[van Durme and Pasca, AAAI 2008]  Uses &lt;Class&gt; such as &lt;Instance&gt; patterns</p>
    <p>Extracts both class (musician) and instance (Johnny Cash)</p>
  </div>
  <div class="page">
    <p>Extraction Techniques .... What Other Musicians Would Fans of the Album Listen to: Storytelling musicians come to mind. Musicians such as Johnny Cash, and Woodie Guthrie. What is Distinctive About this Release?: Every song on the album has its own unique sound. From the fast paced That Texas Girl to the acoustic ....</p>
    <p>[van Durme and Pasca, AAAI 2008]  Uses &lt;Class&gt; such as &lt;Instance&gt; patterns</p>
    <p>Extracts both class (musician) and instance (Johnny Cash)</p>
    <p>Extractions from HTML lists and tables</p>
    <p>[Wang and Cohen, ICDM 2007]  WebTables [Cafarella et al., VLDB 2008], 154 million HTML tables</p>
  </div>
  <div class="page">
    <p>Extraction Techniques .... What Other Musicians Would Fans of the Album Listen to: Storytelling musicians come to mind. Musicians such as Johnny Cash, and Woodie Guthrie. What is Distinctive About this Release?: Every song on the album has its own unique sound. From the fast paced That Texas Girl to the acoustic ....</p>
    <p>[van Durme and Pasca, AAAI 2008]  Uses &lt;Class&gt; such as &lt;Instance&gt; patterns</p>
    <p>Extracts both class (musician) and instance (Johnny Cash)</p>
    <p>Extractions from HTML lists and tables</p>
    <p>[Wang and Cohen, ICDM 2007]  WebTables [Cafarella et al., VLDB 2008], 154 million HTML tables</p>
    <p>Pattern-based methods are usually tuned for high-precision, resulting in low coverage</p>
    <p>Can we combine extractions from all methods (and sources) to improve coverage?</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Extraction Confidence</p>
  </div>
  <div class="page">
    <p>Graph Construction</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Bi-partite graph (not a k-NN graph)  Set nodes encourage members of the set to have similar labels  Natural way to represent extractions from many sources and methods</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician Singer</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician</p>
    <p>Singer Musician</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician</p>
    <p>Singer</p>
    <p>Musician</p>
    <p>Musician</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Pattern Table Mining</p>
    <p>Set 1</p>
    <p>Bob Dylan (0.95)</p>
    <p>Johnny Cash (0.87)</p>
    <p>Billy Joel (0.82)</p>
    <p>Set 2</p>
    <p>Billy Joel (0.72)</p>
    <p>Johnny Cash (0.73)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician</p>
    <p>Singer</p>
    <p>Can we infer that Bob</p>
    <p>Dylan is also a musician?</p>
    <p>Musician</p>
    <p>Musician</p>
  </div>
  <div class="page">
    <p>Graph Propagation</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
  </div>
  <div class="page">
    <p>Graph Propagation</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Seed 96</p>
  </div>
  <div class="page">
    <p>Graph Propagation</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>SeedPrediction 96</p>
  </div>
  <div class="page">
    <p>Graph Propagation</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0Musician,1.0Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>SeedPrediction 96</p>
  </div>
  <div class="page">
    <p>Graph Propagation</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0Musician,1.0Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,0.8</p>
    <p>SeedPrediction 96</p>
  </div>
  <div class="page">
    <p>Graph Propagation</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Bob Dylan</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0Musician,1.0Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,1.0</p>
    <p>Musician,0.6</p>
    <p>Musician,0.8</p>
    <p>SeedPrediction 96</p>
  </div>
  <div class="page">
    <p>Evaluation Metric</p>
    <p>MRR = 1</p>
    <p>|test-set|</p>
    <p>vtest-set</p>
    <p>rankv(class(v))</p>
    <p>Mean Reciprocal Rank</p>
  </div>
  <div class="page">
    <p>Evaluation Metric</p>
    <p>MRR = 1</p>
    <p>|test-set|</p>
    <p>vtest-set</p>
    <p>rankv(class(v))</p>
    <p>Mean Reciprocal Rank</p>
    <p>Billy Joel</p>
    <p>Linguist, 0.6</p>
    <p>Musician,0.4</p>
  </div>
  <div class="page">
    <p>Evaluation Metric</p>
    <p>MRR = 1</p>
    <p>|test-set|</p>
    <p>vtest-set</p>
    <p>rankv(class(v))</p>
    <p>Mean Reciprocal Rank</p>
    <p>Billy Joel</p>
    <p>Linguist, 0.6</p>
    <p>Musician,0.4</p>
    <p>Musician</p>
    <p>Gold Label</p>
  </div>
  <div class="page">
    <p>Evaluation Metric</p>
    <p>MRR = 1</p>
    <p>|test-set|</p>
    <p>vtest-set</p>
    <p>rankv(class(v))</p>
    <p>Mean Reciprocal Rank</p>
    <p>Billy Joel</p>
    <p>Linguist, 0.6</p>
    <p>Musician,0.4</p>
    <p>Musician</p>
    <p>Gold Label MRR 0.5</p>
  </div>
  <div class="page">
    <p>Extraction for Known Instances Graph with 1.4m nodes,</p>
    <p>Evaluation against WordNet Dataset (38 classes, 8910 instances)</p>
    <p>M ea</p>
    <p>n R</p>
    <p>ec ip</p>
    <p>ro ca</p>
    <p>l R an</p>
    <p>k (M</p>
    <p>R R</p>
    <p>)</p>
    <p>Recall</p>
    <p>Patterns Adsorption WebTables</p>
  </div>
  <div class="page">
    <p>Extraction for Known Instances Graph with 1.4m nodes,</p>
    <p>Evaluation against WordNet Dataset (38 classes, 8910 instances)</p>
    <p>M ea</p>
    <p>n R</p>
    <p>ec ip</p>
    <p>ro ca</p>
    <p>l R an</p>
    <p>k (M</p>
    <p>R R</p>
    <p>)</p>
    <p>Recall</p>
    <p>Patterns Adsorption WebTables</p>
    <p>Adsorption is able to assign better class labels to more instances.</p>
  </div>
  <div class="page">
    <p>Extracted Pairs</p>
    <p>Class A few non-seed Instances found by Adsorption</p>
    <p>Scientific Journals Journal of Physics, Nature, Structural and Molecular Biology, Sciences Sociales et sante, Kidney and Blood Pressure Research, American Journal of Physiology-Cell Physiology,</p>
    <p>NFL Players Tony Gonzales, Thabiti Davis, Taylor Stubblefield, Ron Dixon, Rodney Hannan,</p>
    <p>Book Publishers Small Night Shade Books, House of Ansari Press, Highwater Books, Distributed Art Publishers, Cooper Canyon Press,</p>
    <p>Total classes: 9081</p>
  </div>
  <div class="page">
    <p>Extracted Pairs</p>
    <p>Class A few non-seed Instances found by Adsorption</p>
    <p>Scientific Journals Journal of Physics, Nature, Structural and Molecular Biology, Sciences Sociales et sante, Kidney and Blood Pressure Research, American Journal of Physiology-Cell Physiology,</p>
    <p>NFL Players Tony Gonzales, Thabiti Davis, Taylor Stubblefield, Ron Dixon, Rodney Hannan,</p>
    <p>Book Publishers Small Night Shade Books, House of Ansari Press, Highwater Books, Distributed Art Publishers, Cooper Canyon Press,</p>
    <p>Total classes: 9081</p>
    <p>Graph-based methods can easily handle large number of classes</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>TextRunner Graph, 170 WordNet Classes M</p>
    <p>e a n</p>
    <p>R e</p>
    <p>ci p</p>
    <p>ro ca</p>
    <p>l R</p>
    <p>a n</p>
    <p>k (</p>
    <p>M R</p>
    <p>R )</p>
    <p>Amount of Supervision</p>
    <p>LP-ZGL Adsorption MAD</p>
    <p>Graph with 175k nodes 529k edges</p>
    <p>Data available @ http://www.talukdar.net/datasets/class_inst/</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Freebase-2 Graph, 192 WordNet Classes</p>
    <p>M e</p>
    <p>a n</p>
    <p>R e</p>
    <p>ci p</p>
    <p>ro ca</p>
    <p>l R</p>
    <p>a n</p>
    <p>k (</p>
    <p>M R</p>
    <p>R )</p>
    <p>Amount of Supervision</p>
    <p>LP-ZGL Adsorption MAD</p>
    <p>Graph with 303k nodes 2.3m edges</p>
  </div>
  <div class="page">
    <p>Semantic Constraints</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
  </div>
  <div class="page">
    <p>Semantic Constraints</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Suppose we knew that both Johnny Cash and Billy Joel have albums.</p>
    <p>How do we encode this constraint?</p>
  </div>
  <div class="page">
    <p>Solution (I)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
  </div>
  <div class="page">
    <p>Solution (I)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
  </div>
  <div class="page">
    <p>Solution (I)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
    <p>Graph is no longer bi-partite (not necessarily bad)  Can lead to cliques of size of number of instances in the constraint (bad)</p>
  </div>
  <div class="page">
    <p>Solution (II)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
    <p>[Talukdar &amp; Periera, ACL 2010] 104</p>
  </div>
  <div class="page">
    <p>Solution (II)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
    <p>Have_Album</p>
    <p>[Talukdar &amp; Periera, ACL 2010] 104</p>
  </div>
  <div class="page">
    <p>Solution (II)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
    <p>Have_Album</p>
    <p>[Talukdar &amp; Periera, ACL 2010] 104</p>
  </div>
  <div class="page">
    <p>Solution (II)</p>
    <p>Set 1</p>
    <p>Set 2</p>
    <p>Isaac Newton</p>
    <p>Johnny Cash</p>
    <p>Billy Joel</p>
    <p>Both Johnny Cash and Billy Joel have albums.</p>
    <p>Have_Album</p>
    <p>Semantic Constraints may be easily encoded [Talukdar &amp; Periera, ACL 2010]</p>
  </div>
  <div class="page">
    <p>Results with Semantic Constraints</p>
    <p>e a n</p>
    <p>R e</p>
    <p>ci p</p>
    <p>ro ca</p>
    <p>l R</p>
    <p>a n</p>
    <p>k (</p>
    <p>M R</p>
    <p>R )</p>
    <p>TextRunner Graph YAGO Graph TextRunner + YAGO Graph</p>
    <p>[Talukdar &amp; Periera, ACL 2010] 105</p>
  </div>
  <div class="page">
    <p>Results with Semantic Constraints</p>
    <p>e a n</p>
    <p>R e</p>
    <p>ci p</p>
    <p>ro ca</p>
    <p>l R</p>
    <p>a n</p>
    <p>k (</p>
    <p>M R</p>
    <p>R )</p>
    <p>TextRunner Graph YAGO Graph TextRunner + YAGO Graph</p>
    <p>With Semantic Constraints</p>
    <p>[Talukdar &amp; Periera, ACL 2010] 105</p>
  </div>
  <div class="page">
    <p>Results with Semantic Constraints</p>
    <p>e a n</p>
    <p>R e</p>
    <p>ci p</p>
    <p>ro ca</p>
    <p>l R</p>
    <p>a n</p>
    <p>k (</p>
    <p>M R</p>
    <p>R )</p>
    <p>TextRunner Graph YAGO Graph TextRunner + YAGO Graph</p>
    <p>Additional semantic constraints help</p>
    <p>improve performance significantly.</p>
    <p>With Semantic Constraints</p>
    <p>[Talukdar &amp; Periera, ACL 2010] 105</p>
  </div>
  <div class="page">
    <p>- Text Categorization - Sentiment Analysis - Class Instance Acquisition - POS Tagging [Subramanya et. al., EMNLP 2008]</p>
    <p>- MultiLingual POS Tagging - Semantic Parsing</p>
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Domain Adaptation</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>... bought a book detailing the ...</p>
    <p>... wanted to book a flight to ...</p>
    <p>... the book is about the ...</p>
    <p>Domain Adaptation</p>
    <p>... VBD DT NN VBG DT ...</p>
    <p>... VBD TO VB DT NN TO ...</p>
    <p>... DT NN VBZ PP DT ...</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>... how to book a band ... can you book a day room ...</p>
    <p>... bought a book detailing the ...</p>
    <p>... wanted to book a flight to ...</p>
    <p>... the book is about the ...</p>
    <p>Domain Adaptation</p>
    <p>... VBD DT NN VBG DT ...</p>
    <p>... VBD TO VB DT NN TO ...</p>
    <p>... DT NN VBZ PP DT ...</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>... how to book a band ... can you book a day room ...</p>
    <p>... bought a book detailing the ...</p>
    <p>... wanted to book a flight to ...</p>
    <p>... the book is about the ...</p>
    <p>Domain Adaptation</p>
    <p>... VBD DT NN VBG DT ...</p>
    <p>... VBD TO VB DT NN TO ...</p>
    <p>... DT NN VBZ PP DT ...</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>... how to book a band ... can you book a day room ...</p>
    <p>... bought a book detailing the ...</p>
    <p>... wanted to book a flight to ...</p>
    <p>... the book is about the ...</p>
    <p>Domain Adaptation</p>
    <p>... VBD DT NN VBG DT ...</p>
    <p>... VBD TO VB DT NN TO ...</p>
    <p>... DT NN VBZ PP DT ...</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>... how to book a band ... can you book a day room ...</p>
    <p>... bought a book detailing the ...</p>
    <p>... wanted to book a flight to ...</p>
    <p>... the book is about the ... ... how to unrar a file ...</p>
    <p>Domain Adaptation</p>
    <p>... VBD DT NN VBG DT ...</p>
    <p>... VBD TO VB DT NN TO ...</p>
    <p>... DT NN VBZ PP DT ...</p>
    <p>Unlabeled Data</p>
    <p>Small amounts of labeled</p>
    <p>source domain data</p>
    <p>Large amounts of unlabeled</p>
    <p>target domain data</p>
  </div>
  <div class="page">
    <p>Graph Construction (I)</p>
    <p>when do you book plane tickets?</p>
    <p>do you read a book on the plane?</p>
  </div>
  <div class="page">
    <p>Graph Construction (I)</p>
    <p>when do you book plane tickets?</p>
    <p>do you read a book on the plane?</p>
    <p>Similarity</p>
  </div>
  <div class="page">
    <p>Graph Construction (I)</p>
    <p>when do you book plane tickets? WRB VBP PRP VB NN NNS</p>
    <p>VBP PRP VB DT NN IN DT NN</p>
    <p>do you read a book on the plane?</p>
    <p>Similarity</p>
  </div>
  <div class="page">
    <p>Graph Construction (II)</p>
    <p>how much does it cost to book a band ?</p>
    <p>what was the book that has no letter e ?</p>
    <p>how to get a book agent ?</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
  </div>
  <div class="page">
    <p>Graph Construction (II)</p>
    <p>how much does it cost to book a band ?</p>
    <p>what was the book that has no letter e ?</p>
    <p>how to get a book agent ?</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
  </div>
  <div class="page">
    <p>Graph Construction (II)</p>
    <p>how much does it cost to book a band ?</p>
    <p>what was the book that has no letter e ?</p>
    <p>how to get a book agent ?</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
  </div>
  <div class="page">
    <p>Graph Construction (II)</p>
    <p>you book a the book that</p>
    <p>to book a</p>
    <p>a book agent</p>
  </div>
  <div class="page">
    <p>Graph Construction (II)</p>
    <p>you book a the book that</p>
    <p>to book a</p>
    <p>a book agent</p>
    <p>k-nearest neighbors?</p>
  </div>
  <div class="page">
    <p>Graph Construction (III)</p>
    <p>you book a the book that</p>
    <p>to book a</p>
    <p>a book agent</p>
    <p>the city that</p>
    <p>to run a</p>
    <p>to become a</p>
    <p>you start a</p>
    <p>a movie agent</p>
  </div>
  <div class="page">
    <p>Graph Construction (III)</p>
    <p>you book a the book that</p>
    <p>to book a</p>
    <p>a book agent</p>
    <p>the city that</p>
    <p>to run a</p>
    <p>to become a</p>
    <p>you start a</p>
    <p>a movie agent</p>
  </div>
  <div class="page">
    <p>Graph Construction (III)</p>
    <p>you book a the book that</p>
    <p>to book a</p>
    <p>a book agent</p>
    <p>the city that</p>
    <p>to run a</p>
    <p>to become a</p>
    <p>you start a</p>
    <p>a movie agent</p>
    <p>you unrar a</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?to book acost to book a band</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?to book acost to book a band</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word book</p>
    <p>Trigram - Center Word to ____ a</p>
    <p>Left Word + Right Context to ____ a band</p>
    <p>Left Context + Right Word cost to ____ a</p>
    <p>Suffix none</p>
    <p>to book a</p>
    <p>cost to book a band</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word book</p>
    <p>Trigram - Center Word to ____ a</p>
    <p>Left Word + Right Context to ____ a band</p>
    <p>Left Context + Right Word cost to ____ a</p>
    <p>Suffix none</p>
    <p>to book a</p>
    <p>cost to book a band</p>
    <p>cost to</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word book</p>
    <p>Trigram - Center Word to ____ a</p>
    <p>Left Word + Right Context to ____ a band</p>
    <p>Left Context + Right Word cost to ____ a</p>
    <p>Suffix none</p>
    <p>to book a</p>
    <p>cost to book a band</p>
    <p>cost to</p>
    <p>a band</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word book</p>
    <p>Trigram - Center Word to ____ a</p>
    <p>Left Word + Right Context to ____ a band</p>
    <p>Left Context + Right Word cost to ____ a</p>
    <p>Suffix none</p>
    <p>to book a</p>
    <p>cost to book a band</p>
    <p>cost to</p>
    <p>a band</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word book</p>
    <p>Trigram - Center Word to ____ a</p>
    <p>Left Word + Right Context to ____ a band</p>
    <p>Left Context + Right Word cost to ____ a</p>
    <p>Suffix none</p>
    <p>to book a</p>
    <p>cost to book a band</p>
    <p>cost to</p>
    <p>a band</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?</p>
    <p>how much to book a flight to paris?</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?to book a</p>
    <p>how much to book a flight to paris?</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?to book a</p>
    <p>how much to book a flight to paris?</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>how much does it cost to book a band ?to book a</p>
    <p>how much to book a flight to paris?</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>to book ato book a</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>to book a</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word</p>
    <p>Trigram - Center Word</p>
    <p>Left Word + Right Context</p>
    <p>Left Context + Right Word</p>
    <p>Suffix</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>to book a</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word</p>
    <p>Trigram - Center Word</p>
    <p>Left Word + Right Context</p>
    <p>Left Context + Right Word</p>
    <p>Suffix</p>
    <p>Poin t-wis</p>
    <p>e Mu tual</p>
    <p>Infor mati</p>
    <p>on</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>to book a</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word</p>
    <p>Trigram - Center Word</p>
    <p>Left Word + Right Context</p>
    <p>Left Context + Right Word</p>
    <p>Suffix</p>
    <p>Point-wise Mutual Information</p>
  </div>
  <div class="page">
    <p>Graph Construction - Features</p>
    <p>to book a</p>
    <p>Trigram + Context</p>
    <p>Left Context</p>
    <p>Right Context</p>
    <p>Center Word</p>
    <p>Trigram - Center Word</p>
    <p>Left Word + Right Context</p>
    <p>Left Context + Right Word</p>
    <p>Suffix</p>
  </div>
  <div class="page">
    <p>Similarity Function</p>
    <p>to book a 0.10.4 . . .</p>
  </div>
  <div class="page">
    <p>Similarity Function</p>
    <p>to book a 0.10.4 . . .</p>
    <p>you unrar a</p>
  </div>
  <div class="page">
    <p>Similarity Function</p>
    <p>to book a</p>
    <p>you unrar a</p>
  </div>
  <div class="page">
    <p>Similarity Function</p>
    <p>to book a</p>
  </div>
  <div class="page">
    <p>Similarity Function</p>
    <p>to book a</p>
    <p>you unrar a</p>
  </div>
  <div class="page">
    <p>Approach (I) 1. Train a CRF on labeled data 2. While not converged do:</p>
  </div>
  <div class="page">
    <p>Approach (I)</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
    <p>how to get a book agent ?</p>
    <p>how do you book a flight to multiple cities ?</p>
    <p>how to unrar a zipped file ?</p>
  </div>
  <div class="page">
    <p>Approach (I)</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
    <p>how to get a book agent ?</p>
    <p>CRF</p>
    <p>how do you book a flight to multiple cities ?</p>
    <p>how to unrar a zipped file ?</p>
  </div>
  <div class="page">
    <p>Approach (I)</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
    <p>how to get a book agent ?</p>
    <p>how do you book a flight to multiple cities ?</p>
    <p>how to unrar a zipped file ?</p>
  </div>
  <div class="page">
    <p>Approach (II) 1. Train a CRF on labeled data 2. While not converged do:</p>
  </div>
  <div class="page">
    <p>Approach (II) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
    <p>how do you book a flight to multiple cities ?</p>
  </div>
  <div class="page">
    <p>Approach (II) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
    <p>how do you book a flight to multiple cities ?</p>
    <p>you book a</p>
  </div>
  <div class="page">
    <p>Approach (II) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>can you book a day room at hilton hawaiian village ?</p>
    <p>how do you book a flight to multiple cities ?</p>
    <p>you book a 1 n</p>
  </div>
  <div class="page">
    <p>Approach (III) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>you unrar a</p>
    <p>you start a</p>
    <p>you book a</p>
  </div>
  <div class="page">
    <p>Approach (III) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>you unrar a</p>
    <p>you start a</p>
    <p>you book a NN VB ....... 0.2 0.7</p>
    <p>.......</p>
    <p>NN VB ....... 0.3 0.5</p>
    <p>.......</p>
    <p>NN VB ....... 0.2 0.2</p>
    <p>.......</p>
  </div>
  <div class="page">
    <p>Approach (III) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>you unrar a</p>
    <p>you start a</p>
    <p>you book a NN VB ....... 0.2 0.7</p>
    <p>.......</p>
    <p>NN VB ....... 0.3 0.5</p>
    <p>.......</p>
    <p>NN VB ....... 0.2 0.2</p>
    <p>.......</p>
  </div>
  <div class="page">
    <p>Approach (III) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>you unrar a</p>
    <p>you start a</p>
    <p>you book a NN VB ....... 0.2 0.7</p>
    <p>.......</p>
    <p>NN VB ....... 0.3 0.5</p>
    <p>.......</p>
    <p>NN VB ....... 0.1 0.6</p>
    <p>.......</p>
  </div>
  <div class="page">
    <p>Approach (III) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>you unrar a</p>
    <p>you start a</p>
    <p>you book a NN VB ....... 0.2 0.7</p>
    <p>.......</p>
    <p>NN VB ....... 0.3 0.5</p>
    <p>.......</p>
    <p>NN VB ....... 0.1 0.6</p>
    <p>.......</p>
    <p>If two n-grams are similar according to the graph then their output distributions should be similar</p>
  </div>
  <div class="page">
    <p>Approach (IV) 1. Train a CRF on labeled data 2. While not converged do:</p>
  </div>
  <div class="page">
    <p>Approach (IV) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>Can you unrar a zipped file?</p>
  </div>
  <div class="page">
    <p>Approach (IV) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>Can you unrar a zipped file?</p>
  </div>
  <div class="page">
    <p>Approach (IV) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>Can you unrar a zipped file?</p>
    <p>NN VB ....... 0.2 0.2</p>
    <p>.......</p>
    <p>Posterior Decode</p>
  </div>
  <div class="page">
    <p>Approach (IV) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>Can you unrar a zipped file?</p>
    <p>NN VB ....... 0.1 0.6</p>
    <p>.......NN VB ....... 0.2 0.2</p>
    <p>.......</p>
    <p>Posterior Decode</p>
    <p>Graph Propagation</p>
  </div>
  <div class="page">
    <p>Approach (IV) 1. Train a CRF on labeled data 2. While not converged do:</p>
    <p>Can you unrar a zipped file?</p>
    <p>NN VB ....... 0.1 0.6</p>
    <p>.......NN VB ....... 0.2 0.2</p>
    <p>.......</p>
    <p>+Convex Combination</p>
    <p>Can you unrar a zipped file?</p>
    <p>NN VB ....... 0.15 0.45</p>
    <p>.......</p>
    <p>Posterior Decode</p>
    <p>Graph Propagation</p>
  </div>
  <div class="page">
    <p>Approach (V)</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>p0(y|x)Space of all distributions realizable using a CRF</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>p0(y|x)</p>
    <p>q(y|x)</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>p0(y|x)</p>
    <p>q(y|x)</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Converged graph posterior</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>p0(y|x)</p>
    <p>KL Projection</p>
    <p>q(y|x)</p>
    <p>q(y|x)</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Converged graph posterior</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>p0(y|x)</p>
    <p>KL Projection</p>
    <p>q(y|x)</p>
    <p>q(y|x)</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Converged graph posterior</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Viterbi Decoding : Intuition</p>
    <p>p0(y|x)</p>
    <p>KL Projection</p>
    <p>q(y|x)</p>
    <p>q(y|x)</p>
    <p>p(y|x)</p>
    <p>Space of all distributions realizable using a CRF</p>
    <p>Converged graph posterior</p>
    <p>Current estimate</p>
  </div>
  <div class="page">
    <p>Corpora</p>
    <p>Source Domain (labeled): Wall Street Journal (WSJ) section of the Penn Treebank.</p>
    <p>Target Domain:</p>
    <p>QuestionBank: 4000 labeled sentences  Penn BioTreebank: 1061 labeled sentences</p>
  </div>
  <div class="page">
    <p>Graph Construction: Question Bank</p>
    <p>WSJ Unlabeled Data</p>
  </div>
  <div class="page">
    <p>Graph Construction: Question Bank</p>
    <p>WSJ Unlabeled Data PMI</p>
    <p>Statistics</p>
  </div>
  <div class="page">
    <p>Graph Construction: Question Bank</p>
    <p>WSJ Unlabeled Data</p>
    <p>Similarity Graph Construction</p>
    <p>PMI Statistics</p>
  </div>
  <div class="page">
    <p>Graph Construction: Question Bank</p>
    <p>WSJ Unlabeled Data</p>
    <p>Similarity Graph Construction</p>
    <p>PMI Statistics</p>
    <p>Labels are not used during graph construction</p>
  </div>
  <div class="page">
    <p>Graph Construction: Question Bank</p>
    <p>WSJ Unlabeled Data</p>
    <p>Similarity Graph Construction</p>
    <p>PMI Statistics</p>
    <p>Labels are not used during graph construction</p>
    <p>Search Queries</p>
  </div>
  <div class="page">
    <p>Graph Construction: Question Bank</p>
    <p>WSJ Unlabeled Data</p>
    <p>Similarity Graph Construction</p>
    <p>PMI Statistics</p>
    <p>Labels are not used during graph construction</p>
    <p>Search Queries</p>
  </div>
  <div class="page">
    <p>Graph Construction: Bio</p>
    <p>WSJ Unlabeled Data</p>
    <p>Similarity Graph Construction</p>
    <p>PMI Statistics</p>
    <p>Labels are not used during graph construction</p>
    <p>(Blitzer et al. 2006)</p>
  </div>
  <div class="page">
    <p>Baseline (Supervised)</p>
    <p>Features: word identity, suffixes, prefixes &amp; special character detectors (dashes, digits, etc.).</p>
    <p>Achieves 97.17% accuracy on WSJ development set.</p>
    <p>Not the same as features used</p>
    <p>using graph construction</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Questions Bio</p>
    <p>Baseline 83.8 86.2</p>
    <p>Self-training 84.0 87.1</p>
    <p>Semi-supervised CRF</p>
  </div>
  <div class="page">
    <p>Analysis</p>
    <p>Questions Bio</p>
    <p>percentage of unlabeled trigrams not connected to and any labeled trigram 12.4 46.8</p>
    <p>average path length between an unlabeled trigram and its nearest labeled trigram 9.4 22.4</p>
  </div>
  <div class="page">
    <p>Analysis</p>
    <p>Questions Bio</p>
    <p>percentage of unlabeled trigrams not connected to and any labeled trigram 12.4 46.8</p>
    <p>average path length between an unlabeled trigram and its nearest labeled trigram 9.4 22.4</p>
    <p>Sparse Graph</p>
  </div>
  <div class="page">
    <p>Analysis</p>
    <p>Pros  Inductive  Produces a CRF (standard CRF inference</p>
    <p>infrastructure may be used)</p>
    <p>Issues  Graph construction  Graph is not integrated with CRF training</p>
  </div>
  <div class="page">
    <p>- Text Categorization - Sentiment Analysis - Class Instance Acquisition - POS Tagging - MultiLingual POS Tagging [Das &amp; Petrov, ACL 2011]</p>
    <p>- Semantic Parsing</p>
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Supervised POS taggers for English have accuracies in the high 90s for most domains  By comparison taggers in other languages are not as accurate  Performance ranges from between 60 - 80%</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Supervised POS taggers for English have accuracies in the high 90s for most domains  By comparison taggers in other languages are not as accurate  Performance ranges from between 60 - 80%</p>
    <p>Model in resource-rich language (e.g.,</p>
    <p>English)</p>
    <p>Model in resource-poor</p>
    <p>language</p>
    <p>Transfer Knowledge</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>The food at Google is good .</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>The food at Google is good . DET NOUN ADP NOUN VERB ADJ .</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>The food at Google is good . DET NOUN ADP NOUN VERB ADJ .</p>
    <p>Das Essen ist gut bei Google .</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>The food at Google is good . DET NOUN ADP NOUN VERB ADJ .</p>
    <p>Das Essen ist gut bei Google .</p>
    <p>Automatic alignments from translation data (available for more than 50 languages)</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>Das Essen ist gut bei Google.</p>
    <p>The DET</p>
    <p>food NOUN</p>
    <p>at ADP</p>
    <p>Google NOUN</p>
    <p>is VERB</p>
    <p>good ADJ</p>
    <p>.</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>DasEssen</p>
    <p>ist</p>
    <p>gut</p>
    <p>bei Google</p>
    <p>.</p>
    <p>The DET</p>
    <p>food NOUN</p>
    <p>at ADP</p>
    <p>Google NOUN</p>
    <p>is VERB</p>
    <p>good ADJ</p>
    <p>.</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>DasEssen</p>
    <p>ist</p>
    <p>gut</p>
    <p>bei Google</p>
    <p>.</p>
    <p>The DET</p>
    <p>food NOUN</p>
    <p>at ADP</p>
    <p>Google NOUN</p>
    <p>is VERB</p>
    <p>good ADJ</p>
    <p>.</p>
    <p>.</p>
    <p>bag of alignments</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>DasEssen</p>
    <p>ist</p>
    <p>gut</p>
    <p>bei Google</p>
    <p>.</p>
    <p>The DET</p>
    <p>food NOUN</p>
    <p>at ADP</p>
    <p>Google NOUN</p>
    <p>is VERB</p>
    <p>good ADJ</p>
    <p>.</p>
    <p>.</p>
    <p>bag of alignments</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>.</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection</p>
    <p>DasEssen</p>
    <p>ist</p>
    <p>gut</p>
    <p>bei Google</p>
    <p>.</p>
    <p>The DET</p>
    <p>food NOUN</p>
    <p>at ADP</p>
    <p>Google NOUN</p>
    <p>is VERB</p>
    <p>good ADJ</p>
    <p>.</p>
    <p>.</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>.</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>It PRON</p>
    <p>google VERB</p>
    <p>more alignments!</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>. PRON</p>
    <p>nicely ADV</p>
    <p>NOUN VERB</p>
    <p>ADJ ADP DET</p>
    <p>. PRON</p>
    <p>ADV</p>
    <p>fine ADJ</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection Results</p>
    <p>Danish Dutch German Greek Italian Portuguese Spanish Swedish Average</p>
    <p>FeatureHMM</p>
    <p>Feature-HMM [Berg-Kirkpatrick, NAACL 2010] 132</p>
  </div>
  <div class="page">
    <p>Cross-Lingual Projection Results</p>
    <p>Direct Projection 73.6 77.0 83.2 79.3 79.7 82.6 80.1 74.7 78.8</p>
    <p>Danish Dutch German Greek Italian Portuguese Spanish Swedish Average</p>
    <p>FeatureHMM</p>
    <p>Feature-HMM [Berg-Kirkpatrick, NAACL 2010] 132</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen ,</p>
    <p>ist wichtig bei</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen ,</p>
    <p>NOUN</p>
    <p>VERB</p>
    <p>separation!</p>
    <p>ist wichtig bei</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist wichtig bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen , eat</p>
    <p>food</p>
    <p>eat</p>
    <p>eating</p>
    <p>NOUN VERB</p>
    <p>VERB VERB</p>
    <p>good ADJ</p>
    <p>nicely ADV</p>
    <p>fine ADJ</p>
    <p>important ADJ</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist wichtig bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen , eat</p>
    <p>food</p>
    <p>eat</p>
    <p>eating</p>
    <p>NOUN VERB</p>
    <p>VERB VERB</p>
    <p>good ADJ</p>
    <p>nicely ADV</p>
    <p>fine ADJ</p>
    <p>important ADJ</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist wichtig bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen , eat</p>
    <p>food</p>
    <p>eat</p>
    <p>eating</p>
    <p>NOUN VERB</p>
    <p>VERB VERB</p>
    <p>good ADJ</p>
    <p>nicely ADV</p>
    <p>fine ADJ</p>
    <p>important ADJ</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist wichtig bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen , eat</p>
    <p>food</p>
    <p>eat</p>
    <p>eating</p>
    <p>NOUN VERB</p>
    <p>VERB VERB</p>
    <p>good ADJ</p>
    <p>nicely ADV</p>
    <p>fine ADJ</p>
    <p>important ADJ</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist wichtig bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen , eat</p>
    <p>food</p>
    <p>eat</p>
    <p>eating</p>
    <p>NOUN VERB</p>
    <p>VERB VERB</p>
    <p>good ADJ</p>
    <p>nicely ADV</p>
    <p>fine ADJ</p>
    <p>important ADJ</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>VERB</p>
    <p>NOUN</p>
  </div>
  <div class="page">
    <p>Graph Regularization</p>
    <p>ist gut bei</p>
    <p>ist lebhafter bei</p>
    <p>ist wichtig bei</p>
    <p>ist fein bei</p>
    <p>gutem Essen zugetan</p>
    <p>fuers Essen drauf</p>
    <p>zum Essen niederlassen</p>
    <p>zu realisieren ,</p>
    <p>zu essen ,</p>
    <p>zu stecken , zu erreichen , eat</p>
    <p>food</p>
    <p>eat</p>
    <p>eating</p>
    <p>NOUN VERB</p>
    <p>VERB VERB</p>
    <p>good ADJ</p>
    <p>nicely ADV</p>
    <p>fine ADJ</p>
    <p>important ADJ</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>ADJ</p>
    <p>ADV</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>VERB</p>
    <p>NOUN</p>
    <p>Continues till convergence...</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Danish Dutch German Greek Italian Portugese Spanish Swedish Average</p>
    <p>FeatureHMM</p>
    <p>Direct Projection</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Graphbased</p>
    <p>Projection 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4</p>
    <p>Danish Dutch German Greek Italian Portugese Spanish Swedish Average</p>
    <p>FeatureHMM</p>
    <p>Direct Projection</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Graphbased</p>
    <p>Projection 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4</p>
    <p>Danish Dutch German Greek Italian Portugese Spanish Swedish Average</p>
    <p>FeatureHMM</p>
    <p>Direct Projection</p>
    <p>Oracle (Supervised) 96.9 94.9 98.2 97.8 95.8 97.2 96.8 94.8 96.6</p>
  </div>
  <div class="page">
    <p>- Text Categorization - Sentiment Analysis - Class Instance Acquisition - POS Tagging - MultiLingual POS Tagging - Semantic Parsing [Das &amp; Smith, ACL 2011]</p>
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Extract shallow semantic structure: Frames and Roles</p>
    <p>I want to go to Jeju Island on Sunday</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Extract shallow semantic structure: Frames and Roles</p>
    <p>I want to go to Jeju Island on SundayI want to go to Jeju Island on Sunday</p>
    <p>Target (Predicate)</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Extract shallow semantic structure: Frames and Roles</p>
    <p>I want to go to Jeju Island on SundayI want to go to Jeju Island on Sunday</p>
    <p>Target (Predicate)</p>
    <p>TRAVEL</p>
    <p>Frame</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Extract shallow semantic structure: Frames and Roles</p>
    <p>I want to go to Jeju Island on SundayI want to go to Jeju Island on Sunday</p>
    <p>Target (Predicate)</p>
    <p>TRAVEL</p>
    <p>Frame</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Extract shallow semantic structure: Frames and Roles</p>
    <p>I want to go to Jeju Island on SundayI want to go to Jeju Island on Sunday</p>
    <p>Target (Predicate)</p>
    <p>TRAVEL</p>
    <p>Frame</p>
    <p>Traveller Goal Time</p>
    <p>Role</p>
    <p>Argument</p>
  </div>
  <div class="page">
    <p>Problem Description</p>
    <p>Target identification  Most approaches assume this is given  Frame identification  Argument identification</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>All Predicates</p>
    <p>F-Measure</p>
    <p>Unknown Predicates</p>
    <p>F-Measure</p>
    <p>Frame Identification</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>All Predicates</p>
    <p>F-Measure</p>
    <p>Unknown Predicates</p>
    <p>F-Measure</p>
    <p>All Predicates</p>
    <p>F-Measure</p>
    <p>Unknown Predicates</p>
    <p>F-Measure</p>
    <p>Frame Identification</p>
    <p>Full Parsing</p>
  </div>
  <div class="page">
    <p>Sparse label data</p>
    <p>Labeled data has only about 9,263 labeled predicates (targets)</p>
    <p>English on the other hand has a lot more potential predicates (~65,000 in newswire)</p>
  </div>
  <div class="page">
    <p>Sparse label data</p>
    <p>Labeled data has only about 9,263 labeled predicates (targets)</p>
    <p>English on the other hand has a lot more potential predicates (~65,000 in newswire)</p>
    <p>Construct a graph with potential predicates as vertices</p>
    <p>Expand the lexicon by using graph-based SSL</p>
  </div>
  <div class="page">
    <p>Graph Propagation (I)</p>
    <p>Seed predicates</p>
  </div>
  <div class="page">
    <p>Graph Propagation (II)</p>
    <p>Seed predicates Unseen predicates</p>
  </div>
  <div class="page">
    <p>Graph Propagation (III)</p>
  </div>
  <div class="page">
    <p>Graph Propagation (IV)</p>
  </div>
  <div class="page">
    <p>Results on Unknown Predicates</p>
    <p>Supervised Self-Training Graph-Based</p>
    <p>F-MeasureFrame Identification</p>
  </div>
  <div class="page">
    <p>Results on Unknown Predicates</p>
    <p>Supervised Self-Training Graph-Based</p>
    <p>F-Measure</p>
    <p>Supervised Self-Training Graph-Based</p>
    <p>F-Measure</p>
    <p>Frame Identification</p>
    <p>Full Parsing</p>
  </div>
  <div class="page">
    <p>Results on Unknown Predicates</p>
    <p>Supervised Self-Training Graph-Based</p>
    <p>F-Measure</p>
    <p>Supervised Self-Training Graph-Based</p>
    <p>F-Measure</p>
    <p>Frame Identification</p>
    <p>Full Parsing Gains from SSL</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Graph Construction  Inference Methods  Scalability  Applications  Conclusion &amp; Future Work</p>
  </div>
  <div class="page">
    <p>When to use Graph-based SSL and which method?</p>
    <p>When input data itself is a graph  or, when the data is expected to lie on a manifold</p>
    <p>Measure Propagation (MP)  for probabilistic interpretation</p>
    <p>Quadratic Criteria (QC), MAD, MADDL  when labels are not mutually exclusive</p>
    <p>Manifold Regularization  for generalization to unseen data (induction)</p>
  </div>
  <div class="page">
    <p>Graph-based SSL: Summary</p>
    <p>Provide flexible representation  for both IID and relational data</p>
    <p>Graph construction can be key  Scalable: Node Reordering and MapReduce  Can handle labeled as well as unlabeled data  Can handle multi class, multi label settings  Effective in practice</p>
  </div>
  <div class="page">
    <p>Open Challenges</p>
    <p>Use in structured prediction problems  Constituency and dependency parsing</p>
    <p>Combining Inductive and Graph-based methods  Joint optimization and parallel training [Altun et al., NIPS 2006]</p>
    <p>Scalable graph construction, especially with multi-modal data</p>
    <p>Extensions with other loss functions, sparsity, etc.  Using side information</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>National Science Foundation (NSF) IIS-0447972  DARPA HRO1107-1-0029, FA8750-09-C-0179  Google Research Award  Dipanjan Das (Google), Ryan McDonald (Google),</p>
    <p>Fernando Pereira (Google), Slav Petrov (Google), Noah Smith (CMU)</p>
  </div>
  <div class="page">
    <p>References (I) [1] A. Alexandrescu and K. Kirchhoff. Data-driven graph construction for semi- supervised graph-based learning in NLP. In NAACL HLT, 2007. [2] Y. Altun, D. McAllester, and M. Belkin. Maximum margin semi-supervised learn- ing for structured variables. NIPS, 2006. [3] R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. Distributional word clusters vs. words for text categorization. J. Mach. Learn. Res., 3:11831208, 2003. [4] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:23992434, 2006. [5] Y. Bengio, O. Delalleau, and N. Le Roux. Label propagation and quadratic criterion. Semi-supervised learning, 2006. [6] T. Berg-Kirkpatrick, A. Bouchard-Cot e, J. DeNero, and D. Klein. Painless unsupervised learning with features. In HLTNAACL, 2010. [7] J. Bilmes and A. Subramanya. Scaling up Machine Learning: Parallel and Distributed Approaches, chapter Parallel GraphBased Semi-Supervised Learning. 2011. [8] S. Blair-goldensohn, T. Neylon, K. Hannan, G. A. Reis, R. Mcdonald, and J. Reynar. Building a sentiment summarizer for local service reviews. In In NLP in the Information Explosion Era, 2008. [9] M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. Webtables: exploring the power of tables on the web. VLDB, 2008. [10] O. Chapelle, B. Scho lkopf, A. Zien, et al. Semi-supervised learning. MIT press Cambridge, MA:, 2006. [11] Y. Choi and C. Cardie. Adapting a polarity lexicon using integer linear program- ming for domain specific sentiment classification. In EMNLP, 2009. [12] S. Daitch, J. Kelner, and D. Spielman. Fitting a graph to vector data. In ICML, 2009. [13] D. Das and S. Petrov. Unsupervised part-of-speech tagging with bilingual graph- based projections. In ACL, 2011. [14] D. Das, N. Schneider, D. Chen, and N. A. Smith. Probabilistic frame-semantic parsing. In NAACL-HLT, 2010. [15] D. Das and N. Smith. Graph-based lexicon expansion with sparsity-inducing penalties. NAACL-HLT, 2012. [16] D. Das and N. A. Smith. Semi-supervised frame-semantic parsing for unknown predicates. In ACL, 2011. [17] J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. Information-theoretic metric learning. In ICML, 2007. [18] O. Delalleau, Y. Bengio, and N. L. Roux. Efficient non-parametric function induction in semi-supervised learning. In AISTATS, 2005. [19] P. Dhillon, P. Talukdar, and K. Crammer. Inference-driven metric learning for graph construction. Technical report, MSCIS-10-18, University of Pennsylvania, 2010. [20] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms and representations for text categorization. In CIKM, 1998.</p>
  </div>
  <div class="page">
    <p>References (II) [21] J. Friedman, J. Bentley, and R. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transaction on Mathematical Software, 3, 1977. [22] J. Garcke and M. Griebel. Data mining with sparse grids using simplicial basis functions. In KDD, 2001. [23] A. Goldberg and X. Zhu. Seeing stars when there arent many stars: graph-based semi-supervised learning for sentiment categorization. In Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing, 2006. [24] A. Goldberg, X. Zhu, and S. Wright. Dissimilarity in graph-based semi-supervised classification. AISTATS, 2007. [25] M. Hu and B. Liu. Mining and summarizing customer reviews. In KDD, 2004. [26] T. Jebara, J. Wang, and S. Chang. Graph construction and b-matching for semisupervised learning. In ICML, 2009. [27] T. Joachims. Transductive inference for text classification using support vector machines. In ICML, 1999. [28] T. Joachims. Transductive learning via spectral graph partitioning. In ICML, 2003. [29] M. Karlen, J. Weston, A. Erkan, and R. Collobert. Large scale manifold transduction. In ICML, 2008. [30] S.-M. Kim and E. Hovy. Determining the sentiment of opinions. In Proceedings of the 20th International conference on Computational Linguistics, 2004. [31] F. Kschischang, B. Frey, and H. Loeliger. Factor graphs and the sum-product algorithm. Information Theory, IEEE Transactions on, 47(2):498519, 2001 [32] K. Lerman, S. Blair-Goldensohn, and R. McDonald. Sentiment summarization: evaluating and learning user preferences. In EACL, 2009. [33] D.Lewisetal.Reuters-21578.http://www.daviddlewis.com/resources/testcollections/reuters21578, 1987. [34] J. Malkin, A. Subramanya, and J. Bilmes. On the semi-supervised learning of multi-layered perceptrons. In InterSpeech, 2009. [35] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP, 2002. [36] D. Rao and D. Ravichandran. Semi-supervised polarity lexicon induction. In EACL, 2009. [37] A. Subramanya and J. Bilmes. Soft-supervised learning for text classification. In EMNLP, 2008. [38] A. Subramanya and J. Bilmes. Entropic graph regularization in non-parametric semi-supervised classification. NIPS, 2009. [39] A. Subramanya and J. Bilmes. Semi-supervised learning with measure propagation. The Journal of Machine Learning Research, 2011. [40] A. Subramanya, S. Petrov, and F. Pereira. Efficient graph-based semi-supervised learning of structured tagging models. In EMNLP, 2010.</p>
  </div>
  <div class="page">
    <p>References (III) [41] P. Talukdar. Topics in graph construction for semi-supervised learning. Technical report, MS-CIS-09-13, University of Pennsylvania, 2009. [42] P. Talukdar and K. Crammer. New regularized algorithms for transductive learning. ECML, 2009. [43] P. Talukdar and F. Pereira. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In ACL, 2010. [44] P. Talukdar, J. Reisinger, M. Pa sca, D. Ravichandran, R. Bhagat, and F. Pereira. Weakly-supervised acquisition of labeled class instances using graph random walks. In EMNLP, 2008. [45] B. Van Durme and M. Pasca. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In AAAI, 2008. [46] L. Velikovich, S. Blair-Goldensohn, K. Hannan, and R. McDonald. The viability of web-derived polarity lexicons. In HLTNAACL, 2010. [47] F. Wang and C. Zhang. Label propagation through linear neighborhoods. In ICML, 2006. [48] J. Wang, T. Jebara, and S. Chang. Graph transduction via alternating minimization. In ICML, 2008. [49] R. Wang and W. Cohen. Language-independent set expansion of named entities using the web. In ICDM, 2007. [50] K. Weinberger and L. Saul. Distance metric learning for large margin nearest neighbor classification. The Journal of Machine Learning Research, 10:207244, 2009. [51] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase- level sentiment analysis. In HLT-EMNLP, 2005. [52] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scho lkopf. Learning with local and global consistency. NIPS, 2004. [53] D. Zhou, J. Huang, and B. Scho lkopf. Learning from labeled and un- labeled data on a directed graph. In ICML, 2005. [54] D. Zhou, B. Scho lkopf, and T. Hofmann. Semi-supervised learning on directed graphs. In NIPS, 2005. [55] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical report, CMUCALD-02-107, Carnegie Mellon University, 2002. [56] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical report, Carnegie Mellon University, 2002. [57] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, 2003. [58] X. Zhu and J. Lafferty. Harmonic mixtures: combining mixture models and graph- based methods for inductive and scalable semi-supervised learning. In ICML, 2005.</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Web: http://graph-ssl.wikidot.com/</p>
  </div>
</Presentation>
