<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Evaluating MapReduce for</p>
    <p>Multicore and Multiprocessor Systems</p>
    <p>Colby Ranger, Ramanan Raghuraman,</p>
    <p>Arun Penmetsa, Gary Bradski, Christos Kozyrakis</p>
    <p>Computer Systems Laboratory</p>
    <p>Stanford University</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 2</p>
    <p>Googles MapReduce</p>
    <p>A general-purpose environment for large-scale data processing</p>
    <p>Programming model (API) and runtime system for large clusters</p>
    <p>Functional representation of data parallel tasks</p>
    <p>Easy to use &amp; very successful within Google</p>
    <p>Indexing system, distributed grep &amp; sort, document clustering, machine learning, statistical machine translation,</p>
    <p>MapReduce supports</p>
    <p>Automatic parallelization and distribution  Abstracts parallelization, synchronization, and communication issues</p>
    <p>Fault tolerance  Task monitoring and replication</p>
    <p>I/O scheduling</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 3</p>
    <p>// Applied to each input element</p>
    <p>// &lt;key, val&gt; intermediate pairs</p>
    <p>// Applied to all pairs with same key</p>
    <p>// Final output sorted by key</p>
    <p>WordCount Example [OSDI04]</p>
    <p>// input: a document // intermediate output: key=word; value=1 Map(void *input) {</p>
    <p>for each word w in input</p>
    <p>EmitIntermediate(w, 1);</p>
    <p>}</p>
    <p>// intermediate output: key=word; value=1 // output: key=word; value=occurrences Reduce(String key, Iterator values) {</p>
    <p>int result = 0;</p>
    <p>for each v in values</p>
    <p>result += v;</p>
    <p>Emit(key, result);</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 4</p>
    <p>The Phoenix System</p>
    <p>Question: is MapReduce applicable to multicore programming?</p>
    <p>What is the performance?</p>
    <p>Is the performance scalable &amp; portable?</p>
    <p>Does it help with locality and fault management?</p>
    <p>How does it compare to other parallel programming approaches?</p>
    <p>Phoenix: a shared-memory implementation of MapReduce</p>
    <p>Uses threads instead of cluster nodes for parallelism</p>
    <p>Communicates through shared memory instead of network messages  Works with CMP and SMP systems</p>
    <p>Current version works with C/C++ and uses P-threads  Easy to port to other languages or thread environments</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 5</p>
    <p>The Phoenix API</p>
    <p>System-defined functions</p>
    <p>int phoenix_scheduler (scheduler_args_t *args)</p>
    <p>Initializes the runtime system</p>
    <p>void emit_intermediate (void *key, void *val, int key_size)</p>
    <p>void emit (void *key, void *val)</p>
    <p>User-defined functions</p>
    <p>void (*map_t) (map_args_t *args)</p>
    <p>Map function applied on each input element</p>
    <p>void (*reduce_t) (void *key, void **buffer, int count)</p>
    <p>Reduce function applied on intermediate pairs with same key</p>
    <p>int (*key_cmp_t) (const void *key1, const void *key2)</p>
    <p>Function that compares two keys</p>
    <p>int (*splitter_t) (void *input, int size, map args t *args)</p>
    <p>Splits input data across Map tasks (optional)</p>
    <p>Simple &amp; narrow API. Similar to Googles API</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 6</p>
    <p>The Phoenix Runtime</p>
    <p>Orchestrates program execution across multiple threads</p>
    <p>Initiates and terminates threads (workers)</p>
    <p>Assigns map &amp; reduce tasks to workers</p>
    <p>Handles buffer allocation and communication</p>
    <p>Key runtime features</p>
    <p>Dynamic scheduling of tasks for load balancing</p>
    <p>Communication through pointer exchange (when possible)</p>
    <p>Locality optimization through granularity adjustment</p>
    <p>Support for failure recovery</p>
    <p>Details of parallel execution are hidden from programmer</p>
    <p>Low-level threading, communication, scheduling,</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 7</p>
    <p>Phoenix Execution Overview In p u t</p>
    <p>.. .</p>
    <p>.. .</p>
    <p>O u tp u t</p>
    <p>.. .</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 8</p>
    <p>Input Data Splitting</p>
    <p>Divides input data to chunks for map tasks</p>
    <p>Small chunk  map overhead; large chunk  locality issues</p>
    <p>Phoenix: chunk size determined by cache size for locality</p>
    <p>Or programmer provides custom splitter</p>
    <p>In p u t</p>
    <p>.. .</p>
    <p>.. .</p>
    <p>O u tp u t</p>
    <p>.. .</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 9</p>
    <p>Map Stage</p>
    <p>Each task applies map function to an input chunk</p>
    <p>Phoenix: typically 100s of tasks multiplexed to available workers</p>
    <p>No reduce tasks are started before map tasks complete</p>
    <p>Intermediate pairs partitioned to reduce queues based on keys</p>
    <p>Partitioning can introduce significant overhead!</p>
    <p>In p u t</p>
    <p>.. .</p>
    <p>.. .</p>
    <p>O u tp u t</p>
    <p>.. .</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 10</p>
    <p>Reduce Stage</p>
    <p>Each task processes a key set from the reduce queues</p>
    <p>Dynamic scheduling used for reduce tasks as well</p>
    <p>A poor partition function can lead to significant imbalance</p>
    <p>Default partition function based on key hashing</p>
    <p>Programmer can provide custom partition function</p>
    <p>In p u t</p>
    <p>.. .</p>
    <p>.. .</p>
    <p>O u tp u t</p>
    <p>.. .</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 11</p>
    <p>Merge Stage</p>
    <p>Combines reduce output queues to single sorted output</p>
    <p>May be unnecessary for some applications</p>
    <p>But merge time tends to be small compared to map/reduce time</p>
    <p>Phoenix: binary merging of reduce queues into single queue</p>
    <p>Overhead increases with number of reduce tasks</p>
    <p>In p u t</p>
    <p>.. .</p>
    <p>.. .</p>
    <p>O u tp u t</p>
    <p>.. .</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 12</p>
    <p>Potential Performance Detractors</p>
    <p>Significant detractors  Partitioning overhead: communication and grouping requirements</p>
    <p>Model overhead: particularly due to calls to emit/emit_intermediate</p>
    <p>Key management: some apps do not naturally associate keys with data</p>
    <p>Repeated Map/Reduce invocations: necessary for some apps</p>
    <p>Practically insignificant issues  Final merging &amp; sorting: insignificant compared to other tasks</p>
    <p>Buffer management: extensive (re-)use of pre-allocated buffers</p>
    <p>Reduce imbalance: handled through dynamic scheduling</p>
    <p>Serialized input splitting: most map tasks involve non-trivial work</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 13</p>
    <p>Phoenix Fault Tolerance</p>
    <p>Focus: transient or permanent errors in workers</p>
    <p>Error detection: worker time-out</p>
    <p>Execution time of similar tasks used as yardstick</p>
    <p>Error recovery</p>
    <p>Restart or potentially re-assign affected tasks</p>
    <p>Handle input/output buffer management</p>
    <p>Future work</p>
    <p>Fault tolerance for the scheduler</p>
    <p>Error detection and isolation through worker sandboxing</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 14</p>
    <p>Evaluation Methodology</p>
    <p>Shared-memory systems</p>
    <p>CMP: Niagara-based Sun Fire T1200 (8 CPUs, 4 threads/CPU)</p>
    <p>SMP: Sun Ultra E6000 (24 CPUs)  SMP results similar to CMP  portable performance</p>
    <p>See paper for details</p>
    <p>8 applications</p>
    <p>Domains: enterprise, scientific, consumer</p>
    <p>Three code versions: sequential, MapReduce (Phoenix), P-threads  Optimized independently</p>
    <p>Experiments</p>
    <p>Talk: performance &amp; scalability, Phoenix Vs. P-threads</p>
    <p>See paper for: dependency to data-set size, dependency to input task granularity, (soft &amp; hard) fault injection experiment</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 15</p>
    <p>Applications</p>
    <p>Word count  determine frequency of words in documents</p>
    <p>String match  search file with keys for an encrypted word</p>
    <p>Reverse Index  build reverse index for links in HTML files</p>
    <p>Linear regression  find the best fit line for a set of points</p>
    <p>Matrix multiply  dense integer matrix multiplication</p>
    <p>MapReduce version introduces coarse-grain coordinate variables</p>
    <p>Kmeans  clustering algorithm for 3D data points</p>
    <p>Multiple MapReduce invocation with translation step</p>
    <p>PCA  principal component analysis on a matrix</p>
    <p>MapReduce version introduces coordinate variables</p>
    <p>Histogram  frequency of RGB components in images</p>
    <p>There is no need for keys in original algorithm</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 16</p>
    <p>CMP Speedup</p>
    <p>Good scalability across all applications</p>
    <p>Absolute speedup depends on importance of various detractors</p>
    <p>Note for CMP: 1 core = 4 threads (4 workers)</p>
    <p>Improved locality leads to significant improvements for some apps</p>
    <p>At high core counts: some bandwidth saturation or load imbalance</p>
    <p>WordCount MatrixMult StringMatch Kmeans ReverseIndex PCA Histogram LinearReg</p>
    <p>C M</p>
    <p>P S</p>
    <p>p e e d u p</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 17</p>
    <p>Phoenix Vs. P-threads</p>
    <p>Phoenix equal to P-threads if algorithm matches MapReduce model</p>
    <p>Note that P-threads low-level API is more flexible</p>
    <p>Anecdote: we looked at Phoenix behavior to tune some Pthreads codes</p>
    <p>P-threads is better for algorithms that do not fit MapReduce model</p>
    <p>Does not use keys, requires multiple MapReduce iterations,</p>
    <p>Wordcount MatrixMult StringMatch Kmeans ReverseIndex PCA Histogram LinearReg</p>
    <p>C M</p>
    <p>P S</p>
    <p>p e e d u p ( 8 c</p>
    <p>o re</p>
    <p>s )</p>
    <p>Pthreads</p>
    <p>Phoenix</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 18</p>
    <p>Conclusions</p>
    <p>Phoenix: a shared-memory implementation of MapReduce</p>
    <p>MapReduce API and runtime system for C/C++</p>
    <p>Uses threads instead of cluster nodes for parallelism</p>
    <p>Communicates through shared memory instead of network</p>
    <p>messages</p>
    <p>Dynamic scheduling, locality management, fault recovery,</p>
    <p>Scalable &amp; portable performance, compares well to P-threads</p>
    <p>Future work</p>
    <p>Improve queue structures</p>
    <p>Automatic configuration detection</p>
    <p>Better fault detection</p>
    <p>Experiment with more systems &amp; apps</p>
  </div>
  <div class="page">
    <p>C. Ranger et al, HPCA13, February 2007 19</p>
    <p>Questions?</p>
    <p>Want a copy of Phoenix?</p>
    <p>Will post the source code at http://csl.stanford.edu/~christos</p>
  </div>
</Presentation>
