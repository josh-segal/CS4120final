<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Stefan Kaestle, Reto Achermann, Roni Haecki, Moritz Hoffmann, Sabela Ramos, Timothy Roscoe</p>
    <p>Systems Group, Department of Computer Science, ETH Zurich</p>
    <p>Smelt: Machine-aware Atomic Broadcast Trees for Multicores</p>
  </div>
  <div class="page">
    <p>Large number of trees: topologies and send orders</p>
    <p>Number of cross NUMA</p>
    <p>links?</p>
    <p>Tree Topology?</p>
    <p>Root of the tree?</p>
    <p>Send order of messages?</p>
    <p>Maximum out degree?</p>
  </div>
  <div class="page">
    <p>Large number of trees: topologies and send orders</p>
    <p>Number of cross NUMA</p>
    <p>links?</p>
    <p>Tree Topology?</p>
    <p>Root of the tree?</p>
    <p>Send order of messages?</p>
    <p>Maximum out degree?</p>
    <p>Large number of possible trees</p>
    <p>topology + message send order</p>
    <p>!1 = (2  2)!</p>
    <p>1 !</p>
  </div>
  <div class="page">
    <p>Barrier Reduction Broadcast 2PC</p>
    <p>Binary Tree Cluster Sequential MST Fibonacci</p>
    <p>There is no globally optimal tree structure</p>
    <p>Barrier Reduction Broadcast 2PC</p>
    <p>Cluster topology wins Binary tree or Fibonacci win</p>
    <p>AMD Interlagos (4 Socket x 4 Cores x 2 Threads) Intel Xeon Phi (61 Cores x 1 Thread)</p>
    <p>Execution Time [kCycles] Execution Time [kCycles]</p>
  </div>
  <div class="page">
    <p>Tree Generation</p>
    <p>Time</p>
    <p>Multicore Model</p>
    <p>Smelt: Automatic optimization of broadcast and reduction trees</p>
  </div>
  <div class="page">
    <p>Example: Building fast and simple barriers</p>
    <p>Barrier implementation</p>
    <p>Dramatic improvement through automatic optimization of communication patterns</p>
    <p>Barrier Benchmark on Intel Sandy Bridge 4x8x2 Execution Time [kCycles]</p>
    <p>Dissemination Parlib MCS Smelt</p>
  </div>
  <div class="page">
    <p>Broadcasts and reductions are central building blocks for parallel programs</p>
    <p>Performance</p>
    <p>Atomic broadcasts</p>
    <p>Replication for data locality</p>
    <p>e.g. Shoal, Carrefour, SMMP OS, FOS</p>
    <p>Fault-Tolerance</p>
    <p>Agreement protocols, atomic broadcasts</p>
    <p>Replication for failure resilience</p>
    <p>e.g. 1Paxos</p>
    <p>Execution Control</p>
    <p>Reductions, broadcast, barriers</p>
    <p>Thread synchronization, data gathering</p>
    <p>e.g. OpenMP</p>
  </div>
  <div class="page">
    <p>Multicore hardware is complex</p>
    <p>Interconnect / coherence protocol complexity</p>
    <p>Distributed resources</p>
    <p>Hardware parallelism</p>
    <p>Hierarchical: thread/cores</p>
    <p>caches/memory</p>
    <p>Intel Sandy Bridge 4x8x2</p>
  </div>
  <div class="page">
    <p>Multicore hardware is complex</p>
    <p>Interconnect / coherence protocol complexity</p>
    <p>Distributed resources</p>
    <p>Hardware parallelism</p>
    <p>Hierarchical: thread/cores</p>
    <p>caches/memory</p>
    <p>Intel Sandy Bridge 4x8x2</p>
    <p>Generate a good tree topology and schedules</p>
    <p>automatically</p>
  </div>
  <div class="page">
    <p>Works well for our approach.</p>
    <p>Clear concept: Enables reasoning about send and receive costs</p>
    <p>Smelt is based on peer-to-peer message passing</p>
    <p>cache-line sized slot</p>
    <p>Thread 1 Thread 2</p>
    <p>shared memory page</p>
    <p>shared memory page</p>
  </div>
  <div class="page">
    <p>Core 1</p>
    <p>Core 2</p>
    <p>Time [10ns]</p>
    <p>Core 3</p>
    <p>Core 4</p>
    <p>Machine 1</p>
    <p>Machine 2</p>
    <p>Machine 3</p>
    <p>Machine 4</p>
    <p>Classical Network Multicore interconnect</p>
    <p>Message-passing on multicores is different</p>
    <p>Time [1us]</p>
  </div>
  <div class="page">
    <p>Core 1</p>
    <p>Core 2</p>
    <p>Time [10ns]</p>
    <p>Core 3</p>
    <p>Core 4</p>
    <p>Machine 1</p>
    <p>Machine 2</p>
    <p>Machine 3</p>
    <p>Machine 4</p>
    <p>Classical Network Multicore interconnect</p>
    <p>Message-passing on multicores is different</p>
    <p>Time [1us]</p>
    <p>On multicores send and receive times dominate propagation time</p>
    <p>Goal: Minimize total time of the broadcast</p>
  </div>
  <div class="page">
    <p>=</p>
    <p>Minimize the longest path from the root to the leaves.</p>
    <p>Minimizing the total time of a broadcast</p>
    <p>Root Leaf</p>
    <p>1 21 2</p>
    <p>= ( + )</p>
    <p>We need to know the send and receive cost between any pair of cores</p>
  </div>
  <div class="page">
    <p>Information obtained from hardware discovery</p>
    <p>$ numactl hardware node distances: node 0 1 2 3 4 5 6 7 0: 10 16 16 22 16 22 16 22 1: 16 10 22 16 16 22 22 16 2: 16 22 10 16 16 16 16 16 3: 22 16 16 10 16 16 22 22 4: 16 16 16 16 10 16 16 22 5: 22 22 16 16 16 10 22 16 6: 16 22 16 22 16 22 10 16 7: 22 16 16 22 22 16 16 10</p>
    <p>$ lscpu CPU(s): 64 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 4 NUMA node(s): 8 L1d cache: 16K L1i cache: 64K L2 cache: 2048K L3 cache: 6144K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28 NUMA node1 CPU(s): 32,36,40,44,48,52,56,60 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30 NUMA node3 CPU(s): 34,38,42,46,50,54,58,62 NUMA node4 CPU(s): 3,7,11,15,19,23,27,31 NUMA node5 CPU(s): 35,39,43,47,51,55,59,63 NUMA node6 CPU(s): 1,5,9,13,17,21,25,29 NUMA node7 CPU(s): 33,37,41,45,49,53,57,61</p>
    <p>AMD Interlagos 4x4x2</p>
  </div>
  <div class="page">
    <p>Information obtained from hardware discovery</p>
    <p>$ numactl hardware node distances: node 0 1 2 3 4 5 6 7 0: 10 16 16 22 16 22 16 22 1: 16 10 22 16 16 22 22 16 2: 16 22 10 16 16 16 16 16 3: 22 16 16 10 16 16 22 22 4: 16 16 16 16 10 16 16 22 5: 22 22 16 16 16 10 22 16 6: 16 22 16 22 16 22 10 16 7: 22 16 16 22 22 16 16 10</p>
    <p>$ lscpu CPU(s): 64 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 4 NUMA node(s): 8 L1d cache: 16K L1i cache: 64K L2 cache: 2048K L3 cache: 6144K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28 NUMA node1 CPU(s): 32,36,40,44,48,52,56,60 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30 NUMA node3 CPU(s): 34,38,42,46,50,54,58,62 NUMA node4 CPU(s): 3,7,11,15,19,23,27,31 NUMA node5 CPU(s): 35,39,43,47,51,55,59,63 NUMA node6 CPU(s): 1,5,9,13,17,21,25,29 NUMA node7 CPU(s): 33,37,41,45,49,53,57,61</p>
    <p>AMD Interlagos 4x4x2</p>
    <p>Doesnt distinguish between send() and recv()</p>
    <p>NUMA distance: abstract value</p>
  </div>
  <div class="page">
    <p>Complement with microbenchmarks: pairwise send and receive</p>
    <p>Cost of Send Operation [Cycles] Cost of Receive Operation [cycles]</p>
    <p>sending core sending core</p>
    <p>receiving core receiving core</p>
    <p>AMD Interlagos 4x4x2</p>
    <p>min = 43 max = 166</p>
    <p>min = 43 max = 360</p>
  </div>
  <div class="page">
    <p>Complement with microbenchmarks: pairwise send and receive</p>
    <p>Cost of Send Operation [Cycles] Cost of Receive Operation [cycles]</p>
    <p>sending core sending core</p>
    <p>receiving core receiving core</p>
    <p>AMD Interlagos 4x4x2</p>
    <p>min = 43 max = 166</p>
    <p>min = 43, max = 360</p>
    <p>Not symmetric!</p>
    <p>Core 10  Core 22: 137 + 282 = 419 cycles Core 22  Core 10: 159 + 351 = 510 cycles</p>
  </div>
  <div class="page">
    <p>Smelt</p>
  </div>
  <div class="page">
    <p>Using Smelt for group communication</p>
    <p>Multicore ModelProgram</p>
    <p>Hardware Discovery # Cores # NUMA nodes</p>
    <p>lstopo; /proc/cpu</p>
    <p>Measurements Micro-benchmarks</p>
    <p>#include &lt;smelt/smelt.h&gt;</p>
    <p>void main() {</p>
    <p>smelt_init();</p>
    <p>smelt_topology_create();</p>
    <p>smelt_broadcast(msg);</p>
    <p>}</p>
    <p>Smelt runtime</p>
    <p>smelt_topology_create() {</p>
    <p>}</p>
    <p>Smelt Algorithm</p>
    <p>Tree topology</p>
  </div>
  <div class="page">
    <p>Smelts tree generator heuristics</p>
    <p>Remote Cores First</p>
    <p>Avoid Expensive Communication</p>
    <p>No redundancy</p>
    <p>r</p>
    <p>Maximize Parallelism</p>
    <p>r</p>
    <p>rr</p>
  </div>
  <div class="page">
    <p>NUMA 3</p>
    <p>NUMA 2</p>
    <p>NUMA 1</p>
    <p>NUMA 0</p>
    <p>Broadcast on AMD Shanghai 4x4x1</p>
    <p>send receive</p>
    <p>Simplified Example</p>
    <p>Time [Cycles]</p>
  </div>
  <div class="page">
    <p>NUMA 3</p>
    <p>NUMA 2</p>
    <p>NUMA 1</p>
    <p>NUMA 0</p>
    <p>Heuristic #1: Expensive first</p>
    <p>Broadcast on AMD Shanghai 4x4x1</p>
    <p>send receive</p>
    <p>Simplified Example</p>
    <p>Time [Cycles]</p>
    <p>Step 2: Start Scheduler Schedule cores to send and receive messages</p>
    <p>Apply Heuristics Decide where to send messages to</p>
    <p>The tree is good, But not optimal!</p>
  </div>
  <div class="page">
    <p>Smelt Tree for Intel Sandy Bridge 4 Sockets x 8 Cores x 2 Threads</p>
    <p>Smelt Tree for Intel Xeon Phi using 61 cores</p>
  </div>
  <div class="page">
    <p>Evaluation Testbed</p>
    <p>Architecture Sockets Cores / Socket</p>
    <p>Threads / Core</p>
    <p>Magny Cours 4 12 1</p>
    <p>Barcelona 8 4 1</p>
    <p>Shanghai 4 4 1</p>
    <p>Interlagos 4 4 2</p>
    <p>Istanbul 4 6 1</p>
    <p>Architecture Sockets Cores / Socket</p>
    <p>Threads / Core</p>
    <p>Ivy Bridge 2 10 2</p>
    <p>Nehalem 4 8 2</p>
    <p>Knights Corner 1 61 4</p>
    <p>Sandy Bridge 4 8 2</p>
    <p>Sandy Bridge 2 10 2</p>
    <p>Bloomfield 2 4 2</p>
    <p>Full set of results online.</p>
    <p>http://machinedb.systems.ethz.ch</p>
    <p>Intel AMD</p>
  </div>
  <div class="page">
    <p>Barrier Reduction Broadcast 2PC</p>
    <p>Binary Tree Cluster Sequential MST Fibonacci Smelt</p>
    <p>Smelt produces good trees across architectures</p>
    <p>Barrier Reduction Broadcast 2PC</p>
    <p>AMD Interlagos (4 Socket x 4 Threads) Intel Xeon Phi (61 Threads) Execution Time [kCycles] Execution Time [kCycles]</p>
    <p>Best other = Cluster Best other = Fibonacci / Binary Tree</p>
  </div>
  <div class="page">
    <p>Smelt produces good trees across architectures</p>
    <p>Manufacturer Codename Sockets x Cores x Threads</p>
    <p>slowdown speedup</p>
  </div>
  <div class="page">
    <p>Cluster Topology on Intel Bloomfield 2x4x2Smelt Tree on Intel Bloomfield 2x4x2</p>
    <p>Fast broadcast trees are good for reductions in most cases</p>
    <p>Additional Cross-NUMA link</p>
  </div>
  <div class="page">
    <p>Barriers based on reduction and broadcast</p>
    <p>Smelt provides simple and fast barriers</p>
    <p>Execution Time [kCycles]</p>
    <p>Barrier Benchmark on Intel Sandy Bridge 4x8x2</p>
    <p>Simple barrier implementation</p>
    <p>void smelt_barrier(void) { smelt_reduce(); smelt_broadcast();</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>OpenMP: EPCC OpenMP Benchmark Collection</p>
    <p>/* epcc openmp barrier benchmark */ void testbar() {</p>
    <p>int j; #pragma omp parallel private(j) {</p>
    <p>for (j = 0; j &lt; innerreps; j++) { delay(delaylength); #pragma omp barrier</p>
    <p>} }</p>
    <p>} Implicit barrier at the end of parallel block</p>
    <p>Explicit barrier</p>
    <p>Remaining results on the website</p>
    <p>PARALLEL BARRIER</p>
    <p>AMD Interlagos 4x4x2</p>
    <p>PARALLEL BARRIER</p>
    <p>Intel SandyBridge 4x8x2</p>
    <p>GOMP Smelt</p>
    <p>Execution Time [us]</p>
    <p>Execution Time [us]</p>
    <p>Replaced GOMP barrier with Smelt</p>
  </div>
  <div class="page">
    <p>Agreement Protocols: 1Paxos</p>
    <p>Number of Replicas</p>
    <p>Original Broadcast Smelt Broadcast</p>
    <p>Execution Time [kCycles]</p>
  </div>
  <div class="page">
    <p>Broadcasts and reductions are central building blocks</p>
    <p>No globally optimal tree topology</p>
    <p>Information from hardware discovery is not sufficient</p>
    <p>Smelts produces good trees</p>
    <p>Summary Talk to us at</p>
    <p>the first poster session</p>
    <p>machinedb.systems.ethz.ch github.com/libsmelt</p>
  </div>
</Presentation>
