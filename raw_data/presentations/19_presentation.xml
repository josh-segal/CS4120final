<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Spectral Graph Segmentation</p>
    <p>Timothe Cour Jianbo Shi</p>
    <p>Nicolas Gogin</p>
    <p>AISTATS 2005</p>
    <p>Computer and Information Science Department University of Pennsylvania</p>
    <p>Computer Science Ecole Polytechnique</p>
  </div>
  <div class="page">
    <p>Graph-based Image Segmentation</p>
    <p>Weighted graph G =(V,W)</p>
    <p>V = vertices (pixels i)  Similarity between pixels i</p>
    <p>and j : Wij = Wji  0</p>
    <p>Wij</p>
    <p>Segmentation = graph partition of pixels</p>
  </div>
  <div class="page">
    <p>Image I Graph Affinities W=W(I,)</p>
    <p>Intensity Color Edges</p>
    <p>Texture</p>
    <p>Spectral Graph Segmentation</p>
  </div>
  <div class="page">
    <p>Image I Graph Affinities W=W(I,)</p>
    <p>Intensity Color Edges</p>
    <p>Texture</p>
    <p>Spectral Graph Segmentation</p>
    <p>( , ) ( , )</p>
    <p>cut A B</p>
    <p>NCut A B Vol A Vol B</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>Image I Graph Affinities W=W(I,)</p>
    <p>Eigenvector X(W)</p>
    <p>Spectral Graph Segmentation</p>
    <p>( , ) ( , )</p>
    <p>cut A B</p>
    <p>NCut A B Vol A Vol B</p>
    <p>=</p>
    <p>WX DX= 1 if</p>
    <p>( ) 0 if A</p>
    <p>i A X i</p>
    <p>i A  =</p>
  </div>
  <div class="page">
    <p>Image I Graph Affinities W=W(I,)</p>
    <p>Eigenvector X(W)</p>
    <p>Spectral Graph Segmentation</p>
    <p>( , ) ( , )</p>
    <p>cut A B</p>
    <p>NCut A B Vol A Vol B</p>
    <p>=</p>
    <p>WX DX=</p>
    <p>= Ai Ai</p>
    <p>iX A if 0 if 1</p>
    <p>)(</p>
    <p>Discretisation</p>
  </div>
  <div class="page">
    <p>Image I Graph Affinities</p>
    <p>W=W(I,) Eigenvector</p>
    <p>X(W)</p>
    <p>Intensity +</p>
    <p>Color +</p>
    <p>Edges +</p>
    <p>Texture +</p>
    <p>Learn graph parameters</p>
    <p>Spectral Graph Segmentation</p>
    <p>Target segmentation</p>
    <p>Reverse pipeline</p>
  </div>
  <div class="page">
    <p>intensity cues edges cues color cues</p>
    <p>[Shi &amp; Malik, 97]</p>
    <p>Texture cuesMultiscale cues [Yu, 04; Fowlkes 04]</p>
  </div>
  <div class="page">
    <p>Edges cues ? Color cues ? Texture cues ?</p>
    <p>Do you use</p>
    <p>Where is Waldo ?</p>
    <p>-Thats not enough, you need Shape cues High-level object priors</p>
  </div>
  <div class="page">
    <p>Image I Graph Affinities</p>
    <p>W=W(I,) Eigenvector</p>
    <p>X(W)</p>
    <p>Intensity +</p>
    <p>Color +</p>
    <p>Edges +</p>
    <p>Texture +</p>
    <p>Learn graph parameters</p>
    <p>Spectral Graph Segmentation</p>
    <p>Target segmentation</p>
  </div>
  <div class="page">
    <p>Image I Affinities W=W(I,) Eigenvector X(W)</p>
    <p>Target X* Target P*~W*</p>
    <p>[1] Meila and Shi (2001) [2] Fowlkes, Martin, Malik (2004)</p>
    <p>Error criterion on affinity matrix W</p>
  </div>
  <div class="page">
    <p>Image I Affinities W=W(I,) Eigenvector X(W)</p>
    <p>Target X* Target P*~W*</p>
    <p>[1] Meila and Shi (2001) [2] Fowlkes, Martin, Malik (2004)</p>
    <p>Error criterion on affinity matrix W</p>
    <p>Constraining Wij is overkill:</p>
    <p>Many W have segmentation W: O(n2) parameters Segments : O(n) parameters</p>
  </div>
  <div class="page">
    <p>Image I Affinities W=W(I,) Eigenvector X(W)</p>
    <p>Target X*</p>
    <p>Error criterion on partition vector X only!</p>
  </div>
  <div class="page">
    <p>Energy function for segmenting 1 image I</p>
    <p>* 2( ) || ( ( , )) ( ) ||IE W X W I X I=</p>
    <p>Target X*</p>
    <p>Eigenvector X(W)</p>
  </div>
  <div class="page">
    <p>* 2</p>
    <p>images</p>
    <p>Min. ( ) || ( ( , )) ( ) || I</p>
    <p>E X W I X I =</p>
    <p>Energy function for segmenting 1 image I</p>
    <p>* 2( ) || ( ( , )) ( ) ||IE W X W I X I=</p>
    <p>Target X*</p>
    <p>Eigenvector X(W)</p>
  </div>
  <div class="page">
    <p>* 2( ) || ( ( , )) ( ) ||IE W X W I X I=</p>
    <p>Energy function for image I</p>
    <p>Target X*</p>
    <p>Eigenvector X(W)</p>
    <p>Can use gradient descent</p>
    <p>* 2</p>
    <p>images</p>
    <p>Min. ( ) || ( ( , )) ( ) || I</p>
    <p>E X W I X I =</p>
    <p>( ) ...but ( ) is only implicitly defined, by ( , ) - 0, ( , )</p>
    <p>X W W I D X</p>
    <p>X f W  =</p>
    <p>Can Not backtrack easily</p>
  </div>
  <div class="page">
    <p>Gradient descent:</p>
    <p>E E X W X W</p>
    <p>= =</p>
    <p>* 2</p>
    <p>images</p>
    <p>Min. ( ) || ( ( , )) ( ) || I</p>
    <p>E X W I X I =</p>
  </div>
  <div class="page">
    <p>Gradient descent:</p>
    <p>E E X W X W</p>
    <p>= =</p>
    <p>* 2</p>
    <p>images</p>
    <p>Min. ( ) || ( ( , )) ( ) || I</p>
    <p>E X W I X I =</p>
    <p>X(W) is implicit</p>
    <p>E(X) is quadratic depends on W()</p>
  </div>
  <div class="page">
    <p>( )</p>
    <p>( )</p>
    <p>The map W (X, ) is C over and we can express the derivatives over any C path ( ) as :</p>
    <p>' '( ( ))</p>
    <p>( ( )) = ' '</p>
    <p>T</p>
    <p>T</p>
    <p>W t</p>
    <p>X W D Xd W t dt X DX</p>
    <p>dX W t d W D W D</p>
    <p>dt</p>
    <p>=</p>
    <p>Theorem : Derivative of NCut eigenvectors</p>
    <p>D X dt</p>
    <p>E X W X W</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>Feasible set in the space of graph weight matrices : iff 1) is symmetric matrix</p>
    <p>W W n n W</p>
    <p>WX DX</p>
    <p>&gt; =</p>
    <p>( )</p>
    <p>( )</p>
    <p>The map W (X, ) is C over and we can express the derivatives over any C path ( ) as :</p>
    <p>' '( ( ))</p>
    <p>( ( )) = ' '</p>
    <p>T</p>
    <p>T</p>
    <p>W t</p>
    <p>X W D Xd W t dt X DX</p>
    <p>dX W t d W D W D</p>
    <p>dt</p>
    <p>=</p>
    <p>Theorem : Derivative of NCut eigenvectors</p>
    <p>D X dt</p>
    <p>E X W X W</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>( )</p>
    <p>( )</p>
    <p>The map W (X, ) is C over and we can express the derivatives over any C path ( ) as :</p>
    <p>' '( ( ))</p>
    <p>( ( )) = ' '</p>
    <p>T</p>
    <p>T</p>
    <p>W t</p>
    <p>X W D Xd W t dt X DX</p>
    <p>dX W t d W D W D</p>
    <p>dt</p>
    <p>=</p>
    <p>Theorem : Derivative of NCut eigenvectors</p>
    <p>D X dt</p>
    <p>E X W X W</p>
    <p>=</p>
    <p>[Meila, Shortreed, Xu, 05]</p>
  </div>
  <div class="page">
    <p>( )parameters : ,x y =</p>
    <p>Task 1: Learning 2D point segmentation</p>
  </div>
  <div class="page">
    <p>( )2 2Learn exp ( ) ( )xij i j i jyW x x y y =</p>
    <p>( ) * 2x y x y( , ) || ( , ) ||E X W X   =</p>
    <p>Ground truth segmentation</p>
    <p>X* and X(W) learned</p>
  </div>
  <div class="page">
    <p>Learning 2D point segmentation</p>
    <p>( )2 2Learn exp ( ) ( )xij i j i jyW x x y y =</p>
    <p>( ) * 2x y x y( , ) || ( , ) ||E X W X   =  Ground truth segmentation</p>
    <p>X* and X(W) learned</p>
  </div>
  <div class="page">
    <p>( )</p>
    <p>The PDE either</p>
    <p>converges to a global minimum : E W(t) 0, exponentially</p>
    <p>or escapes any compact K this happens when 1) ( ) 1</p>
    <p>E W</p>
    <p>W W</p>
    <p>t</p>
    <p>=</p>
    <p>Proposition : Exponential convergence</p>
    <p>( )</p>
    <p>or ( ) ( ) 0 2) ( , ) 0 for some W t</p>
    <p>t t D i i i</p>
  </div>
  <div class="page">
    <p>Is there a ground truth segmentation ?</p>
    <p>{ }Graph nodes edge at ( , ) with angle i i i ie x y =</p>
  </div>
  <div class="page">
    <p>Task 2: Learning rectangular shape</p>
    <p>Convex Concave Impossible</p>
    <p>( , ) ( , , ; , , )i j i i i j j jW e e f x y x y =</p>
    <p>Edge location Edge orientation</p>
    <p>,i i i</p>
    <p>x y</p>
  </div>
  <div class="page">
    <p>Task 2: Learning rectangular shape</p>
    <p>Convex Concave Impossible</p>
    <p>( , ) ( , , ; , , )i j i i i j j jW e e f x y x y =</p>
    <p>=   =</p>
    <p>,Edge location Edge orientaion</p>
    <p>i i</p>
    <p>i</p>
    <p>x y</p>
  </div>
  <div class="page">
    <p>Task 2: Learning rectangular shape</p>
    <p>Convex Concave Impossible</p>
    <p>( , ) ( , , ; , , )i j i i i j j jW e e f x y x y =</p>
    <p>Edge location , Edge orientaion</p>
    <p>i i</p>
    <p>i</p>
    <p>x y</p>
    <p>( , ) ( , , , )i j j i j i i jW e e f x x y y  =   Invariance through parameterization :</p>
  </div>
  <div class="page">
    <p>training</p>
    <p>Input edges</p>
    <p>testing</p>
    <p>Input edges</p>
    <p>Ncut eigenvector after training</p>
    <p>Ncut eigenvector</p>
    <p>target</p>
    <p>Synthetic case</p>
  </div>
  <div class="page">
    <p>Testing on real images Learning with precise edges</p>
  </div>
  <div class="page">
    <p>Comparison of the spectral graph learning schemes</p>
    <p>Our method</p>
    <p>*argmin || ( ) || W</p>
    <p>X W X</p>
    <p>Bach-Jordan</p>
    <p>),(minarg *XWJ W</p>
    <p>D W D W X</p>
    <p>Meila and Shi, Fowlkes, Martin and Malik</p>
    <p>Bach and Jordan (2003) Learning spectral clustering Use a differentiable approximation</p>
    <p>of eigenvectors</p>
    <p>Our work Exact analytical form computationally efficient solution</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Supervise the un-supervised spectral clustering</p>
    <p>Exact and efficient learning of graph weights using derivatives of eigenvectors</p>
  </div>
</Presentation>
