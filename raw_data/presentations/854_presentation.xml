<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Nadav HarEl Abel Gordon Alex Landau</p>
    <p>Muli Ben-Yehuda, Avishay Traegerx Razya Ladelskyx</p>
    <p>IBM Research  Haifa  Technion and Hypervisor Consulting</p>
    <p>Efficient and scaLable paraVirtual I/o System (ELVIS)</p>
  </div>
  <div class="page">
    <p>Why (not) software-based I/O interposition in virtual environments?</p>
    <p>Pros</p>
    <p>Software Defined Networking</p>
    <p>File based images</p>
    <p>Live Migration</p>
    <p>Fault Tolerance</p>
    <p>Security</p>
    <p>.</p>
    <p>Cons</p>
    <p>Scalability Limitations</p>
    <p>Performance Degradation</p>
    <p>Scalability Limitations</p>
    <p>Performance Degradation</p>
  </div>
  <div class="page">
    <p>I/O Virtualization Models</p>
    <p>S c a</p>
    <p>la b</p>
    <p>il it y a</p>
    <p>n d P</p>
    <p>e rf</p>
    <p>o rm</p>
    <p>a n c e</p>
    <p>Flexibility</p>
    <p>Bare-metal I/O (no VM) SR-IOV +</p>
    <p>ELI</p>
    <p>Para-Virtual I/O</p>
    <p>SR-IOV</p>
    <p>ELVIS</p>
    <p>Emulated I/O</p>
    <p>Non-virtualizable</p>
    <p>Unmodified GuestNon-interposable</p>
    <p>Interposable</p>
  </div>
  <div class="page">
    <p>I/OI/O</p>
    <p>GuestGuest</p>
    <p>HypervisorHypervisor</p>
    <p>The guest posts I/O requests in ring-queue (shared with the</p>
    <p>hypervisor) and sends a request notification</p>
    <p>The hypervisor processes the requests and sends a reply</p>
    <p>notification</p>
    <p>How Paravirtual I/O works today</p>
    <p>I/O Request</p>
    <p>Notification I/O Reply</p>
    <p>Notification</p>
    <p>Ring Queue</p>
    <p>I/O DeviceI/O Device</p>
  </div>
  <div class="page">
    <p>How I/O notifications are sent/received</p>
    <p>VCPU Thread (Core X)</p>
    <p>guest</p>
    <p>hypervisor</p>
    <p>I/O Thread (Core Y)</p>
    <p>hypervisor</p>
    <p>I/O notification Guest-to-Host</p>
    <p>I/O notification Host-to-Guest</p>
    <p>Process I/O Request</p>
    <p>Complete I/O Request</p>
    <p>PIO</p>
    <p>Virtual Interrupt Injection</p>
    <p>CPU context switch (exits and entries) I/O processing Guest execution</p>
    <p>Forced Exit (via IPI)</p>
    <p>1 thread per virtual CPU (VCPU)</p>
    <p>1 thread per virtual I/O device</p>
  </div>
  <div class="page">
    <p>Is this model scalable with the number of guests and I/O bandwidth ?</p>
    <p>Core 1</p>
    <p>VM1</p>
    <p>Core N+1</p>
    <p>I/O VM1</p>
    <p>Core N</p>
    <p>Core 2</p>
    <p>VM2</p>
    <p>Exit I/O</p>
    <p>VM2</p>
    <p>Exit</p>
    <p>VM2</p>
    <p>VM1 VM2</p>
    <p>VM1</p>
    <p>Exit</p>
    <p>VM2 VM1</p>
    <p>ExitExit</p>
    <p>VMj</p>
    <p>I/O VMj</p>
    <p>Exit</p>
    <p>VMi</p>
    <p>VM1</p>
    <p>I/O VM1</p>
    <p>VM2</p>
    <p>I/O VM2</p>
    <p>Exit</p>
    <p>VCPU and I/O thread-based scheduling for all cores</p>
    <p>E x e c u ti o n T</p>
    <p>im e</p>
    <p>Depends on the host thread scheduler but</p>
    <p>the scheduler has no information about the</p>
    <p>I/O activity of the virtual devices.</p>
  </div>
  <div class="page">
    <p>Facts and Trends</p>
    <p>Notifications cause exits (context switches) == overhead!</p>
    <p>Current trend is:</p>
    <p>Towards multi-core systems with an increasing numbers of</p>
    <p>cores per socket (4-&gt;6-&gt;8-&gt;16-&gt;32) and guests per host</p>
    <p>Faster networks with expectation of lower latency and</p>
    <p>higher bandwidth (1GbE-&gt;10GbE-&gt;40GbE-&gt;100GbE)</p>
    <p>I/O virtualization is a CPU intensive task, and may require</p>
    <p>more cycles than the available in a single core</p>
    <p>We need a new efficient and scalable</p>
    <p>Paravirtual I/O model!</p>
  </div>
  <div class="page">
    <p>I/O Core</p>
    <p>ELVIS: use fine-grained I/O scheduling and dedicate cores to</p>
    <p>improve scalability and efficiency</p>
    <p>Core 1</p>
    <p>VM1</p>
    <p>I/O Core</p>
    <p>I/O VM1</p>
    <p>Core N</p>
    <p>VMi I/O</p>
    <p>VM2 I/O</p>
    <p>VMn</p>
    <p>fine-grained I/O scheduling</p>
    <p>Core 2</p>
    <p>VM2</p>
    <p>I/O VM2 I/O VMi</p>
    <p>thread-based scheduling</p>
    <p>E x e c u ti o n T</p>
    <p>im e</p>
    <p>VM2 VM1</p>
    <p>VMj</p>
    <p>VMi I/O</p>
    <p>Core I/O</p>
    <p>Core Core 1</p>
    <p>VM1</p>
    <p>I/O Core</p>
    <p>I/O VM1</p>
    <p>Core N</p>
    <p>VMi I/O</p>
    <p>VM2 I/O VMj</p>
    <p>Core 2</p>
    <p>VM2</p>
    <p>I/O VM2 I/O VMi</p>
    <p>E x e c u ti o n T</p>
    <p>im e</p>
    <p>VM2 VM1</p>
    <p>VMj</p>
    <p>VMi</p>
    <p>Process queues based on the I/O activity  Balance between throughput and latency  No process/thread context switches for I/O  Exitless communication (next slide)</p>
    <p>requests of many VMs</p>
  </div>
  <div class="page">
    <p>ELVIS: remove notifications overhead to further improve efficiency</p>
    <p>VCPU Thread (Core X)</p>
    <p>guest</p>
    <p>hypervisor</p>
    <p>(time)</p>
    <p>I/O Thread (Core Y)</p>
    <p>hypervisor</p>
    <p>I/O notification Guest-to-Host</p>
    <p>I/O notification Host-to-Guest</p>
    <p>Process I/O Request</p>
    <p>Complete I/O Request</p>
    <p>ELVIS</p>
    <p>VCPU Thread (Core X)</p>
    <p>guest</p>
    <p>hypervisor</p>
    <p>(time)</p>
    <p>I/O Thread (Core Y)</p>
    <p>hypervisor</p>
    <p>I/O notification Guest-to-Host</p>
    <p>I/O notification Host-to-Guest</p>
    <p>Process I/O Request</p>
    <p>Complete I/O Request</p>
    <p>Traditional Paravirtual</p>
    <p>I/O</p>
    <p>Polling</p>
    <p>Exitless virtual interrupt injection (via ELI)</p>
  </div>
  <div class="page">
    <p>Single thread in a dedicated core monitors the activity of each</p>
    <p>queue (VMs I/O)</p>
    <p>Decide which queue should be processed and for how long</p>
    <p>ELVIS: Fine-grained I/O scheduling in a nutshell</p>
    <p>Min data</p>
    <p>Max data</p>
    <p>Q2 is stuck</p>
    <p>Q2: latency sensitiveQ1: throughput intensive Q3: throughput intensive</p>
    <p>Dedicated I/O Core</p>
    <p>Q2 is Stuck but not passed min</p>
    <p>Check queues activity</p>
  </div>
  <div class="page">
    <p>ELVIS: Placement of threads, memory and interrupts</p>
    <p>Dedicate 1 I/O core per CPU socket</p>
    <p>Cores per socket continue to increase year by year</p>
    <p>More cores are required to virtualize more bandwidth at lower latencies (network links continue to be improved)</p>
    <p>NUMA awareness: shared LLC cache and memory controller, DDIO technology</p>
    <p>Deliver interrupts to the corresponding I/O core</p>
    <p>Interrupts are processed by I/O cores and do not disturb</p>
    <p>the running the guests</p>
    <p>Improve locality</p>
    <p>Multi-port and SR-IOV adapters can dedicate interrupts</p>
    <p>per port or virtual function</p>
  </div>
  <div class="page">
    <p>Implementation and Experimental Setup</p>
    <p>Implementation</p>
    <p>Based on KVM Hypervisor (Linux Kernel 3.1 / QEMU 0.14)</p>
    <p>With VHOST, in-kernel paravirtual I/O framework</p>
    <p>Use ELI patches to enable exitless replies and to improve hardware-assisted non-interposable I/O (SR-IOV)</p>
    <p>Experimental Setup</p>
    <p>IBM System x3550 M4, dual socket 8 cores per socket Intel Xeon E2660 2.2GHz (SandyBridge)</p>
    <p>Dual port 10GbE Intel x520 SRIOV NIC</p>
    <p>2 identical servers: one used to host the VMs and the other used to generate load on bare-metal</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Repeated experiments using 1 to 14 UP VMs</p>
    <p>1x10GbE when running up-to 7 VMs</p>
    <p>2x10GbE when running more than 7 VMs</p>
    <p>Compared ELVIS against 3 other configurations</p>
    <p>No interposition</p>
    <p>Each VM runs on a dedicated core and has a SR-IOV VF assigned using ELI</p>
    <p>The closer ELVIS is to this configuration, the smaller the overhead is (used to evaluate ELVIS efficiency)</p>
  </div>
  <div class="page">
    <p>Methodology (cont.)</p>
    <p>N=number of VMs (1 to 14)</p>
    <p>Used N+1 cores (N 7) or N+2 cores (N&gt;7)</p>
    <p>This is the resource overhead for I/O interposition</p>
    <p>ELVIS</p>
    <p>1 dedicated core per VCPU (VM)</p>
    <p>1 core (N&lt;=7) or (N&gt;7) 2 cores dedicated for I/O</p>
    <p>Baseline</p>
    <p>N+1 cores (N  7) or N+2 cores (N&gt;7) to run VCPU and I/O threads (no thread affinity)</p>
    <p>Baseline+Affinity</p>
    <p>Baseline but dedicate 1 core per VCPU and pin I/O threads to dedicated I/O cores</p>
  </div>
  <div class="page">
    <p>Netperf  TCP Stream 64Bytes (throughput intensive)</p>
    <p>ELVIS: 1 core dedicated for I/O and 1</p>
    <p>dedicated core per VM (N+1 total)</p>
    <p>Baseline: N+1 cores (to handle I/O and to</p>
    <p>run the VMs)</p>
    <p>No Interposition: N cores to run the VMs</p>
    <p>Numbers of VMs</p>
    <p>ELVIS: 2 cores dedicated for I/O and 1</p>
    <p>dedicated core per VM (N+2 total)</p>
    <p>Baseline: N+2 cores (to handle I/O and to</p>
    <p>run the VMs)</p>
    <p>No Interposition: N cores to run the VMs</p>
    <p>Scaled perfectly</p>
    <p>1 core managed to handle I/O for 7 VMs (cores)</p>
    <p>Maximum throughput</p>
    <p>Coalesced more interrupts than the SR-IOV device (4K-11K vs. 30K ints/sec)</p>
  </div>
  <div class="page">
    <p>Netperf  UDP Request Response (latency sensitive)</p>
    <p>Latency slightly increased with more VMs</p>
    <p>Better than No Interposition in some cases because enabling SR-IOV in the NIC increases latency by 22% (ELVIS disables SR-IOV)</p>
  </div>
  <div class="page">
    <p>Memcached - 90% get, 10% set, 32 concurrent requests per VM 1KB value size, 64B key size</p>
    <p>I/O core saturated after 3 VMs</p>
    <p>ELVIS was up to 30% slower than No interposition when the I/O core was not saturated, but was always 30%-115% better than Baseline</p>
    <p>I/O core saturated</p>
  </div>
  <div class="page">
    <p>Improving I/O Virtualization - Related Work</p>
    <p>Paravirtual I/O</p>
    <p>Polling</p>
    <p>Spatial division of cores / core dedication</p>
    <p>Exitless Interrupts</p>
    <p>We extended many of these ideas and integrated them with a</p>
    <p>fine-grained I/O scheduling to build a new Efficient and Scalable paravirtual I/O System (ELVIS)</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Most data centers and cloud providers use paravirtual I/O (required to enable many useful virtualization features)</p>
    <p>Current trend towards multi-core systems and towards faster networks makes paravirtual I/O inefficient and not scalable</p>
    <p>ELVIS presents a new efficient and scalable I/O virtualization system that removes paravirtual I/O deficiencies</p>
    <p>Future Work</p>
    <p>Improve fine-grained I/O scheduling to consider VMs SLAs</p>
    <p>Dynamically allocate or release I/O cores based on the system load and guests workloads</p>
    <p>Core Specialization: I/O core &lt;&gt; VCPU cores</p>
  </div>
  <div class="page">
    <p>Questions ?</p>
  </div>
</Presentation>
