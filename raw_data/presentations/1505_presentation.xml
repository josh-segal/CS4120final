<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SDGen: Mimicking Datasets for Content</p>
    <p>Generation in Storage Benchmarks</p>
    <p>Ral Gracia-Tinedo (Universitat Rovira i Virgili, Spain)</p>
    <p>Danny Harnik, Dalit Naor, Dmitry Sotnikov (IBM Research-Haifa, Israel)</p>
    <p>Sivan Toledo, Aviad Zuck (Tel-Aviv University, Israel)</p>
  </div>
  <div class="page">
    <p>Pre-Introduction</p>
    <p>Random</p>
    <p>Data Zero Data</p>
    <p>Stones in the backpack!!! Just thin air</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Benchmarking is essential to evaluate storage systems:</p>
    <p>File systems, databases, micro-benchmarks</p>
    <p>FileBench, LinkBench, Bonnie++, YCSB,</p>
    <p>Many storage benchmarks try to recreate real workloads:</p>
    <p>Operations per unit of time, R/W behavior,</p>
    <p>But, what about the data generated during a benchmark?</p>
    <p>Real dataset: representative, proprietary, potentially large</p>
    <p>Simple synthetic data (zeros, random data): not-representative,</p>
    <p>easy to create, reproducible</p>
  </div>
  <div class="page">
    <p>The Problem</p>
    <p>Does the benchmarking data actually matter?</p>
    <p>ZFS Example: A file system with built-in compression</p>
    <p>ZFS is significantly content-sensitive if compression enabled</p>
    <p>The throughput also varies depending on the compressor</p>
    <p>Conclusion: Yes, it matters if data reduction is involved!</p>
  </div>
  <div class="page">
    <p>Current Solutions</p>
    <p>Some benchmarks try to emulate the compressibility of data</p>
    <p>(LinkBench, Fio, VDBench): Mixing compressible/incompressible data</p>
    <p>at right proportion.</p>
    <p>Problems (LinkBench data vs real data):</p>
    <p>Accurate compression ratios</p>
    <p>but insensitive to compressor</p>
    <p>Unrealistic compression times</p>
    <p>Heterogeneity is not captured</p>
    <p>zlib - Text Data (Calgary Corpus)</p>
    <p>Rand Zeros Rand Zeros 50% compressible!</p>
  </div>
  <div class="page">
    <p>Our Mission</p>
    <p>Complex situation:</p>
    <p>Most storage benchmarks generate unrealistic contents</p>
    <p>Representative data is normally not shared due to privacy issues</p>
    <p>Not good for the performance evaluation of storage</p>
    <p>systems with data reduction techniques built-in.</p>
    <p>We need a common approach to generate realistic and</p>
    <p>reproducible benchmarking data.</p>
    <p>In this work, we focus on compression benchmarking.</p>
  </div>
  <div class="page">
    <p>Summary of our Work</p>
    <p>Synthetic Data GENerator (SDGen): open and extensible</p>
    <p>framework to generate realistic data for storage benchmarks.</p>
    <p>Goal: mimic real datasets.</p>
    <p>Compact, reusable and anonymized dataset representation.</p>
    <p>Mimicking compression: identify the properties of data that</p>
    <p>are key to the performance of popular lossless compressors (e.g.</p>
    <p>zlib, lz4).</p>
    <p>Usability and integration: SDGen is available for download and</p>
    <p>has been integrated in popular benchmarks (LinkBench,</p>
    <p>Impressions).</p>
  </div>
  <div class="page">
    <p>SDGen: Concept &amp; Overview</p>
    <p>Scan Dataset</p>
    <p>Build Characteri</p>
    <p>zation Share it</p>
    <p>Load Characteri</p>
    <p>zation</p>
    <p>Generate Data</p>
    <p>Mimicking method: capture the characteristics of data that affect data reduction techniques to generate similar synthetic data.</p>
    <p>SDGen works in two main phases:</p>
    <p>SDGen can do full scans or use sampling.</p>
    <p>SDGen requires knowing what to scan for and how to generate data.</p>
    <p>S D</p>
    <p>G e n L</p>
    <p>if e c y c le</p>
    <p>Scan Phase Generation Phase</p>
  </div>
  <div class="page">
    <p>Mimicking data for compression</p>
    <p>We empirically found two properties that affect the behavior of</p>
    <p>compression engines:</p>
    <p>Repetition length distribution</p>
    <p>Key for compression time &amp; ratio</p>
    <p>Typically follows a power-law</p>
    <p>Byte frequency</p>
    <p>Impacts on entropy coding</p>
    <p>Changes importantly depending</p>
    <p>on data</p>
  </div>
  <div class="page">
    <p>Generating synthetic data</p>
    <p>Goals:</p>
    <p>Generate data with similar properties (repetition lengths, byte freq.)</p>
    <p>Fast generation throughput</p>
    <p>At high-level, we generate a data chunk as follows:</p>
    <p>Synthetic</p>
    <p>chunk</p>
    <p>Repeated sequence 1) Random decision: repetition or not?</p>
    <p>repetition histogram</p>
    <p>Histogram</p>
    <p>repetition histogram</p>
    <p>Rep. Len.</p>
    <p>Histogram</p>
    <p>generator</p>
    <p>Initialize</p>
    <p>source of</p>
    <p>repetitions</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Objective Metrics Additional Mimicked Properties</p>
    <p>Compression ratio</p>
    <p>Compression time</p>
    <p>Repetition length</p>
    <p>Entropy (byte frequencies)</p>
    <p>Datasets</p>
    <p>Calgary/Canterbury corpus</p>
    <p>Silesia Corpus</p>
    <p>PDFs (FAST conferences)</p>
    <p>Media (IBM engineers)</p>
    <p>Sensor network data</p>
    <p>Enwiki9</p>
    <p>Private Mix (VMs, .xml, .html,)</p>
    <p>Compressors</p>
    <p>Target: Lossless compression based on byte level repetition finding and/or on entropy encoding (zlib, lz4).</p>
    <p>We tested other families of compressors (bzip2, lzma).</p>
  </div>
  <div class="page">
    <p>Evaluation: Mimicked Properties</p>
    <p>Experiment: compare repetition length distributions and</p>
    <p>byte entropy in real and SDGen data.</p>
    <p>SDGen generates data that closely mimics these metrics.</p>
  </div>
  <div class="page">
    <p>Evaluation: Compression Ratio &amp; Time</p>
    <p>Per-chunk compression ratio</p>
    <p>Compression ratios are</p>
    <p>closely mimicked</p>
    <p>Heterogeneity is also well</p>
    <p>captured within a dataset Per-chunk compression time</p>
    <p>Compression times are harder</p>
    <p>to mimic (especially for lz4)</p>
    <p>Still, for most data types</p>
    <p>compressors behave similarly</p>
    <p>Experiment: Capture per-chunk compression ratios and times</p>
    <p>for both synthetic and real datasets.</p>
  </div>
  <div class="page">
    <p>Evaluation: Performance of ZFS</p>
    <p>Experiment: write to ZFS 1GB files augmenting</p>
    <p>previous datasets.</p>
    <p>ZFS exhibits similar behavior for both real and our</p>
    <p>synthetic data.</p>
    <p>ZFS digests faster LinkBench data (+12% to +44%).</p>
    <p>DNA sequencing files in Calgary corpus are specially hard to compress</p>
  </div>
  <div class="page">
    <p>Evaluation: Integration with LinkBench</p>
    <p>Experiment: LinkBench write workload using distinct data</p>
    <p>types (ZFS + SSD storage).</p>
    <p>SDGen serves as data generation layer for LinkBench.</p>
    <p>Write latency is similar in both synthetic and text dataset.</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Future Directions</p>
    <p>Data is an important aspect of storage benchmarking</p>
    <p>when data reduction is involved (compression, dedup).</p>
    <p>We presented SDGen: a framework for generating</p>
    <p>realistic and sharable benchmarking data.</p>
    <p>Idea: scan data, build a characterization, share it, generate data</p>
    <p>We designed a method to mimic data compression</p>
    <p>ratios and times for popular lossless compressors.</p>
    <p>We plan to extend SDGen to mimic data deduplication.</p>
  </div>
  <div class="page">
    <p>Q&amp;A</p>
    <p>Thanks for your attention!</p>
    <p>SDGen code: https://github.com/iostackproject/SDGen</p>
    <p>Funding projects:</p>
    <p>http://cloudspaces.eu</p>
    <p>http://iostack.eu</p>
    <p>Towards the next generation of open Personal Clouds</p>
    <p>Software-Defined Storage for Big Data</p>
  </div>
  <div class="page">
    <p>Backup: Generation Performance</p>
    <p>Characterizations of chunks can be used in parallel for</p>
    <p>generation.</p>
    <p>Generating uncompressible data is more expensive.</p>
    <p>We plan optimizations to wisely reuse random data for boosting</p>
    <p>throughput.</p>
  </div>
</Presentation>
