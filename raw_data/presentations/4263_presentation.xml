<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Discourse-level Diversity for Neural Dialog Models Using Conditional Variational Autoencoders</p>
    <p>Tiancheng Zhao, Ran Zhao and Maxine Eskenazi Language Technologies Institute Carnegie Mellon University</p>
    <p>Code&amp;Data: https://github.com/snakeztc/NeuralDialog-CVAE</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>End-to-end dialog models based on encoder-decoder models have shown great promises for</p>
    <p>modeling open-domain conversations, due to its flexibility and scalability.</p>
    <p>System Response</p>
    <p>Encoder Decoder</p>
    <p>Dialog History/Context</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>However, dull response problem! [Li et al 2015, Serban et al. 2016]. Current solutions include:</p>
    <p>Add more info to the dialog context [Xing et al 2016, Li et al 2016]</p>
    <p>Improve decoding algorithm, e.g. beam search [Wiseman and Rush 2016]</p>
    <p>YesI dont knowsure</p>
    <p>Encoder Decoder</p>
    <p>User: I am feeling quite happy today.  (previous utterances)</p>
  </div>
  <div class="page">
    <p>Our Key Insights</p>
    <p>Response generation in conversation is a ONE-TO-MANY mapping problem at the</p>
    <p>discourse level.</p>
    <p>A similar dialog context can have many different yet valid responses.</p>
    <p>Learn a probabilistic distribution over the valid responses instead of only keep the</p>
    <p>most likely one.</p>
  </div>
  <div class="page">
    <p>Our Key Insights</p>
    <p>Response generation in conversation is a ONE-TO-MANY mapping problem at the discourse</p>
    <p>level.  A similar dialog context can have many different yet valid responses.</p>
    <p>Learn a probabilistic distribution over the valid responses instead of only keep the most likely</p>
    <p>one.</p>
  </div>
  <div class="page">
    <p>Our Contributions</p>
    <p>(CVAE).</p>
  </div>
  <div class="page">
    <p>Conditional Variational Auto Encoder (CVAE)</p>
    <p>C is dialog context</p>
    <p>B: Do you like cats? A: Yes I do</p>
    <p>Z is the latent variable (gaussian)</p>
    <p>X is the next response</p>
    <p>B: So do I.</p>
  </div>
  <div class="page">
    <p>Conditional Variational Auto Encoder (CVAE)</p>
    <p>C is dialog context</p>
    <p>B: Do you like cats? A: Yes I do</p>
    <p>Z is the latent variable (gaussian)</p>
    <p>X is the next response</p>
    <p>B: So do I.</p>
    <p>Trained by Stochastic Gradient Variational</p>
    <p>Bayes (SGVB) [Kingma and Welling 2013]</p>
  </div>
  <div class="page">
    <p>Knowledge-Guided CVAE (kgCVAE)</p>
    <p>Y is linguistic features extracted from responses</p>
    <p>Dialog act: statement -&gt; So do I.</p>
    <p>Use Y to guide the learning of latent Z</p>
  </div>
  <div class="page">
    <p>Training of (kg)CVAE Reconstruction loss</p>
    <p>KL-divergence loss</p>
  </div>
  <div class="page">
    <p>Testing of (kg)CVAE</p>
  </div>
  <div class="page">
    <p>Optimization Challenge Training CVAE with RNN decoder is hard due to the vanishing latent variable problem</p>
    <p>[Bowman et al., 2015]</p>
    <p>RNN decoder can cheat by using LM information and ignore Z!</p>
    <p>Bowman et al. [2015] described two methods to alleviate the problem :</p>
    <p>picking).</p>
  </div>
  <div class="page">
    <p>BOW Loss  Predict the bag-of-words in the responses X at once (word counts in the response)</p>
    <p>Break the dependency between words and eliminate the chance of cheating based on LM.</p>
    <p>z</p>
    <p>c</p>
    <p>RNN Lossx</p>
  </div>
  <div class="page">
    <p>BOW Loss  Predict the bag-of-words in the responses X at once (word counts in the response)</p>
    <p>Break the dependency between words and eliminate the chance of cheating based on LM.</p>
    <p>z</p>
    <p>c</p>
    <p>RNN Lossx</p>
    <p>x wo</p>
    <p>FF Bag-of-word Loss</p>
  </div>
  <div class="page">
    <p>Dataset</p>
    <p>Data Name Switchboard Release 2</p>
    <p>Number of dialogs 2,400 (2316/60/62 - train/valid/test)</p>
    <p>Number of context-response pairs 207,833/5,225/5,481</p>
    <p>Vocabulary Size Top 10K</p>
    <p>Dialog Act Labels 42 types, tagged by SVM and human</p>
    <p>Number of Topics 70 tagged by humans</p>
  </div>
  <div class="page">
    <p>Quantitative Metrics</p>
    <p>Ref resp1</p>
    <p>Ref resp Mc</p>
    <p>Context</p>
    <p>Hyp resp 1</p>
    <p>Hyp resp N</p>
    <p>ModelHuman ... ...</p>
  </div>
  <div class="page">
    <p>Quantitative Metrics</p>
    <p>d(r, h) is a distance function [0, 1] to measure the similarity between a reference and a hypothesis.</p>
    <p>Appropriateness</p>
    <p>Diversity</p>
    <p>Ref resp1</p>
    <p>Ref resp Mc</p>
    <p>Context</p>
    <p>Hyp resp 1</p>
    <p>Hyp resp N</p>
    <p>ModelHuman ... ...</p>
  </div>
  <div class="page">
    <p>Distance Functions used for Evaluation 1. Smoothed Sentence-level BLEU (1/2/3/4): lexical similarity</p>
    <p>(pre-trained Glove embedding on twitter)</p>
    <p>a. Average of embeddings (A-bow)</p>
    <p>b. Extrema of embeddings (E-bow)</p>
    <p>a. (Use pre-trained dialog act tagger for tagging)</p>
  </div>
  <div class="page">
    <p>Models (trained with BOW loss)</p>
    <p>Encoder Sampling Decoder</p>
    <p>Encoder Greedy Decoder</p>
    <p>Encoder Greedy Decoder</p>
    <p>z</p>
    <p>z</p>
    <p>y</p>
    <p>sampling</p>
    <p>sampling</p>
    <p>Baseline</p>
    <p>CVAE</p>
    <p>kgCVAE</p>
  </div>
  <div class="page">
    <p>Quantitative Analysis Results</p>
    <p>Metrics Perplexi ty (KL)</p>
    <p>BLEU-1 (p/r)</p>
    <p>BLEU-2 (p/r)</p>
    <p>BLEU-3 (p/r)</p>
    <p>BLEU-4 (p/r)</p>
    <p>A-bow (p/r)</p>
    <p>E-bow (p/r)</p>
    <p>DA (p/r)</p>
    <p>Baseline (sample)</p>
    <p>CVAE (greedy)</p>
    <p>kgCVAE (greedy)</p>
    <p>Note: BLEU are normalized into [0, 1] to be valid precision and recall distance function</p>
  </div>
  <div class="page">
    <p>Qualitative Analysis Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target (statement): well at my workplace we have places for aluminium cans</p>
    <p>Baseline + Sampling kgCVAE + Greedy</p>
  </div>
  <div class="page">
    <p>Latent Space Visualization</p>
    <p>Visualization of the posterior Z on the test</p>
    <p>dataset in 2D space using t-SNE.</p>
    <p>Assign different colors to the top 8 frequent</p>
    <p>dialog acts.</p>
    <p>The size of circle represents the response</p>
    <p>length.</p>
    <p>Exhibit clear clusterings of responses w.r.t the</p>
    <p>dialog act</p>
  </div>
  <div class="page">
    <p>The Effect of BOW Loss</p>
    <p>Same setup on PennTree Bank for LM [Bowman 2015]. Compare 4 setups:</p>
    <p>Goal: low reconstruction loss + small but non-trivial KL cost</p>
    <p>Model Perplexity KL Cost</p>
    <p>Standard 122.0 0.05</p>
    <p>KLA 111.5 2.02</p>
    <p>BOW 97.72 7.41</p>
    <p>BOW+KLA 73.04 15.94</p>
  </div>
  <div class="page">
    <p>KL Cost during Training</p>
    <p>Standard model suffers from vanishing</p>
    <p>latent variable.</p>
    <p>KLA requires early stopping.</p>
    <p>BOW leads to stable convergence</p>
    <p>with/without KLA.</p>
    <p>The same trend is observed on CVAE.</p>
  </div>
  <div class="page">
    <p>Conclusion and Future Work  Identify the ONE-TO-MANY nature of open-domain dialog modeling</p>
    <p>Propose two novel models based on latent variables models for generating diverse yet</p>
    <p>appropriate responses.</p>
    <p>Explore further in the direction of leveraging both past linguistic findings and deep models</p>
    <p>for controllability and explainability.</p>
    <p>Utilize crowdsourcing to yield more robust evaluation.</p>
    <p>Code available here! https://github.com/snakeztc/NeuralDialog-CVAE</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>References</p>
  </div>
  <div class="page">
    <p>Training Details</p>
    <p>Word Embedding 200 Glove pre-trained on Twitter</p>
    <p>Utterance Encoder Hidden Size 300</p>
    <p>Context Encoder Hidden Size 600</p>
    <p>Response Decoder Hidden Size 400</p>
    <p>Latent Z Size 200</p>
    <p>Context Window Size 10 utterances</p>
    <p>Optimizer Adam learning rate=0.001</p>
  </div>
  <div class="page">
    <p>Testset Creation</p>
    <p>Use 10-nearest neighbour to collect similar context in the training data</p>
    <p>Label a subset of the appropriateness of the 10 responses by 2 human</p>
    <p>annotators</p>
    <p>bootstrap via SVM on the whole test set (5481 context/response)</p>
    <p>Resulting 6.79 Avg references responses/context</p>
    <p>Distinct reference dialog acts 4.2</p>
  </div>
</Presentation>
