<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ISCA 2007 1</p>
    <p>Comparing Memory Systems for Chip Multiprocessors</p>
    <p>Jacob Leverich Hideho Arakida, Alex Solomatnikov, Amin Firoozshahian, Mark Horowitz,</p>
    <p>Christos Kozyrakis</p>
    <p>Computer Systems Laboratory Stanford University</p>
  </div>
  <div class="page">
    <p>Cores are the New GHz</p>
    <p>M</p>
    <p>P</p>
    <p>M M M M M</p>
    <p>P P P P P</p>
    <p>P P P P P P</p>
    <p>M M M M M M</p>
  </div>
  <div class="page">
    <p>What is the New Memory System?</p>
    <p>M M M M M M</p>
    <p>P P P P P P</p>
    <p>P P P P P P</p>
    <p>M M M M M M</p>
    <p>Data ArrayTags</p>
    <p>Cache Controller DMA Engine</p>
    <p>Local Storage</p>
    <p>CacheCache--based Memorybased Memory Streaming MemoryStreaming Memory</p>
  </div>
  <div class="page">
    <p>The Role of Local Memory</p>
    <p>Exploit spatial &amp; temporal locality Reduce average memory access time</p>
    <p>Enable data re-use Amortize latency over several accesses</p>
    <p>Minimize off-chip bandwidth Keep useful data local</p>
    <p>Data ArrayTags</p>
    <p>Cache Controller DMA Engine</p>
    <p>Local Storage</p>
    <p>CacheCache--based Memorybased Memory Streaming MemoryStreaming Memory</p>
  </div>
  <div class="page">
    <p>Who Manages Local Memory?</p>
    <p>Cache-based Streaming</p>
    <p>Locality Data Fetch Reactive Proactive Placement Limited mapping Arbitrary Replacement Fixed-policy Arbitrary Granularity Cache block Arbitrary</p>
    <p>Communication Coherence Hardware Software</p>
    <p>Cache-based: Hardware-managed Streaming: Software-managed</p>
  </div>
  <div class="page">
    <p>Potential Advantages of Streaming Memory</p>
    <p>Better latency hiding Overlap DMA transfers with computation Double buffering is macroscopic prefetching</p>
    <p>Lower off-chip bandwidth requirements Avoid conflict misses Avoid superfluous refills for output data Avoid write-back of dead data Avoid fetching whole lines for sparse accesses</p>
    <p>Better energy and area efficiency No tag &amp; associativity overhead Fewer off-chip accesses</p>
  </div>
  <div class="page">
    <p>How Much Advantage over Caching?</p>
    <p>How do they differ in Performance?</p>
    <p>How do they differ in Scaling?</p>
    <p>How do they differ in Energy Efficiency?</p>
    <p>How do they differ in Programmability?</p>
  </div>
  <div class="page">
    <p>Our Contribution: A Head to Head Comparison</p>
    <p>CacheCache--based Memorybased Memory vs.vs.</p>
    <p>Streaming MemoryStreaming Memory</p>
    <p>Unified set of constraints Same processor core Same capacity of local storage per core Same on-chip interconnect Same off-chip memory channel</p>
    <p>Justification VLSI constraints (e.g., local storage capacity) No fundamental differences (e.g., core type)</p>
  </div>
  <div class="page">
    <p>Our Conclusions Caching performs &amp; scales as well as Streaming</p>
    <p>Well-known cache enhancements eliminate differences</p>
    <p>Stream Programming benefits Caching Memory Enhances locality patterns Improves bandwidth and efficiency of caches</p>
    <p>Stream Programming easier with Caches Makes memory system amenable to irregular &amp; unpredictable workloads</p>
    <p>Streaming Memory likely to be replaced or at least augmented by Caching Memory</p>
  </div>
  <div class="page">
    <p>Simulation Parameters 1  16 cores: Tensilica LX, 3-way VLIW, 2 FPUs</p>
    <p>Clock frequency: 800 MHz  3.2 GHz</p>
    <p>On-chip data memory Cache-based: 32kB cache, 32B block, 2-way, MESI Streaming: 24kB scratch pad</p>
    <p>DMA engine 8kB cache, 32B block, 2-way</p>
    <p>Both: 512kB L2 cache, 32B block, 16-way</p>
    <p>System Hierarchical on-chip interconnect Simple main memory model (3.2 GB/s  12.8 GB/s)</p>
  </div>
  <div class="page">
    <p>Benchmark Applications No SPEC Streaming</p>
    <p>Few available apps with streaming &amp; caching versions</p>
    <p>Selected 10 streaming applications Some used to motivate or evaluate Streaming Memory</p>
    <p>Co-developed apps for both systems Caching: C, threads Streaming: C, threads, DMA library</p>
    <p>Optimized both versions as best we could</p>
  </div>
  <div class="page">
    <p>Benchmark Applications Video processing</p>
    <p>Stereo Depth Extraction H.264 Encoding MPEG-2 Encoding</p>
    <p>Image processing JPEG Encode/Decode KD-tree Raytracer 179.art</p>
    <p>Scientific and data-intensive 2D Finite Element Method 1D Finite Impulse Response Merge Sort Bitonic Sort</p>
    <p>Irregular</p>
    <p>Unpredictable</p>
  </div>
  <div class="page">
    <p>Our Conclusions Caching performs &amp; scales as well as Streaming</p>
    <p>Well-known cache enhancements eliminate differences</p>
    <p>Stream Programming benefits Caching Memory Enhances locality patterns Improves bandwidth and efficiency of caches</p>
    <p>Stream Programming easier with Caches Makes memory system amenable to irregular &amp; unpredictable workloads</p>
    <p>Streaming Memory likely to be replaced or at least augmented by Caching Memory</p>
  </div>
  <div class="page">
    <p>Parallelism Independent of Memory System</p>
    <p>MPEG-2 Encoder @ 3.2 GHz</p>
    <p>N o</p>
    <p>rm al</p>
    <p>iz ed</p>
    <p>T im</p>
    <p>e</p>
    <p>Cache Streaming</p>
    <p>FEM @ 3.2 GHz</p>
    <p>N o</p>
    <p>rm al</p>
    <p>iz ed</p>
    <p>T im</p>
    <p>e</p>
    <p>Cache Streaming</p>
  </div>
  <div class="page">
    <p>Local Memory Not Critical For Compute-Intensive Applications</p>
    <p>Cache Stream Cache Stream</p>
    <p>MPEG-2 FEM</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>T im</p>
    <p>e</p>
    <p>Useful Data Sync</p>
    <p>Intuition Apps limited by compute Good data reuse, even with large datasets Low misses/instruction</p>
    <p>Note: Sync includes Barriers and DMA wait</p>
  </div>
  <div class="page">
    <p>Double-Buffering Hides Latency For Streaming Memory Systems</p>
    <p>Intuition Non-local accesses entirely overlapped with computation DMAs perform efficient SW prefetching</p>
    <p>Note The case for memoryintensive apps not bound by memory BW</p>
    <p>Cache Stream</p>
    <p>FIR</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>T im</p>
    <p>e</p>
    <p>Useful Data Sync</p>
  </div>
  <div class="page">
    <p>Prefetching Hides Latency For Cache-Based Memory Systems</p>
    <p>Cache Prefetch Stream</p>
    <p>FIR</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>T im</p>
    <p>e</p>
    <p>Useful Data Sync</p>
    <p>Intuition HW stream prefetcher overlaps misses with computation as well Predictable &amp; regular access patterns</p>
  </div>
  <div class="page">
    <p>Streaming Memory Often Incurs Less Off-Chip Traffic</p>
    <p>The case for apps with large output streams Avoids superfluous refills for output streams Not the case for write-allocate, fetch-on-miss caches</p>
    <p>Cac Str Cac Str Cac Str</p>
    <p>FIR Merge Sort MPEG-2</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>O ff</p>
    <p>-c hi</p>
    <p>p Tr</p>
    <p>af fic</p>
    <p>Write Read</p>
  </div>
  <div class="page">
    <p>SW-Guided Cache Policies Improve Bandwidth Efficiency</p>
    <p>Our system: Prepare For Store cache hint Allocates cache line but avoid refill of old data</p>
    <p>Xbox360: write-buffer for non allocating writes</p>
    <p>Cac PFS Str Cac PFS Str Cac PFS Str</p>
    <p>FIR Merge Sort MPEG-2</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>O ff</p>
    <p>-c hi</p>
    <p>p Tr</p>
    <p>af fic</p>
    <p>Write Read</p>
  </div>
  <div class="page">
    <p>Energy Efficiency Does not Depend on Local Memory</p>
    <p>Intuition Energy dominated by DRAM accesses and processor core Local store ~2x energyefficiency of cache, but small portion of total energy</p>
    <p>Note The case for computeintensive applications</p>
    <p>C ac</p>
    <p>he</p>
    <p>S tr</p>
    <p>ea m</p>
    <p>C ac</p>
    <p>he</p>
    <p>S tr</p>
    <p>ea m</p>
    <p>C ac</p>
    <p>he</p>
    <p>S tr</p>
    <p>ea m</p>
    <p>MPEG-2 FEM FIR</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E ne</p>
    <p>rg y</p>
    <p>DRAM L2-cache L-store D-cache I-cache Core</p>
  </div>
  <div class="page">
    <p>Optimized Bandwidth Yields Optimized Energy Efficiency</p>
    <p>Superfluous off-chip accesses are expensive! Streaming &amp; SW-guided caching reduce them</p>
    <p>Cache PFS Stream</p>
    <p>FIR</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>E ne</p>
    <p>rg y</p>
    <p>DRAM L2-cache L-store D-cache I-cache Core</p>
    <p>Cache PFS Stream</p>
    <p>FIR</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>O ff</p>
    <p>-c hi</p>
    <p>p Tr</p>
    <p>af fic</p>
    <p>Write Read</p>
  </div>
  <div class="page">
    <p>Our Conclusions Caching performs &amp; scales as well as Streaming</p>
    <p>Well-known cache enhancements eliminate differences</p>
    <p>Stream Programming benefits Caching Memory Enhances locality patterns Improves bandwidth and efficiency of caches</p>
    <p>Stream Programming easier with Caches Makes memory system amenable to irregular &amp; unpredictable workloads</p>
    <p>Streaming Memory likely to be replaced or at least augmented by Caching Memory</p>
  </div>
  <div class="page">
    <p>Stream Programming for Caches: MPEG-2 Example</p>
    <p>MPEG-2 example P() generates a video frame later consumed by T() Whole frame is too large to fit in local memory No temporal locality</p>
    <p>Opportunity Computation on frame blocks are independent</p>
    <p>P PredictedVideo Frame T</p>
  </div>
  <div class="page">
    <p>Stream Programming for Caches: MPEG-2 Example</p>
    <p>Introducing temporal locality Loop fusion for P() and T() at block level Intermediate data are dead once T() done</p>
    <p>P Predicted</p>
    <p>Video Frame T</p>
    <p>Predicted block</p>
  </div>
  <div class="page">
    <p>Stream Programming for Caches: MPEG-2 Example</p>
    <p>Exploiting producer-consumer locality Re-use the predicted block buffer Dynamic working set reduced Fits in local memory; no off-chip traffic</p>
    <p>P T</p>
    <p>Predicted block</p>
  </div>
  <div class="page">
    <p>Stream Programming for Caches: MPEG-2 Example</p>
    <p>Stream programming beneficial for any Memory System</p>
    <p>Exposes locality that improves bandwidth and energy efficiency of local memory</p>
    <p>Stream programming toolchains helpful</p>
    <p>Un op</p>
    <p>tim ize</p>
    <p>d</p>
    <p>Op tim</p>
    <p>ize d</p>
    <p>St re</p>
    <p>am</p>
    <p>MPEG-2</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>O ff</p>
    <p>-c hi</p>
    <p>p Tr</p>
    <p>af fic</p>
    <p>W rite Read</p>
  </div>
  <div class="page">
    <p>Our Conclusions Caching performs &amp; scales as well as Streaming</p>
    <p>Well-known cache enhancements eliminate differences</p>
    <p>Stream Programming benefits Caching Memory Enhances locality patterns Improves bandwidth and efficiency of caches</p>
    <p>Stream Programming easier with Caches Makes memory system amenable to irregular &amp; unpredictable workloads</p>
    <p>Streaming Memory likely to be replaced or at least augmented by Caching Memory</p>
  </div>
  <div class="page">
    <p>Stream Programming is Easier with Caches</p>
    <p>Stream programming necessary for correctness on Streaming Memory</p>
    <p>Must refactor all dataflow</p>
    <p>Caches can use Stream programming for performance, not correctness</p>
    <p>Incremental tuning Doesnt require up-front holistic analysis</p>
    <p>Why is this important? Many streaming apps include some unpredictable patterns</p>
  </div>
  <div class="page">
    <p>Specific Examples</p>
    <p>Raytracing Unpredictable tree accesses Software caching on Cell (Benthin 06)</p>
    <p>Emulation overhead, DMA latency for refills</p>
    <p>Tree accesses have good locality on HW caches</p>
  </div>
  <div class="page">
    <p>Our Conclusions Caching performs &amp; scales as well as Streaming</p>
    <p>Well-known cache enhancements eliminate differences</p>
    <p>Stream Programming benefits Caching Memory Enhances locality patterns Improves bandwidth and efficiency of caches</p>
    <p>Stream Programming easier with Caches Makes memory system amenable to irregular &amp; unpredictable workloads</p>
    <p>Streaming Memory likely to be replaced or at least augmented by Caching Memory</p>
  </div>
  <div class="page">
    <p>Limitations of This Work</p>
    <p>Did not scale beyond 16 cores Does cache coherence scale?</p>
    <p>Application scope May not generalize to other domains General-purpose != application-specific</p>
    <p>Sensitivity to local storage capacity Intractable without language/compiler support</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Scale beyond 16 cores Exploit streaming SW to assist HW coherence</p>
    <p>Extend application scope Generalize to other domains Consider further optimizations</p>
    <p>Study sensitivity to local storage capacity Introduce language/compiler support</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>Streaming Memory and L2 Caches</p>
    <p>L2 caches mitigate overfetch in Streaming apps</p>
    <p>Unstructured meshes Motion estimation</p>
    <p>search window reference frames</p>
    <p>Motion Estimation</p>
    <p>Cache Stream Cache Stream</p>
    <p>FEM MPEG-2</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>O ff</p>
    <p>-c hi</p>
    <p>p Tr</p>
    <p>af fic</p>
    <p>Write Read</p>
  </div>
  <div class="page">
    <p>Streaming Memory Occasionally Consumes More Bandwidth</p>
    <p>The problem Data-dependent write pattern</p>
    <p>Caching Automatically track modified state Write back only dirty data</p>
    <p>Streaming Writes back everything Programming burden &amp; overhead to track modified state</p>
    <p>Bitonic Sort</p>
    <p>Cache Stream</p>
    <p>N o</p>
    <p>rm al</p>
    <p>iz ed</p>
    <p>O ff</p>
    <p>-c h</p>
    <p>ip T</p>
    <p>ra ff</p>
    <p>ic</p>
    <p>Write Read</p>
  </div>
</Presentation>
