<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Statistical Phrase-Based Translation</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu</p>
    <p>koehn@isi.edu, och@isi.edu, marcu@isi.edu</p>
    <p>Information Sciences Institute</p>
    <p>University of Southern California</p>
    <p>p.1</p>
  </div>
  <div class="page">
    <p>Motivation p</p>
    <p>Phrase-based translation is the best way to do statistical machine translation</p>
    <p>best performance in recent DARPA evaluations</p>
    <p>also fairly simple</p>
    <p>tools are freely available</p>
    <p>How do I construct a phrase translation table?</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 2  p.2</p>
  </div>
  <div class="page">
    <p>Goals p</p>
    <p>Compare different approaches to learn phrases</p>
    <p>Examine properties of phrase-based translation</p>
    <p>Syntax and phrases</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 3  p.3</p>
  </div>
  <div class="page">
    <p>Overview p</p>
    <p>Evaluation framework</p>
    <p>unified model</p>
    <p>decoder</p>
    <p>corpus</p>
    <p>Three methods for learning phrases</p>
    <p>word-alignment induced phrases</p>
    <p>syntactic phrases</p>
    <p>phrase-alignment</p>
    <p>Experiments</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 4  p.4</p>
  </div>
  <div class="page">
    <p>Model p</p>
    <p>Morgen fliege ich nach Kanada zur Konferenz</p>
    <p>Tomorrow I will fly to the conference in Canada</p>
    <p>Bayes rule: argmax       argmax</p>
    <p>Foreign sentence</p>
    <p>is segmented into</p>
    <p>phrases</p>
    <p>Each phrase is translated with</p>
    <p>Phrases are reordered with</p>
    <p>Use of language model</p>
    <p>LM</p>
    <p>and word penalty</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 5  p.5</p>
  </div>
  <div class="page">
    <p>Decoder: Beam Search p</p>
    <p>e: Mary f: *-------p: .534</p>
    <p>e: witch f: -------*p: .182</p>
    <p>e: f: ---------p: 1</p>
    <p>e: ... did f: *-------p: .122</p>
    <p>e: ... slap f: *-***---p: .043</p>
    <p>Build English by hypothesis expansion</p>
    <p>from left to right</p>
    <p>search space exponential with sentence length</p>
    <p>reduction by pruning weak hypothesis aided by future cost estimate</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 6  p.6</p>
  </div>
  <div class="page">
    <p>Evaluation on Europarl Corpus p</p>
    <p>Collected from the European Parliament Proceedings</p>
    <p>Available at http://www.isi.edu/ koehn/</p>
    <p>11 languages, 20 million words each</p>
    <p>Test set</p>
    <p>German-English</p>
    <p>1755 sentence of length 5-15</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 7  p.7</p>
  </div>
  <div class="page">
    <p>Three Methods for Learning Phrases p</p>
    <p>Word-alignment induced phrases</p>
    <p>similar to alignment templates [Och et al., 1999]</p>
    <p>Syntactic phrases</p>
    <p>only syntactic phrases are learned</p>
    <p>same restriction as in recently proposed syntactic transfer models</p>
    <p>Phrase-alignment</p>
    <p>joint model [Marcu and Wong, 2002]</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 8  p.8</p>
  </div>
  <div class="page">
    <p>Word Alignment Induced Phrases p</p>
    <p>Word alignment is generated using IBM Model 4</p>
    <p>bidirectional alignments e</p>
    <p>f, f</p>
    <p>e</p>
    <p>intersect alignments</p>
    <p>grow additional alignment points with heuristics</p>
    <p>Collect phrase pairs consistent with word alignment</p>
    <p>This is alignment templates without word classes</p>
    <p>[Och et al., 1999]</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 9  p.9</p>
  </div>
  <div class="page">
    <p>Word Alignment Induced Phrases (2) p Maria no daba una</p>
    <p>bofetada a la</p>
    <p>bruja verde</p>
    <p>Mary</p>
    <p>witch</p>
    <p>green</p>
    <p>the</p>
    <p>slap</p>
    <p>not</p>
    <p>did</p>
    <p>(Maria, Mary), (no, did not), (slap, daba una bofetada), (a la, the), (bruja, witch),</p>
    <p>(verde, green), (Maria no, Mary did not), (no daba una bofetada, did not slap),</p>
    <p>(daba una bofetada a la, slap the), (bruja verde, green witch),</p>
    <p>(Maria no daba una bofetada, Mary did not slap),</p>
    <p>(no daba una bofetada a la, did not slap the), (a la bruja verde, the green witch),</p>
    <p>(Maria no daba una bofetada a la, Mary did not slap the),</p>
    <p>(daba una bofetada a la bruja verde, slap the green witch),</p>
    <p>(no daba una bofetada a la bruja verde, did not slap the green witch),</p>
    <p>(Maria no daba una bofetada a la bruja verde, Mary did not slap the green witch)</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 10  p.10</p>
  </div>
  <div class="page">
    <p>Syntactic Phrases p</p>
    <p>Syntactic phrases span whole constituents in parse tree</p>
    <p>Motivation</p>
    <p>only these phrases used syntactic transfer models,</p>
    <p>e.g., [Yamada and Knight, 2002]</p>
    <p>does syntax help or hurt?</p>
    <p>Extract syntactic phrase pairs</p>
    <p>parse both sides (with statistical parsers)</p>
    <p>use word alignment as before</p>
    <p>limit to phrases to syntactic constituents in parse tree</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 11  p.11</p>
  </div>
  <div class="page">
    <p>Phrase Alignment p</p>
    <p>Morgen fliege ich nach Kanada zur Konferenz</p>
    <p>Tomorrow I will fly to the conference in Canada</p>
    <p>Direct Phrase Alignment of Parallel Corpus</p>
    <p>[Marcu and Wong, 2002]</p>
    <p>Generative Story</p>
    <p>a number of concepts are created</p>
    <p>each concept generates a foreign and English phrase</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 12  p.12</p>
  </div>
  <div class="page">
    <p>Experiments p</p>
    <p>Comparison of core methods</p>
    <p>Maximum phrase length</p>
    <p>Lexical weighting</p>
    <p>Phrase extraction heuristics</p>
    <p>Simpler word alignment models</p>
    <p>Other language pairs</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 13  p.13</p>
  </div>
  <div class="page">
    <p>Comparison of Core Methods p</p>
    <p>Same decoder, same training data, same language model</p>
    <p>except for IBM Model 4: uses greedy decoder [Germann et al., 2001]</p>
    <p>WAIPh best, syntactic phrases very bad</p>
    <p>Training Corpus Size</p>
    <p>BLEU</p>
    <p>WAIPh</p>
    <p>Joint</p>
    <p>Syn</p>
    <p>M4</p>
    <p>All following experiments on WAIPh only</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 14  p.14</p>
  </div>
  <div class="page">
    <p>Maximum Phrase Length p</p>
    <p>Maximum limit on length of phrases</p>
    <p>higher limit</p>
    <p>larger phrase translation table</p>
    <p>all tables still fit into memory of modern machines</p>
    <p>Max. Training corpus size</p>
    <p>Length 10k 20k 40k 80k 160k 320k</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 15  p.15</p>
  </div>
  <div class="page">
    <p>Maximum Phrase Length (2) p</p>
    <p>Impact of limit on translation quality</p>
    <p>not much improvement if maximum length is extended beyond 3</p>
    <p>independent of training corpus size</p>
    <p>Training Corpus Size</p>
    <p>BLEU</p>
    <p>max2 max3 max4 max5 max7</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 16  p.16</p>
  </div>
  <div class="page">
    <p>Lexical Weighting p</p>
    <p>Augment phrase translation probability</p>
    <p>with lexical</p>
    <p>translation probabilities</p>
    <p>la bruja verde</p>
    <p>the ### --- --</p>
    <p>green --- --- ###</p>
    <p>witch --- ### --</p>
    <p>Lexical weight:</p>
    <p>la</p>
    <p>the</p>
    <p>bruja</p>
    <p>witch</p>
    <p>verde</p>
    <p>green</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 17  p.17</p>
  </div>
  <div class="page">
    <p>Lexical Weighting p</p>
    <p>Improves translation quality</p>
    <p>Training Corpus Size</p>
    <p>BLEU</p>
    <p>no-lex lex</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 18  p.18</p>
  </div>
  <div class="page">
    <p>Phrase Extraction Heuristics p</p>
    <p>Recall: word alignment based on intersection of</p>
    <p>bidirectional IBM Model 4 alignments + heuristics</p>
    <p>Maria no daba una bofetada</p>
    <p>a la bruja</p>
    <p>verde</p>
    <p>Mary</p>
    <p>witch</p>
    <p>green</p>
    <p>the</p>
    <p>slap</p>
    <p>not</p>
    <p>did</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 19  p.19</p>
  </div>
  <div class="page">
    <p>Phrase Extraction Heuristics (2) p</p>
    <p>Different phrases are learned, if heuristic to create word</p>
    <p>alignment is changed.</p>
    <p>Variations in heuristics:</p>
    <p>only to directly neighboring</p>
    <p>also to diagonally neighboring</p>
    <p>also to non-neighboring</p>
    <p>prefer English-foreign or foreign-to-English</p>
    <p>use lexical probabilities or frequencies</p>
    <p>extend only to unaligned words</p>
    <p>...</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 20  p.20</p>
  </div>
  <div class="page">
    <p>Phrase Extraction Heuristics (3) p</p>
    <p>No clear advantage to any strategy</p>
    <p>large differences, but ...</p>
    <p>... depending on corpus size</p>
    <p>... depending on language pair</p>
    <p>Training Corpus Size</p>
    <p>BLEU</p>
    <p>diag-and diag base e2f f2e union</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 21  p.21</p>
  </div>
  <div class="page">
    <p>Simpler Word Alignment Models p</p>
    <p>Using simpler IBM Models for word alignment</p>
    <p>not much impact, if simpler models used</p>
    <p>simpler models computationally much cheaper</p>
    <p>Training Corpus Size</p>
    <p>BLEU</p>
    <p>m4 m3 m2 m1</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 22  p.22</p>
  </div>
  <div class="page">
    <p>Other Language Pairs p</p>
    <p>Finding hold for other language pairs, other corpora</p>
    <p>Phrase translation better than IBM Model 4</p>
    <p>Lexicalization helps (about +0.01 BLEU)</p>
    <p>Language Pair Model4 Phrase Lex</p>
    <p>English-German 0.2040 0.2361 0.2449</p>
    <p>French-English 0.2787 0.3294 0.3389</p>
    <p>English-French 0.2555 0.3145 0.3247</p>
    <p>Finnish-English 0.2178 0.2742 0.2806</p>
    <p>Swedish-English 0.3137 0.3459 0.3554</p>
    <p>Chinese-English 0.1190 0.1395 0.1418</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 23  p.23</p>
  </div>
  <div class="page">
    <p>Conclusions p</p>
    <p>Phrase-based translation better than word-based</p>
    <p>translation</p>
    <p>Limit to syntactic phrases hurts a lot</p>
    <p>Small phrases (up to 3 words) good enough</p>
    <p>Lexical weighting helpful</p>
    <p>Phrase extraction heuristics matter, but best heuristics</p>
    <p>vary on corpus size, language pair</p>
    <p>Philipp Koehn, Franz Och, Daniel Marcu  USC/ISI 24  p.24</p>
  </div>
</Presentation>
