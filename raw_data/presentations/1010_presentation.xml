<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-Hypervisor Virtual Machines: Enabling An Ecosystem of Hypervisor-level Services</p>
    <p>Kartik Gopalan, Rohith Kugve, Hardik Bagdi, Yaohui Hu  Binghamton University Dan Williams, Nilton Bila  IBM T.J. Watson Research Center</p>
    <p>Funded by NSF</p>
    <p>Dan Williams, Yaohui Hu, Umesh Deshpande, Piush K Sinha, Nilton Bila, Kartik Gopalan, Hani Jamjoom</p>
    <p>IBM T.J. Watson Research Center Binghamton University IBM Almaden Research Center</p>
    <p>Funded in part by the NSF</p>
    <p>Enabling Efficient Hypervisor-as-a-Service Clouds with Ephemeral Virtualization</p>
  </div>
  <div class="page">
    <p>Hypervisors</p>
    <p>A thin and secure layer in the cloud -- or -</p>
    <p>Guest 1</p>
    <p>Hypervisor</p>
  </div>
  <div class="page">
    <p>Hypervisors</p>
    <p>A thin and secure layer in the cloud -- or -</p>
    <p>Feature-filled cloud differentiators  Migration  Checkpointing  High availability  Live Guest Patching  Network monitoring  Intrusion detection  Other VMI</p>
    <p>Guest 1</p>
    <p>Hypervisor</p>
    <p>Service A</p>
    <p>Service B</p>
    <p>Service C</p>
  </div>
  <div class="page">
    <p>Lots of third-party interest in hypervisor-level services</p>
    <p>Ravello  Bromium  XenBlanket  McCafe DeepDefender  Secvisor  Cloudvisor  And more</p>
    <p>But limited support for third party services from base hypervisor.</p>
  </div>
  <div class="page">
    <p>How can a guest use multiple third-party hypervisor-level services?</p>
    <p>Our Solution: Span virtualization</p>
    <p>One guest controlled by multiple coresident hypervisors.</p>
    <p>Hypervisor</p>
    <p>Guest 1</p>
    <p>L0</p>
    <p>L1s Service</p>
    <p>A</p>
    <p>Guest 2</p>
    <p>Service B</p>
    <p>Service C</p>
    <p>Service D</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Why multi-hypervisor virtual machines?  Design of Span Virtualization  Evaluations  Related Work  Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Hypervisor</p>
    <p>Option 1: Fat hypervisor</p>
    <p>All services run at the most privileged level.</p>
    <p>Buthypervisor cannot trust thirdparty services in privileged mode.</p>
    <p>Guest 1 Guest 2</p>
    <p>Service A</p>
    <p>Service B</p>
    <p>Service C</p>
  </div>
  <div class="page">
    <p>Option 2: Native user space services</p>
    <p>Services run natively in the user space of the hypervisor</p>
    <p>Services control guest indirectly via the hypervisor</p>
    <p>E.g. QEMU with KVM, uDenali</p>
    <p>ButPotentially large user-kernel interface  event interposition and system calls</p>
    <p>Guest 1</p>
    <p>Hypervisor</p>
    <p>Service A</p>
    <p>Service B</p>
    <p>Service C</p>
    <p>Hypervisor User space</p>
    <p>Cloud providers reluctant to run thirdparty services natively, even if in user space.</p>
  </div>
  <div class="page">
    <p>Option 3: Service VMs</p>
    <p>Run services inside deprivileged VMs</p>
    <p>Services control guest indirectly via hypercalls and events</p>
    <p>Single trusted Service VM  Runs all services  E.g. Domain0 in Xen</p>
    <p>-- or -</p>
    <p>Guest 1</p>
    <p>Hypervisor</p>
    <p>Service A</p>
    <p>Service B</p>
    <p>Service C</p>
    <p>Service VM</p>
  </div>
  <div class="page">
    <p>Option 3: Service VMs</p>
    <p>Run services inside deprivileged VMs</p>
    <p>Services control guest indirectly via hypercalls and events</p>
    <p>Multiple service VMs  One per service  Deprivileged and restartable  E.g. Service Domains in Xoar</p>
    <p>Guest 1</p>
    <p>Hypervisor</p>
    <p>Service A</p>
    <p>Service B</p>
    <p>Service C</p>
    <p>Service VMs</p>
  </div>
  <div class="page">
    <p>Option 3: Service VMs</p>
    <p>Run services inside deprivileged VMs</p>
    <p>Services control guest indirectly via hypercalls and events</p>
    <p>Multiple service VMs  One per service  Deprivileged and restartable  E.g. Service Domains in Xoar</p>
    <p>Guest 1</p>
    <p>Hypervisor</p>
    <p>Service A</p>
    <p>Service B</p>
    <p>Service C</p>
    <p>Service VMs</p>
    <p>Lack direct control over ISA-level guest state  Memory mappings, VCPU scheduling, port-mapped I/O, etc.</p>
  </div>
  <div class="page">
    <p>Hypervisor</p>
    <p>Option 4: Nested Virtualization</p>
    <p>Services run in a deprivileged L1 hypervisor, which runs on L0.</p>
    <p>Services control guest at virtualized ISA level.</p>
    <p>But  multiple services must reside in the same L1, i.e. fat L1.</p>
    <p>Vertically Stack L1 hypervisors?  More than two levels of nesting is inefficient.</p>
    <p>A B C D</p>
    <p>L0</p>
    <p>L1</p>
    <p>Guest 1 Guest 2</p>
  </div>
  <div class="page">
    <p>Hypervisor</p>
    <p>Our solution: Span Virtualization</p>
    <p>Allow multiple coresident L1s to concurrently control a common guest  i.e. Horizontal layering of L1 hypervisors</p>
    <p>Guest is a multi-hypervisor virtual machine</p>
    <p>Each L1  Offers guest services that augment L0s services.  Controls one or more guest resources</p>
    <p>Guest 1</p>
    <p>L0</p>
    <p>L1s A B C D</p>
    <p>Guest 2</p>
  </div>
  <div class="page">
    <p>Design Goals of Span Virtualization</p>
    <p>Guest Transparency  Guest remains unmodified</p>
    <p>Service Isolation  L1s controlling the same guest are unaware of each other.</p>
    <p>Hypervisor</p>
    <p>Guest 1</p>
    <p>L0</p>
    <p>L1s A B C D</p>
    <p>Guest 2</p>
  </div>
  <div class="page">
    <p>Guest Control operations</p>
    <p>Operations:  Attach an L1 to a specified guest resource</p>
    <p>Detach an L1 from a guest resource</p>
    <p>Subscribe an attached L1 to receive guest events (currently memory events)</p>
    <p>Unsubscribe an L1 from a subscribed guest event</p>
    <p>L0 supervises which L1 controls which Guest resource  Memory, VCPU and I/O</p>
    <p>L0 and L1s communicate via Traps/Faults (implicit) and Messages (explicit)</p>
    <p>L0 Hypervisor</p>
    <p>Virtual Guest EPT Virtual Guest EPT</p>
    <p>L1 Hypervisor(s)</p>
    <p>Span Guest (unmodified)</p>
    <p>Message Channel L1 Traps Guest Faults</p>
  </div>
  <div class="page">
    <p>Control over Guest Resources</p>
    <p>Guest Memory  Shared: All hypervisors have the same consistent view of guest memory</p>
    <p>Guest VCPUs  Exclusive: All guest VCPUs are controlled by one hypervisor at any instant</p>
    <p>Guest I/O devices  Exclusive: Different virtual I/O devices of a guest may be controlled by different hypervisors</p>
    <p>Control Transfer  Control over guest VCPUs and I/O devices can be transferred from one L1 to another via L0.</p>
  </div>
  <div class="page">
    <p>Memory Translation Isolation and Communication: Another design goal</p>
    <p>is to compartmentalize L1 services, from each other and from L0. First, L1s must have lower execution privilege compared to L0. Secondly, L1s must remain isolated from each other. These two goals are achieved by deprivileging L1s using nested virtualization and executing them as separate guests on L0. Finally, L1s must remain unaware of each other during execution. This goal is achieved by requiring L1s to receive any subscribed guest events that are generated on other L1s only via L0.</p>
    <p>There are two ways that L0 communicates with L1s: implicitly via traps and explicitly via messages. Traps allow L0 to transparently intercept certain memory management operations by L1 on the guest. Explicit messages allow an L1 to directly request guest control from L0. An Event Processing module in L0 traps runtime updates to guest memory mappings by any L1 and synchronizes guest mappings across different L1s. The event processing module also relays guest memory faults that need to be handled by L1. A bidirectional Message Channel relays explicit messages between L0 and L1s including attach/detach requests, memory event subscription/notification, guest I/O requests, and virtual interrupts. Some explicit messages, such as guest I/O requests and virtual interrupts, could be replaced with implicit traps. Our choice of which to use is largely based on ease of implementation on a case-by-case basis.</p>
    <p>Continuous vs. Transient Control: Span virtualization allows L1s control over guest resources to be either continuous or transient. Continuous control means that an L1 exerts uninterrupted control over one or more guest resources for an extended period of time. For example, an intrusion detection service in L1 that must monitor guest system calls, VM exits, or network traffic, would require continuous control of guest memory, VCPUs, and network device. Transient control means that an L1 acquires full control over guest resources for a brief duration, provides a short service to the guest, and releases guest control back to L0. For instance, an L1 that periodically checkpoints the guest would need transient control of guest memory, VCPUs, and I/O devices.</p>
    <p>A Span VM has a single guest physical address space which is mapped into the address space of all attached L1s. Thus any memory write on a guest page is immediately visible to all hypervisors controlling the guest. Note that all L1s have the same visibility into the guest memory due to the horizontal layering of Span virtualization, unlike the vertical stacking of nested virtualization, which somewhat obscures the guest to lower layers.</p>
    <p>EPTGuest</p>
    <p>Page Table EPTVA GPA HPA</p>
    <p>Page Table</p>
    <p>VA GPA L1PA</p>
    <p>Shadow EPT Shadow EPT</p>
    <p>Shadow EPT</p>
    <p>Virtual EPTVirtual EPTVirtual EPT</p>
    <p>Virtual EPTVirtual EPTEPTL1</p>
    <p>Page Table</p>
    <p>Virtual EPTVA GPA</p>
    <p>L1PA EPTL1 HPA</p>
    <p>Shadow EPT</p>
    <p>(a) Single-level</p>
    <p>(b) Nested</p>
    <p>(c) Span</p>
    <p>HPA</p>
    <p>Figure 3: Memory translation for single-level, nested, and Span VMs. VA = Virtual Address; GPA = Guest Physical Address; L1PA = L1 Physical Address; HPA = Host Physical Address.</p>
    <p>Single-level virtualization: Figure 3(a) shows that for single-level virtualization, the guest page tables map virtual addresses to guest physical addresses (VA to GPA in the figure). The hypervisor uses an EPT to map guest physical addresses to host physical addresses (GPA to HPA). Guest memory permissions are controlled by the combination of permissions in guest page table and EPT.</p>
    <p>Whenever the guest attempts to access a page that is either not present or protected in the EPT, the hardware generates an EPT fault and traps into the hypervisor, which handles the fault by mapping a new page, emulating an instruction, or taking other actions. On the other hand, the hypervisor grants complete control over the traditional paging hardware to the guest. A guest OS is free to maintain the mappings between its virtual and guest physical address space and update them as it sees fit, without trapping into the hypervisor.</p>
    <p>Nested virtualization: Figure 3(b) shows that for nested virtualization, the guest is similarly granted control over the traditional paging hardware to map virtual addresses to its guest physical address space. L1 maintains a Virtual EPT to map the guest pages to pages in L1s physical addresses space, or L1 pages. Finally, one more translation is required: L0 maintains EPTL1 to map L1 pages to physical pages. However, x86 processors can translate only two levels of addresses in hardware, from guest virtual to guest physical to host physical address. Hence the Virtual EPT maintained by L1 needs to be shadowed by L0, meaning that the Virtual EPT and EPTL1 must be compacted by L0 during runtime into a</p>
    <p>Single-Level Virtualization</p>
  </div>
  <div class="page">
    <p>Memory Translation Isolation and Communication: Another design goal</p>
    <p>is to compartmentalize L1 services, from each other and from L0. First, L1s must have lower execution privilege compared to L0. Secondly, L1s must remain isolated from each other. These two goals are achieved by deprivileging L1s using nested virtualization and executing them as separate guests on L0. Finally, L1s must remain unaware of each other during execution. This goal is achieved by requiring L1s to receive any subscribed guest events that are generated on other L1s only via L0.</p>
    <p>There are two ways that L0 communicates with L1s: implicitly via traps and explicitly via messages. Traps allow L0 to transparently intercept certain memory management operations by L1 on the guest. Explicit messages allow an L1 to directly request guest control from L0. An Event Processing module in L0 traps runtime updates to guest memory mappings by any L1 and synchronizes guest mappings across different L1s. The event processing module also relays guest memory faults that need to be handled by L1. A bidirectional Message Channel relays explicit messages between L0 and L1s including attach/detach requests, memory event subscription/notification, guest I/O requests, and virtual interrupts. Some explicit messages, such as guest I/O requests and virtual interrupts, could be replaced with implicit traps. Our choice of which to use is largely based on ease of implementation on a case-by-case basis.</p>
    <p>Continuous vs. Transient Control: Span virtualization allows L1s control over guest resources to be either continuous or transient. Continuous control means that an L1 exerts uninterrupted control over one or more guest resources for an extended period of time. For example, an intrusion detection service in L1 that must monitor guest system calls, VM exits, or network traffic, would require continuous control of guest memory, VCPUs, and network device. Transient control means that an L1 acquires full control over guest resources for a brief duration, provides a short service to the guest, and releases guest control back to L0. For instance, an L1 that periodically checkpoints the guest would need transient control of guest memory, VCPUs, and I/O devices.</p>
    <p>A Span VM has a single guest physical address space which is mapped into the address space of all attached L1s. Thus any memory write on a guest page is immediately visible to all hypervisors controlling the guest. Note that all L1s have the same visibility into the guest memory due to the horizontal layering of Span virtualization, unlike the vertical stacking of nested virtualization, which somewhat obscures the guest to lower layers.</p>
    <p>EPTGuest</p>
    <p>Page Table EPTVA GPA HPA</p>
    <p>Page Table</p>
    <p>VA GPA L1PA</p>
    <p>Shadow EPT Shadow EPT</p>
    <p>Shadow EPT</p>
    <p>Virtual EPTVirtual EPTVirtual EPT</p>
    <p>Virtual EPTVirtual EPTEPTL1</p>
    <p>Page Table</p>
    <p>Virtual EPTVA GPA</p>
    <p>L1PA EPTL1 HPA</p>
    <p>Shadow EPT</p>
    <p>(a) Single-level</p>
    <p>(b) Nested</p>
    <p>(c) Span</p>
    <p>HPA</p>
    <p>Figure 3: Memory translation for single-level, nested, and Span VMs. VA = Virtual Address; GPA = Guest Physical Address; L1PA = L1 Physical Address; HPA = Host Physical Address.</p>
    <p>Single-level virtualization: Figure 3(a) shows that for single-level virtualization, the guest page tables map virtual addresses to guest physical addresses (VA to GPA in the figure). The hypervisor uses an EPT to map guest physical addresses to host physical addresses (GPA to HPA). Guest memory permissions are controlled by the combination of permissions in guest page table and EPT.</p>
    <p>Whenever the guest attempts to access a page that is either not present or protected in the EPT, the hardware generates an EPT fault and traps into the hypervisor, which handles the fault by mapping a new page, emulating an instruction, or taking other actions. On the other hand, the hypervisor grants complete control over the traditional paging hardware to the guest. A guest OS is free to maintain the mappings between its virtual and guest physical address space and update them as it sees fit, without trapping into the hypervisor.</p>
    <p>Nested virtualization: Figure 3(b) shows that for nested virtualization, the guest is similarly granted control over the traditional paging hardware to map virtual addresses to its guest physical address space. L1 maintains a Virtual EPT to map the guest pages to pages in L1s physical addresses space, or L1 pages. Finally, one more translation is required: L0 maintains EPTL1 to map L1 pages to physical pages. However, x86 processors can translate only two levels of addresses in hardware, from guest virtual to guest physical to host physical address. Hence the Virtual EPT maintained by L1 needs to be shadowed by L0, meaning that the Virtual EPT and EPTL1 must be compacted by L0 during runtime into a</p>
    <p>Single-Level Virtualization</p>
    <p>Nested Virtualization</p>
    <p>Isolation and Communication: Another design goal is to compartmentalize L1 services, from each other and from L0. First, L1s must have lower execution privilege compared to L0. Secondly, L1s must remain isolated from each other. These two goals are achieved by deprivileging L1s using nested virtualization and executing them as separate guests on L0. Finally, L1s must remain unaware of each other during execution. This goal is achieved by requiring L1s to receive any subscribed guest events that are generated on other L1s only via L0.</p>
    <p>There are two ways that L0 communicates with L1s: implicitly via traps and explicitly via messages. Traps allow L0 to transparently intercept certain memory management operations by L1 on the guest. Explicit messages allow an L1 to directly request guest control from L0. An Event Processing module in L0 traps runtime updates to guest memory mappings by any L1 and synchronizes guest mappings across different L1s. The event processing module also relays guest memory faults that need to be handled by L1. A bidirectional Message Channel relays explicit messages between L0 and L1s including attach/detach requests, memory event subscription/notification, guest I/O requests, and virtual interrupts. Some explicit messages, such as guest I/O requests and virtual interrupts, could be replaced with implicit traps. Our choice of which to use is largely based on ease of implementation on a case-by-case basis.</p>
    <p>Continuous vs. Transient Control: Span virtualization allows L1s control over guest resources to be either continuous or transient. Continuous control means that an L1 exerts uninterrupted control over one or more guest resources for an extended period of time. For example, an intrusion detection service in L1 that must monitor guest system calls, VM exits, or network traffic, would require continuous control of guest memory, VCPUs, and network device. Transient control means that an L1 acquires full control over guest resources for a brief duration, provides a short service to the guest, and releases guest control back to L0. For instance, an L1 that periodically checkpoints the guest would need transient control of guest memory, VCPUs, and I/O devices.</p>
    <p>A Span VM has a single guest physical address space which is mapped into the address space of all attached L1s. Thus any memory write on a guest page is immediately visible to all hypervisors controlling the guest. Note that all L1s have the same visibility into the guest memory due to the horizontal layering of Span virtualization, unlike the vertical stacking of nested virtualization, which somewhat obscures the guest to lower layers.</p>
    <p>EPTGuest</p>
    <p>Page Table EPTVA GPA HPA</p>
    <p>Page Table</p>
    <p>VA GPA L1PA</p>
    <p>Shadow EPT Shadow EPT</p>
    <p>Shadow EPT</p>
    <p>Virtual EPTVirtual EPTVirtual EPT</p>
    <p>Virtual EPTVirtual EPTEPTL1</p>
    <p>Page Table</p>
    <p>Virtual EPTVA GPA</p>
    <p>L1PA EPTL1 HPA</p>
    <p>Shadow EPT</p>
    <p>(a) Single-level</p>
    <p>(b) Nested</p>
    <p>(c) Span</p>
    <p>HPA</p>
    <p>Figure 3: Memory translation for single-level, nested, and Span VMs. VA = Virtual Address; GPA = Guest Physical Address; L1PA = L1 Physical Address; HPA = Host Physical Address.</p>
    <p>Single-level virtualization: Figure 3(a) shows that for single-level virtualization, the guest page tables map virtual addresses to guest physical addresses (VA to GPA in the figure). The hypervisor uses an EPT to map guest physical addresses to host physical addresses (GPA to HPA). Guest memory permissions are controlled by the combination of permissions in guest page table and EPT.</p>
    <p>Whenever the guest attempts to access a page that is either not present or protected in the EPT, the hardware generates an EPT fault and traps into the hypervisor, which handles the fault by mapping a new page, emulating an instruction, or taking other actions. On the other hand, the hypervisor grants complete control over the traditional paging hardware to the guest. A guest OS is free to maintain the mappings between its virtual and guest physical address space and update them as it sees fit, without trapping into the hypervisor.</p>
    <p>Nested virtualization: Figure 3(b) shows that for nested virtualization, the guest is similarly granted control over the traditional paging hardware to map virtual addresses to its guest physical address space. L1 maintains a Virtual EPT to map the guest pages to pages in L1s physical addresses space, or L1 pages. Finally, one more translation is required: L0 maintains EPTL1 to map L1 pages to physical pages. However, x86 processors can translate only two levels of addresses in hardware, from guest virtual to guest physical to host physical address. Hence the Virtual EPT maintained by L1 needs to be shadowed by L0, meaning that the Virtual EPT and EPTL1 must be compacted by L0 during runtime into a</p>
  </div>
  <div class="page">
    <p>Memory Translation Isolation and Communication: Another design goal</p>
    <p>is to compartmentalize L1 services, from each other and from L0. First, L1s must have lower execution privilege compared to L0. Secondly, L1s must remain isolated from each other. These two goals are achieved by deprivileging L1s using nested virtualization and executing them as separate guests on L0. Finally, L1s must remain unaware of each other during execution. This goal is achieved by requiring L1s to receive any subscribed guest events that are generated on other L1s only via L0.</p>
    <p>There are two ways that L0 communicates with L1s: implicitly via traps and explicitly via messages. Traps allow L0 to transparently intercept certain memory management operations by L1 on the guest. Explicit messages allow an L1 to directly request guest control from L0. An Event Processing module in L0 traps runtime updates to guest memory mappings by any L1 and synchronizes guest mappings across different L1s. The event processing module also relays guest memory faults that need to be handled by L1. A bidirectional Message Channel relays explicit messages between L0 and L1s including attach/detach requests, memory event subscription/notification, guest I/O requests, and virtual interrupts. Some explicit messages, such as guest I/O requests and virtual interrupts, could be replaced with implicit traps. Our choice of which to use is largely based on ease of implementation on a case-by-case basis.</p>
    <p>Continuous vs. Transient Control: Span virtualization allows L1s control over guest resources to be either continuous or transient. Continuous control means that an L1 exerts uninterrupted control over one or more guest resources for an extended period of time. For example, an intrusion detection service in L1 that must monitor guest system calls, VM exits, or network traffic, would require continuous control of guest memory, VCPUs, and network device. Transient control means that an L1 acquires full control over guest resources for a brief duration, provides a short service to the guest, and releases guest control back to L0. For instance, an L1 that periodically checkpoints the guest would need transient control of guest memory, VCPUs, and I/O devices.</p>
    <p>A Span VM has a single guest physical address space which is mapped into the address space of all attached L1s. Thus any memory write on a guest page is immediately visible to all hypervisors controlling the guest. Note that all L1s have the same visibility into the guest memory due to the horizontal layering of Span virtualization, unlike the vertical stacking of nested virtualization, which somewhat obscures the guest to lower layers.</p>
    <p>EPTGuest</p>
    <p>Page Table EPTVA GPA HPA</p>
    <p>Page Table</p>
    <p>VA GPA L1PA</p>
    <p>Shadow EPT Shadow EPT</p>
    <p>Shadow EPT</p>
    <p>Virtual EPTVirtual EPTVirtual EPT</p>
    <p>Virtual EPTVirtual EPTEPTL1</p>
    <p>Page Table</p>
    <p>Virtual EPTVA GPA</p>
    <p>L1PA EPTL1 HPA</p>
    <p>Shadow EPT</p>
    <p>(a) Single-level</p>
    <p>(b) Nested</p>
    <p>(c) Span</p>
    <p>HPA</p>
    <p>Figure 3: Memory translation for single-level, nested, and Span VMs. VA = Virtual Address; GPA = Guest Physical Address; L1PA = L1 Physical Address; HPA = Host Physical Address.</p>
    <p>Single-level virtualization: Figure 3(a) shows that for single-level virtualization, the guest page tables map virtual addresses to guest physical addresses (VA to GPA in the figure). The hypervisor uses an EPT to map guest physical addresses to host physical addresses (GPA to HPA). Guest memory permissions are controlled by the combination of permissions in guest page table and EPT.</p>
    <p>Whenever the guest attempts to access a page that is either not present or protected in the EPT, the hardware generates an EPT fault and traps into the hypervisor, which handles the fault by mapping a new page, emulating an instruction, or taking other actions. On the other hand, the hypervisor grants complete control over the traditional paging hardware to the guest. A guest OS is free to maintain the mappings between its virtual and guest physical address space and update them as it sees fit, without trapping into the hypervisor.</p>
    <p>Nested virtualization: Figure 3(b) shows that for nested virtualization, the guest is similarly granted control over the traditional paging hardware to map virtual addresses to its guest physical address space. L1 maintains a Virtual EPT to map the guest pages to pages in L1s physical addresses space, or L1 pages. Finally, one more translation is required: L0 maintains EPTL1 to map L1 pages to physical pages. However, x86 processors can translate only two levels of addresses in hardware, from guest virtual to guest physical to host physical address. Hence the Virtual EPT maintained by L1 needs to be shadowed by L0, meaning that the Virtual EPT and EPTL1 must be compacted by L0 during runtime into a</p>
    <p>Single-Level Virtualization</p>
    <p>Nested Virtualization</p>
    <p>Isolation and Communication: Another design goal is to compartmentalize L1 services, from each other and from L0. First, L1s must have lower execution privilege compared to L0. Secondly, L1s must remain isolated from each other. These two goals are achieved by deprivileging L1s using nested virtualization and executing them as separate guests on L0. Finally, L1s must remain unaware of each other during execution. This goal is achieved by requiring L1s to receive any subscribed guest events that are generated on other L1s only via L0.</p>
    <p>There are two ways that L0 communicates with L1s: implicitly via traps and explicitly via messages. Traps allow L0 to transparently intercept certain memory management operations by L1 on the guest. Explicit messages allow an L1 to directly request guest control from L0. An Event Processing module in L0 traps runtime updates to guest memory mappings by any L1 and synchronizes guest mappings across different L1s. The event processing module also relays guest memory faults that need to be handled by L1. A bidirectional Message Channel relays explicit messages between L0 and L1s including attach/detach requests, memory event subscription/notification, guest I/O requests, and virtual interrupts. Some explicit messages, such as guest I/O requests and virtual interrupts, could be replaced with implicit traps. Our choice of which to use is largely based on ease of implementation on a case-by-case basis.</p>
    <p>Continuous vs. Transient Control: Span virtualization allows L1s control over guest resources to be either continuous or transient. Continuous control means that an L1 exerts uninterrupted control over one or more guest resources for an extended period of time. For example, an intrusion detection service in L1 that must monitor guest system calls, VM exits, or network traffic, would require continuous control of guest memory, VCPUs, and network device. Transient control means that an L1 acquires full control over guest resources for a brief duration, provides a short service to the guest, and releases guest control back to L0. For instance, an L1 that periodically checkpoints the guest would need transient control of guest memory, VCPUs, and I/O devices.</p>
    <p>A Span VM has a single guest physical address space which is mapped into the address space of all attached L1s. Thus any memory write on a guest page is immediately visible to all hypervisors controlling the guest. Note that all L1s have the same visibility into the guest memory due to the horizontal layering of Span virtualization, unlike the vertical stacking of nested virtualization, which somewhat obscures the guest to lower layers.</p>
    <p>EPTGuest</p>
    <p>Page Table EPTVA GPA HPA</p>
    <p>Page Table</p>
    <p>VA GPA L1PA</p>
    <p>Shadow EPT Shadow EPT</p>
    <p>Shadow EPT</p>
    <p>Virtual EPTVirtual EPTVirtual EPT</p>
    <p>Virtual EPTVirtual EPTEPTL1</p>
    <p>Page Table</p>
    <p>Virtual EPTVA GPA</p>
    <p>L1PA EPTL1 HPA</p>
    <p>Shadow EPT</p>
    <p>(a) Single-level</p>
    <p>(b) Nested</p>
    <p>(c) Span</p>
    <p>HPA</p>
    <p>Figure 3: Memory translation for single-level, nested, and Span VMs. VA = Virtual Address; GPA = Guest Physical Address; L1PA = L1 Physical Address; HPA = Host Physical Address.</p>
    <p>Single-level virtualization: Figure 3(a) shows that for single-level virtualization, the guest page tables map virtual addresses to guest physical addresses (VA to GPA in the figure). The hypervisor uses an EPT to map guest physical addresses to host physical addresses (GPA to HPA). Guest memory permissions are controlled by the combination of permissions in guest page table and EPT.</p>
    <p>Whenever the guest attempts to access a page that is either not present or protected in the EPT, the hardware generates an EPT fault and traps into the hypervisor, which handles the fault by mapping a new page, emulating an instruction, or taking other actions. On the other hand, the hypervisor grants complete control over the traditional paging hardware to the guest. A guest OS is free to maintain the mappings between its virtual and guest physical address space and update them as it sees fit, without trapping into the hypervisor.</p>
    <p>Nested virtualization: Figure 3(b) shows that for nested virtualization, the guest is similarly granted control over the traditional paging hardware to map virtual addresses to its guest physical address space. L1 maintains a Virtual EPT to map the guest pages to pages in L1s physical addresses space, or L1 pages. Finally, one more translation is required: L0 maintains EPTL1 to map L1 pages to physical pages. However, x86 processors can translate only two levels of addresses in hardware, from guest virtual to guest physical to host physical address. Hence the Virtual EPT maintained by L1 needs to be shadowed by L0, meaning that the Virtual EPT and EPTL1 must be compacted by L0 during runtime into a</p>
    <p>Span Virtualization</p>
    <p>Isolation and Communication: Another design goal is to compartmentalize L1 services, from each other and from L0. First, L1s must have lower execution privilege compared to L0. Secondly, L1s must remain isolated from each other. These two goals are achieved by deprivileging L1s using nested virtualization and executing them as separate guests on L0. Finally, L1s must remain unaware of each other during execution. This goal is achieved by requiring L1s to receive any subscribed guest events that are generated on other L1s only via L0.</p>
    <p>There are two ways that L0 communicates with L1s: implicitly via traps and explicitly via messages. Traps allow L0 to transparently intercept certain memory management operations by L1 on the guest. Explicit messages allow an L1 to directly request guest control from L0. An Event Processing module in L0 traps runtime updates to guest memory mappings by any L1 and synchronizes guest mappings across different L1s. The event processing module also relays guest memory faults that need to be handled by L1. A bidirectional Message Channel relays explicit messages between L0 and L1s including attach/detach requests, memory event subscription/notification, guest I/O requests, and virtual interrupts. Some explicit messages, such as guest I/O requests and virtual interrupts, could be replaced with implicit traps. Our choice of which to use is largely based on ease of implementation on a case-by-case basis.</p>
    <p>Continuous vs. Transient Control: Span virtualization allows L1s control over guest resources to be either continuous or transient. Continuous control means that an L1 exerts uninterrupted control over one or more guest resources for an extended period of time. For example, an intrusion detection service in L1 that must monitor guest system calls, VM exits, or network traffic, would require continuous control of guest memory, VCPUs, and network device. Transient control means that an L1 acquires full control over guest resources for a brief duration, provides a short service to the guest, and releases guest control back to L0. For instance, an L1 that periodically checkpoints the guest would need transient control of guest memory, VCPUs, and I/O devices.</p>
    <p>A Span VM has a single guest physical address space which is mapped into the address space of all attached L1s. Thus any memory write on a guest page is immediately visible to all hypervisors controlling the guest. Note that all L1s have the same visibility into the guest memory due to the horizontal layering of Span virtualization, unlike the vertical stacking of nested virtualization, which somewhat obscures the guest to lower layers.</p>
    <p>EPTGuest</p>
    <p>Page Table EPTVA GPA HPA</p>
    <p>Page Table</p>
    <p>VA GPA L1PA</p>
    <p>Shadow EPT Shadow EPT</p>
    <p>Shadow EPT</p>
    <p>Virtual EPTVirtual EPTVirtual EPT</p>
    <p>Virtual EPTVirtual EPTEPTL1</p>
    <p>Page Table</p>
    <p>Virtual EPTVA GPA</p>
    <p>L1PA EPTL1 HPA</p>
    <p>Shadow EPT</p>
    <p>(a) Single-level</p>
    <p>(b) Nested</p>
    <p>(c) Span</p>
    <p>HPA</p>
    <p>Figure 3: Memory translation for single-level, nested, and Span VMs. VA = Virtual Address; GPA = Guest Physical Address; L1PA = L1 Physical Address; HPA = Host Physical Address.</p>
    <p>Single-level virtualization: Figure 3(a) shows that for single-level virtualization, the guest page tables map virtual addresses to guest physical addresses (VA to GPA in the figure). The hypervisor uses an EPT to map guest physical addresses to host physical addresses (GPA to HPA). Guest memory permissions are controlled by the combination of permissions in guest page table and EPT.</p>
    <p>Whenever the guest attempts to access a page that is either not present or protected in the EPT, the hardware generates an EPT fault and traps into the hypervisor, which handles the fault by mapping a new page, emulating an instruction, or taking other actions. On the other hand, the hypervisor grants complete control over the traditional paging hardware to the guest. A guest OS is free to maintain the mappings between its virtual and guest physical address space and update them as it sees fit, without trapping into the hypervisor.</p>
    <p>Nested virtualization: Figure 3(b) shows that for nested virtualization, the guest is similarly granted control over the traditional paging hardware to map virtual addresses to its guest physical address space. L1 maintains a Virtual EPT to map the guest pages to pages in L1s physical addresses space, or L1 pages. Finally, one more translation is required: L0 maintains EPTL1 to map L1 pages to physical pages. However, x86 processors can translate only two levels of addresses in hardware, from guest virtual to guest physical to host physical address. Hence the Virtual EPT maintained by L1 needs to be shadowed by L0, meaning that the Virtual EPT and EPTL1 must be compacted by L0 during runtime into a</p>
  </div>
  <div class="page">
    <p>Guest physical memory to Host physical memory translation should be the same regardless of the translation path.</p>
    <p>L0 syncs Shadow EPTs and EPTL1s  Guest faults  Virtual EPT modifications by L1  When L1 directly accesses guest memory</p>
    <p>L1s subscribe to guest memory events via L0  E.g. to track write events for dirty page tracking</p>
    <p>Synchronizing Guest Memory Maps</p>
    <p>Shadow EPT that directly maps guest pages to physical pages. To accomplish this, manipulations to the Virtual EPT by L1 trigger traps to L0. Whenever L1 loads a Virtual EPT, L0 receives a trap and activates the appropriate Shadow EPT. This style of nested page table management is also called multi-dimensional paging [10].</p>
    <p>EPT faults on guest memory can be due to (a) the guest accessing its own pages that have invalid Shadow EPT entries, and (b) the L1 directly accessing guest pages that have invalid EPTL1 entries to perform tasks such as I/O processing and VM introspection (VMI). Both kinds of EPT faults are first intercepted by L0. L0 examines a Shadow EPT fault to further determine whether it is due to a invalid Virtual EPT entry; such faults are forwarded to L1 for processing. Otherwise, faults due to invalid EPTL1 entries are handled by L0.</p>
    <p>Finally, an L1 may modify the Virtual EPT it maintains for a guest in the course of performing its own memory management. However, since the Virtual EPT is shadowed by L0, all Virtual EPT modifications cause traps to L0 for validation and a Shadow EPT update.</p>
    <p>Figure 4 illustrates that L1 reserves a range in the L1 physical address space for guest memory and then informs L0 of this range. Next, L1 constructs a Virtual EPT for the guest which is shadowed by L0, as in the nested case. Note that the reservation in L1 physical address space does not immediately allocate physical memory. Rather, physical memory is allocated lazily upon guest memory faults. L0 dynamically populates the reserved address range in L1 by adjusting the mappings in EPTL1</p>
    <p>Span Guest</p>
    <p>Shadow EPT</p>
    <p>Virtual Guest EPT</p>
    <p>EPTL1</p>
    <p>Virtual Guest EPT</p>
    <p>L1 Hypervisor(s) Virtual EPT</p>
    <p>Virtual EPT Trap Handler</p>
    <p>Guest Event Handling</p>
    <p>Memory Event Emulator</p>
    <p>L0 Virtual EPT Modifications</p>
    <p>Shadow EPTShadow EPT</p>
    <p>L1PA</p>
    <p>HPA</p>
    <p>Page Table</p>
    <p>Process VA</p>
    <p>Event Subscription Service</p>
    <p>Memory EventsGPA</p>
    <p>Event Notifications</p>
    <p>Figure 4: Span memory management overview.</p>
    <p>and the Shadow EPT. A memory-detach operation correspondingly undoes the EPTL1 mappings for guest and releases the reserved L1 address range.</p>
    <p>Fault handling for Span VMs extends the corresponding mechanism for nested VMs described earlier in Section 4.1. The key difference in the Span case is that L0 first checks if a host physical page has already been mapped to the faulting guest page. If so, the existing physical page mapping is used to resolve the fault, else a new physical page is allocated.</p>
    <p>As with the nested case, modifications by an L1 to establish Virtual EPT mappings trap to a Virtual EPT trap handler in L0, shown in Figure 4. When the handler receives a trap due to a protection modification, it updates each corresponding EPTL1 with the new least-permissive combination of page protection. Our current prototype allows protection modifications but disallows changes to established GPA-to-L1PA mappings to avoid having to change mappings in multiple EPTs.</p>
    <p>In Span virtualization, since multiple L1s can be attached to a guest, the L1 controlling the guests VCPUs may differ from the L1s requiring the memory event notification. Hence L0 provides a Memory Event Subscrip</p>
    <p>USENIX Association 2017 USENIX Annual Technical Conference 239</p>
  </div>
  <div class="page">
    <p>I/O Control</p>
    <p>Hypervisor</p>
    <p>Guest</p>
    <p>Backend</p>
    <p>Frontend</p>
    <p>Ring BufferI/O Request</p>
    <p>I/O ResponseKick</p>
    <p>Interrupt</p>
    <p>Native Device I/O</p>
    <p>Traditional Para-virtual I/O</p>
    <p>We consider para-virtual I/O in this work</p>
  </div>
  <div class="page">
    <p>I/O Control  We consider para-virtual I/O in this work  A Span Guests I/O device and VCPUs may be controlled by different L1s</p>
    <p>tion interface to enable L1s to independently subscribe to guest memory events. An L1 subscribes with L0, via the message channel, requesting notifications when a specific type of event occurs on certain pages of a given guest. When the L0 intercepts the subscribed events, it notifies all L1 subscribers via the message channel. Upon receiving the event notification, a memory event emulator in each L1, shown in Figure 4, processes the event and responds back to L0, either allowing or disallowing the guests memory access which triggered the event. The response from the L1 also specifies whether to maintain or discontinue the L1s event subscription on the guest page. For example, upon receiving a write event notification, an L1 that performs dirty page tracking will instruct L0 to allow the guest to write to the page, and cancel the subscription for future write events on the page, since the page has been recorded as being dirty. On the other hand, an intrusion detection service in L1 might disallow write events on guest pages containing kernel code and maintain future subscription. L0 concurrently delivers event notifications to all L1 subscribers. Guest memory access is allowed to proceed only if all subscribed L1s allow the event in their responses.</p>
    <p>To intercept a subscribed memory event on a page, the L0 applies the events mask to the corresponding EPTL1 entry of each L1 attached to the guest. Updating EPTL1 prompts L0 to update the guests Shadow EPT entry with the mask, to capture guest-triggered memory events. Updating EPTL1 entries also captures the events resulting from direct accesses to guest memory by an L1 instead of the guest. For instance, to track write events on a guest page, the EPT entry could be marked read-only after saving the original permissions for later restoration.</p>
    <p>In this work, guests use paravirtual devices [54, 6] which provide better performance than device emulation [59] and provide greater physical device sharing among guests than direct device assignment [11, 12, 50].</p>
    <p>For single-level virtualization, the guest OS runs a set of paravirtual frontend drivers, one for each virtual device, including block and network devices. The hypervisor runs the corresponding backend driver. The frontend and the backend drivers communicate via a shared ring buffer to issue I/O requests and receive responses. The frontend places an I/O request in the ring buffer and notifies the backend through a kick event, which triggers a VM exit to the hypervisor. The backend removes the I/O request from the ring buffer, completes the request, places the I/O response in the ring buffer, and injects an I/O completion interrupt to the guest. The interrupt handler in the frontend then picks up the I/O response from the ring buffer for processing. For nested guests, paravir</p>
    <p>L1a</p>
    <p>Guest</p>
    <p>Backend</p>
    <p>Frontend</p>
    <p>Ring Buffer</p>
    <p>I/O Request</p>
    <p>I/O Response Forwarded</p>
    <p>Interrupt</p>
    <p>L0</p>
    <p>L1b</p>
    <p>Forwarded Kick</p>
    <p>Native I/O via L0</p>
    <p>Figure 5: Paravirtual I/O for Span VMs. L1a controls the guest I/O device and L1b controls the VCPUs. Kicks from L1b and interrupts from L1a are forwarded via L0.</p>
    <p>tual drivers are used at both levels. For Span guests, different L1s may control guest VC</p>
    <p>PUs and I/O devices. If the same L1 controls both guest VCPUs and the device backend then I/O processing proceeds as in the nested case. Figure 5 illustrates the other case, when different L1s control guest VCPUs and backends. L1a controls the backend and L1b controls the guest VCPUs. The frontend in the guest and backend in L1a exchange I/O requests and responses via the ring buffer. However, I/O kicks are generated by guest VCPUs controlled by L1b, which forward the kicks to L1a. Likewise, L1a forwards any virtual interrupts from the backend to L1b, which injects the interrupt to the guest VCPUs. Kicks from the frontend and virtual interrupts from the backend are forwarded between L1s via L0 using the message channel.</p>
    <p>In single-level virtualization, L0 controls the scheduling of guest VCPUs. In nested virtualization, L0 delegates guest VCPU scheduling to an L1. The L1 schedules guest VCPUs on its own VCPUs and L0 schedules the L1s VCPUs on PCPUs. This hierarchical scheduling provides the L1 some degree of control over customized scheduling for its guests.</p>
    <p>Span virtualization can leverage either single-level or nested VCPU scheduling depending on whether the L0 or an L1 controls a guests VCPUs. Our current design requires that all VCPUs of a guest be controlled by one of the hypervisors at any instant. However, control over guest VCPUs can be transferred between hypervisors if needed. When L0 initiates a Span VM, it initializes the all the VCPUs as it would for a single-level guest. After the guest boots up, the control of guest VCPUs can be transferred to/from an L1 using attach/detach operations.</p>
    <p>Hypervisor</p>
    <p>Guest</p>
    <p>Backend</p>
    <p>Frontend</p>
    <p>Ring BufferI/O Request</p>
    <p>I/O ResponseKick</p>
    <p>Interrupt</p>
    <p>Native Device I/O</p>
    <p>Traditional Para-virtual I/O Para-virtual I/O in Span Virtualization</p>
    <p>Guest VCPUS</p>
    <p>I/O via L0</p>
  </div>
  <div class="page">
    <p>VCPU control</p>
    <p>Simple for now.</p>
    <p>All VCPUs controlled by one hypervisor  Either by L0 or one of the L1s</p>
    <p>Can we distribute VCPUs among L1s?  Possible, but no good reason why.  Requires expensive IPI forwarding across L1s  Complicates memory synchronization.</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>L0 L0 L0 KVM KVM KVM</p>
    <p>Guest GuestQEMU</p>
    <p>L1b L1</p>
    <p>L1 QEMU</p>
    <p>Guest GuestQEMU Span Guest</p>
    <p>NestedSingle Span</p>
    <p>L1a QEMU</p>
    <p>Guest QEMU In L1a</p>
    <p>Guest QEMU</p>
    <p>Guest QEMU In L1b</p>
    <p>L1a</p>
    <p>L1b QEMU</p>
    <p>KVM KVM KVM</p>
    <p>Figure 6: Roles of QEMU (Guest Controller) and KVM (hypervisor) for Single-level, Nested, and Span VMs.</p>
    <p>Code size and memory footprint: Our implementation required about 2200 lines of code changes in KVM/QEMU, which is roughly 980+ lines in KVM and 500+ lines in QEMU for L0, 300+ in KVM and 200+ in QEMU for L1, and another 180+ in the virtio backend. We disabled unnecessary kernel components in both L0 and L1 implementations to reduce their footprint. When idle, L0 was observed to have 600MB usage at startup. When running an idle Span guest attached to an idle L1, L0s memory usage increased to 1756MB after excluding usage by the guest and the L1. The L1s initial memory usage, as measured from L0, was 1GB after excluding the guest footprint. This is an initial prototype to validate our ideas. The footprints of L0 and L1 implementations could be further reduced using one of many lightweight Linux distributions [14].</p>
    <p>Guest Controller: A user-level control process, called the Guest Controller, runs on the hypervisor alongside each guest. In KVM/QEMU, the Guest Controller is a QEMU process which assists the KVM hypervisor with various control tasks on a guest, including guest ini</p>
    <p>tialization, I/O emulation, checkpointing, and migration. Figure 6 shows the position of the Guest Controller in different virtualization models. In both single-level and nested virtualization, there is only one Guest Controller per guest, since each guest is completely controlled by one hypervisor. Additionally, in the nested case, each L1 has its own Guest Controller that runs on L0. In Span virtualization, each guest is associated with multiple Guest Controllers, one per attached hypervisor. For instance, the Span Guest in Figure 6 is associated with three Guest Controllers, one each on L0, L1a, and L1b. During attach/detach operations, the Guest Controller in an L1 initiates the mapping/unmapping of guest memory into the L1s address space and, if needed, acquires/releases control over the guests VCPU and virtual I/O devices.</p>
    <p>Paravirtual I/O Architecture: The Guest Controller also performs I/O emulation of virtual I/O devices controlled by its corresponding hypervisor. The paravirtual device model described in Section 5 is called virtio in KVM/QEMU [54]. For nested guests, the virtio drivers are used at two levels: once between L0 and each L1 and again between an L1 and the guest. This design is also called virtio-over-virtio. A kick is implemented in virtio as a software trap from the frontend leading to a VM exit to KVM, which delivers the kick to the Guest Controller as a signal. Upon I/O completion, the Guest Controller requests KVM to inject a virtual interrupt into the guest. Kicks and interrupts are forwarded across hypervisors using the message channel. Redirected interrupts are received and injected into the guest by a modified version of KVMs virtual IOAPIC code.</p>
    <p>VCPU Control: The Guest Controllers in different hypervisors communicate with the Guest Controller in L0 to acquire or relinquish guest VCPU control. The Guest Controller represents each guest VCPU as a user space thread. A newly attached L1 hypervisor does not initialize guest VCPU state from scratch. Rather, the Guest Controller in the L1 accepts a checkpointed guest VCPU state from its counterpart in L0 using a technique similar to that used for live VM migration between physical hosts. After guest VCPU states are transferred from L0 to L1, the L1 Guest Controller resumes the guest VCPU threads while the L0 Guest Controller pauses its VCPU threads. A VCPU detach operation similarly transfers a checkpoint of guest VCPU states from L1 to L0. Transfer of guest VCPU states from one L1 to another is presently accomplished through a combination of detaching the source L1 from the guest VCPUs followed by attaching to the destination L1 (although a direct transfer could be potentially more efficient).</p>
    <p>Message Channel: The message channel between L0 and each L1 is implemented using a combination of hypercalls and UDP messages. Hypercalls from an L1 to L0 are used for attach/detach operations on guest</p>
    <p>USENIX Association 2017 USENIX Annual Technical Conference 241</p>
    <p>Guest: Unmodified Ubuntu 15.10, Linux 4.2</p>
    <p>L0 and L1  QEMU 1.2 and Linux 3.14.2  Modified nesting support in KVM/QEMU  L0 : 980+ lines in KVM and 500+ lines in QEMU  L1: 300+ lines in KVM and 380+ in QEMU</p>
    <p>Guest controller  User space QEMU process  Guest initialization, I/O emulation, Control</p>
    <p>Transfer, Migration, etc</p>
    <p>I/O: virtio-over-virtio  Direct assignment: future work</p>
    <p>Message channel  For I/O kick and interrupt forwarding  Currently using UDP messages and hypercalls</p>
    <p>Control Transfer  Guest VCPUs and virtio devices can be transferred between L1s and L0  Using attach/detach operations</p>
  </div>
  <div class="page">
    <p>Example 1: Two L1s controlling one Guest</p>
    <p>memory. UDP messages between an L1 and L0 are used for relaying I/O requests, device interrupts, memory subscription messages, and attach/detach operations on guest VCPU and I/O devices. UDP messages are presently used for ease of implementation and will be replaced by better alternatives such as hypercalls, callbacks, or shared buffers.</p>
    <p>Use Case 1  Network Monitoring and VM Introspection: In the first use case, the two L1s passively examine the guest state, while L0 supervises resource control. L1a controls the guests virtual network device whereas L1b controls the guest VCPUs. L1a performs network traffic monitoring by running the tcpdump tool to capture packets on the guests virtual network interface. Here we use tcpdump as a stand-in for other more complex packet filtering and analysis tools.</p>
    <p>L1b performs VM introspection (VMI) using a tool called Volatility [3] which continuously inspects a guests memory using a utility such as pmemsave to extract an accurate list of all processes running inside the guest. The guest OS is infected by a rootkit, Kernel Beast [38], which can hide malicious activity and present an inaccurate process list to the compromised guest. Volatility, running in L1b, can nevertheless extract an accurate guest process list using VM introspection.</p>
    <p>Figure 7 shows a screenshot, where the top window shows the tcpdump output in L1a, specifically the SSH traffic from the guest. The bottom right window shows that the rootkit KBeast in the guest OS hides a process evil, i.e. it prevents the process evil from being listed using the ps command in the guest. The bottom left window shows that Volatility, running in L1b, successfully detects the process evil hidden by the KBeast rootkit in the guest.</p>
    <p>This use case highlights several salient features of our design. First, an unmodified guest executes correctly</p>
    <p>L1a: Network Monitoring</p>
    <p>L1b: Volatility</p>
    <p>Guest infected with KBeast</p>
    <p>Figure 7: A screenshot of Span VM simultaneously using services from two L1s.</p>
    <p>even though its resources are controlled by multiple hypervisors. Second, an L1 can transparently examine guest memory. Third, an L1 controlling a guest virtual device (here network interface) can examine all I/O requests specific to the device even if the I/O requests are initiated from guest VCPUs controlled by another hypervisor. Thus an I/O device can be delegated to an L1 that does not control the guest VCPUs.</p>
    <p>Use Case 2  Guest Mirroring and VM Introspection: In this use case, we demonstrate an L1 that subscribes to guest memory events from L0. Hypervisors can provide a high availability service that protects unmodified guests from a failure of the physical machine. Solutions, such as Remus [24], typically work by continually transferring live incremental checkpoints of the guest state to a remote backup server, an operation that we call guest mirroring. When the primary VM fails, its backup image is activated, and the VM continues running as if failure never happened. To checkpoint incrementally, hypervisors typically use a feature called dirty page tracking. The hypervisor maintains a dirty bitmap, i.e. the set of pages that were dirtied since the last checkpoint. The dirty bitmap is constructed by marking all guest pages read-only in the EPT and recording dirtied pages upon write traps. The pages listed in the dirty bitmap are incrementally copied to the backup server.</p>
    <p>As a first approximation of guest mirroring, we modified the pre-copy live migration code in KVM/QEMU to periodically copy all dirtied guest pages to a backup server at a given frequency. In our setup, L1a mirrors a Span guest while L1b runs Volatility and controls guest VCPUs. L1a uses memory event subscription to track write events, construct the dirty bitmap, and periodically transfer any dirty pages to the backup server. We measured the average bandwidth reported by the iPerf [1] client benchmark running in the guest when L1a mirrors the guest memory at different frequencies. When guest mirroring happens every 12 seconds, iPerf delivers 800Mbps average bandwidth which is about the same as</p>
    <p>Guest: Infected with rootkit</p>
    <p>L1a: Monitoring network traffic</p>
    <p>L1b: Running VMI (Volatility)</p>
  </div>
  <div class="page">
    <p>Example 2: Guest mirroring</p>
    <p>L1a runs Volatility</p>
    <p>L1b runs Guest Mirroring  Periodically copy dirty guest pages  Requires subscription on write events</p>
    <p>Guest runs iPerf  ~800Mbps when mirrored every 12 seconds. Same as standard nested.</p>
    <p>~600Mbps every 1 second.  25% impact with high frequency dirty page tracking</p>
    <p>Guest Mirroring Service</p>
    <p>VM Introspection</p>
    <p>Service</p>
    <p>L0</p>
    <p>iPerf</p>
    <p>L1a</p>
    <p>Guest</p>
    <p>VCPUS</p>
    <p>Memory Event Subscription</p>
    <p>Incremental copy to a remote node</p>
    <p>L1b</p>
  </div>
  <div class="page">
    <p>Example 3: Live Hypervisor Replacement  Replace hypervisor underneath a live Guest</p>
    <p>L1 runs a full hypervisor  L0 acts as a thin switching layer</p>
    <p>Replacement operation  Attach new L1  Detach old L1</p>
    <p>740ms replacement latency, including memory co-mapping</p>
    <p>70ms guest downtime  During VCPU and I/O state transfer</p>
    <p>New Hypervisor</p>
    <p>Old Hypervisor</p>
    <p>L0 : Switches the L1 Hypervisor</p>
    <p>Guest</p>
  </div>
  <div class="page">
    <p>Macrobenchmarks Guest Workloads  Kernbench: repeatedly compiles the kernel  Quicksort: repeatedly sorts 400MB data  iPerf: Measures bandwidth to another host</p>
    <p>Hypervisor-level Services  Network monitoring (tcpdump)  VMI (Volatility)</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>U Ut</p>
    <p>ili za</p>
    <p>tio n</p>
    <p>(% )</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0.2</p>
    <p>++</p>
    <p>++- +- +</p>
    <p>(a) Kernbench</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>CP U</p>
    <p>Ut ili</p>
    <p>za tio</p>
    <p>n (%</p>
    <p>)</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0.0 0.1</p>
    <p>++</p>
    <p>+</p>
    <p>++- +- +</p>
    <p>(b) Quicksort</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>CP U</p>
    <p>Ut ili</p>
    <p>za tio</p>
    <p>n (%</p>
    <p>)</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0</p>
    <p>Mbps</p>
    <p>Mbps MbpsMbps</p>
    <p>(c) iPerf</p>
    <p>Figure 9: Service Mode: Normalized performance with hypervisor-level services network monitoring and Volatility. For single-level, L0 runs both services. For nested, L1 runs both services. For Span0 and Span1, L1a runs network monitoring and controls the guests network device; L1b runs Volatility; L0 controls guests block device.</p>
    <p>tio network interface with L0. We observed that if L1a controls the guest network device as well, then iPerf in the Span1 guest performs as well as in the nested guest.</p>
    <p>For iPerf in service mode (Figure 9(c)), nested, Span0, and Span1 guests perform about 1415% worse than the single-level guest, due to the combined effect of virtioover-virtio overhead and tcpdump running in L1a. Further, for Span0, the guest VCPU is controlled by L0 whereas the network device is controlled by L1a. Thus forwarding of I/O kicks and interrupts between L0 and L1a via the UDP-based message channel balances out any gains from having guest VCPUs run on L0.</p>
    <p>Figure 8(c) shows that the average CPU utilization increases significantly for iPerf in no-op mode  from 2.7% for the native host to 100+% for the single-level and Span0 configurations and 180+% for the nested and Span1 configurations. The increase appears to be due to the virtio network device implementation in QEMU, since we observed this higher CPU utilization even with newer versions of (unmodified) QEMU (v2.7) and Linux (v4.4.2). Figures 8(c) and 9(c) also show higher CPU utilization for the nested and Span1 cases compared to the single-level case. This is because guest VCPUs are controlled by L1s in the nested and Span1 cases, making nested VM exits more expensive.</p>
    <p>Attaching VCPUs to one of the L1s takes about 50ms. Attaching virtual I/O devices takes 135ms. When I/O control has to be transferred between hypervisors, the VCPUs need to be paused. The VCPUs could be running on any of the L1s and hence L0 needs to coordinate pausing and resuming the VCPUs during the transfer. The</p>
    <p>Figure 10: Overhead of attaching an L1 to a guest. Single Nested Span</p>
    <p>EPT Fault 2.4 2.8 3.3 Virtual EPT Fault - 23.3 24.1 Shadow EPT Fault - 3.7 4.1 Message Channel - - 53 Memory Event Notify - - 103.5</p>
    <p>Table 3: Low-level latencies( s) in Span virtualization.</p>
    <p>detach operation for VCPUs and I/O devices has similar overhead.</p>
    <p>Page Fault Servicing: Table 3 shows the latency of page fault handling and message channel. We measured the average service times for EPT faults in Span at both levels of nesting. It takes on the average 3.3 s to resolve a fault caused against EPTL1 and on the average 24.1 s to resolve a fault against the Virtual EPT. In contrast, the corresponding values measured for the nested case are 2.8 s and 23.3 s. For the single-level case, EPT-fault processing takes 2.4 s. The difference is due to the extra synchronization work in the EPT-fault handler in L0.</p>
    <p>Message Channel and Memory Events: The message channel is used in Span virtualization to exchange events and requests between L0 and L1s. It takes on the average 53 s to send a message between L0 and an L1. We also measured the overhead of notifying L1 subscribers from L0 for write events on a guest page. Without any subscribers, the write-fault processing takes on the average 3.5 s in L0. Notifying the write event over</p>
    <p>(a) Kernbench (b) Quicksort (c) iPerf</p>
    <p>Figure 8: No-op Mode: Normalized performance when no services run in host, L0, or L1s. The L0 controls the virtio block and network devices of the guest.</p>
    <p>with a nested guest. When guest mirroring happens every second, the average bandwidth drops to 600Mbps, indicating a 25% performance impact of event subscription at very high mirroring frequencies.</p>
    <p>Use Case 3  Proactive Refresh: Hypervisor-level services may contain latent bugs, such as memory leaks, or other vulnerabilities that become worse over time, making a monolithic hypervisor unreliable for guests. Techniques like Microreboot[18] and ReHype[43] have been proposed to improve hypervisor availability, either proactively or post-failure. We have already seen how Span virtualization can compartmentalize unreliable hypervisor-level services in an isolated deprivileged L1. Here, we go one step further and proactively replace unreliable L1s with a fresh reliable instance while the guest and the base L0 hypervisor keep running. In our setup, an old L1 (L1a) was attached to a 3GB Span guest. To perform hypervisor refresh, we attached a new pre-booted replacement hypervisor (L1b) to the guest memory. Then L1a was detached from the guest by transferring guest VCPU and I/O devices to L1b via L0. In our implementation, the entire refresh operation from attaching L1b to detaching L1a completes on the average within 740ms. Of this, 670ms are spent in attaching L1b to guest memory while the guest is running. The remaining 70ms is the guest downtime due to the transfer of VCPU and I/O states. Thus Span virtualization achieves sub-second L1 refresh latency. If we attach the replacement L1b to guest memory well in advance, then the VCPU and I/O state transfer can be triggered on-demand by events, such as unusual memory pressure or CPU usage, yielding sub100ms guest downtime and event response latency. In contrast, using pre-copy [22] to live migrate a guest from L1a to L1b can take several seconds depending on guest size and workload [65].</p>
    <p>L0 L1 L2 Mem CPUs Mem VCPUs Mem VCPUs</p>
    <p>Host 128GB 12 N/A N/A N/A N/A Single 128GB 12 3GB 1 N/A N/A Nested 128GB 12 16GB 8 3GB 1 Span0 128GB 12 8GB 4 3GB 1 on L0 Span1 128GB 12 8GB 4 3GB 1 on L1</p>
    <p>Table 2: Memory and CPU assignments for experiments.</p>
    <p>ways has 128GB and 12 physical CPU cores. In the nested configuration, L1 has 16GB memory and 8 VCPUs. The guest VCPU in the Span0 configuration is controlled by L0, and in Span1 by an L1. Finally, in both Span0 and Span1, L1a and L1b each have 8GB of memory and 4VCPUs, so their sums match the L1 in the nested setting.</p>
    <p>The guest runs one of the following three benchmarks: (a) Kernbench [41] compiles the Linux kernel. (b) Quicksort sorts 400MB of data in memory. (c) iPerf [1] measures network bandwidth to another host.</p>
    <p>The benchmarks run in two modes: No-op Mode, when no hypervisor-level services run, and Service Mode, when network monitoring and VM introspection services run at either L0 or L1s. The figures report each benchmarks normalized performance against the best case and system-wide average CPU utilization, which is measured in L0 using the atop command each second during experiments.</p>
    <p>From Figures 8(a) and (b) and Figures 9(a) and (b), in both modes for Kernbench and Quicksort, Span0 performs comparably with the single-level setting and Span1 performs comparably with the nested setting, with similar CPU utilization.</p>
    <p>For iPerf in No-op mode (Figure 8(c)), we observe that the Span1 guest experiences about 6% degradation over the nested guest with notable bandwidth fluctuation and 7% more CPU utilization. This is because the guests VCPU in Span1 is controlled by L1a, but the guests network device is controlled by L0. Hence, guest I/O requests (kicks) and responses are forwarded from L1a to L0 via the message channel. The message channel is currently implemented using UDP messages, which compete with guests iPerf client traffic on the L1s vir</p>
    <p>USENIX Association 2017 USENIX Annual Technical Conference 243</p>
  </div>
  <div class="page">
    <p>Macrobenchmarks</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>U Ut</p>
    <p>ili za</p>
    <p>tio n</p>
    <p>(% )</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0.2</p>
    <p>++</p>
    <p>++- +- +</p>
    <p>(a) Kernbench</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>CP U</p>
    <p>Ut ili</p>
    <p>za tio</p>
    <p>n (%</p>
    <p>)</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0.0 0.1</p>
    <p>++</p>
    <p>+</p>
    <p>++- +- +</p>
    <p>(b) Quicksort</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>CP U</p>
    <p>Ut ili</p>
    <p>za tio</p>
    <p>n (%</p>
    <p>)</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0</p>
    <p>Mbps</p>
    <p>Mbps MbpsMbps</p>
    <p>(c) iPerf</p>
    <p>Figure 9: Service Mode: Normalized performance with hypervisor-level services network monitoring and Volatility. For single-level, L0 runs both services. For nested, L1 runs both services. For Span0 and Span1, L1a runs network monitoring and controls the guests network device; L1b runs Volatility; L0 controls guests block device.</p>
    <p>tio network interface with L0. We observed that if L1a controls the guest network device as well, then iPerf in the Span1 guest performs as well as in the nested guest.</p>
    <p>For iPerf in service mode (Figure 9(c)), nested, Span0, and Span1 guests perform about 1415% worse than the single-level guest, due to the combined effect of virtioover-virtio overhead and tcpdump running in L1a. Further, for Span0, the guest VCPU is controlled by L0 whereas the network device is controlled by L1a. Thus forwarding of I/O kicks and interrupts between L0 and L1a via the UDP-based message channel balances out any gains from having guest VCPUs run on L0.</p>
    <p>Figure 8(c) shows that the average CPU utilization increases significantly for iPerf in no-op mode  from 2.7% for the native host to 100+% for the single-level and Span0 configurations and 180+% for the nested and Span1 configurations. The increase appears to be due to the virtio network device implementation in QEMU, since we observed this higher CPU utilization even with newer versions of (unmodified) QEMU (v2.7) and Linux (v4.4.2). Figures 8(c) and 9(c) also show higher CPU utilization for the nested and Span1 cases compared to the single-level case. This is because guest VCPUs are controlled by L1s in the nested and Span1 cases, making nested VM exits more expensive.</p>
    <p>Attaching VCPUs to one of the L1s takes about 50ms. Attaching virtual I/O devices takes 135ms. When I/O control has to be transferred between hypervisors, the VCPUs need to be paused. The VCPUs could be running on any of the L1s and hence L0 needs to coordinate pausing and resuming the VCPUs during the transfer. The</p>
    <p>Figure 10: Overhead of attaching an L1 to a guest. Single Nested Span</p>
    <p>EPT Fault 2.4 2.8 3.3 Virtual EPT Fault - 23.3 24.1 Shadow EPT Fault - 3.7 4.1 Message Channel - - 53 Memory Event Notify - - 103.5</p>
    <p>Table 3: Low-level latencies( s) in Span virtualization.</p>
    <p>detach operation for VCPUs and I/O devices has similar overhead.</p>
    <p>Page Fault Servicing: Table 3 shows the latency of page fault handling and message channel. We measured the average service times for EPT faults in Span at both levels of nesting. It takes on the average 3.3 s to resolve a fault caused against EPTL1 and on the average 24.1 s to resolve a fault against the Virtual EPT. In contrast, the corresponding values measured for the nested case are 2.8 s and 23.3 s. For the single-level case, EPT-fault processing takes 2.4 s. The difference is due to the extra synchronization work in the EPT-fault handler in L0.</p>
    <p>Message Channel and Memory Events: The message channel is used in Span virtualization to exchange events and requests between L0 and L1s. It takes on the average 53 s to send a message between L0 and an L1. We also measured the overhead of notifying L1 subscribers from L0 for write events on a guest page. Without any subscribers, the write-fault processing takes on the average 3.5 s in L0. Notifying the write event over</p>
    <p>(a) Kernbench (b) Quicksort (c) iPerf</p>
    <p>Figure 8: No-op Mode: Normalized performance when no services run in host, L0, or L1s. The L0 controls the virtio block and network devices of the guest.</p>
    <p>with a nested guest. When guest mirroring happens every second, the average bandwidth drops to 600Mbps, indicating a 25% performance impact of event subscription at very high mirroring frequencies.</p>
    <p>Use Case 3  Proactive Refresh: Hypervisor-level services may contain latent bugs, such as memory leaks, or other vulnerabilities that become worse over time, making a monolithic hypervisor unreliable for guests. Techniques like Microreboot[18] and ReHype[43] have been proposed to improve hypervisor availability, either proactively or post-failure. We have already seen how Span virtualization can compartmentalize unreliable hypervisor-level services in an isolated deprivileged L1. Here, we go one step further and proactively replace unreliable L1s with a fresh reliable instance while the guest and the base L0 hypervisor keep running. In our setup, an old L1 (L1a) was attached to a 3GB Span guest. To perform hypervisor refresh, we attached a new pre-booted replacement hypervisor (L1b) to the guest memory. Then L1a was detached from the guest by transferring guest VCPU and I/O devices to L1b via L0. In our implementation, the entire refresh operation from attaching L1b to detaching L1a completes on the average within 740ms. Of this, 670ms are spent in attaching L1b to guest memory while the guest is running. The remaining 70ms is the guest downtime due to the transfer of VCPU and I/O states. Thus Span virtualization achieves sub-second L1 refresh latency. If we attach the replacement L1b to guest memory well in advance, then the VCPU and I/O state transfer can be triggered on-demand by events, such as unusual memory pressure or CPU usage, yielding sub100ms guest downtime and event response latency. In contrast, using pre-copy [22] to live migrate a guest from L1a to L1b can take several seconds depending on guest size and workload [65].</p>
    <p>L0 L1 L2 Mem CPUs Mem VCPUs Mem VCPUs</p>
    <p>Host 128GB 12 N/A N/A N/A N/A Single 128GB 12 3GB 1 N/A N/A Nested 128GB 12 16GB 8 3GB 1 Span0 128GB 12 8GB 4 3GB 1 on L0 Span1 128GB 12 8GB 4 3GB 1 on L1</p>
    <p>Table 2: Memory and CPU assignments for experiments.</p>
    <p>ways has 128GB and 12 physical CPU cores. In the nested configuration, L1 has 16GB memory and 8 VCPUs. The guest VCPU in the Span0 configuration is controlled by L0, and in Span1 by an L1. Finally, in both Span0 and Span1, L1a and L1b each have 8GB of memory and 4VCPUs, so their sums match the L1 in the nested setting.</p>
    <p>The guest runs one of the following three benchmarks: (a) Kernbench [41] compiles the Linux kernel. (b) Quicksort sorts 400MB of data in memory. (c) iPerf [1] measures network bandwidth to another host.</p>
    <p>The benchmarks run in two modes: No-op Mode, when no hypervisor-level services run, and Service Mode, when network monitoring and VM introspection services run at either L0 or L1s. The figures report each benchmarks normalized performance against the best case and system-wide average CPU utilization, which is measured in L0 using the atop command each second during experiments.</p>
    <p>From Figures 8(a) and (b) and Figures 9(a) and (b), in both modes for Kernbench and Quicksort, Span0 performs comparably with the single-level setting and Span1 performs comparably with the nested setting, with similar CPU utilization.</p>
    <p>For iPerf in No-op mode (Figure 8(c)), we observe that the Span1 guest experiences about 6% degradation over the nested guest with notable bandwidth fluctuation and 7% more CPU utilization. This is because the guests VCPU in Span1 is controlled by L1a, but the guests network device is controlled by L0. Hence, guest I/O requests (kicks) and responses are forwarded from L1a to L0 via the message channel. The message channel is currently implemented using UDP messages, which compete with guests iPerf client traffic on the L1s vir</p>
    <p>USENIX Association 2017 USENIX Annual Technical Conference 243</p>
    <p>Guest Workloads  Kernbench: repeatedly compiles the kernel  Quicksort: repeatedly sorts 400MB data  iPerf: Measures bandwidth to another host</p>
    <p>Hypervisor-level Services  Network monitoring (tcpdump)  VMI (Volatility)</p>
  </div>
  <div class="page">
    <p>Macrobenchmarks</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>U Ut</p>
    <p>ili za</p>
    <p>tio n</p>
    <p>(% )</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0.2</p>
    <p>++</p>
    <p>++- +- +</p>
    <p>(a) Kernbench</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>CP U</p>
    <p>Ut ili</p>
    <p>za tio</p>
    <p>n (%</p>
    <p>)</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0.0 0.1</p>
    <p>++</p>
    <p>+</p>
    <p>++- +- +</p>
    <p>(b) Quicksort</p>
    <p>Single Nested Span0 Span10</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>CP U</p>
    <p>Ut ili</p>
    <p>za tio</p>
    <p>n (%</p>
    <p>)</p>
    <p>Normalized Performance CPU Utilization</p>
    <p>+- 0</p>
    <p>Mbps</p>
    <p>Mbps MbpsMbps</p>
    <p>(c) iPerf</p>
    <p>Figure 9: Service Mode: Normalized performance with hypervisor-level services network monitoring and Volatility. For single-level, L0 runs both services. For nested, L1 runs both services. For Span0 and Span1, L1a runs network monitoring and controls the guests network device; L1b runs Volatility; L0 controls guests block device.</p>
    <p>tio network interface with L0. We observed that if L1a controls the guest network device as well, then iPerf in the Span1 guest performs as well as in the nested guest.</p>
    <p>For iPerf in service mode (Figure 9(c)), nested, Span0, and Span1 guests perform about 1415% worse than the single-level guest, due to the combined effect of virtioover-virtio overhead and tcpdump running in L1a. Further, for Span0, the guest VCPU is controlled by L0 whereas the network device is controlled by L1a. Thus forwarding of I/O kicks and interrupts between L0 and L1a via the UDP-based message channel balances out any gains from having guest VCPUs run on L0.</p>
    <p>Figure 8(c) shows that the average CPU utilization increases significantly for iPerf in no-op mode  from 2.7% for the native host to 100+% for the single-level and Span0 configurations and 180+% for the nested and Span1 configurations. The increase appears to be due to the virtio network device implementation in QEMU, since we observed this higher CPU utilization even with newer versions of (unmodified) QEMU (v2.7) and Linux (v4.4.2). Figures 8(c) and 9(c) also show higher CPU utilization for the nested and Span1 cases compared to the single-level case. This is because guest VCPUs are controlled by L1s in the nested and Span1 cases, making nested VM exits more expensive.</p>
    <p>Attaching VCPUs to one of the L1s takes about 50ms. Attaching virtual I/O devices takes 135ms. When I/O control has to be transferred between hypervisors, the VCPUs need to be paused. The VCPUs could be running on any of the L1s and hence L0 needs to coordinate pausing and resuming the VCPUs during the transfer. The</p>
    <p>Figure 10: Overhead of attaching an L1 to a guest. Single Nested Span</p>
    <p>EPT Fault 2.4 2.8 3.3 Virtual EPT Fault - 23.3 24.1 Shadow EPT Fault - 3.7 4.1 Message Channel - - 53 Memory Event Notify - - 103.5</p>
    <p>Table 3: Low-level latencies( s) in Span virtualization.</p>
    <p>detach operation for VCPUs and I/O devices has similar overhead.</p>
    <p>Page Fault Servicing: Table 3 shows the latency of page fault handling and message channel. We measured the average service times for EPT faults in Span at both levels of nesting. It takes on the average 3.3 s to resolve a fault caused against EPTL1 and on the average 24.1 s to resolve a fault against the Virtual EPT. In contrast, the corresponding values measured for the nested case are 2.8 s and 23.3 s. For the single-level case, EPT-fault processing takes 2.4 s. The difference is due to the extra synchronization work in the EPT-fault handler in L0.</p>
    <p>Message Channel and Memory Events: The message channel is used in Span virtualization to exchange events and requests between L0 and L1s. It takes on the average 53 s to send a message between L0 and an L1. We also measured the overhead of notifying L1 subscribers from L0 for write events on a guest page. Without any subscribers, the write-fault processing takes on the average 3.5 s in L0. Notifying the write event over</p>
    <p>(a) Kernbench (b) Quicksort (c) iPerf</p>
    <p>Figure 8: No-op Mode: Normalized performance when no services run in host, L0, or L1s. The L0 controls the virtio block and network devices of the guest.</p>
    <p>with a nested guest. When guest mirroring happens every second, the average bandwidth drops to 600Mbps, indicating a 25% performance impact of event subscription at very high mirroring frequencies.</p>
    <p>Use Case 3  Proactive Refresh: Hypervisor-level services may contain latent bugs, such as memory leaks, or other vulnerabilities that become worse over time, making a monolithic hypervisor unreliable for guests. Techniques like Microreboot[18] and ReHype[43] have been proposed to improve hypervisor availability, either proactively or post-failure. We have already seen how Span virtualization can compartmentalize unreliable hypervisor-level services in an isolated deprivileged L1. Here, we go one step further and proactively replace unreliable L1s with a fresh reliable instance while the guest and the base L0 hypervisor keep running. In our setup, an old L1 (L1a) was attached to a 3GB Span guest. To perform hypervisor refresh, we attached a new pre-booted replacement hypervisor (L1b) to the guest memory. Then L1a was detached from the guest by transferring guest VCPU and I/O devices to L1b via L0. In our implementation, the entire refresh operation from attaching L1b to detaching L1a completes on the average within 740ms. Of this, 670ms are spent in attaching L1b to guest memory while the guest is running. The remaining 70ms is the guest downtime due to the transfer of VCPU and I/O states. Thus Span virtualization achieves sub-second L1 refresh latency. If we attach the replacement L1b to guest memory well in advance, then the VCPU and I/O state transfer can be triggered on-demand by events, such as unusual memory pressure or CPU usage, yielding sub100ms guest downtime and event response latency. In contrast, using pre-copy [22] to live migrate a guest from L1a to L1b can take several seconds depending on guest size and workload [65].</p>
    <p>L0 L1 L2 Mem CPUs Mem VCPUs Mem VCPUs</p>
    <p>Host 128GB 12 N/A N/A N/A N/A Single 128GB 12 3GB 1 N/A N/A Nested 128GB 12 16GB 8 3GB 1 Span0 128GB 12 8GB 4 3GB 1 on L0 Span1 128GB 12 8GB 4 3GB 1 on L1</p>
    <p>Table 2: Memory and CPU assignments for experiments.</p>
    <p>ways has 128GB and 12 physical CPU cores. In the nested configuration, L1 has 16GB memory and 8 VCPUs. The guest VCPU in the Span0 configuration is controlled by L0, and in Span1 by an L1. Finally, in both Span0 and Span1, L1a and L1b each have 8GB of memory and 4VCPUs, so their sums match the L1 in the nested setting.</p>
    <p>The guest runs one of the following three benchmarks: (a) Kernbench [41] compiles the Linux kernel. (b) Quicksort sorts 400MB of data in memory. (c) iPerf [1] measures network bandwidth to another host.</p>
    <p>The benchmarks run in two modes: No-op Mode, when no hypervisor-level services run, and Service Mode, when network monitoring and VM introspection services run at either L0 or L1s. The figures report each benchmarks normalized performance against the best case and system-wide average CPU utilization, which is measured in L0 using the atop command each second during experiments.</p>
    <p>From Figures 8(a) and (b) and Figures 9(a) and (b), in both modes for Kernbench and Quicksort, Span0 performs comparably with the single-level setting and Span1 performs comparably with the nested setting, with similar CPU utilization.</p>
    <p>For iPerf in No-op mode (Figure 8(c)), we observe that the Span1 guest experiences about 6% degradation over the nested guest with notable bandwidth fluctuation and 7% more CPU utilization. This is because the guests VCPU in Span1 is controlled by L1a, but the guests network device is controlled by L0. Hence, guest I/O requests (kicks) and responses are forwarded from L1a to L0 via the message channel. The message channel is currently implemented using UDP messages, which compete with guests iPerf client traffic on the L1s vir</p>
    <p>USENIX Association 2017 USENIX Annual Technical Conference 243</p>
    <p>Guest Workloads  Kernbench: repeatedly compiles the kernel  Quicksort: repeatedly sorts 400MB data  iPerf: Measures bandwidth to another host</p>
    <p>Hypervisor-level Services  Network monitoring (tcpdump)  VMI (Volatility)</p>
  </div>
  <div class="page">
    <p>Microbenchmarks</p>
    <p>(a) Kernbench (b) Quicksort (c) iPerf</p>
    <p>Figure 9: Service Mode: Normalized performance with hypervisor-level services network monitoring and Volatility. For single-level, L0 runs both services. For nested, L1 runs both services. For Span0 and Span1, L1a runs network monitoring and controls the guests network device; L1b runs Volatility; L0 controls guests block device.</p>
    <p>tio network interface with L0. We observed that if L1a controls the guest network device as well, then iPerf in the Span1 guest performs as well as in the nested guest.</p>
    <p>For iPerf in service mode (Figure 9(c)), nested, Span0, and Span1 guests perform about 1415% worse than the single-level guest, due to the combined effect of virtioover-virtio overhead and tcpdump running in L1a. Further, for Span0, the guest VCPU is controlled by L0 whereas the network device is controlled by L1a. Thus forwarding of I/O kicks and interrupts between L0 and L1a via the UDP-based message channel balances out any gains from having guest VCPUs run on L0.</p>
    <p>Figure 8(c) shows that the average CPU utilization increases significantly for iPerf in no-op mode  from 2.7% for the native host to 100+% for the single-level and Span0 configurations and 180+% for the nested and Span1 configurations. The increase appears to be due to the virtio network device implementation in QEMU, since we observed this higher CPU utilization even with newer versions of (unmodified) QEMU (v2.7) and Linux (v4.4.2). Figures 8(c) and 9(c) also show higher CPU utilization for the nested and Span1 cases compared to the single-level case. This is because guest VCPUs are controlled by L1s in the nested and Span1 cases, making nested VM exits more expensive.</p>
    <p>Attaching VCPUs to one of the L1s takes about 50ms. Attaching virtual I/O devices takes 135ms. When I/O control has to be transferred between hypervisors, the VCPUs need to be paused. The VCPUs could be running on any of the L1s and hence L0 needs to coordinate pausing and resuming the VCPUs during the transfer. The</p>
    <p>Figure 10: Overhead of attaching an L1 to a guest. Single Nested Span</p>
    <p>EPT Fault 2.4 2.8 3.3 Virtual EPT Fault - 23.3 24.1 Shadow EPT Fault - 3.7 4.1 Message Channel - - 53 Memory Event Notify - - 103.5</p>
    <p>Table 3: Low-level latencies( s) in Span virtualization.</p>
    <p>detach operation for VCPUs and I/O devices has similar overhead.</p>
    <p>Page Fault Servicing: Table 3 shows the latency of page fault handling and message channel. We measured the average service times for EPT faults in Span at both levels of nesting. It takes on the average 3.3 s to resolve a fault caused against EPTL1 and on the average 24.1 s to resolve a fault against the Virtual EPT. In contrast, the corresponding values measured for the nested case are 2.8 s and 23.3 s. For the single-level case, EPT-fault processing takes 2.4 s. The difference is due to the extra synchronization work in the EPT-fault handler in L0.</p>
    <p>Message Channel and Memory Events: The message channel is used in Span virtualization to exchange events and requests between L0 and L1s. It takes on the average 53 s to send a message between L0 and an L1. We also measured the overhead of notifying L1 subscribers from L0 for write events on a guest page. Without any subscribers, the write-fault processing takes on the average 3.5 s in L0. Notifying the write event over</p>
    <p>Low-level latencies in Span virtualization</p>
  </div>
  <div class="page">
    <p>Related Work  User space Services</p>
    <p>Microkernels, library OS, uDenali, KVM/QEMU, NOVA</p>
    <p>Service VMs  Dom0 in Xen, Xoar, Self-Service Cloud</p>
    <p>Nested virtualization  Belpaire &amp; Hsu, Ford et. al, Graf &amp; Roedel, Turtles  Ravello, XenBlanket, Bromium, DeepDefender, Dichotomy</p>
    <p>Span virtualization is the first to address multiple third-party hypervisor-level services to a common guest</p>
  </div>
  <div class="page">
    <p>Summary: Span Virtualization</p>
    <p>We introduced the concept of a multi-hypervisor virtual machine  that can be concurrently controlled by multiple coresident hypervisors</p>
    <p>Another tool in a cloud providers toolbox  to offer compartmentalized guest-facing third-party services</p>
    <p>Future work  Faster event notification and processing  Direct device assignment to L1s or Guest  Possible to support unmodified L1s?</p>
    <p>Requires L1s to support partial guest control. Current L1s assume full control.</p>
    <p>Code to be released after porting to newer KVM/QEMU</p>
  </div>
  <div class="page">
    <p>Single-level Nested Span</p>
    <p>Span</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>Backup slides</p>
  </div>
  <div class="page">
    <p>Comparison</p>
    <p>Level of Guest Control Impact of Service Failure Additional Virtualized Partial or L0 Coresident Guests Performance</p>
    <p>ISA Full Services Overheads Single-level Yes Full Fails Fail All None User space No Partial Protected Protected Attached Process switching Service VM No Partial Protected Protected Attached VM switching Nested Yes Full Protected Protected in Attached L1 switching + nesting</p>
    <p>L1 user space Span Yes Both Protected Protected Attached L1 switching + nesting</p>
    <p>Table 1: Alternatives for providing multiple services to a common guest, assuming one service per user space process, service VM, or Span L1.</p>
    <p>L0 Hypervisor</p>
    <p>Virtual Guest EPT Virtual Guest EPT</p>
    <p>L1 Hypervisor(s)</p>
    <p>Span Guest (unmodified)</p>
    <p>Guest Control Requester</p>
    <p>Memory Manager</p>
    <p>I/O Manager</p>
    <p>VCPU Manager</p>
    <p>Event Producer/Consumer</p>
    <p>Guest Controller (attach/detach/subscribe/unsubscribe)</p>
    <p>Event Processing (Relay/Emulation)</p>
    <p>Message Channel L1 Traps Guest Faults</p>
    <p>Messages Traps Fault Handler</p>
    <p>Memory Manager</p>
    <p>I/O Manager VCPU</p>
    <p>Manager</p>
    <p>Figure 2: High-level architecture for Span virtualization.</p>
    <p>space does not. Service VMs and Span virtualization isolate coresident services in individual VM-level compartments. Thus, failure of a service VM or Span L1 does not affect coresident services.</p>
    <p>Finally, consider additional performance overhead over the single-level case. User space services introduce context switching overhead among processes. Service VMs introduce VM context switching overhead, which is more expensive. Nesting adds the overhead of emulating privileged guest operations in L1. Span virtualization uses nesting but supports partial guest control by L1s. Hence, nesting overhead applies only to the guest resources that an L1 controls.</p>
    <p>Memory: All hypervisors must have the same consistent view of the guest memory.</p>
    <p>VCPUs: All guest VCPUs must be controlled by one hypervisor at a given instant.</p>
    <p>I/O Devices: Different virtual I/O devices of the same guest may be controlled exclusively by differ</p>
    <p>ent hypervisors at a given instant.  Control Transfer: Control of guest VCPUs and/or</p>
    <p>virtual I/O devices can be transferred from one hypervisor to another, but only via L0.</p>
    <p>Figure 2 shows the high-level architecture. A Span guest begins as a single-level VM on L0. One or more L1s can then attach to one or more guest resources and optionally subscribe with L0 for specific guest events.</p>
    <p>Guest Control Operations: The Guest Controller in L0 supervises control over a guest by multiple L1s through the following operations.</p>
    <p>[attach L1, Guest, Resource]: Gives L1 control over the Resource in Guest. Resources include guest memory, VCPU, and I/O devices. Control over memory is shared among multiple attached L1s, whereas control over guest VCPUs and virtual I/O devices is exclusive to an attached L1. Attaching to guest VCPUs or I/O device resources requires attaching to the guest memory resource.</p>
    <p>[detach L1, Guest, Resource]: Releases L1s control over Resource in Guest. Detaching from the guest memory resource requires detaching from guest VCPUs and I/O devices.</p>
    <p>[subscribe L1, Guest, Event, &lt;GFN Range&gt;] Registers L1 with L0 to receive Event from Guest. The GFN Range option specifies the range of frames in the guest address space on which to track the memory event. Presently we support only memory event subscription. Other guest events of interest could include SYSENTER instructions, port-mapped I/O, etc.</p>
    <p>[unsubscribe L1, Guest, Event, &lt;GFN Range&gt;] Unsubscribes L1 Guest Event.</p>
    <p>The Guest Controller also uses administrative policies to resolve apriori any potential conflicts over a guest control by multiple L1s. While this paper focuses on mechanisms rather than specific policies, we note that the problem of conflict resolution among services is not unique to Span. Alternative techniques also need ways to prevent conflicting services from controlling the same guest.</p>
    <p>USENIX Association 2017 USENIX Annual Technical Conference 237</p>
  </div>
  <div class="page">
    <p>Continuous and Transient Control</p>
    <p>Hypervisor</p>
    <p>Guest</p>
    <p>L1a L1b</p>
    <p>Hypervisor</p>
    <p>Guest</p>
    <p>L1Guest</p>
    <p>Continuous Control L1s always attached to guest</p>
    <p>Transient Control L1 attaches/detaches from guest as needed</p>
  </div>
</Presentation>
