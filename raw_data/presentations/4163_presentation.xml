<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Improving historical spelling normalization with bi-directional LSTMs and multi-task learning</p>
    <p>Marcel Bollmann1 Anders Sgaard2</p>
    <p>COLING 2016</p>
    <p>December 13, 2016</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>The Anselm corpus Dealing with spelling variation</p>
    <p>Motivation</p>
    <p>Sample of a manuscript from Early New High German</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>The Anselm corpus Dealing with spelling variation</p>
    <p>A corpus of Early New High German</p>
    <p>I Medieval religious treatise Interrogatio Sancti Anselmi de Passione Domini</p>
    <p>I &gt; 50 manuscripts and prints (in German)</p>
    <p>I 14th16th century I Various dialects</p>
    <p>I Bavarian I Middle German I Low German I . . .</p>
    <p>Sample from an Anselm manuscript</p>
    <p>http://www.linguistics.rub.de/anselm/</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>The Anselm corpus Dealing with spelling variation</p>
    <p>Examples for historical spellings</p>
    <p>Frau (woman) fraw, frawe, frwe, frauwe, frawe, frow, frouw, vraw, vrow, vorwe, vrauwe, vrouwe</p>
    <p>Kind (child) chind, chinde, chindt, chint, kind, kinde, kindi, kindt, kint, kinth, kynde, kynt</p>
    <p>Mutter (mother) moder, moeder, mueter, meter, muoter, muotter, muter, mutter, mvoter, mvter, mweter</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>The Anselm corpus Dealing with spelling variation</p>
    <p>Dealing with spelling variation</p>
    <p>The problems. . .</p>
    <p>I Difficult to annotate with tools aimed at modern data</p>
    <p>I High variance in spelling I None/very little training</p>
    <p>data</p>
    <p>Normalization. . .</p>
    <p>I Removes variance I Enables re-using of</p>
    <p>existing tools I Useful annotation layer</p>
    <p>(e.g. for corpus query)</p>
    <p>Normalization as the mapping of historical spellings to their modern-day equivalents.</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>The Anselm corpus Dealing with spelling variation</p>
    <p>Dealing with spelling variation</p>
    <p>The problems. . .</p>
    <p>I Difficult to annotate with tools aimed at modern data</p>
    <p>I High variance in spelling I None/very little training</p>
    <p>data</p>
    <p>Normalization. . .</p>
    <p>I Removes variance I Enables re-using of</p>
    <p>existing tools I Useful annotation layer</p>
    <p>(e.g. for corpus query)</p>
    <p>Normalization as the mapping of historical spellings to their modern-day equivalents.</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>I Character-based sequence labelling</p>
    <p>Hist vrow</p>
    <p>Norm frau</p>
    <p>I Not all examples are so straightforward. . .</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>I Character-based sequence labelling</p>
    <p>Hist v r o w</p>
    <p>Norm f r a u</p>
    <p>I Not all examples are so straightforward. . .</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>I Character-based sequence labelling</p>
    <p>Hist v r o w</p>
    <p>Norm f r a u</p>
    <p>I Not all examples are so straightforward. . .</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>Hist vsfuret</p>
    <p>Norm ausfhrt</p>
    <p>I Iterated Levenshtein distance alignment (Wieling et al., 2009)</p>
    <p>I Epsilon label for deletions</p>
    <p>I Leftward merging of insertions</p>
    <p>I Special beginning of word symbol</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>Hist v s f u r e t</p>
    <p>Norm a u s f  h r</p>
    <p>t</p>
    <p>I Iterated Levenshtein distance alignment (Wieling et al., 2009)</p>
    <p>I Epsilon label for deletions</p>
    <p>I Leftward merging of insertions</p>
    <p>I Special beginning of word symbol</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>Hist v s f u r e t</p>
    <p>Norm a u s f  h r  t</p>
    <p>I Iterated Levenshtein distance alignment (Wieling et al., 2009)</p>
    <p>I Epsilon label for deletions</p>
    <p>I Leftward merging of insertions</p>
    <p>I Special beginning of word symbol</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>Hist</p>
    <p>_</p>
    <p>v s f u r e t</p>
    <p>Norm a u s f h r  t</p>
    <p>I Iterated Levenshtein distance alignment (Wieling et al., 2009)</p>
    <p>I Epsilon label for deletions</p>
    <p>I Leftward merging of insertions</p>
    <p>I Special beginning of word symbol</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our approach</p>
    <p>Hist _ v s f u r e t</p>
    <p>Norm a u s f h r  t</p>
    <p>I Iterated Levenshtein distance alignment (Wieling et al., 2009)</p>
    <p>I Epsilon label for deletions</p>
    <p>I Leftward merging of insertions</p>
    <p>I Special beginning of word symbol</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Our model</p>
    <p>&lt;BOS&gt; v r o w</p>
    <p>f r a u</p>
    <p>embedding layer</p>
    <p>stack of bi-LSTM layers</p>
    <p>prediction layer</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Evaluation</p>
    <p>I 44 texts from the Anselm corpus</p>
    <p>I  4,200  13,200 tokens per text (average: 7,353 tokens)</p>
    <p>I 1,000 tokens for evaluation I 1,000 tokens for development (not used) I Remaining tokens for training</p>
    <p>I Pre-processing I Remove punctuation I Lowercase all words</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Methods for comparison</p>
    <p>I Norma (Bollmann, 2012)</p>
    <p>I Developed on the same corpus I Methods</p>
    <p>I Automatically learned replacement rules I Weighted Levenshtein distance</p>
    <p>I Requires lexical resource</p>
    <p>I CRFsuite (Okazaki, 2007)</p>
    <p>I Same input as the bi-LSTM model I Features: two surrounding characters</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Normalization as sequence labelling Bi-LSTM model Evaluation</p>
    <p>Results</p>
    <p>ID Region Norma CRF Bi-LSTM</p>
    <p>B2 West Central 76.10% 74.60% 82.00% D3 East Central 80.50% 77.20% 80.10% M East Upper 74.30% 72.80% 83.90% M5 East Upper 80.60% 76.40% 77.70% St2 West Upper 73.20% 73.20% 78.20%</p>
    <p>... ...</p>
    <p>... Average 77.83% 75.73% 79.90%</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Multi-task learning</p>
    <p>&lt;BOS&gt; v r o w&lt;BOS&gt; f r a w</p>
    <p>Stack of bi-LSTMs</p>
    <p>f r a u</p>
    <p>f r a u</p>
    <p>embedding layer</p>
    <p>prediction layer</p>
    <p>prediction layer for A</p>
    <p>prediction layer for B</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Multi-task learning</p>
    <p>&lt;BOS&gt; v r o w&lt;BOS&gt; f r a w</p>
    <p>Stack of bi-LSTMs</p>
    <p>f r a u</p>
    <p>f r a u</p>
    <p>embedding layer</p>
    <p>prediction layer</p>
    <p>prediction layer for A</p>
    <p>prediction layer for B</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Multi-task learning</p>
    <p>&lt;BOS&gt; v r o w</p>
    <p>&lt;BOS&gt; f r a w</p>
    <p>Stack of bi-LSTMs</p>
    <p>f r a u</p>
    <p>f r a u</p>
    <p>embedding layer</p>
    <p>prediction layer</p>
    <p>prediction layer for A</p>
    <p>prediction layer for B</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Multi-task learning</p>
    <p>&lt;BOS&gt; v r o w</p>
    <p>&lt;BOS&gt; f r a w</p>
    <p>Stack of bi-LSTMs</p>
    <p>f r a u</p>
    <p>f r a u</p>
    <p>embedding layer</p>
    <p>prediction layer</p>
    <p>prediction layer for A</p>
    <p>prediction layer for B</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>One prediction layer for each text</p>
    <p>. . .</p>
    <p>Embedding</p>
    <p>Bi-LSTM Stack</p>
    <p>Predict (B2) Predict (D3) Predict (M5)          Predict (St2)</p>
    <p>. . . . . . . . . . . .</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Evaluation</p>
    <p>I Each of the 44 texts as a separate task</p>
    <p>I Training: Randomly sample from all texts I Evaluation: Use the prediction layer for the current task</p>
    <p>I For comparison: Norma/CRF</p>
    <p>I Augment training set with 10,000 randomly sampled instances</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Results</p>
    <p>ID Region Norma Bi-LSTM</p>
    <p>Plain Aug. Plain MTL</p>
    <p>B2 West Central 76.10% 77.60% 82.00% 79.60% D3 East Central 80.50% 80.20% 80.10% 81.20% M East Upper 74.30% 74.40% 83.90% 80.90% M5 East Upper 80.60% 80.70% 77.70% 82.90% St2 West Upper 73.20% 73.40% 78.20% 79.90%</p>
    <p>... ...</p>
    <p>... Average 77.83% 77.48% 79.90% 80.55%</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Results</p>
    <p>ID Region Norma Bi-LSTM</p>
    <p>Plain Aug. Plain MTL</p>
    <p>B2 West Central 76.10% 77.60% 82.00% 79.60% D3 East Central 80.50% 80.20% 80.10% 81.20% M East Upper 74.30% 74.40% 83.90% 80.90% M5 East Upper 80.60% 80.70% 77.70% 82.90% St2 West Upper 73.20% 73.40% 78.20% 79.90%</p>
    <p>... ...</p>
    <p>... Average 77.83% 77.48% 79.90% 80.55%</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Learning a joint model Evaluation Conclusion</p>
    <p>Conclusion</p>
    <p>I Deep learning works for historical spelling normalization</p>
    <p>I . . . despite small datasets ( 4,200  13,200 tokens per text)</p>
    <p>I Outperforms Norma &amp; CRF baseline</p>
    <p>I . . . despite not using a lexical resource (like Norma)</p>
    <p>I Multi-task learning setup improves results</p>
    <p>I Way to deal with data sparsity problem I Many improvements conceivable</p>
  </div>
  <div class="page">
    <p>Problem definition Neural network approach</p>
    <p>Multi-task learning</p>
    <p>Thank you for listening!</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References</p>
    <p>Bollmann, M. (2012). (Semi-)automatic normalization of historical texts using distance measures and the Norma tool. In Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ACRH-2). Lisbon, Portugal.</p>
    <p>Okazaki, N. (2007). CRFsuite: a fast implementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/. Retrieved from http://www.chokkan.org/software/crfsuite/</p>
    <p>Wieling, M., Prokic, J., &amp; Nerbonne, J. (2009). Evaluating the pairwise string alignment of pronunciations. In Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education (LaTeCH  SHELT&amp;R 2009) (pp. 2634). Athens, Greece.</p>
  </div>
</Presentation>
