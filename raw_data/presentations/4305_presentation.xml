<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Semantically Equivalent Adversarial Rules for</p>
    <p>Debugging NLP Models Marco Tulio Ribeiro</p>
    <p>Carlos GuestrinSameer Singh (UC Irvine)</p>
  </div>
  <div class="page">
    <p>NLP / ML models are getting smarter: VQA What type of road sign is shown?</p>
    <p>&gt; STOP.</p>
    <p>Visual7A [Zhu et al 2016]</p>
  </div>
  <div class="page">
    <p>NLP / ML models are getting smarter: MC (SQuAD)</p>
    <p>The biggest city on the river Rhine is Cologne, Germany with a population of more than 1,050,000 people. It is the second-longest river in Central and Western Europe (after the Danube), at about 1,230 km (760 mi)</p>
    <p>How long is the Rhine?</p>
    <p>Question: are they prone to oversensitivity? 3</p>
    <p>BiDAF [Seo et al 2017]</p>
  </div>
  <div class="page">
    <p>Oversensitivity in images</p>
    <p>Adversaries are indistinguishable to humansBut unlikely in the real world (except for attacks)</p>
    <p>panda 57.7% confidence</p>
    <p>gibbon 99.3% confidence</p>
  </div>
  <div class="page">
    <p>Adversarial examples</p>
    <p>Find closest example with different prediction 5</p>
  </div>
  <div class="page">
    <p>What about text? What type of road sign is shown?</p>
    <p>&gt; STOP.</p>
    <p>What type of road sign is shown?</p>
    <p>Perceptible by humans, unlikely in real world</p>
    <p>What type of road sign is sho wn?</p>
  </div>
  <div class="page">
    <p>What about text? What type of road sign is shown?</p>
    <p>&gt; STOP.</p>
    <p>What type of road sign is shown?</p>
    <p>A single word changes too much! 7</p>
  </div>
  <div class="page">
    <p>Semantics matter What type of road sign is shown?</p>
    <p>&gt; Do not Enter.</p>
    <p>&gt; STOP.</p>
    <p>What type of road sign is shown?</p>
    <p>Bug, and likely in the real world 8</p>
  </div>
  <div class="page">
    <p>Semantics matter</p>
    <p>The biggest city on the river Rhine is Cologne, Germany with a population of more than 1,050,000 people. It is the second-longest river in Central and Western Europe (after the Danube), at about 1,230 km (760 mi)</p>
    <p>How long is the Rhine?</p>
    <p>&gt; More than 1,050,000</p>
    <p>&gt; 1230km</p>
    <p>How long is the Rhine?</p>
    <p>Not all changes are the same: semantics matter 9</p>
  </div>
  <div class="page">
    <p>Adversarial Rules</p>
    <p>Find rule that generates many adversaries 10</p>
  </div>
  <div class="page">
    <p>Generalizing adversaries What type of road sign is shown?</p>
    <p>&gt; Do not Enter.</p>
    <p>&gt; STOP.</p>
    <p>What type of road sign is shown?</p>
    <p>- flips 3.9% of examplesRule What NOUN Which NOUN 11</p>
  </div>
  <div class="page">
    <p>Semantics matter What color is the sky?</p>
    <p>&gt; Gray.</p>
    <p>&gt; Blue.</p>
    <p>What color is the sky?</p>
    <p>- flips 3.9% of examplesRule What NOUN Which NOUN 12</p>
  </div>
  <div class="page">
    <p>Semantics matter</p>
    <p>The biggest city on the river Rhine is Cologne, Germany with a population of more than 1,050,000 people. It is the second-longest river in Central and Western Europe (after the Danube), at about 1,230 km (760 mi)</p>
    <p>How long is the Rhine?</p>
    <p>&gt; More than 1,050,000</p>
    <p>&gt; 1230km</p>
    <p>How long is the Rhine?</p>
    <p>- flips 3% of examplesRule ? ?? 13</p>
  </div>
  <div class="page">
    <p>Semantics matter</p>
    <p>Detailed investigation of chum salmon, Oncorhynchus keta, showed that these fish digest ctenophores 20 times as fast as an equal weight of shrimps.</p>
    <p>What is the oncorhynchus also called?</p>
    <p>&gt; Oncorhynchus keta</p>
    <p>What is the oncorhynchus also called?</p>
    <p>- flips 3% of examplesRule ? ??</p>
    <p>&gt; chum salmon</p>
  </div>
  <div class="page">
    <p>Adversarial Rules</p>
    <p>Rules are global and actionable, more interesting than individual adversaries</p>
  </div>
  <div class="page">
    <p>Semantically Equivalent Adversary (SEA)</p>
  </div>
  <div class="page">
    <p>Ingredients</p>
    <p>Semantic score function</p>
    <p>A black box model</p>
    <p>Semantically Equivalent</p>
    <p>Different prediction</p>
  </div>
  <div class="page">
    <p>Revisiting adversaries</p>
    <p>Find closest example with different prediction</p>
  </div>
  <div class="page">
    <p>Sentence X</p>
    <p>en - pt</p>
    <p>en - fr</p>
    <p>Portuguese Translation</p>
    <p>French Translation</p>
    <p>fr - en</p>
    <p>pt - en</p>
    <p>Semantic Similarity: Paraphrasing</p>
    <p>Good movie!</p>
    <p>Bom filme!</p>
    <p>Bon film!</p>
    <p>Translators Back translators</p>
    <p>Score</p>
    <p>Good movie Good film</p>
    <p>Great movie</p>
    <p>Movie good 0.35 0.34 0.1</p>
    <p>Language model</p>
    <p>comes for free 19</p>
    <p>[Mallinson et al, 2017]</p>
  </div>
  <div class="page">
    <p>Finding an adversary What color is the tray? Pink</p>
    <p>What colour is the tray? Green Which color is the tray? Green What color is it? Green What color is tray? PinkHow color is the tray? Green</p>
  </div>
  <div class="page">
    <p>Semantically Equivalent Adversarial Rules (SEARs)</p>
  </div>
  <div class="page">
    <p>From SEAs to Rules</p>
    <p>Find SEAs Propose</p>
    <p>Candidate Rules</p>
    <p>Select Small</p>
    <p>Rule Set</p>
  </div>
  <div class="page">
    <p>Proposing Candidate Rules</p>
    <p>(What  Which)</p>
    <p>(What NOUN  Which NOUN) (WP type  Which type) (WP NOUN  Which NOUN)</p>
    <p>(What type  Which type)</p>
    <p>What type of road sign is shown?</p>
    <p>What type of road sign is shown?</p>
    <p>Candidate Rules: Exact Match</p>
    <p>Context</p>
    <p>POS Tags 23</p>
    <p>What Which type of road sign is shown? What Which is the person looking at? What Which was I thinking?</p>
    <p>Must not change semantics</p>
  </div>
  <div class="page">
    <p>From SEAs to Rules</p>
    <p>Find SEAs Propose</p>
    <p>Candidate Rules</p>
    <p>Select Small</p>
    <p>Rule Set</p>
  </div>
  <div class="page">
    <p>Semantically Equivalent Adversarial Rules (SEARS)</p>
    <p>Induces many flipped predictionsFlips different predictions</p>
    <p>High Adversary Count Non-Redundancy</p>
    <p>What NOUN  Which NOUN</p>
    <p>What type  Which type color  colour</p>
    <p>Selected Rules</p>
  </div>
  <div class="page">
    <p>Examples: VQA</p>
    <p>Visual7a-Telling [Zhu et al 2016]</p>
  </div>
  <div class="page">
    <p>Examples: Machine Comprehension</p>
    <p>BiDAF [Seo et al 2017]</p>
  </div>
  <div class="page">
    <p>Examples: Movie Review Sentiment Analysis</p>
    <p>FastText [Joulin et al 2016]</p>
  </div>
  <div class="page">
    <p>Experiments</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Set up Humans Top scored SEA SEA (top 5) + Human</p>
    <p>Evaluate adversaries for semantic equivalence</p>
  </div>
  <div class="page">
    <p>How often can SEAs be produced?</p>
    <p>Human SEA Human + SEA</p>
    <p>Human SEA Human + SEA</p>
    <p>Human SEA Human + SEA</p>
    <p>Human SEA Human + SEA</p>
    <p>Visual Question Answering Sentiment Analysis</p>
    <p>SEAs find equivalent adversaries as often as HumansSEAs + Humans better than Humans</p>
  </div>
  <div class="page">
    <p>Humans produce different adversaries:</p>
    <p>Humans did not produce these:</p>
    <p>But they did produce these:</p>
    <p>They are so easy to love What kind of meat is on the boys plate?</p>
    <p>How many suitcases? Also great directing and photography</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Part 1: experts come up with rules</p>
  </div>
  <div class="page">
    <p>Part 2: experts evaluate our SEARs</p>
    <p>Experts only accept good rules</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Visual QA Sentiment</p>
    <p>Experts SEARs</p>
    <p>Visual QA Sentiment</p>
    <p>Finding Rules Evaluating SEARs</p>
    <p>% correct predictions flipped Time (minutes)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Closing the loop</p>
    <p>(color  colour) (WP VBZ  WPs)</p>
    <p>Filter out bad rules</p>
    <p>Augment training</p>
    <p>Retrain model</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Visual QA Sentiment</p>
    <p>Original Augmented</p>
    <p>% o</p>
    <p>f fl</p>
    <p>ip s</p>
    <p>d u e t</p>
    <p>o b u gs</p>
    <p>Fix bugs, no loss in accuracy</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Semantics matter</p>
    <p>SEA SEARS</p>
    <p>Models are prone to these bugs</p>
    <p>SEAs and SEARs help find and fix them</p>
  </div>
  <div class="page">
    <p>Semantically Equivalent Adversarial Rules for</p>
    <p>Debugging NLP Models Marco Tulio Ribeiro</p>
    <p>Carlos GuestrinSameer Singh (UC Irvine)</p>
  </div>
  <div class="page">
    <p>Semantic scoring is still a research problem</p>
    <p>Also: inaccurate for long texts</p>
  </div>
  <div class="page">
    <p>Problem: not comparable across instances Good movie Good film</p>
    <p>Great movie</p>
    <p>good great</p>
    <p>excellent</p>
    <p>good movie good</p>
  </div>
  <div class="page">
    <p>Examples: VQA</p>
  </div>
  <div class="page">
    <p>Examples: Movie Review Sentiment Analysis</p>
    <p>FastText [Joulin et al 2016]</p>
  </div>
  <div class="page"/>
</Presentation>
