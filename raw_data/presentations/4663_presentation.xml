<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Generalized Maximum Margin Clustering &amp; Unsupervised Kernel Learning</p>
    <p>Hamed Valizadegan , Rong Jin</p>
    <p>Department of Computer Science and Engineering Michigan State University, USA</p>
  </div>
  <div class="page">
    <p>Outline Maximum margin data clustering Generalized maximum margin clustering Unsupervised kernel learning Empirical studies Future work</p>
  </div>
  <div class="page">
    <p>Data Clustering Two different criteria</p>
    <p>Compactness, e.g., k-means, mixture models Connectivity, e.g., spectral clustering, maximum margin clustering</p>
    <p>Compactness Connectivity</p>
  </div>
  <div class="page">
    <p>Maximum Margin Clustering (MMC) Key idea: extend SVM for unsupervised learning</p>
    <p>Training examples:</p>
    <p>Example weights:</p>
    <p>C: weight for classification error</p>
    <p>The dual problem of SVM</p>
    <p>max i</p>
    <p>nX i=1</p>
    <p>i  1</p>
    <p>nX i,j=1</p>
    <p>ijyiyjKi,j</p>
    <p>s. t. nX i=1</p>
    <p>iyi = 0</p>
    <p>Kernel similarity: Ki,j = K(xi,xj)</p>
    <p>i, i = 1, . . . ,n</p>
    <p>D = {(xi,yi}ni=1</p>
  </div>
  <div class="page">
    <p>Maximum Margin Clustering (MMC) Key idea: extend SVM for unsupervised learning</p>
    <p>Training examples:</p>
    <p>Example weights:</p>
    <p>C: weight for classification error</p>
    <p>The dual problem of SVM</p>
    <p>Kernel similarity: Ki,j = K(xi,xj)</p>
    <p>i, i = 1, . . . ,n</p>
    <p>D = {(xi,yi}ni=1max i,yi{1,+1}</p>
    <p>nX i=1</p>
    <p>i  1</p>
    <p>nX i,j=1</p>
    <p>ijyiyjKi,j</p>
    <p>s. t. nX i=1</p>
    <p>iyi = 0</p>
  </div>
  <div class="page">
    <p>MMC: Convex Programming Converting into a convex programming prob.</p>
    <p>A K: element wise product between matrix A and B , : vectors of size n 1</p>
    <p>min y,,,</p>
    <p>t</p>
    <p>s. t.</p>
    <p>(yy&gt;)K e +    + y</p>
    <p>(e +    + y)&gt; t2C&gt;e</p>
    <p>0</p>
    <p>0,   0</p>
  </div>
  <div class="page">
    <p>Maximum Margin Clustering Key idea: convert into a convex programming prob.</p>
    <p>min y,,</p>
    <p>t</p>
    <p>s. t.</p>
    <p>(yy&gt;)K e+   (e+  )&gt; t2C&gt;e</p>
    <p>0</p>
    <p>0,   0 yy&gt; M Rnn (1) M  0, (2) Mi,i = 1, i = 1, . . . ,n,</p>
    <p>(3) rank(M) = 1</p>
  </div>
  <div class="page">
    <p>MMC: Final Formulism</p>
    <p>Disadvantages High computational cost: SDP, M is a nn matrix Assume clustering boundaries pass through the origins. Sensitive to the choice of kernel functions</p>
    <p>min M,,</p>
    <p>t</p>
    <p>s. t.</p>
    <p>M K e +</p>
    <p>(e +   )&gt; t2C&gt;e</p>
    <p>0</p>
    <p>0,   0, M  0 Mi,i = 1, i = 1,2, . . . ,n</p>
  </div>
  <div class="page">
    <p>MMC: Scalability Issue</p>
    <p>Number of Samples</p>
    <p>T im</p>
    <p>e (</p>
    <p>se co</p>
    <p>n d s)</p>
    <p>Time comparision</p>
    <p>Generalized Maxmium Marging Clustering Maximum Margin Clustering</p>
    <p>MMC</p>
    <p>Proposed Alg.</p>
  </div>
  <div class="page">
    <p>MMC: Sensitivity to Kernel Function</p>
    <p>1 0.5 0 0.5 1</p>
    <p>1</p>
    <p>0.5</p>
    <p>Kernel Width (% of data range) in RBF function</p>
    <p>C lu</p>
    <p>st e ri n g E</p>
    <p>rr o r</p>
    <p>Dataset MMC clustering using RBF kernel</p>
    <p>Ki,j = exp  kxixj)k</p>
  </div>
  <div class="page">
    <p>Generalized Maximum Margin Clustering Simple Case: Hard Margin</p>
    <p>min ,y,</p>
    <p>s. t.   0, y  {+1,1}n</p>
    <p>z = (e+ )y</p>
  </div>
  <div class="page">
    <p>Generalized Maximum Margin Clustering Simple Case: Hard Margin</p>
    <p>z2i = (1+ i) 2y2i = (1+ i)</p>
    <p>min ,y,</p>
    <p>s. t.   0, y  {+1,1}n</p>
    <p>z = (e+ )y min z,</p>
    <p>s. t. zi 2  1, i = 1,2, . . . ,n</p>
  </div>
  <div class="page">
    <p>GMMC: Translation Invariance</p>
    <p>z0 = z+ e</p>
    <p>0 =  . f(z,) = f(z,)</p>
    <p>Related to the non-unique solution for threshold b in SVM</p>
    <p>f(z,) = min z,</p>
    <p>s. t. z2i  1, i = 1,2, . . . ,n</p>
  </div>
  <div class="page">
    <p>GMMC: Translation Invariance</p>
    <p>Remove translation invariance z&gt;e  0</p>
    <p>w = (z;)</p>
    <p>P = (In,e)</p>
    <p>min z,</p>
    <p>&gt;e)2</p>
    <p>s. t. z2i  1, i = 1,2, . . . ,n</p>
    <p>min wRn+1</p>
    <p>wTPTK1Pw+Ce(e &gt; 0 w)</p>
    <p>s. t. w2i  1, i = 1,2, . . . ,n</p>
  </div>
  <div class="page">
    <p>GMMC: Non-Convex Constraint</p>
    <p>Non-convex Constraint |w|</p>
    <p>min wRn+1</p>
    <p>wTPTK1Pw + Ce(e &gt; 0 w)</p>
    <p>s. t. w2i  1, i = 1,2, . . . ,n</p>
  </div>
  <div class="page">
    <p>GMMC: Dual Approximation Approximate the original problem by its dual</p>
    <p>The number of variables n A standard SemiDefinite Programming (SDP) problem</p>
    <p>e0 = (0n;1) Ikn+1</p>
    <p>i,j</p>
    <p>=</p>
    <p>1 i = j = k 0 otherwise</p>
    <p>max Rn</p>
    <p>nX i=1</p>
    <p>i</p>
    <p>s. t. PTK1P +Cee0e &gt; 0</p>
    <p>nX i=1</p>
    <p>iI i n+1  0</p>
    <p>i  0, i = 1,2, . . . ,n</p>
  </div>
  <div class="page">
    <p>Karush-Kuhn-Tucker (KKT) Conditions</p>
    <p>w is the zero eigenvector of matrix</p>
    <p>PTK1P + Cee0e</p>
    <p>&gt; 0</p>
    <p>nX i=1</p>
    <p>iI i n+1</p>
    <p>! w = 0n+1</p>
    <p>PTK1P +Cee0e</p>
    <p>&gt; 0</p>
    <p>Pn i=1 iI</p>
    <p>i n+1</p>
  </div>
  <div class="page">
    <p>GMMC: Procedure 1. Compute 1, 2, , n by solving the dual problem 2. Compute the zero eigenvector w of matrix</p>
    <p>PTK1P  Pn</p>
    <p>i=1 iI i n+1</p>
  </div>
  <div class="page">
    <p>GMMC: Procedure 1. Compute 1, 2, , n by solving the dual problem 2. Compute the zero eigenvector w of matrix</p>
    <p>PTK1P +Cee0e</p>
    <p>&gt; 0</p>
    <p>Pn i=1 iI</p>
    <p>i n+1</p>
  </div>
  <div class="page">
    <p>Understand the Approximation Compute the dual of the dual</p>
    <p>min M</p>
    <p>tr(P&gt;K1PM)+Cee &gt; 0 Me0</p>
    <p>s. t. M  0, Mi,i  1, i = 1,2, . . . ,n</p>
    <p>min w</p>
    <p>wTPTK1Pw + Ce(e &gt; 0 w)</p>
    <p>s. t. w2i  1, i = 1,2, . . . ,n</p>
    <p>ww&gt;  M (1) M  0, (2) Mi,i  1,(3)rank(M) = 1</p>
  </div>
  <div class="page">
    <p>Relation to Spectral Clustering</p>
    <p>Set</p>
    <p>is the minimum eigenvector of kernel matrix K Solution w is related to the minimum eigenvector of K Spectral Clustering</p>
    <p>i = ,Ce  1</p>
    <p>max 0</p>
    <p>s. t. K1  In</p>
  </div>
  <div class="page">
    <p>GMMC: Soft Margin</p>
    <p>max Rn</p>
    <p>nX i=1</p>
    <p>i</p>
    <p>s. t. P&gt;K1P +Cee0e &gt; 0</p>
    <p>nX i=1</p>
    <p>iI i n+1  0</p>
    <p>C: upper bound for the assigned weight i</p>
  </div>
  <div class="page">
    <p>Empirical Studies: Datasets Three synthesized datasets</p>
    <p>Overlapped Gaussian, two circles, and two connected circles</p>
    <p>Four UCI datasets Vote, digits, ionosphere, and breast</p>
    <p>1 0.5 0 0.5 1</p>
    <p>1</p>
    <p>0.5</p>
    <p>0.6 0.4 0.2 0 0.2 0.4 0.6</p>
    <p>0.6</p>
    <p>0.4</p>
    <p>0.2</p>
    <p>Overlapped Gaussian Two circles Two connected circles</p>
  </div>
  <div class="page">
    <p>GMMC: Scalability</p>
    <p>Number of Samples</p>
    <p>T im</p>
    <p>e (</p>
    <p>se co</p>
    <p>n d s)</p>
    <p>Time comparision</p>
    <p>Generalized Maxmium Marging Clustering Maximum Margin Clustering</p>
    <p>MMC</p>
    <p>GMMC</p>
  </div>
  <div class="page">
    <p>Empirical Studies: Results</p>
    <p>Using RBF kernel with the optimal kernel width</p>
    <p>NC: Normalized Cut MMC: Maximum Margin Clustering GMMC: Generalized Maximum Margin Clustering</p>
  </div>
  <div class="page">
    <p>Unsupervised Kernel Learning Caveat: the clustering results heavily rely on the appropriate kernel function How to learn the right kernel matrix without any supervision information?</p>
  </div>
  <div class="page">
    <p>Unsupervised Kernel Learning Given m kernel matrices , find the right combination</p>
    <p>{Ki}mi=1 K =</p>
    <p>Pm i=1 iKi</p>
    <p>max ,</p>
    <p>nX i=1</p>
    <p>i</p>
    <p>s. t. P&gt;K1P + Cee0e &gt; 0</p>
    <p>nX i=1</p>
    <p>iI i n+1  0</p>
    <p>i = 1, i  0, i = 1,2, . . . ,m</p>
  </div>
  <div class="page">
    <p>Unsupervised Kernel Learning Given m kernel matrices , find the right combination</p>
    <p>{Ki}mi=1 K =</p>
    <p>Pm i=1 iKi</p>
    <p>Non-convex constraintmax ,</p>
    <p>nX i=1</p>
    <p>i</p>
    <p>s. t. P&gt;</p>
    <p>mX i=1</p>
    <p>iKi</p>
    <p>! 1P +Cee0e</p>
    <p>&gt; 0</p>
    <p>nX i=1</p>
    <p>iI i n+1  0</p>
    <p>i = 1, i  0, i = 1,2, . . . ,m</p>
  </div>
  <div class="page">
    <p>Unsupervised Kernel Learning Replace K-1 with combinatorial Laplacian L(K)</p>
    <p>L(K) = D(K)K</p>
    <p>[D(K)]i,j =</p>
    <p>0 i 6= jPn k=1 Ki,k i = j</p>
    <p>K1 =</p>
    <p>mX i=1</p>
    <p>iKi</p>
    <p>!1 L(K) =</p>
    <p>mX i=1</p>
    <p>iL(Ki)</p>
    <p>Replacing arithmetic mean by harmonic mean</p>
  </div>
  <div class="page">
    <p>Unsupervised Kernel Learning Replace K-1 with combinatorial Laplacian L(K)</p>
    <p>Li: the combinatorial Laplacian for Ki</p>
    <p>max ,</p>
    <p>nX i=1</p>
    <p>i</p>
    <p>s. t. P&gt;</p>
    <p>mX i=1</p>
    <p>iLi</p>
    <p>! P +Cee0e</p>
    <p>&gt; 0</p>
    <p>nX i=1</p>
    <p>iI i n+1  0</p>
    <p>i = 1, i  0, i = 1,2, . . . ,m</p>
  </div>
  <div class="page">
    <p>Empirical Results</p>
    <p>Baseline: self-tune spectral clustering (with param k) Finding RBF kernel with the optimal kernel width</p>
  </div>
  <div class="page">
    <p>Conclusion We proposed a generalized version of maximum margin clustering</p>
    <p>Reduce the computational cost Remove the constraints on the clustering decision boundaries by the maximum margin clustering Learn kernel matrices unsupervisedly</p>
    <p>Future work Computational efficiency Semi-supervised learning</p>
  </div>
</Presentation>
