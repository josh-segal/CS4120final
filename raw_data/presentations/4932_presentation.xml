<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Beyond Equilibrium: Predicting Human Behavior in Normal-Form Games</p>
    <p>Kevin Leyton-Brown, University of British Columbia</p>
    <p>Based on joint work with James R. Wright</p>
    <p>OpLog Seminar, September 10, 2012</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Context</p>
    <p>Motivation: Predict human behavior in strategic settings.</p>
    <p>Our focus: Unrepeated initial play in simultaneous-move, 2-player games.</p>
    <p>Game theory: Studies idealized rational agents, not human agents.</p>
    <p>Behavioral game theory: Aims to extend game theory to modeling human agents.</p>
    <p>There are a wide range of BGT models in the literature. Historically, BGT has been most concerned with explaining behavior, often on particular games, rather than predicting it. No study compares a wide range of models, considers predictive performance, or looks at such a large, heterogeneous set of games.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Contribution</p>
    <p>Our contributions:</p>
    <p>Compared predictive performance of the most prominent solution concepts for our setting:</p>
    <p>Nash equilibrium, plus Four models from behavioral game theory</p>
    <p>. . . using nine experimental datasets from the literature</p>
    <p>Bayesian sensitivity analysis:</p>
    <p>Yields new insight into existing model (Poisson-CH) Argues for a novel simplification of an existing model (Quantal level-k)</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>Travelers Dilemma has a unique Nash equilibrium.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>Travelers Dilemma has a unique Nash equilibrium.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>Travelers Dilemma has a unique Nash equilibrium.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Example: Travelers Dilemma</p>
    <p>Two players pick a number (2-100) simultaneously.</p>
    <p>If they pick the same number, that is their payoff.</p>
    <p>If they pick different numbers:</p>
    <p>Lower player gets lower number, plus bonus of 2. Higher player gets lower number, minus penalty of 2.</p>
    <p>Travelers Dilemma has a unique Nash equilibrium.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Nash equilibrium and human subjects</p>
    <p>Nash equilibrium often makes counterintuitive predictions.</p>
    <p>In Travelers Dilemma: The vast majority of human players choose 97100. The Nash equilibrium is 2.</p>
    <p>Modifications to a game that dont change Nash equilibrium predictions at all can cause large changes in how human subjects play the game [Goeree &amp; Holt 2001].</p>
    <p>In Travelers Dilemma: When the penalty is large, people play much closer to Nash equilibrium. But the size of the penalty does not affect equilibrium.</p>
    <p>Clearly Nash equilibrium is not the whole story.</p>
    <p>Behavioral game theory proposes a number of models to better explain human behavior.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Nash equilibrium and human subjects</p>
    <p>Nash equilibrium often makes counterintuitive predictions.</p>
    <p>In Travelers Dilemma: The vast majority of human players choose 97100. The Nash equilibrium is 2.</p>
    <p>Modifications to a game that dont change Nash equilibrium predictions at all can cause large changes in how human subjects play the game [Goeree &amp; Holt 2001].</p>
    <p>In Travelers Dilemma: When the penalty is large, people play much closer to Nash equilibrium. But the size of the penalty does not affect equilibrium.</p>
    <p>Clearly Nash equilibrium is not the whole story.</p>
    <p>Behavioral game theory proposes a number of models to better explain human behavior.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>BGT model: Quantal response equilibrium (QRE)</p>
    <p>Cost-proportional errors: Agents are less likely to make high-cost mistakes than low-cost mistakes.</p>
    <p>QRE model [McKelvey &amp; Palfrey 1995] parameter: ()</p>
    <p>Agents quantally best respond to each other.</p>
    <p>QBRi(si,)(ai) = eui(ai,si)</p>
    <p>aiAi eui(a</p>
    <p>i,si)</p>
    <p>Precision parameter   [0,) indicates how sensitive agents are to utility differences.</p>
    <p>= 0 means agents choose actions uniformly at random. As  , QBR approaches best response.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Nice storybut is QRE a good model?</p>
    <p>Lets say we pay a bunch of people to play games against each other, and gather some data. Now wed like to know how good a job our QRE model does. How would we do that?</p>
    <p>Two issues:</p>
    <p>have to set the models parameter () to use it at all;</p>
    <p>must ensure that we do this in a way that generalizes to new play by the same people.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Scoring a models performance</p>
    <p>We randomly partition our data into different sets: D = Dtrain Dtest We choose parameter value(s) that maximize the likelihood of the training data:</p>
    <p>#</p>
    <p>= arg max #</p>
    <p>Pr(Dtrain |M, #</p>
    <p>).</p>
    <p>a tricky non-convex optimization problem</p>
    <p>We score the performance of a model by the likelihood of the test data:</p>
    <p>Pr(Dtest |M, #</p>
    <p>).</p>
    <p>To reduce variance, we repeat this process multiple times with different random partitions and average the results</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>BGT models: Iterative strategic reasoning</p>
    <p>Level-0 agents choose actions non-strategically.</p>
    <p>In this work (and most others), uniformly at random</p>
    <p>Level-1 agents reason about level-0 agents.</p>
    <p>Level-2 agents reason about level-1 agents.</p>
    <p>Theres a probability distribution over levels.</p>
    <p>Higher-level agents are smarter; scarcer</p>
    <p>Predicting the distribution of play: weighted sum of the distributions for each level.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>BGT model: Lk</p>
    <p>Lk model [Costa-Gomes et al. 2001] parameters: (1,2,1,2)</p>
    <p>Each agent has one of 3 levels: level-0, level-1, or level-2.</p>
    <p>Distribution of level [2,1,0] agents is [2,1,(11 2)] Each level-k agent makes a mistake with prob k, or best responds to level-(k 1) opponent with prob 1 k.</p>
    <p>Level-k agents believe all opponents are level-(k 1). Level-k agents arent aware that level-(k 1) agents will make mistakes.</p>
    <p>IBRi,0 = Ai,</p>
    <p>IBRi,k = BRi(IBRi,k1),</p>
    <p>Lki,0 (ai) = |Ai| 1,</p>
    <p>Lki,k(ai) =</p>
    <p>{ (1 k)/|IBRi,k| if ai  IBRi,k, k/(|Ai| |IBRi,k|) otherwise.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>BGT model: Cognitive hierarchy</p>
    <p>Cognitive hierarchy model [Camerer et al. 2004] parameter: ()</p>
    <p>An agent of level m best responds to the truncated, true distribution of levels from 0 to m1. Poisson-CH: Levels are assumed to have a Poisson distribution with mean .</p>
    <p>PCHi,0 (ai) = |Ai| 1,</p>
    <p>PCHi,m (ai) =</p>
    <p>BRi (PCHi,0:m1)1 if ai  BRi (PCHi,0:m1) , 0 otherwise.</p>
    <p>PCHi,0:m1 =</p>
    <p>m1 `=0</p>
    <p>PCH i,` Pr(Poisson() = `)m1</p>
    <p>`=0 Pr(Poisson() = `)</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>BGT model: QLk</p>
    <p>QLk model [Stahl &amp; Wilson 1994] parameters: (1,2,1,2,1(2))</p>
    <p>Distribution of level [2,1,0] agents is [2,1,(11 2)] Each agent quantally responds to next-lower level.</p>
    <p>Each QLk agent level has its own precision (k), and its own beliefs about lower-level agents precisions (`(k)).</p>
    <p>QLk i,0 (ai) = |Ai|</p>
    <p>1,</p>
    <p>QLk i,1 = QBRi(</p>
    <p>QLk i,0 ,1),</p>
    <p>QLk j,1(2)</p>
    <p>= QBRj( QLk j,0,1(2)),</p>
    <p>QLk i,2 = QBRi(</p>
    <p>QLk i,1(2),2).</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model comparisons: Nash equilibrium vs. BGT</p>
    <p>COMBO9 SW94 SW95 CGCB98 GH01 HSW01 CVH03 HS07 SH08 RPC09</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u ni</p>
    <p>fo rm</p>
    <p>d is</p>
    <p>tri bu</p>
    <p>tio n</p>
    <p>NEE Best BGT</p>
    <p>Worst BGT</p>
    <p>Average NEE virtually always worse than every BGT model (only exception: SW95).</p>
    <p>All NEE significantly worse than best BGT model in most datasets.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model comparisons: Nash equilibrium vs. BGT</p>
    <p>COMBO9 SW94 SW95 CGCB98 GH01 HSW01 CVH03 HS07 SH08 RPC09</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u ni</p>
    <p>fo rm</p>
    <p>d is</p>
    <p>tri bu</p>
    <p>tio n</p>
    <p>NEE Best BGT</p>
    <p>Worst BGT</p>
    <p>Average NEE virtually always worse than every BGT model (only exception: SW95).</p>
    <p>All NEE significantly worse than best BGT model in most datasets.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model comparisons: Lk and CH vs. QRE</p>
    <p>COMBO9 SW94 SW95 CGCB98 GH01 HSW01 CVH03 HS07 SH08 RPC09</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u ni</p>
    <p>fo rm</p>
    <p>d is</p>
    <p>tri bu</p>
    <p>tio n</p>
    <p>Lk Poisson-CH</p>
    <p>Lk and Poisson-CH performance was strikingly similar.</p>
    <p>No consistent ordering between Lk/Poisson-CH and QRE. Iterative strategic reasoning and quantal response appear to capture distinct phenomena.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model comparisons: Lk and CH vs. QRE</p>
    <p>COMBO9 SW94 SW95 CGCB98 GH01 HSW01 CVH03 HS07 SH08 RPC09</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u ni</p>
    <p>fo rm</p>
    <p>d is</p>
    <p>tri bu</p>
    <p>tio n</p>
    <p>Lk Poisson-CH</p>
    <p>QRE</p>
    <p>Lk and Poisson-CH performance was strikingly similar. No consistent ordering between Lk/Poisson-CH and QRE.</p>
    <p>Iterative strategic reasoning and quantal response appear to capture distinct phenomena.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model comparisons: QLk</p>
    <p>COMBO9 SW94 SW95 CGCB98 GH01 HSW01 CVH03 HS07 SH08 RPC09</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u ni</p>
    <p>fo rm</p>
    <p>d is</p>
    <p>tri bu</p>
    <p>tio n</p>
    <p>Lk Poisson-CH</p>
    <p>QRE</p>
    <p>So perhaps a model with both iterative and quantal response components would perform best?</p>
    <p>In fact, on every dataset, QLk is either the best predictive model or very similar to the best.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model comparisons: QLk</p>
    <p>COMBO9 SW94 SW95 CGCB98 GH01 HSW01 CVH03 HS07 SH08 RPC09</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u ni</p>
    <p>fo rm</p>
    <p>d is</p>
    <p>tri bu</p>
    <p>tio n</p>
    <p>Lk Poisson-CH</p>
    <p>QRE QLk</p>
    <p>So perhaps a model with both iterative and quantal response components would perform best? In fact, on every dataset, QLk is either the best predictive model or very similar to the best.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Taking Stock of What We Have Done</p>
    <p>Take-home message so far</p>
    <p>QLk is the best of the models for prediction.</p>
    <p>Question</p>
    <p>How strongly does the data argue for particular parameter values?</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Posterior distributions</p>
    <p>A posterior distribution gives the probability of each possible combination of parameter values, given the data, e.g.:</p>
    <p>Pr(1 = 0.1,2 = 0.3, = 0.1 |D)</p>
    <p>Maximum likelihood only tells us the most likely parameter setting, given the data.</p>
    <p>The posterior distribution over parameter settings describes the relative probability of all possible parameter settings.</p>
    <p>Individual parameters can be analyzed by inspecting the marginal posterior distribution.</p>
    <p>Pr(1 = 0.1 |D) =</p>
    <p>Pr(1 = 0.1,2 =   2, =</p>
    <p>|D)d2d</p>
    <p>Flat distributions indicate less important parameter values. Sharp distributions indicate a high degree of certainty.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Warm-up: Poisson-CH</p>
    <p>Regarding the single parameter () for the Poisson-CH model:</p>
    <p>Indeed, values of  between 1 and 2 explain empirical results for nearly 100 games, suggesting that a  value of 1.5 could give reliable predictions for many other games as well. [Camerer et al. 2004]</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Warm-up: Poisson-CHs Posterior Distribution</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Our analysis gives 99% posterior probability that the best value of  is 0.59 or less.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Refresher: QLks Parameters</p>
    <p>QLk has 5 different parameters:</p>
    <p>1: Proportion of level-1 agents.</p>
    <p>2: Proportion of level-2 agents.</p>
    <p>1: Precision of level-1 agents.</p>
    <p>2: Precision of level-2 agents.</p>
    <p>1(2): Level-2 agents belief about level-1 agents precision.</p>
    <p>QLk i,0 (ai) = |Ai|</p>
    <p>1,</p>
    <p>QLk i,1 = QBRi(</p>
    <p>QLk i,0 ,1),</p>
    <p>QLk j,1(2)</p>
    <p>= QBRj( QLk j,0,1(2)),</p>
    <p>QLk i,2 = QBRi(</p>
    <p>QLk i,1(2),2).</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Posterior distributions: QLk</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Level proportions</p>
    <p>1 2</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Precisions</p>
    <p>1 2</p>
    <p>1(2)</p>
    <p>Some surprises:</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Posterior distributions: QLk</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Level proportions</p>
    <p>1 2</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Precisions</p>
    <p>1 2</p>
    <p>1(2)</p>
    <p>Some surprises:</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Posterior distributions: QLk</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Level proportions</p>
    <p>1 2</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Precisions</p>
    <p>1 2</p>
    <p>1(2)</p>
    <p>Some surprises:</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Maybe QLk isnt quite the right model</p>
    <p>We constructed a family of models by systematically varying QLk: 1 Top level:</p>
    <p>We evaluated all variations leading to  8 parameters.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model variations: Efficient frontier</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u .a</p>
    <p>.r.</p>
    <p>Number of parameters</p>
    <p>Model performance</p>
    <p>ah-QLkp</p>
    <p>gi-QLk2</p>
    <p>ai-QLk2</p>
    <p>gh-QLk2</p>
    <p>ah-QLk2</p>
    <p>gi-QCH2</p>
    <p>ai-QCH2</p>
    <p>gh-QCH2</p>
    <p>ah-QCH2</p>
    <p>gi-QLk3</p>
    <p>ai-QLk3</p>
    <p>ah-QLk3</p>
    <p>gi-QCH3</p>
    <p>ai-QCH3</p>
    <p>ah-QLk4 ah-QLk5 ah-QLk6 ah-QLk7</p>
    <p>ah-QCH6 ah-QCH7 ai-QLk4 ai-QCH4</p>
    <p>gh-QCH3gh-QLk3</p>
    <p>ah-QCHp</p>
    <p>ah-QCH3</p>
    <p>ah-QCH4</p>
    <p>ah-QCH5</p>
    <p>Efficient frontier: best performance for # of parameters.</p>
    <p>QLk (gi-QLk2) is not on the efficient frontier.</p>
    <p>Best models all have accurate precision beliefs, homogeneous precision, cognitive hierarchy population beliefs.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Model variations: Efficient frontier</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u .a</p>
    <p>.r.</p>
    <p>Number of parameters</p>
    <p>Model performance Efficient frontier</p>
    <p>ah-QLkp</p>
    <p>gi-QLk2</p>
    <p>ai-QLk2</p>
    <p>gh-QLk2</p>
    <p>ah-QLk2</p>
    <p>gi-QCH2</p>
    <p>ai-QCH2</p>
    <p>gh-QCH2</p>
    <p>ah-QCH2</p>
    <p>gi-QLk3</p>
    <p>ai-QLk3</p>
    <p>ah-QLk3</p>
    <p>gi-QCH3</p>
    <p>ai-QCH3</p>
    <p>ah-QLk4 ah-QLk5 ah-QLk6 ah-QLk7</p>
    <p>ah-QCH6 ah-QCH7 ai-QLk4 ai-QCH4</p>
    <p>gh-QCH3gh-QLk3</p>
    <p>ah-QCHp</p>
    <p>ah-QCH3</p>
    <p>ah-QCH4</p>
    <p>ah-QCH5</p>
    <p>Efficient frontier: best performance for # of parameters.</p>
    <p>QLk (gi-QLk2) is not on the efficient frontier.</p>
    <p>Best models all have accurate precision beliefs, homogeneous precision, cognitive hierarchy population beliefs.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Thinking back to QLk</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Level proportions</p>
    <p>1 2</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Precisions</p>
    <p>1 2</p>
    <p>1(2)</p>
    <p>Recall...</p>
    <p>1,2: Best fits predict more level-2 agents than level-1.</p>
    <p>1,2: Level-2 agents have lower precision than level-1 agents.</p>
    <p>1, 1(2): Level-2 agents beliefs are very wrong.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>ah-QCH3: Posterior distribution</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Level proportions</p>
    <p>1 2 3</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Precisions</p>
    <p>More robust model: small parameter changes less likely to change prediction quality.</p>
    <p>Smooth, unimodal distributions for level proportions.</p>
    <p>Distribution for  is unimodal, with narrow confidence region</p>
    <p>Still more agents of type 2 than 1.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Marginal distributions comparison</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Proportion of L0 ah-QCHp ah-QCH3 ah-QCH4 ah-QCH5</p>
    <p>Proportion of L1</p>
    <p>Proportion of L2</p>
    <p>Proportion of L3</p>
    <p>Proportion of L4</p>
    <p>Proportion of L5</p>
    <p>Poisson QCH matches tabular L0 proportions very closely. To do so, forced to match most other proportions poorly.</p>
    <p>If L0 were treated specially, could Poisson match others?</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Marginal distributions comparison</p>
    <p>C um</p>
    <p>ul at</p>
    <p>iv e</p>
    <p>pr ob</p>
    <p>ab ili</p>
    <p>ty</p>
    <p>Proportion of L0 ah-QCHp ah-QCH3 ah-QCH4 ah-QCH5</p>
    <p>Proportion of L1</p>
    <p>Proportion of L2</p>
    <p>Proportion of L3</p>
    <p>Proportion of L4</p>
    <p>Proportion of L5</p>
    <p>Poisson QCH matches tabular L0 proportions very closely. To do so, forced to match most other proportions poorly. If L0 were treated specially, could Poisson match others?</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Spike-Poisson model</p>
    <p>Spike-Poisson QCH model parameters: (,,)</p>
    <p>An ah-QCH model with precision .</p>
    <p>Proportion distribution f is a mixture of Poisson distribution and a spike distribution of L0 agents:</p>
    <p>f(m) =</p>
    <p>{  + (1 )Poisson(m;) if m = 0, (1 )Poisson(m;) otherwise.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Spike-Poisson performance</p>
    <p>Li ke</p>
    <p>lih oo</p>
    <p>d im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r u .a</p>
    <p>.r.</p>
    <p>Number of parameters</p>
    <p>Model performance Efficient frontier</p>
    <p>ah-QCHp</p>
    <p>ah-QCH-sp</p>
    <p>ah-QCH2</p>
    <p>ah-QCH3</p>
    <p>gi-QLk2 (QLk)</p>
    <p>ah-QCH4</p>
    <p>ah-QCH5</p>
    <p>Spike-Poisson QCH outperforms all other ah-QCH models except for ah-QCH5.</p>
    <p>Only three parameters, fewer even than ah-QCH3.</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Compared predictive performance of four BGT models.</p>
    <p>BGT models typically predict human behavior better than Nash equilibrium-based model. QLk has best performance of the four.</p>
    <p>Bayesian sensitivity analysis of parameters.</p>
    <p>Parameters for QLk are counterintuitive, hard to identify. Using CH beliefs and a single precision for all agents yields more identifiable parameter values, superior predictive performance.</p>
    <p>Even with fewer parameters!</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Compared predictive performance of four BGT models.</p>
    <p>BGT models typically predict human behavior better than Nash equilibrium-based model. QLk has best performance of the four.</p>
    <p>Bayesian sensitivity analysis of parameters.</p>
    <p>Parameters for QLk are counterintuitive, hard to identify. Using CH beliefs and a single precision for all agents yields more identifiable parameter values, superior predictive performance.</p>
    <p>Even with fewer parameters!</p>
    <p>September 10, 2012: OpLog Kevin Leyton-Brown</p>
  </div>
</Presentation>
