<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Coherence Stalls or Latency Tolerance:</p>
    <p>Informed CPU Scheduling for Socket</p>
    <p>and Core Sharing</p>
    <p>Sharanyan Srikanthan</p>
    <p>Sandhya Dwarkadas</p>
    <p>Kai Shen</p>
    <p>Department of Computer Science</p>
    <p>University of Rochester</p>
  </div>
  <div class="page">
    <p>Performance Transparency Challenge:</p>
    <p>Modern Multi-core Machines</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Memory Controller</p>
    <p>Shared Last Level Cache</p>
    <p>Socket 2</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Shared Last Level Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Memory Controller</p>
  </div>
  <div class="page">
    <p>Performance Transparency Challenge:</p>
    <p>Resource Sharing</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Memory Controller</p>
    <p>Shared Last Level Cache</p>
    <p>Socket 2</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Shared Last Level Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Memory Controller</p>
    <p>Problem: Simultaneous multi-threading</p>
  </div>
  <div class="page">
    <p>Performance Transparency Challenge:</p>
    <p>Resource Sharing</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Memory Controller</p>
    <p>Shared Last Level Cache</p>
    <p>Socket 2</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Shared Last Level Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Memory Controller</p>
    <p>Problem: Intra-processor resource sharing</p>
  </div>
  <div class="page">
    <p>Performance Transparency Challenge:</p>
    <p>Resource Sharing</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Memory Controller</p>
    <p>Shared Last Level Cache</p>
    <p>Socket 2</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Shared Last Level Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Memory Controller</p>
    <p>Problem: Inter-processor resource sharing</p>
  </div>
  <div class="page">
    <p>Performance Transparency Challenge:</p>
    <p>Non-Uniform Access Latencies</p>
    <p>Ref: https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Socket 2</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>Problem: Communication costs a function of thread/data placement</p>
  </div>
  <div class="page">
    <p>Impact of Thread Placement on Data</p>
    <p>Sharing Costs</p>
    <p>Ref: https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Socket 2</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
  </div>
  <div class="page">
    <p>Impact of Thread Placement on Data</p>
    <p>Sharing Costs</p>
    <p>Ref: https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Socket 2</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
  </div>
  <div class="page">
    <p>Impact of Thread Placement on Data</p>
    <p>Sharing Costs</p>
    <p>Ref: https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf</p>
    <p>Inter-processor interconnect</p>
    <p>Socket 1</p>
    <p>Socket 2</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
    <p>CPU</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>CPU</p>
    <p>L1 Cache</p>
    <p>L2 Cache</p>
    <p>..</p>
  </div>
  <div class="page">
    <p>Sharing Aware Mapper (SAM)</p>
    <p>Srikanthan et al. [USENIX ATC 2015]</p>
    <p>Uses low overhead performance counters to</p>
    <p>identify data sharing, resource demand</p>
    <p>Map processes to CPUs to minimize</p>
    <p>Communication cost due to data sharing</p>
    <p>Resource contention</p>
    <p>Memory access latency</p>
  </div>
  <div class="page">
    <p>Sharing Aware Mapper</p>
    <p>(SAM-MPH)</p>
    <p>Remaining challenges:</p>
    <p>Understand impact of coherence: Execution stalls</p>
    <p>or latency tolerance</p>
    <p>Analyze impact of hyper-threading</p>
    <p>Our approach:</p>
    <p>M: Metrics to identify and use latency tolerance for</p>
    <p>data sharing cost to prioritization</p>
    <p>P: Phase detection adds hysteresis to recognize</p>
    <p>and avoid reacting to transient phases</p>
    <p>H: Hyper-threading related cost/benefits</p>
  </div>
  <div class="page">
    <p>Importance of Task Placement</p>
    <p>Micro-benchmark forces data to move from one task to the</p>
    <p>other, generating coherence activity</p>
    <p>Rate of coherence activity varied by varying ratio of private</p>
    <p>to shared variable access</p>
  </div>
  <div class="page">
    <p>Prioritizing Applications</p>
    <p>Coherence Activity?</p>
  </div>
  <div class="page">
    <p>Metrics: SPC and IPC</p>
    <p>SPC  Stalls per inter</p>
    <p>socket coherence activity</p>
    <p>SPC helps identify latency</p>
    <p>hiding capability of</p>
    <p>application</p>
    <p>IPC  Instructions per</p>
    <p>cycle</p>
    <p>IPC helps identify</p>
    <p>computational contention</p>
    <p>on processor core</p>
  </div>
  <div class="page">
    <p>Take Placement:</p>
    <p>Importance of SPC SPC &gt; 550</p>
  </div>
  <div class="page">
    <p>Task Placement:</p>
    <p>Importance of IPC</p>
    <p>IPC &gt; 0.9 IPC &gt; 0.9</p>
  </div>
  <div class="page">
    <p>Prioritizing Applications:</p>
    <p>High Coherence Activity</p>
    <p>High Coherence</p>
    <p>Activity</p>
    <p>SPC, Priority</p>
    <p>SPC ~</p>
    <p>SPC &gt; 550</p>
    <p>Prioritize logical thread</p>
    <p>co-location</p>
    <p>IPC &gt; 0.9</p>
    <p>Avoid co-location on</p>
    <p>hyper-threads even if it</p>
    <p>results in distributing</p>
    <p>across sockets</p>
    <p>Stalls on cache accesses</p>
    <p>Inter-socket coherence activity</p>
  </div>
  <div class="page">
    <p>Prioritizing Applications:</p>
    <p>Moderate Coherence Activity Moderate</p>
    <p>Coherence</p>
    <p>Activity</p>
    <p>IPC, Priority</p>
    <p>IPC &gt; 0.9</p>
    <p>Avoid co-location on</p>
    <p>hyper-threads even if it</p>
    <p>results in distributing</p>
    <p>across sockets</p>
  </div>
  <div class="page">
    <p>Prioritizing Applications:</p>
    <p>Low Coherence Activity Low</p>
    <p>Coherence</p>
    <p>Activity</p>
    <p>No preference to be</p>
    <p>distributed across sockets</p>
    <p>or consolidated within a</p>
    <p>socket</p>
    <p>IPC &gt; 0.9</p>
    <p>Avoid co-locating on</p>
    <p>hyper-threads even if it</p>
    <p>results in distributing</p>
    <p>across sockets</p>
  </div>
  <div class="page">
    <p>Implementation Context</p>
    <p>A B D C</p>
    <p>Select which applications run</p>
    <p>together (Linux Scheduler)</p>
    <p>X</p>
    <p>..</p>
    <p>Map threads to cores</p>
    <p>(Tasks to be scheduled)</p>
    <p>Implemented as a daemon running periodically</p>
    <p>using CPU affinity masks to control placement</p>
  </div>
  <div class="page">
    <p>Monitoring Using Performance</p>
    <p>Counters  5 metrics identified: 8 counter events need to be</p>
    <p>monitored</p>
    <p>Inter-socket coherence activity</p>
    <p>Last level cache misses served by remote cache</p>
    <p>Intra-socket coherence activity</p>
    <p>Last private level cache misses  (sum of hits and misses</p>
    <p>in LLC)</p>
    <p>Stalled cycles on coherence activity</p>
    <p>Local Memory Accesses</p>
    <p>Approximated by LLC misses</p>
    <p>Remote Memory Accesses</p>
    <p>4 hardware programmable counters available: requires</p>
    <p>multiplexing</p>
  </div>
  <div class="page">
    <p>Experimental Environment</p>
    <p>Fedora 19, Linux 3.14.8</p>
    <p>Dual socket machine  IvyBridge processor (40 logical cores, 2.20 GHz)</p>
    <p>Quad socket machine  Haswell processor (80 logical cores, 1.90 GHz)</p>
    <p>Benchmarks</p>
    <p>Microbenchmarks</p>
    <p>SPECCPU 06 (CPU &amp; Memory bound workloads)</p>
    <p>PARSEC 3.0 (Parallel workloads  light on data sharing)</p>
    <p>Machine learning and data mining algorithms like: ALS, Stochastic Gradient Descent, Single Value Decomposition, etc</p>
    <p>Service Oriented  MongoDB</p>
  </div>
  <div class="page">
    <p>Standalone Applications</p>
    <p>Improvement over</p>
    <p>Linux: Mean = 20%</p>
    <p>SAM: Mean = 6%</p>
    <p>Baseline (Normalization factor): Best static mapping determined by exhaustive search</p>
    <p>Dual Socket IvyBridge</p>
  </div>
  <div class="page">
    <p>Multiple Applications Dual Socket IvyBridge</p>
    <p>Improvement over</p>
    <p>Linux: Mean = 27% (Max: 43%)</p>
    <p>SAM: Mean = 9% (Max: 24%)</p>
    <p>Improvement in fairness:</p>
    <p>Linux: Avg min speedup: 0.71, Avg max speedup: 0.84</p>
    <p>SAM: Avg min speedup: 0.86, Avg max speedup: 0.93</p>
    <p>SAM-MPH: Avg min speedup: 0.95, Avg max speedup: 1.0003</p>
  </div>
  <div class="page">
    <p>Standalone Applications Quad Socket Haswell</p>
    <p>Baseline (Normalization factor): Best static mapping determined by exhaustive search</p>
    <p>Improvement over</p>
    <p>Linux: Mean = 45%</p>
    <p>SAM: Mean = 3%</p>
  </div>
  <div class="page">
    <p>Multiple Applications Quad Socket Haswell</p>
    <p>Improvement over</p>
    <p>Linux: Mean = 43% (Max: 61%)</p>
    <p>SAM: Mean = 21% (Max: 27%)</p>
    <p>Improvement in fairness:</p>
    <p>Linux: Avg min speedup: 0.57, Avg max speedup: 0.79</p>
    <p>SAM: Avg min speedup: 0.73, Avg max speedup: 0.82</p>
    <p>SAM-MPH: Avg min speedup: 0.89, Avg max speedup: 0.99</p>
  </div>
  <div class="page">
    <p>Overheads and Scaling</p>
    <p>Overall overhead (40 cores)</p>
    <p>Performance counter reading  Invoked every tick  1msec  8.9 s per tick</p>
    <p>Constant time overhead</p>
    <p>Data consolidation and decision making is centralized</p>
    <p>Data consolidation  Demon invoked every 100ms</p>
    <p>SAM-MPH: 230us (worst case), 14us (best case)</p>
    <p>SAM: 9.9us</p>
    <p>Decision making  Invoked every 100ms  negligible time related to data</p>
    <p>consolidation</p>
    <p>O(n2) complexity, time spent is within measurement error</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Information from performance counters is sufficient to</p>
    <p>Identify and prioritize latency intolerant applications</p>
    <p>Separate data sharing from resource contention</p>
    <p>Significant contributors to improving performance:</p>
    <p>Minimizing expensive communication due to data</p>
    <p>sharing</p>
    <p>Identifying impact of data sharing on performance to</p>
    <p>prioritize applications</p>
    <p>Identifying resource contention within the core,</p>
    <p>outside of the core, and outside of the chip</p>
  </div>
  <div class="page">
    <p>Thank you</p>
    <p>Questions?</p>
    <p>Coherence Stalls or Latency Tolerance:</p>
    <p>Informed CPU Scheduling for Socket and</p>
    <p>Core Sharing</p>
    <p>Sharanyan Srikanthan</p>
    <p>Sandhya Dwarkadas</p>
    <p>Kai Shen</p>
    <p>Department of Computer Science</p>
    <p>University of Rochester</p>
  </div>
</Presentation>
