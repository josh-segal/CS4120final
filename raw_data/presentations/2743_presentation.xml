<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Democratically Finding The Cause of Packet Drops</p>
    <p>Behnaz Arzani, Selim Ciraci, Luiz Chamon, Yibo Zhu, Hongqiang (Harry) Liu, Jitu Padhye,</p>
    <p>Geoff Outhred, Boon Thau Loo</p>
  </div>
  <div class="page">
    <p>The ultimate goal of network diagnosis: Find the cause of every packet drop</p>
    <p>Sherlock- SigComm 2007</p>
    <p>Netclinic- VAST 2010</p>
    <p>Netprofiler- P2Psys 2005</p>
    <p>Marple- SigComm 2017</p>
    <p>In this talk I will show how to: Find the cause of every TCP packet drop*</p>
    <p>*As long as it is not caused by noise</p>
  </div>
  <div class="page">
    <p>Not all faults are the same</p>
    <p>Associate failed links with</p>
    <p>problems they cause</p>
    <p>High drop rate</p>
    <p>lower drop rate</p>
    <p>My connections to service X are</p>
    <p>failing</p>
  </div>
  <div class="page">
    <p>Mapping complaints to faulty links</p>
    <p>But operators dont always know where the failures are either</p>
  </div>
  <div class="page">
    <p>Clouds operate at massive scales</p>
    <p>Each Data center has millions of devices</p>
  </div>
  <div class="page">
    <p>Low congestion drop rates add noise</p>
    <p>One-off, transient, drops do occur on many links and add noise to diagnosis*</p>
    <p>One-off packet drop</p>
    <p>Fault: Systemic causes of packet drops whether transient or not</p>
  </div>
  <div class="page">
    <p>Noise: One-off packet drop due to buffer overflows</p>
    <p>Fault: Systemic causes of packet drops whether transient or not</p>
  </div>
  <div class="page">
    <p>Talk outline  Solution requirements  A strawman solution and why its impractical  The 007 solution  Design  How it finds the cause of every TCP flows drops  Theoretical guarantees</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Solution Requirements  Detect short-lived failures  Detect concurrent failures  Robust to noise</p>
  </div>
  <div class="page">
    <p>Want to avoid infrastructure changes</p>
    <p>Costly to implement and maintain  Sometimes not even an option  Example: changes to flow destinations (not in the DC)</p>
  </div>
  <div class="page">
    <p>A strawman solution  Suppose  we knew the path of all flows  we knew of every packet drop</p>
    <p>Tomography can find where failures are</p>
    <p>If we assume there are enough flows</p>
  </div>
  <div class="page">
    <p>Example of doing tomography</p>
    <p>Only solvable if we have N independent equations N = number of links in the network</p>
  </div>
  <div class="page">
    <p>Tomography is not always practical</p>
    <p>Theoretical challenges Engineering challenges</p>
    <p>Set of equations doesn't fully specify a solution  Number of active flows may not be sufficient  Becomes NP hard</p>
    <p>Many approximate solutions  MAX_COVERAGE (PathDump-OSDI 2016)  They are sensitive to noise</p>
  </div>
  <div class="page">
    <p>Assume small number of failed links</p>
    <p>AND</p>
    <p>Fate Sharing across flows</p>
  </div>
  <div class="page">
    <p>Tomography is not always practical</p>
    <p>Finding path of all flows is hard  Pre-compute paths</p>
    <p>ECMP changes with every reboot/link failure  Hard to keep track of these changes</p>
    <p>Traceroute (TCP)  ICMP messages use up switch CPU  NATs and Software load balancers</p>
    <p>Infrastructure changes  Labeling packets, adding metadata  Costly</p>
    <p>Engineering challenges</p>
  </div>
  <div class="page">
    <p>We show in this work  Simple traceroute-based solution  Minimal overhead on switches  Tractable (not NP hard)  Resilient to noise  No infrastructure changes (host based app)</p>
    <p>We prove its accurate</p>
  </div>
  <div class="page">
    <p>We can fix problems with traceroute</p>
    <p>Overhead on switch CPU  Only find paths of flows with packet drops  Limit number of traceroutes from each host  Explicit rules on the switch to limit responses</p>
    <p>NATs and Software load balancer  See paper for details</p>
  </div>
  <div class="page">
    <p>How the system works</p>
    <p>Monitoring agent: Deployed on all hosts</p>
    <p>Notified of each TCP retransmission (ETW) Path discovery agent finds the path of the failed flows</p>
    <p>Flows vote on the status of links</p>
    <p>Votes: if you dont know who to blame just blame everyone!</p>
  </div>
  <div class="page">
    <p>How the system works</p>
  </div>
  <div class="page">
    <p>Can diagnose TCP flows  Using votes to compare drop rates  For each flow we know the links involved  Link with most votes most likely cause of drops</p>
    <p>Assume small number of failed links and fate sharing across flows</p>
  </div>
  <div class="page">
    <p>Attractive features of 007  Resilient to noise  Intuitive and easy to implement  Requires no changes to the network</p>
  </div>
  <div class="page">
    <p>We give theoretical guarantees</p>
    <p>We ensure minimal impact on switch CPU  Theorem bounding number of traceroutes</p>
    <p>We prove the voting scheme is 100% accurate when the noise is bounded  Depends on the network topology and failure</p>
    <p>drop rate</p>
  </div>
  <div class="page">
    <p>Questions to answer in evaluation</p>
    <p>Does 007 work in practice?  Capture the right path for each flow?  Find the cause of drops for each flow correctly?</p>
    <p>Are votes a good indicator of packet drop rate?  What level of noise can 007 tolerate?  What level of traffic skew can 007 tolerate?</p>
  </div>
  <div class="page">
    <p>Two month deployment  Types of problems found in production:  Software bugs  FCS errors  Route flaps  Switch reconfigurations</p>
    <p>YES</p>
    <p>Does 007 work in practice</p>
    <p>YES</p>
  </div>
  <div class="page">
    <p>Are votes correlated with drops?</p>
    <p>A ccuracy</p>
    <p>Drop rate 1% Drop rate 0.1% Drop rate 0.05%</p>
  </div>
  <div class="page">
    <p>Are votes correlated with drops?</p>
    <p>Test cluster (we know ground truth)</p>
    <p>False positive</p>
  </div>
  <div class="page">
    <p>Comparison to MAX_COVERAGE</p>
    <p>MAX_COVERAGE (PathDump- OSDI 2016)  Approximate solution to a binary optimization  See 007 extended version for proof  Highly sensitive to noise</p>
    <p>Integer optimization  Improvement on the binary optimization approach  Reduces sensitivity to noise</p>
  </div>
  <div class="page">
    <p>Binary optimization underperforms  Clos topology  2 pods  4000 links</p>
    <p>Drop rates between 0.01%-1% uniform at random  Noise uniformly at random between 0-0.0001%</p>
    <p>%</p>
  </div>
  <div class="page">
    <p>Is 007 robust to noise?</p>
  </div>
  <div class="page">
    <p>Skewed traffic causes problems</p>
    <p>We dont care about this particular case, because The failure isnt impacting any traffic</p>
    <p>But what if it had?</p>
  </div>
  <div class="page">
    <p>Is 007 impacted by traffic skew?</p>
    <p>More simulation results in the paper</p>
  </div>
  <div class="page">
    <p>Conclusion  007: simple voting scheme  Finds cause of problems for each flow  Allows operators to prioritize fixes  Analytically proven to be accurate  Contained at the end host as an application  No changes to the network or destinations</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Adi Aditya  Alec Wolman  Andreas Haeberlen  Ang Chen  Deepal Dhariwal  Ishai Menache  Jiaxin Cao  Monia Ghobadi</p>
    <p>Mina Tahmasbi  Omid AlipourFard  Stefan Saroiu  Trevor Adams</p>
  </div>
  <div class="page">
    <p>An example closer to home</p>
  </div>
  <div class="page">
    <p>Guaranteed Accurate</p>
    <p>Theorem: For Vigil will rank with probability the bad links that drop packets with probability higher than all good links that drop packets with probability if</p>
    <p>where is the total number of connections between hosts, and are lower and upper bounds, respectively, on the number of packets per connection.</p>
  </div>
  <div class="page">
    <p>Minimal impact on switch CPU</p>
    <p>Theorem: The rate of ICMP packets generated by any switch due to a traceroute is below if the rate at which hosts trigger traceroutes is upper bounded as</p>
    <p>Where are the number of ToR, T1 , and T2 switches respectively and is the number of hosts under each ToR.</p>
    <p>n0, n1, n2</p>
  </div>
  <div class="page">
    <p>Failures are complicated</p>
  </div>
  <div class="page">
    <p>We can now prioritize fixes  We can answer questions like:  Why are connections to storage failing?  What is causing problems for SQL connections?  Why do I have bad throughput to a.b.c.d?</p>
  </div>
  <div class="page">
    <p>An example closer to home</p>
  </div>
  <div class="page">
    <p>More than finding a few failed links</p>
  </div>
  <div class="page">
    <p>Past solutions dont help</p>
    <p>Dont allow for always on monitoring  Pingmesh [SIGCOMM-15]  EverFlow [SIGCOMM-15]  TRAT [SIGCOMM-02]  Other Tomography work</p>
    <p>Require changes to network/remote hosts  Marple [SIGCOMM-17]  PathDump [OSDI-16]  Link-based anomaly detection [NSDI-17]</p>
  </div>
  <div class="page">
    <p>Finding paths is also hard  Infrastructure changes are costly  DSCP bit reserved for other tasks  Cannot deploy any changes on the destination end-point</p>
    <p>Reverse engineering ECMP also difficult  Can get the ECMP functions from vendors  Seed changes with every reboot/link failure  Hard to keep track of these changes</p>
    <p>Only option left: Traceroute  ICMP messages use up switch CPU  We cannot find the path of all flows Problem is not always fully specified  Approximate solutions are NP hard  And the approach is sensitive to noise 42</p>
  </div>
  <div class="page">
    <p>Our Solution</p>
    <p>It detects retransmissions as soon as they happen</p>
  </div>
  <div class="page">
    <p>Mapping DIPs to VIPs</p>
    <p>Connections are to Virtual IPs  SYN packets go to a Software Load Balancer (SLB)  The host gets configured with a physical IP  All other packets in the connections use the physical IP</p>
    <p>Traceroute packets must use the physical IP</p>
  </div>
  <div class="page">
    <p>An evaluation with skewed traffic</p>
    <p>Traffic concentrated in one part of network  Extreme example: most flows go to one ToR  Small fraction of traffic goes over failed links  Votes can become skewed  We call this a hot ToR scenario</p>
  </div>
  <div class="page">
    <p>Our Solution</p>
    <p>It detects retransmissions as soon as they happen</p>
  </div>
  <div class="page">
    <p>Observation</p>
    <p>Data gathered using the monitoring agent of NetPoirot Uses ETW to get notifications of TCP retransmissions</p>
  </div>
  <div class="page">
    <p>If path of all flows was known</p>
    <p>Given TCP statistics for existing flows  We know the paths that have problems  Without having to send any probe traffic  Without having to rely on packet captures</p>
    <p>We can also find the failed links</p>
  </div>
  <div class="page">
    <p>We can now prioritize fixes  We can answer questions like:  Why are connections to storage failing?  What is causing problems for SQL connections?  Why do I have bad throughput to a.b.c.d?</p>
    <p>Just one catch:  Needs to know retransmissions  Ok for infrastructure traffic (e.g. storage)  See paper on how to extend to VM traffic</p>
  </div>
  <div class="page">
    <p>SLB</p>
    <p>Get the DIP to VIP mapping from SLBSend traceroute like packetsEach connection votes on the status of links good links get a vote of 0</p>
  </div>
  <div class="page">
    <p>Where in the network?</p>
  </div>
  <div class="page">
    <p>Holding the network accountable</p>
    <p>Given impacted application find links responsible  Allows us to prioritize fixes</p>
    <p>Given a failed device quantify its impact  Estimate cost of failures in customer impact</p>
  </div>
  <div class="page">
    <p>Failures are hard to diagnose</p>
    <p>High CPU load High I/O load</p>
    <p>Reboots Software bugs</p>
    <p>BGP link flaps FCS errors</p>
    <p>misconfigurations Switch Reboots</p>
    <p>Congestion Hardware bug</p>
    <p>+ Millions of devices</p>
    <p>Bad design Software bugs</p>
    <p>High CPU usage High memory usage</p>
  </div>
</Presentation>
