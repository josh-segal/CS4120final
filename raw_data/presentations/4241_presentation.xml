<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>NP</p>
    <p>D N</p>
    <p>a</p>
    <p>NP</p>
    <p>D N</p>
    <p>NP</p>
    <p>D N</p>
    <p>a D N</p>
    <p>a</p>
    <p>NP</p>
    <p>D N NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>hit</p>
    <p>a phosphor</p>
    <p>V</p>
    <p>hit</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>a</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>a</p>
    <p>NP</p>
    <p>D</p>
    <p>VP</p>
    <p>V</p>
    <p>N</p>
    <p>a</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>hit</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>hit NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>hit</p>
    <p>NP</p>
    <p>VP</p>
    <p>V NP</p>
    <p>VP</p>
    <p>V</p>
    <p>hit</p>
    <p>phosphor</p>
    <p>phosphor phosphor</p>
    <p>phosphor</p>
    <p>phosphor phosphor</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>o</p>
    <p>o</p>
    <p>o o</p>
    <p>)x(</p>
    <p>)x( )x(</p>
    <p>)x( )(o</p>
    <p>)(o</p>
    <p>)(o )(o</p>
    <p>ROOT</p>
    <p>SBARQ</p>
    <p>WHADVP</p>
    <p>WRB</p>
    <p>When</p>
    <p>S</p>
    <p>VP</p>
    <p>VBN</p>
    <p>hit</p>
    <p>PP</p>
    <p>IN</p>
    <p>by</p>
    <p>NP</p>
    <p>NNS</p>
    <p>electrons</p>
    <p>,</p>
    <p>,</p>
    <p>NP</p>
    <p>DT</p>
    <p>a</p>
    <p>NN</p>
    <p>phosphor</p>
    <p>VP</p>
    <p>VBZ</p>
    <p>gives</p>
    <p>PRP</p>
    <p>RP</p>
    <p>off</p>
    <p>NP</p>
    <p>NP</p>
    <p>JJ</p>
    <p>electromagnetic</p>
    <p>NN</p>
    <p>energy</p>
    <p>PP</p>
    <p>IN</p>
    <p>in</p>
    <p>NP</p>
    <p>DT</p>
    <p>this</p>
    <p>NN</p>
    <p>form</p>
    <p>ROOT</p>
    <p>SBARQ</p>
    <p>WHADVP</p>
    <p>WRB</p>
    <p>When</p>
    <p>S</p>
    <p>VP</p>
    <p>VBN</p>
    <p>hit</p>
    <p>PP</p>
    <p>IN</p>
    <p>by</p>
    <p>NP</p>
    <p>NNS</p>
    <p>electrons</p>
    <p>,</p>
    <p>,</p>
    <p>NP</p>
    <p>DT</p>
    <p>a</p>
    <p>NN</p>
    <p>phosphor</p>
    <p>VP</p>
    <p>VBZ</p>
    <p>gives</p>
    <p>PRP</p>
    <p>RP</p>
    <p>off</p>
    <p>NP</p>
    <p>NP</p>
    <p>JJ</p>
    <p>electromagnetic</p>
    <p>NN</p>
    <p>energy</p>
    <p>PP</p>
    <p>IN</p>
    <p>in</p>
    <p>NP</p>
    <p>DT</p>
    <p>this</p>
    <p>NN</p>
    <p>form</p>
    <p>PAS</p>
    <p>A0</p>
    <p>electrons</p>
    <p>predicate</p>
    <p>hit</p>
    <p>AM</p>
    <p>When</p>
    <p>PAS</p>
    <p>A0</p>
    <p>a phosphor</p>
    <p>predicate</p>
    <p>give off</p>
    <p>A1</p>
    <p>energy</p>
    <p>AM</p>
    <p>in this form</p>
    <p>PAS</p>
    <p>A0</p>
    <p>electrons</p>
    <p>predicate</p>
    <p>hit</p>
    <p>A1</p>
    <p>it</p>
    <p>AM</p>
    <p>if</p>
    <p>PAS</p>
    <p>A0</p>
    <p>a phosphor</p>
    <p>predicate</p>
    <p>give off</p>
    <p>A1</p>
    <p>photons</p>
  </div>
  <div class="page">
    <p>Outline: Part I  Kernel Machines</p>
    <p>! Motivation (5 min)</p>
    <p>! Kernel Machines (20 min) ! Perceptron</p>
    <p>! Support Vector Machines</p>
    <p>! Kernel Definition (Kernel Trick)</p>
    <p>! Mercer's Conditions</p>
    <p>! Kernel Operators</p>
    <p>! Efficiency issue: when can we use kernels?</p>
  </div>
  <div class="page">
    <p>Outline: Part I  Basic Kernels</p>
    <p>! Basic Kernels and their Feature Spaces (25 min) ! Linear Kernels</p>
    <p>! Polynomial Kernels</p>
    <p>! Lexical Semantic Kernels</p>
    <p>! String and Word Sequence Kernels</p>
    <p>! Syntactic Tree Kernel, Partial Tree kernel (PTK), Semantic Syntactic Tree Kernel, Smoothed PTK</p>
  </div>
  <div class="page">
    <p>Outline: Part II  Applications with Simple Kernels</p>
    <p>! NLP applications with simple kernels (25 min) ! Question Classification in TREC</p>
    <p>! Cue Classification in Jeopardy!</p>
    <p>! Semantic Role Labeling (SRL): FrameNet and PropBank</p>
    <p>! Relation Extraction: ACE</p>
    <p>! Coreference Resolution</p>
  </div>
  <div class="page">
    <p>Outline: Part II  Joint Kernel Models</p>
    <p>! Reranking for (12 min) ! Preference kernel framework</p>
    <p>! Concept Segmentation and Classification of speech</p>
    <p>! Named-Entity Recognition</p>
    <p>! Predicate Argument Structures</p>
    <p>! Relational Kernels (13 min) ! Recognizing Textual Entailment</p>
    <p>! Answer Reranking</p>
  </div>
  <div class="page">
    <p>Outline: Part II  Advanced Topics</p>
    <p>! Fast learning and classification approaches (10 min) ! Cutting Plane Algorithm for SVMs</p>
    <p>! Sampling methods (uSVMs)</p>
    <p>! Compacting space with DAGs</p>
    <p>! Reverse Kernel Engineering (10 min) ! Model linearization</p>
    <p>! Semantic Role Labeling</p>
    <p>! Question Classification</p>
    <p>! Conclusions and Future Research (5 min)</p>
  </div>
  <div class="page">
    <p>Motivation (1)</p>
    <p>! Feature design most difficult aspect in designing a</p>
    <p>learning system</p>
    <p>! complex and difficult phase, e.g., structural feature</p>
    <p>representation:</p>
    <p>! deep knowledge and intuitions are required</p>
    <p>! design problems when the phenomenon is described</p>
    <p>by many features</p>
  </div>
  <div class="page">
    <p>Motivation (2)</p>
    <p>! Kernel methods alleviate such problems</p>
    <p>! Structures represented in terms of substructures</p>
    <p>! High dimensional feature spaces</p>
    <p>! Implicit and abstract feature spaces</p>
    <p>! Generate high number of features</p>
    <p>! Support Vector Machines select the relevant features</p>
    <p>! Automatic feature engineering side-effect</p>
  </div>
  <div class="page">
    <p>Motivation (3)</p>
    <p>! High accuracy especially for new applications and new</p>
    <p>domains</p>
    <p>! Manual engineering still poor, e.g. arabic SRL</p>
    <p>! Inherent higher accuracy when many structural patterns</p>
    <p>are needed, e.g. Relation Extraction</p>
    <p>! Fast prototyping and adaptation for new domains and</p>
    <p>applications</p>
    <p>! The major contribution of kernels is to make easier system</p>
    <p>modeling:</p>
  </div>
  <div class="page">
    <p>Part I: Kernel Machines</p>
  </div>
  <div class="page">
    <p>Classification Problem (on text)</p>
    <p>! Given: ! a set of target categories:</p>
    <p>! the set T of documents,</p>
    <p>define</p>
    <p>f : T  2C</p>
    <p>! VSM (Salton89)</p>
    <p>! Features are dimensions of a Vector Space.</p>
    <p>! Documents and Categories are vectors of feature weights.</p>
    <p>! d is assigned to if</p>
    <p>d   C i &gt; th</p>
    <p>C = C1,..,Cn{ }</p>
    <p>iC</p>
  </div>
  <div class="page">
    <p>More in detail</p>
    <p>! In Text Categorization documents are word</p>
    <p>vectors</p>
    <p>! The dot product counts the number of</p>
    <p>features in common</p>
    <p>! This provides a sort of similarity</p>
    <p>(dx ) =  x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)</p>
    <p>buy acquisition stocks sell market</p>
    <p>zx</p>
    <p>(dz ) =  z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)</p>
    <p>buy company stocks sell</p>
  </div>
  <div class="page">
    <p>Linear Classifier</p>
    <p>f (  x ) =  x   w + b = 0,</p>
    <p>x ,  w  n ,b</p>
    <p>! The equation of a hyperplane is</p>
    <p>! is the vector representing the classifying example</p>
    <p>! is the gradient of the hyperplane</p>
    <p>! The classification function is</p>
    <p>x</p>
    <p>w</p>
    <p>( ) sign( ( ))h x f x=</p>
  </div>
  <div class="page">
    <p>! Mapping vectors in a space where they are linearly separable,</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>o</p>
    <p>o</p>
    <p>o o</p>
    <p>The main idea of Kernel Functions</p>
    <p>)(xx</p>
    <p>)x(</p>
    <p>)x( )x(</p>
    <p>)x( )(o</p>
    <p>)(o</p>
    <p>)(o )(o</p>
  </div>
  <div class="page">
    <p>A kernel-based Machine: Perceptron training</p>
    <p>w 0</p>
    <p>0 ;b0  0;k  0;R  max1il ||</p>
    <p>x i ||</p>
    <p>do for i = 1 to  if yi (</p>
    <p>w k   x i + bk )  0 then</p>
    <p>w k +1 =</p>
    <p>w k + yi</p>
    <p>x i</p>
    <p>bk +1 = bk + yiR 2</p>
    <p>k = k + 1 endif endfor while an error is found return k,(</p>
    <p>w k ,bk )</p>
  </div>
  <div class="page">
    <p>Graphic interpretation of the Perceptron 476 A. Moschitti</p>
    <p>w</p>
    <p>|||| w b</p>
    <p>A</p>
    <p>ix w</p>
    <p>B</p>
    <p>ix</p>
    <p>ii xy! w</p>
    <p>ii xyw !+</p>
    <p>w</p>
    <p>C</p>
    <p>ix iik xyw !+</p>
    <p>||||</p>
    <p>iik</p>
    <p>ik</p>
    <p>xyw Ryb</p>
    <p>! !</p>
    <p>+ +</p>
    <p>Fig. 10. Perceptron algorithm process</p>
    <p>Since the sign of the contribution xi is given by yi, !i is positive and is proportional (through the &quot; factor) to the number of times that xi is incorrectly classified. Difficult points that cause many mistakes will be associated with large !i.</p>
    <p>It is interesting to note that, if we fix the training set S, we can use the !i as alternative coordinates of a dual space to represent the target hypothesis associated with w. The resulting decision function is the following:</p>
    <p>h(x) = sgn(w  x + b) = sgn !&quot; m#</p>
    <p>i=1</p>
    <p>!iyixi $  x + b</p>
    <p>% =</p>
    <p>= sgn</p>
    <p>! m#</p>
    <p>i=1</p>
    <p>!iyi(xi  x) + b %</p>
    <p>(11)</p>
    <p>Given the dual representation, we can adopt a learning algorithm that works in the dual space described in Table 3.</p>
    <p>Note that as the Novikoffs theorem states that the learning rate &quot; only changes the scaling of the hyperplanes, it does not affect the algorithm thus we can set &quot; = 1. On the contrary, if the perceptron algorithm starts with a different initialization, it will find a different separating hyperplane. The reader may wonder if such hyperplanes are all equivalent in terms of the classification accuracy of the test set; the answer is no: different hyperplanes may lead to different error probabilities. In particular, the next</p>
  </div>
  <div class="page">
    <p>! In each step of perceptron only training data is added with a certain weight</p>
    <p>! Hence the classification function results:</p>
    <p>! Note that data only appears in the scalar product</p>
    <p>Dual Representation for Classification</p>
    <p>w =  j</p>
    <p>j =1..  y j</p>
    <p>x j</p>
    <p>sgn(  w   x + b) = sgn  j</p>
    <p>j =1..  y j</p>
    <p>x j   x + b</p>
    <p>%</p>
    <p>&amp; ' '</p>
    <p>(</p>
    <p>) * *</p>
  </div>
  <div class="page">
    <p>Dual Representation for Learning</p>
    <p>! as well as the updating function</p>
    <p>! The learning rate only affects the re-scaling of the hyperplane, it does not affect the algorithm, so we can fix 1. =</p>
    <p>if yi (  j j =1..  y j</p>
    <p>x j   x i + b)  0 then i = i +</p>
  </div>
  <div class="page">
    <p>! We can rewrite the classification function as</p>
    <p>! As well as the updating function</p>
    <p>Dual Perceptron algorithm and Kernel functions</p>
    <p>h(x) = sgn(  w  (</p>
    <p>x ) + b) = sgn(  j</p>
    <p>j =1..  y j(</p>
    <p>x j) (</p>
    <p>x ) + b) =</p>
    <p>= sgn(  j i=1..  y j k(</p>
    <p>x j,  x ) + b)</p>
    <p>if yi  j j =1..  y j k(</p>
    <p>x j ,  x i ) + b</p>
    <p>%</p>
    <p>&amp; ' '</p>
    <p>(</p>
    <p>) * *  0 allora i = i +</p>
  </div>
  <div class="page">
    <p>Support Vector Machines</p>
    <p>Var1</p>
    <p>Var2 kbxw =+</p>
    <p>kbxw =+</p>
    <p>w</p>
    <p>The margin is equal to 2 k w</p>
  </div>
  <div class="page">
    <p>Support Vector Machines</p>
    <p>Var1</p>
    <p>Var2  w   x +b = 1</p>
    <p>w   x +b =1</p>
    <p>w</p>
    <p>The margin is equal to 2 w</p>
    <p>We need to solve</p>
    <p>max 2</p>
    <p>||  w ||</p>
    <p>w   x +b +1, if</p>
    <p>x is positive</p>
    <p>w   x +b  1, if</p>
    <p>x is negative</p>
  </div>
  <div class="page">
    <p>Optimization Problem</p>
    <p>! Optimal Hyperplane: ! Minimize</p>
    <p>! Subject to</p>
    <p>! The dual problem is simpler</p>
    <p>(  w)=</p>
    <p>yi (  w   xi +b)1,i =1,...,l</p>
  </div>
  <div class="page">
    <p>Dual Transformation</p>
    <p>! To solve the dual problem we need to evaluate:</p>
    <p>! Given the Lagrangian associated with our problem</p>
    <p>! Let us impose the derivatives to 0, with respect to</p>
    <p>w</p>
    <p>The above conditions can be applied to evaluate the maximal margin classifier, i.e. the Problem 2.13, but the general approach is to transform Problem 2.13 in an equivalent problem, simpler to solve. The output of such transformation is called dual problem and it is described by the following definition.</p>
    <p>Def. 2.24 Let f ( ~w), hi( ~w) and gi( ~w) be the objective function, the equality constraints and the inequality constraints (i.e. ) of an optimization problem, and let L( ~w, ~, ~) be its Lagrangian, defined as follows:</p>
    <p>L( ~w, ~, ~) = f ( ~w) + mX</p>
    <p>i=1</p>
    <p>igi( ~w) + lX</p>
    <p>i=1</p>
    <p>ihi( ~w)</p>
    <p>The Lagrangian dual problem of the above primal problem is</p>
    <p>maximize (~, ~)</p>
    <p>subject to ~  ~0</p>
    <p>where (~, ~) = infw2W L( ~w, ~, ~)</p>
    <p>The strong duality theorem assures that an optimal solution of the dual is also the optimal solution for the primal problem and vice versa, thus, we can focus on the transformation of Problem 2.13 according to the Definition 2.24.</p>
    <p>First, we observe that the only constraints in Problem 2.13 are the inequalities gi( ~w) = [yi( ~w  ~xi + b)  1 8~xi 2 S].</p>
    <p>Second, the objective function is ~w  ~w. Consequently, the primal Lagrangian4 is</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1], (2.17)</p>
    <p>where i are the Lagrange multipliers and b is the extra variable associated with the threshold.</p>
    <p>Third, to evaluate (~, ~) = infw2W L( ~w, ~, ~), we can find the minimum of the Lagrangian by setting the partial derivatives to 0.</p>
    <p>@L( ~w, b, ~) @ ~w</p>
    <p>= ~w  mX</p>
    <p>i=1</p>
    <p>yii~xi = ~0 ) ~w = mX</p>
    <p>i=1</p>
    <p>yii~xi (2.18)</p>
    <p>~w  ~w is the same optimization function from a solution point of view, we use the 1</p>
    <p>The above conditions can be applied to evaluate the maximal margin classifier, i.e. the Problem 2.13, but the general approach is to transform Problem 2.13 in an equivalent problem, simpler to solve. The output of such transformation is called dual problem and it is described by the following definition.</p>
    <p>Def. 2.24 Let f ( ~w), hi( ~w) and gi( ~w) be the objective function, the equality constraints and the inequality constraints (i.e. ) of an optimization problem, and let L( ~w, ~, ~) be its Lagrangian, defined as follows:</p>
    <p>L( ~w, ~, ~) = f ( ~w) + mX</p>
    <p>i=1</p>
    <p>igi( ~w) + lX</p>
    <p>i=1</p>
    <p>ihi( ~w)</p>
    <p>The Lagrangian dual problem of the above primal problem is</p>
    <p>maximize (~, ~)</p>
    <p>subject to ~  ~0</p>
    <p>where (~, ~) = infw2W L( ~w, ~, ~)</p>
    <p>The strong duality theorem assures that an optimal solution of the dual is also the optimal solution for the primal problem and vice versa, thus, we can focus on the transformation of Problem 2.13 according to the Definition 2.24.</p>
    <p>First, we observe that the only constraints in Problem 2.13 are the inequalities gi( ~w) = [yi( ~w  ~xi + b)  1 8~xi 2 S].</p>
    <p>Second, the objective function is ~w  ~w. Consequently, the primal Lagrangian4 is</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1], (2.17)</p>
    <p>where i are the Lagrange multipliers and b is the extra variable associated with the threshold.</p>
    <p>Third, to evaluate (~, ~) = infw2W L( ~w, ~, ~), we can find the minimum of the Lagrangian by setting the partial derivatives to 0.</p>
    <p>@L( ~w, b, ~) @ ~w</p>
    <p>= ~w  mX</p>
    <p>i=1</p>
    <p>yii~xi = ~0 ) ~w = mX</p>
    <p>i=1</p>
    <p>yii~xi (2.18)</p>
    <p>~w  ~w is the same optimization function from a solution point of view, we use the 1</p>
    <p>The above conditions can be applied to evaluate the maximal margin classifier, i.e. the Problem 2.13, but the general approach is to transform Problem 2.13 in an equivalent problem, simpler to solve. The output of such transformation is called dual problem and it is described by the following definition.</p>
    <p>Def. 2.24 Let f ( ~w), hi( ~w) and gi( ~w) be the objective function, the equality constraints and the inequality constraints (i.e. ) of an optimization problem, and let L( ~w, ~, ~) be its Lagrangian, defined as follows:</p>
    <p>L( ~w, ~, ~) = f ( ~w) + mX</p>
    <p>i=1</p>
    <p>igi( ~w) + lX</p>
    <p>i=1</p>
    <p>ihi( ~w)</p>
    <p>The Lagrangian dual problem of the above primal problem is</p>
    <p>maximize (~, ~)</p>
    <p>subject to ~  ~0</p>
    <p>where (~, ~) = infw2W L( ~w, ~, ~)</p>
    <p>The strong duality theorem assures that an optimal solution of the dual is also the optimal solution for the primal problem and vice versa, thus, we can focus on the transformation of Problem 2.13 according to the Definition 2.24.</p>
    <p>First, we observe that the only constraints in Problem 2.13 are the inequalities gi( ~w) = [yi( ~w  ~xi + b)  1 8~xi 2 S].</p>
    <p>Second, the objective function is ~w  ~w. Consequently, the primal Lagrangian4 is</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1], (2.17)</p>
    <p>where i are the Lagrange multipliers and b is the extra variable associated with the threshold.</p>
    <p>Third, to evaluate (~, ~) = infw2W L( ~w, ~, ~), we can find the minimum of the Lagrangian by setting the partial derivatives to 0.</p>
    <p>@L( ~w, b, ~) @ ~w</p>
    <p>= ~w  mX</p>
    <p>i=1</p>
    <p>yii~xi = ~0 ) ~w = mX</p>
    <p>i=1</p>
    <p>yii~xi (2.18)</p>
    <p>~w  ~w is the same optimization function from a solution point of view, we use the 1</p>
  </div>
  <div class="page">
    <p>Dual Transformation (contd)</p>
    <p>! and wrt b</p>
    <p>! Then we substituted them in the objective function</p>
    <p>@L( ~w, b, ~) @b</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>yii = 0 (2.19)</p>
    <p>Finally, by substituting Eq. 2.18 and 2.19 into the primal Lagrangian we obtain</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1] =</p>
    <p>= 1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj  mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj + mX</p>
    <p>i=1</p>
    <p>i</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>(2.20) which according to the Definition 2.24 is the optimization function of the dual problem subject to i  0. In summary, the final dual optimization problem is the following:</p>
    <p>maximize mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>subject to i  0, i = 1, .., m mX</p>
    <p>i=1</p>
    <p>yii = 0</p>
    <p>where ~w = Pm</p>
    <p>i=1 yii~xi and the Pm</p>
    <p>i=1 yii = 0 are the relation derived from Eq. 2.18 and 2.19. Other conditions establishing interesting properties can be derived by the Khun-Tucker theorem. This provides the following relations for an optimal solution:</p>
    <p>@L( ~w, ~, ~) @ ~w</p>
    <p>= ~0</p>
    <p>@L( ~w, ~, ~) @ ~</p>
    <p>= ~0</p>
    <p>i gi( ~w ) = 0, i = 1, .., m</p>
    <p>gi( ~w)  0, i = 1, .., m i  0, i = 1, .., m</p>
    <p>@L( ~w, b, ~) @b</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>yii = 0 (2.19)</p>
    <p>Finally, by substituting Eq. 2.18 and 2.19 into the primal Lagrangian we obtain</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1] =</p>
    <p>= 1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj  mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj + mX</p>
    <p>i=1</p>
    <p>i</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>(2.20) which according to the Definition 2.24 is the optimization function of the dual problem subject to i  0. In summary, the final dual optimization problem is the following:</p>
    <p>maximize mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>subject to i  0, i = 1, .., m mX</p>
    <p>i=1</p>
    <p>yii = 0</p>
    <p>where ~w = Pm</p>
    <p>i=1 yii~xi and the Pm</p>
    <p>i=1 yii = 0 are the relation derived from Eq. 2.18 and 2.19. Other conditions establishing interesting properties can be derived by the Khun-Tucker theorem. This provides the following relations for an optimal solution:</p>
    <p>@L( ~w, ~, ~) @ ~w</p>
    <p>= ~0</p>
    <p>@L( ~w, ~, ~) @ ~</p>
    <p>= ~0</p>
    <p>i gi( ~w ) = 0, i = 1, .., m</p>
    <p>gi( ~w)  0, i = 1, .., m i  0, i = 1, .., m</p>
  </div>
  <div class="page">
    <p>The Final Dual Optimization Problem</p>
    <p>@L( ~w, b, ~) @b</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>yii = 0 (2.19)</p>
    <p>Finally, by substituting Eq. 2.18 and 2.19 into the primal Lagrangian we obtain</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1] =</p>
    <p>= 1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj  mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj + mX</p>
    <p>i=1</p>
    <p>i</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>(2.20) which according to the Definition 2.24 is the optimization function of the dual problem subject to i  0. In summary, the final dual optimization problem is the following:</p>
    <p>maximize mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>subject to i  0, i = 1, .., m mX</p>
    <p>i=1</p>
    <p>yii = 0</p>
    <p>where ~w = Pm</p>
    <p>i=1 yii~xi and the Pm</p>
    <p>i=1 yii = 0 are the relation derived from Eq. 2.18 and 2.19. Other conditions establishing interesting properties can be derived by the Khun-Tucker theorem. This provides the following relations for an optimal solution:</p>
    <p>@L( ~w, ~, ~) @ ~w</p>
    <p>= ~0</p>
    <p>@L( ~w, ~, ~) @ ~</p>
    <p>= ~0</p>
    <p>i gi( ~w ) = 0, i = 1, .., m</p>
    <p>gi( ~w)  0, i = 1, .., m i  0, i = 1, .., m</p>
  </div>
  <div class="page">
    <p>Soft Margin optimization problem</p>
    <p>respect to ~w, ~ and b:</p>
    <p>@L( ~w, b, ~, ~) @ ~w</p>
    <p>= ~w  mX</p>
    <p>i=1</p>
    <p>yii~xi = ~0 ) ~w = mX</p>
    <p>i=1</p>
    <p>yii~xi</p>
    <p>@L( ~w, b, ~, ~) @~</p>
    <p>= C~  ~ = ~0</p>
    <p>@L( ~w, b, ~, ~) @b</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>yii = 0</p>
    <p>(2.23)</p>
    <p>By substituting the above relations into the primal, we obtain the following dual objective function:</p>
    <p>w(~) = mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj + 1</p>
    <p>~  ~ =</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj  1</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij  ~xi  ~xj +</p>
    <p>ij  ,</p>
    <p>(2.24) where ij = 1 if i = j and 0 otherwise (Kroneckers delta). The objective</p>
    <p>function is subject to the usual constraints: (</p>
    <p>i  0, 8i = 1, .., mPm i=1 yii = 0</p>
    <p>The above dual can be used to find a solution of Problem 2.21, which extends the applicability of linear functions to classification problems not completely linearly separable. The separability property relates not only on the available class of hypotheses, e.g. linear vs. polynomial functions, but it strictly depends on the adopted features. Their roles is to provide a map between the example data and vectors in Rn. Given such mapping, the scalar product provides a measure of the similarity between pairs of examples or, according to a colder interpretation, it provides a partitioning function based on such features.</p>
    <p>The next Section shows that, it is possible to substitute the scalar product of two feature vectors with a function between the data examples directly. This allows us to avoid the explicit feature design and consequently enables us to</p>
    <p>respect to ~w, ~ and b:</p>
    <p>@L( ~w, b, ~, ~) @ ~w</p>
    <p>= ~w  mX</p>
    <p>i=1</p>
    <p>yii~xi = ~0 ) ~w = mX</p>
    <p>i=1</p>
    <p>yii~xi</p>
    <p>@L( ~w, b, ~, ~) @~</p>
    <p>= C~  ~ = ~0</p>
    <p>@L( ~w, b, ~, ~) @b</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>yii = 0</p>
    <p>(2.23)</p>
    <p>By substituting the above relations into the primal, we obtain the following dual objective function:</p>
    <p>w(~) = mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj + 1</p>
    <p>~  ~ =</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj  1</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij  ~xi  ~xj +</p>
    <p>ij  ,</p>
    <p>(2.24) where ij = 1 if i = j and 0 otherwise (Kroneckers delta). The objective</p>
    <p>function is subject to the usual constraints: (</p>
    <p>i  0, 8i = 1, .., mPm i=1 yii = 0</p>
    <p>The above dual can be used to find a solution of Problem 2.21, which extends the applicability of linear functions to classification problems not completely linearly separable. The separability property relates not only on the available class of hypotheses, e.g. linear vs. polynomial functions, but it strictly depends on the adopted features. Their roles is to provide a map between the example data and vectors in Rn. Given such mapping, the scalar product provides a measure of the similarity between pairs of examples or, according to a colder interpretation, it provides a partitioning function based on such features.</p>
    <p>The next Section shows that, it is possible to substitute the scalar product of two feature vectors with a function between the data examples directly. This allows us to avoid the explicit feature design and consequently enables us to</p>
    <p>@L( ~w, b, ~) @b</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>yii = 0 (2.19)</p>
    <p>Finally, by substituting Eq. 2.18 and 2.19 into the primal Lagrangian we obtain</p>
    <p>L( ~w, b, ~) = 1 2</p>
    <p>~w  ~w  mX</p>
    <p>i=1</p>
    <p>i[yi( ~w  ~xi + b)  1] =</p>
    <p>= 1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj  mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj + mX</p>
    <p>i=1</p>
    <p>i</p>
    <p>= mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>(2.20) which according to the Definition 2.24 is the optimization function of the dual problem subject to i  0. In summary, the final dual optimization problem is the following:</p>
    <p>maximize mX</p>
    <p>i=1</p>
    <p>i  1 2</p>
    <p>mX</p>
    <p>i,j=1</p>
    <p>yiyj ij ~xi  ~xj</p>
    <p>subject to i  0, i = 1, .., m mX</p>
    <p>i=1</p>
    <p>yii = 0</p>
    <p>where ~w = Pm</p>
    <p>i=1 yii~xi and the Pm</p>
    <p>i=1 yii = 0 are the relation derived from Eq. 2.18 and 2.19. Other conditions establishing interesting properties can be derived by the Khun-Tucker theorem. This provides the following relations for an optimal solution:</p>
    <p>@L( ~w, ~, ~) @ ~w</p>
    <p>= ~0</p>
    <p>@L( ~w, ~, ~) @ ~</p>
    <p>= ~0</p>
    <p>i gi( ~w ) = 0, i = 1, .., m</p>
    <p>gi( ~w)  0, i = 1, .., m i  0, i = 1, .., m</p>
  </div>
  <div class="page">
    <p>Kernels in Support Vector Machines</p>
    <p>! In Soft Margin SVMs we maximize:</p>
    <p>! By using kernel functions we rewrite the problem as:</p>
    <p>! &quot;&quot;&quot;&quot;&quot;&quot;#</p>
    <p>&quot;&quot;&quot;&quot;&quot;&quot;$</p>
    <p>maximize m%</p>
    <p>i=1</p>
    <p>!i ! 1 2</p>
    <p>m%</p>
    <p>i,j=1</p>
    <p>yiyj !i!j &amp; k(oi, oj ) +</p>
    <p>&quot;ij '</p>
    <p>!i &quot; 0, #i = 1, .., m m%</p>
    <p>i=1</p>
    <p>yi!i = 0</p>
    <p>Moreover, Eq. 10 for the Perceptron appears also in the Soft Margin SVMs (see conditions 24), hence we can rewrite the SVM classification function as in Eq. 11 and use a kernel inside it, i.e.:</p>
    <p>h(x) = sgn</p>
    <p>( m%</p>
    <p>i=1</p>
    <p>!iyik(oi, oj ) + b</p>
    <p>)</p>
    <p>The data object o is mapped in the vector space trough a feature extraction procedure # : o $ (x1, ..., xn) = x, more in general, we can map a vector x from one feature space into another one:</p>
    <p>x = (x1, ..., xn) $ !(x) = (#1(x), ..., #n(x))</p>
    <p>This leads to the general definition of kernel functions:</p>
    <p>Definition 10. A kernel is a function k, such that # x,z % X</p>
    <p>k(x, z) = !(x)  !(z)</p>
    <p>where ! is a mapping from X to an (inner product) feature space.</p>
    <p>Note that, once we have defined a kernel function that is effective for a given learning problem, we do not need to find which mapping # corresponds to. It is enough to know that such mapping exists. The following proposition states the conditions that guarantee such existence.</p>
    <p>Proposition 1. (Mercers conditions) Let X be a finite input space and let K(x, z) be a symmetric function on X. Then</p>
    <p>K(x, z) is a kernel function if and only if the matrix</p>
    <p>k(x, z) = !(x)  !(z)</p>
    <p>is positive semi-definite (has non-negative eigenvalues).</p>
    <p>Proof. Let us consider a symmetric function on a finite space X = {x1, x2, ..., xn}</p>
    <p>K = &amp; K(xi, xj )</p>
    <p>'n i,j=1</p>
    <p>Since K is symmetric there is an orthogonal matrix V such that K = V &quot;V !</p>
    <p>where &quot; is a diagonal matrix containing the eigenvalues $t of K, with corresponding</p>
    <p>Automatic Learning Using Kernels Methods 485</p>
    <p>!i</p>
    <p>B) Soft Margin SVM A) Hard Margin SVM</p>
    <p>ix ix</p>
    <p>Fig. 15. Soft Margin vs. Hard Margin hyperplanes</p>
    <p>where !i are Lagrangian multipliers. The dual problem is obtained by imposing stationarity on the derivatives respect to</p>
    <p>w, ! and b:</p>
    <p>&quot;L(w, b, !, &quot;) &quot;w</p>
    <p>= w ! m!</p>
    <p>i=1</p>
    <p>yi!ixi = 0 &quot; w = m!</p>
    <p>i=1</p>
    <p>yi!ixi</p>
    <p>&quot;L(w, b, !, &quot;) &quot;!</p>
    <p>= C! ! &quot; = 0</p>
    <p>&quot;L(w, b, !, &quot;) &quot;b</p>
    <p>= m!</p>
    <p>i=1</p>
    <p>yi!i = 0</p>
    <p>(24)</p>
    <p>By substituting the above relations into the primal, we obtain the following dual objective function:</p>
    <p>w(&quot;) = m!</p>
    <p>i=1</p>
    <p>!i ! 1 2</p>
    <p>m!</p>
    <p>i,j=1</p>
    <p>yiyj!i!j xi  xj + 1</p>
    <p>&quot;  &quot; =</p>
    <p>= m!</p>
    <p>i=1</p>
    <p>!i ! 1 2</p>
    <p>m!</p>
    <p>i,j=1</p>
    <p>yiyj!i!j xi  xj ! 1</p>
    <p>= m!</p>
    <p>i=1</p>
    <p>!i ! 1 2</p>
    <p>m!</p>
    <p>i,j=1</p>
    <p>yiyj!i!j &quot; xi  xj +</p>
    <p>#ij # ,</p>
    <p>(25)</p>
    <p>where the Kroneckers delta, #ij = 1 if i = j and 0 otherwise. The objective function above is subject to the usual constraints:</p>
    <p>$ !i # 0, $i = 1, .., m%m</p>
    <p>i=1 yi!i = 0</p>
    <p>This dual formulation can be used to find a solution of Problem 22, which extends the applicability of linear functions to classification problems not completely linearly separable. The separability property relates not only to the available class of hypotheses, e.g. linear vs. polynomial functions, but it strictly depends on the adopted features. Their</p>
  </div>
  <div class="page">
    <p>Soft Margin Support Vector Machines</p>
    <p>! The algorithm tries to keep i low and maximize the margin</p>
    <p>! NB: the number of error is not directly minimized (NP-complete</p>
    <p>problem); the distances from the hyperplane are minimized</p>
    <p>! If C, the solution tends to the one of the hard-margin algorithm ! If C increases the number of error decreases. When C tends to infinite</p>
    <p>the number of errors must be 0, i.e. the hard-margin formulation</p>
    <p>min 1 2 ||  w ||2 +C ii</p>
    <p>yi (  w   x i + b) 1i</p>
    <p>x i</p>
    <p>i  0</p>
  </div>
  <div class="page">
    <p>Trade-off between Generalization and Empirical Error</p>
    <p>i</p>
    <p>Var1</p>
    <p>Var2 0=+ bxw</p>
    <p>i</p>
    <p>Var1</p>
    <p>Var2 0=+ bxw</p>
    <p>Soft Margin SVM Hard Margin SVM</p>
  </div>
  <div class="page">
    <p>Parameters</p>
    <p>! C: trade-off parameter</p>
    <p>! J: cost factor</p>
    <p>min 1</p>
    <p>i i</p>
    <p>= min 1</p>
    <p>+  i</p>
    <p>i</p>
    <p>+</p>
    <p>+C</p>
    <p>i</p>
    <p>i</p>
    <p>= min 1</p>
    <p>i i</p>
    <p>+</p>
    <p>+  i</p>
    <p>i</p>
    <p>( )</p>
  </div>
  <div class="page">
    <p>Kernel Function Definition</p>
    <p>! Kernels are the product of mapping functions</p>
    <p>such as</p>
    <p>x  n ,</p>
    <p>(  x ) = (1(</p>
    <p>x ),2 (</p>
    <p>x ),...,m (</p>
    <p>x ))  m</p>
    <p>Def. 2.26 A kernel is a function k, such that 8 ~x,~z 2 X</p>
    <p>k(~x, ~z) = (~x)  (~z)</p>
    <p>where  is a mapping from X to an (inner product) feature space.</p>
    <p>Note that, once we have defined a kernel function that is effective for a given learning problem, we do not need to find which mapping  it corresponds to. It is enough to know that such mapping exists. The following proposition states the conditions that guaranteed such existence.</p>
    <p>Proposition 2.27 (Mercers conditions) Let X be a finite input space with K(~x, ~z) a symmetric function on X. Then K(~x, ~z) is a kernel function if and only if the matrix</p>
    <p>k(~x, ~z) = (~x)  (~z)</p>
    <p>is positive semi-definite (has non-negative eigenvalues).</p>
    <p>The proof of such proposition is the following (from [Cristianini and ShaweTaylor, 2000]). Let us consider a symmetric function on a finite space X = {x1, x2, ..., xn}</p>
    <p>K =  K(xi, xj )</p>
    <p>n i,j=1</p>
    <p>Since K is symmetric there is an orthogonal matrix V such that K = V V 0 where  is a diagonal matrix containing the eigenvalues t of K, with corresponding eigenvectors ~vt = (vti)ni=1, i.e. the columns of V . Now assume that all the eigenvalues are non-negatives and consider the feature mapping:</p>
    <p>: ~xi ! p</p>
    <p>tvti n t=1</p>
    <p>We now have that,</p>
    <p>(~xi)  (~xj ) = nX</p>
    <p>t=1</p>
    <p>tvtivtj = (V V 0)ij = Kij = K(xi, xj ).</p>
    <p>This proves that K(~x, ~z) is a valid kernel function that corresponds to the mapping . Therefore, the only requirement to derive the mapping  is that the eigenvalues of K are non-negatives since if we had a negative eigenvalue s associated with the eigenvector ~vs, the point</p>
    <p>~z = nX</p>
    <p>i=1</p>
    <p>vsi(~xi) = p</p>
    <p>V 0~vs.</p>
  </div>
  <div class="page">
    <p>The Kernel Gram Matrix</p>
    <p>! With KM-based learning, the sole information used from the training data set is the Kernel Gram Matrix</p>
    <p>! If the kernel is valid, K is symmetric definite-positive</p>
    <p>=</p>
    <p>),(...),(),( ............</p>
    <p>),(...),(),( ),(...),(),(</p>
    <p>mmmm</p>
    <p>m</p>
    <p>m</p>
    <p>training</p>
    <p>kkk</p>
    <p>kkk kkk</p>
    <p>K</p>
    <p>xxxxxx</p>
    <p>xxxxxx xxxxxx</p>
  </div>
  <div class="page">
    <p>Valid Kernels</p>
    <p>VI Appendix B. Basic Geometry and Algebraic Concepts</p>
    <p>For example:</p>
    <p>(x, x2) = Z 1</p>
    <p>h1 0</p>
    <p>x4</p>
    <p>The four properties required in Def. B.6 follow immediately from the analogous property of the definite integral:</p>
    <p>(f + h, g) = Z 1</p>
    <p>Z 1</p>
    <p>= Z 1</p>
    <p>Z 1</p>
    <p>Example B.8 The classical scalar product in Rn is the component-wise product</p>
    <p>(u1, u2, .., un)(v1, v2, .., vn) = (u1v1, u2v2, .., unvn)</p>
    <p>We recall that cos(~u, ~v) =</p>
    <p>(~u, ~v) ||~u||  ||~v||</p>
    <p>B.2 Matrixes</p>
    <p>Def. B.9 Transposed Matrix Given a matrix A 2 Rm  Rn of m rows and n columns, we indicate with A0 2 Rn  Rm its transposed, i.e. Aij = A0ji for i = 1, .., m and j = 1, .., n.</p>
    <p>Def. B.10 Diagonal Matrix Given a matrix A 2 Rm  Rn, A is a diagonal matrix iff Aij = 0 for i 6= j i = 1, .., m and j = 1, .., n.</p>
    <p>Def. B.11 Eigen Values Given a matrix A 2 Rm  Rn, an egeinvalue  and an egeinvector ~x 2 Rn  {~0} are such that</p>
    <p>A~x = ~x</p>
    <p>Def. B.12 Symmetric Matrix A square matrix A 2 RnRn is symmetric iff Aij = Aji for i 6= j i = 1, .., m and j = 1, .., n, i.e. iff A = A0. B.2. Matrixes VII</p>
    <p>Def. B.13 Positive (Semi-) definite Matrix A square matrix A 2 Rn  Rn is said to be positive (semi-) definite if its eigenvalues are all positive (non-negative).</p>
    <p>Proposition B.14 Let A be a symmetric matrix. Then A is positive (semi-) definite iff for any vector ~x 6= 0</p>
    <p>~x0A~x &gt; ~x ( 0).</p>
    <p>From the previous proposition it follows that: If we find a decomposition A in M 0M , then A is semi-definite positive matrix as</p>
    <p>~x0A~x = ~x0M 0M ~x = (M ~x)0(M ~x) = M ~x  M ~x = ||M ~x||2  0.</p>
    <p>Theorem B.15 Schur Decomposition, (Real Values) Every square real matrix A is orthogonally similar to an upper block triangular matrix D: A = Q0DQ where each block of D is either a 1#1 matrix or a 2#2 matrix having complex conjugate eigenvalues. D is diagonal iff A is symmetric.</p>
  </div>
  <div class="page">
    <p>Valid Kernels contd</p>
    <p>! If the matrix is positive semi-definite then we can find a mapping  implementing the kernel function</p>
    <p>Note that, once we have defined a kernel function that is effective for a given learning problem, we do not need to find which mapping  corresponds to. It is enough to know that such mapping exists. The following proposition states the conditions that guarantee such existence.</p>
    <p>Proposition 1. (Mercers conditions) Let X be a finite input space and let K(x, z) be a symmetric function on X. Then</p>
    <p>K(x, z) is a kernel function if and only if the matrix</p>
    <p>k(x, z) = (x)  (z)</p>
    <p>is positive semi-definite (has non-negative eigenvalues).</p>
    <p>Proof. Let us consider a symmetric function on a finite space X = {x1, x2, ..., xn}</p>
    <p>K =  K(x</p>
    <p>i</p>
    <p>, x j</p>
    <p>)  n</p>
    <p>i,j=1</p>
    <p>Since K is symmetric there is an orthogonal matrix V such that K = V V 0</p>
    <p>where  is a diagonal matrix containing the eigenvalues  t</p>
    <p>of K, with corresponding eigenvectors v</p>
    <p>t</p>
    <p>= (v ti</p>
    <p>)n i=1, i.e., the columns of V . Now assume that all the eigenvalues</p>
    <p>are non-negatives and consider the feature mapping:</p>
    <p>: x i</p>
    <p>! p</p>
    <p>t</p>
    <p>v ti</p>
    <p>n</p>
    <p>t=1 2 Rn, i = 1, .., n.</p>
    <p>It follows that</p>
    <p>(x i</p>
    <p>)  (x j</p>
    <p>) = nX</p>
    <p>t=1</p>
    <p>t</p>
    <p>v ti</p>
    <p>v tj</p>
    <p>= (V V 0) ij</p>
    <p>= K ij</p>
    <p>= K(x i</p>
    <p>, x j</p>
    <p>).</p>
    <p>This proves that K(x, z) is a valid kernel function that corresponds to the mapping . Therefore, the only requirement to derive the mapping  is that the eigenvalues of K are non-negatives since if we had a negative eigenvalue</p>
    <p>s</p>
    <p>associated with the eigenvector v</p>
    <p>s</p>
    <p>, the point</p>
    <p>z = nX</p>
    <p>i=1</p>
    <p>v</p>
    <p>si</p>
    <p>(x i</p>
    <p>) = p</p>
    <p>V</p>
    <p>s</p>
    <p>.</p>
    <p>in the feature space would have norm squared</p>
    <p>||z||2 = z  z = v0 s</p>
    <p>V</p>
    <p>p</p>
    <p>p V</p>
    <p>s</p>
    <p>= v0 s</p>
    <p>V V</p>
    <p>s</p>
    <p>= v0 s</p>
    <p>Kv</p>
    <p>s</p>
    <p>=  s</p>
    <p>&lt; 0,</p>
    <p>which contradicts the geometry of the space [20].</p>
    <p>The above section has shown that kernel functions can be used to map a vector space in other spaces in which the target classification problem becomes linearly separable (or in general easier). Another advantage is the possibility to map the initial feature space</p>
  </div>
  <div class="page">
    <p>Mercers Theorem (finite space)</p>
    <p>! Let us consider</p>
    <p>K = K(  x i,  x j )( )i, j =1</p>
    <p>n</p>
    <p>! K symmetric   V: for Takagi factorization of a complex-symmetric matrix, where:</p>
    <p>!  is the diagonal matrix of the eigenvalues t of K</p>
    <p>! are the eigenvectors, i.e. the columns of V</p>
    <p>! Let us assume lambda values non-negative</p>
    <p>K = V # V</p>
    <p>v t = vti( )i =1</p>
    <p>n</p>
    <p>:  x i  t vti( )t =1</p>
    <p>n  n , i = 1,.., n</p>
  </div>
  <div class="page">
    <p>Mercers Theorem (sufficient conditions)</p>
    <p>(  x i ) (</p>
    <p>x j ) = tvti</p>
    <p>t =1</p>
    <p>n</p>
    <p>vtj = V ' V ( )ij = K ij = K (  x i,  x j )</p>
    <p>! Therefore</p>
    <p>,</p>
    <p>! which implies that K is a kernel function</p>
  </div>
  <div class="page">
    <p>Mercers Theorem (necessary conditions)</p>
    <p>z 2 =</p>
    <p>z   z =  $ V</p>
    <p>v s  $ V</p>
    <p>v s =  v s' V   $ V</p>
    <p>v s =</p>
    <p>v s' K  v s =</p>
    <p>v s' s  v s = s</p>
    <p>v s</p>
    <p>! Suppose we have negative eigenvalues s and eigenvectors the following point</p>
    <p>! has the following norm:</p>
    <p>this contradicts the geometry of the space.</p>
    <p>v s</p>
    <p>z = vsi (</p>
    <p>x i )</p>
    <p>i=1</p>
    <p>n</p>
    <p>= vsi t vti( )t = i=1</p>
    <p>n</p>
    <p>&amp; V  v s</p>
  </div>
  <div class="page">
    <p>Is it a valid kernel?</p>
    <p>! It may not be a kernel so we can use MM</p>
    <p>B.2. Matrixes VII</p>
    <p>Def. B.13 Positive (Semi-) definite Matrix A square matrix A 2 Rn  Rn is said to be positive (semi-) definite if its eigenvalues are all positive (non-negative).</p>
    <p>Proposition B.14 Let A be a symmetric matrix. Then A is positive (semi-) definite iff for any vector ~x 6= 0</p>
    <p>~x0A~x &gt; ~x ( 0).</p>
    <p>From the previous proposition it follows that: If we find a decomposition A in M 0M , then A is semi-definite positive matrix as</p>
    <p>~x0A~x = ~x0M 0M ~x = (M ~x)0(M ~x) = M ~x  M ~x = ||M ~x||2  0.</p>
    <p>Theorem B.15 Schur Decomposition, (Real Values) Every square real matrix A is orthogonally similar to an upper block triangular matrix D: A = Q0DQ where each block of D is either a 1#1 matrix or a 2#2 matrix having complex conjugate eigenvalues. D is diagonal iff A is symmetric.</p>
  </div>
  <div class="page">
    <p>Valid Kernel operations</p>
    <p>! k(x,z) = k1(x,z)+k2(x,z)</p>
    <p>! k(x,z) = k1(x,z)*k2(x,z)</p>
    <p>! k(x,z) =  k1(x,z)</p>
    <p>! k(x,z) = f(x)f(z)</p>
    <p>! k(x,z) = x'Bz</p>
    <p>! k(x,z) = k1((x),(z))</p>
  </div>
  <div class="page">
    <p>Object Transformation [Moschitti et al, CLJ 2008]</p>
    <p>!</p>
    <p>! Canonical Mapping, M() ! object transformation,</p>
    <p>! e. g., a syntactic parse tree into a verb subcategorization frame tree.</p>
    <p>! Feature Extraction, E() ! maps the canonical structure in all its fragments</p>
    <p>! different fragment spaces, e.g. String and Tree Kernels</p>
    <p>),()()(</p>
    <p>))(())(()()(),( 2121</p>
    <p>SSKSS OOOOOOK</p>
    <p>EEE</p>
    <p>MEME</p>
    <p>== ==</p>
  </div>
  <div class="page">
    <p>Part I: Basic Kernels (Feature Extraction Functions)</p>
  </div>
  <div class="page">
    <p>Basic Kernels for unstructured data</p>
    <p>! Linear Kernel</p>
    <p>! Polynomial Kernel</p>
    <p>! Lexical kernel</p>
    <p>! String Kernel</p>
    <p>! Tree Kernels: Subtree, Syntactic, Partial Tree</p>
    <p>Kernels (PTK), and Smoothed PTK</p>
  </div>
  <div class="page">
    <p>Linear Kernel</p>
    <p>! In Text Categorization documents are word</p>
    <p>vectors</p>
    <p>! The dot product counts the number of</p>
    <p>features in common</p>
    <p>! This provides a sort of similarity</p>
    <p>(dx ) =  x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)</p>
    <p>buy acquisition stocks sell market</p>
    <p>zx</p>
    <p>(dz ) =  z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)</p>
    <p>buy company stocks sell</p>
  </div>
  <div class="page">
    <p>Feature Conjunction (polynomial Kernel)</p>
    <p>! The initial vectors are mapped in a higher space</p>
    <p>! More expressive, as encodes</p>
    <p>Stock+Market vs. Downtown+Market features</p>
    <p>! We can smartly compute the scalar product as</p>
    <p>)1,2,2,2,,(),( 2121 2 2</p>
    <p>),()1()1( 1222</p>
    <p>)1,2,2,2,,()1,2,2,2,,( )()(</p>
    <p>zxKzxzxzx zxzxzzxxzxzx</p>
    <p>zzzzzzxxxxxx zx</p>
    <p>Poly</p>
    <p>=+=++= =+++++=</p>
    <p>== =</p>
    <p>)( 21xx</p>
  </div>
  <div class="page">
    <p>Sub-hierarchies in WordNet</p>
  </div>
  <div class="page">
    <p>Similarity based on WordNet</p>
    <p>Inverted Path Length:</p>
    <p>simIP L(c1, c2) = 1</p>
    <p>(1 + d(c1, c2))</p>
    <p>Wu &amp; Palmer:</p>
    <p>simW U P (c1, c2) =</p>
    <p>Resnik: simRES (c1, c2) =  log P (lso(c1, c2))</p>
    <p>Lin:</p>
    <p>simLIN (c1, c2) = 2 log P (lso(c1, c2))</p>
    <p>log P (c1) + log P (c2)</p>
    <p>Table 1. Measures of semantic similarity.</p>
    <p>The formal description of semantic kernels requires the introduction of some definitions. We denote terms as t1, t2, . . . 2 T and concepts as c1, c2, . . . 2 C; we also sometimes use the somewhat informal disambiguation operator c() to map terms to concept representations. To compute useful notions of semantic similarity among the input terms, we employ semantic reference structures which we call, for simplicity, Semantic Networks. These can be seen as directed graphs semantically linking concepts by means of taxonomic relations (e.g. [cat] is-a [mammal] ). Research in Computational Linguistics has led to a variety of wellknown measures of semantic similarity in semantic networks.</p>
    <p>The measures relevant in the context of this paper are summarized in table 1. These measures make use of several notions. (i) The distance (d) of two concepts c1 and c2, is the number of superconcept edges between c1 and c2. (ii) The depth (dep) of a concept refers to the distance of the concept to the unique root node3. (iii) The lowest super ordinate (lso) of two concepts refers to the concept with maximal depth that subsumes them both. (iv) The probability P (c) of encountering a concept c which can be estimated from corpus statistics. When probabilities are used, the measures follow the trail of information theory in quantifying the information concept (IC) of an observation as the negative log likelihood. We point the interested reader to [17] for a detailed and recent survey of the field.</p>
  </div>
  <div class="page">
    <p>Document Similarity</p>
    <p>industry</p>
    <p>telephone</p>
    <p>market</p>
    <p>company</p>
    <p>product</p>
    <p>Doc 1 Doc 2</p>
  </div>
  <div class="page">
    <p>Lexical Semantic Kernels</p>
    <p>! The document similarity is the SK function:</p>
    <p>! where s is any similarity function between words, e.g. WordNet [Basili et al.,2005] similarity or LSA [Cristianini et al., 2002]</p>
    <p>! Good results when training data is small</p>
    <p>SK(d1,d2) = s(w1,w2) w1 d1,w2 d2</p>
  </div>
  <div class="page">
    <p>String Kernel</p>
    <p>! Given two strings, the number of matches</p>
    <p>between their substrings is evaluated</p>
    <p>! E.g. Bank and Rank</p>
    <p>! B, a, n, k, Ba, Ban, Bank, Bk, an, ank, nk,..</p>
    <p>! R, a , n , k, Ra, Ran, Rank, Rk, an, ank, nk,..</p>
    <p>! String kernel over sentences and texts</p>
    <p>! Huge space but there are efficient algorithms</p>
  </div>
  <div class="page">
    <p>Using character sequences</p>
    <p>zx</p>
    <p>(&quot;bank&quot;) =  x = (0,..,1,..,0,..,1,..,0,......1,..,0,..,1,..,0,..,1,..,0)</p>
    <p>! counts the number of common substrings</p>
    <p>bank ank bnk bk b</p>
    <p>(&quot;rank&quot;) =  z = (1,..,0,..,0,..,1,..,0,......0,..,1,..,0,..,1,..,0,..,1)</p>
    <p>rank ank rnk rk r</p>
    <p>x   z = (&quot;bank&quot;) (&quot;rank&quot;) = k(&quot;bank&quot;,&quot;rank&quot;)</p>
  </div>
  <div class="page">
    <p>Formal Definition</p>
    <p>, where</p>
    <p>, where</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
    <p>for some   1. These features measure the number of occurrences of subsequences in the string s weighting them according to their lengths. Hence, the inner product of the feature vectors for two strings s and t give a sum over all common subsequences weighted according to their frequency of occurrences and lengths, i.e.</p>
    <p>K(s, t) = X</p>
    <p>u2 u(s)  u(t) =</p>
    <p>X</p>
    <p>u2</p>
    <p>X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I)</p>
    <p>X</p>
    <p>~J:u=t[ ~J]</p>
    <p>l( ~J) =</p>
    <p>= X</p>
    <p>u2</p>
    <p>X</p>
    <p>~I:u=s[~I]</p>
    <p>X</p>
    <p>~J:u=t[ ~J]</p>
    <p>l( ~I)+l( ~J) (2.26)</p>
    <p>The above equation defines a class of similarity functions known as string kernels or sequence kernels. These functions are interesting for text categorization as it allows the learning algorithm to quantify the matching between two different words, phrases, sentences or whole documents. For example, given two strings, Bank and Rank:</p>
    <p>B, a, n, k, Ba, Ban, Bank, an, ank, nk, Bn, Bnk, Bk and ak are the substrings of Bank.</p>
    <p>R, a, n, k, Ra, Ran, Rank, an, ank, nk, Rn, Rnk, Rk and ak are the substrings of Rank.</p>
    <p>Such substrings are the features in the  that have non-null weights. These are evaluated by means of Eq. 2.25, e.g. B(Bank) = (i1i1+1) = (11+1) = , k(Bank) = (i1i1+1) = (44+1) = , an(Bank) = (i2i1+1) = (32+1) = 2 and Bk(Bank) = (i2i1+1) = (41+1) = 4.</p>
    <p>Since Eq. 2.26 requires that the substrings in Bank and Rank match, we need to evaluate Eq. 2.25 only for the common substrings, i.e.:</p>
    <p>- a(Bank) = a(Rank) = (i1i1+1) = (22+1) = ,</p>
    <p>- n(Bank) = n(Rank) = (i1i1+1) = (33+1) = ,</p>
    <p>- k(Bank) = k(Rank) = (i1i1+1) = (44+1) = ,</p>
    <p>- an(Bank) = an(Rank) = (i1i2+1) = (32+1) = 2,</p>
    <p>- ank(Bank) = ank(Rank) = (i1i3+1) = (42+1) = 3,</p>
    <p>for some   1. These features measure the number of occurrences of subsequences in the string s weighting them according to their lengths. Hence, the inner product of the feature vectors for two strings s and t give a sum over all common subsequences weighted according to their frequency of occurrences and lengths, i.e.</p>
    <p>K(s, t) = X</p>
    <p>u2 u(s)  u(t) =</p>
    <p>X</p>
    <p>u2</p>
    <p>X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I)</p>
    <p>X</p>
    <p>~J:u=t[ ~J]</p>
    <p>l( ~J) =</p>
    <p>= X</p>
    <p>u2</p>
    <p>X</p>
    <p>~I:u=s[~I]</p>
    <p>X</p>
    <p>~J:u=t[ ~J]</p>
    <p>l( ~I)+l( ~J) (2.26)</p>
    <p>The above equation defines a class of similarity functions known as string kernels or sequence kernels. These functions are interesting for text categorization as it allows the learning algorithm to quantify the matching between two different words, phrases, sentences or whole documents. For example, given two strings, Bank and Rank:</p>
    <p>B, a, n, k, Ba, Ban, Bank, an, ank, nk, Bn, Bnk, Bk and ak are the substrings of Bank.</p>
    <p>R, a, n, k, Ra, Ran, Rank, an, ank, nk, Rn, Rnk, Rk and ak are the substrings of Rank.</p>
    <p>Such substrings are the features in the  that have non-null weights. These are evaluated by means of Eq. 2.25, e.g. B(Bank) = (i1i1+1) = (11+1) = , k(Bank) = (i1i1+1) = (44+1) = , an(Bank) = (i2i1+1) = (32+1) = 2 and Bk(Bank) = (i2i1+1) = (41+1) = 4.</p>
    <p>Since Eq. 2.26 requires that the substrings in Bank and Rank match, we need to evaluate Eq. 2.25 only for the common substrings, i.e.:</p>
    <p>- a(Bank) = a(Rank) = (i1i1+1) = (22+1) = ,</p>
    <p>- n(Bank) = n(Rank) = (i1i1+1) = (33+1) = ,</p>
    <p>- k(Bank) = k(Rank) = (i1i1+1) = (44+1) = ,</p>
    <p>- an(Bank) = an(Rank) = (i1i2+1) = (32+1) = 2,</p>
    <p>- ank(Bank) = ank(Rank) = (i1i3+1) = (42+1) = 3,</p>
    <p>two:</p>
    <p>(~x  ~z + c)2 =  nX</p>
    <p>i=1</p>
    <p>xizi + c 2 =</p>
    <p>nX</p>
    <p>i=1</p>
    <p>xizi + c  nX</p>
    <p>j=1</p>
    <p>xizi + c</p>
    <p>=</p>
    <p>= nX</p>
    <p>i=1</p>
    <p>nX</p>
    <p>j=1</p>
    <p>xixj zizj + 2c nX</p>
    <p>i=1</p>
    <p>xizi + c2 =</p>
    <p>= X</p>
    <p>i,j2{1,..,n}</p>
    <p>(xixj )(zizj ) + 2c nX</p>
    <p>i=1</p>
    <p>p 2cxi</p>
    <p>p 2czi</p>
    <p>+ c2</p>
    <p>Note that the second summation introduces n individual features (i.e. xi) whose weights are controlled by the parameter c which also determines the strength of the degree 0. Thus, we add (n+1) new features to the</p>
    <p>n+1</p>
    <p>features</p>
    <p>of the previous kernel of degree 2. If we consider a generic degree d, i.e. the kernel (~x  ~z + c)d, we will obtain</p>
    <p>n+d1</p>
    <p>d</p>
    <p>+ n + d  1 =</p>
    <p>n+d</p>
    <p>d</p>
    <p>distinct</p>
    <p>features (which have at least distinct weights). These are all monomials up to and including the degree d.</p>
    <p>Kernel functions can be applied also to discrete space. As a first example, we show their potentiality on the space of finite strings.</p>
    <p>Let  be a finite alphabet. A string is a finite sequence of characters from , including the empty sequence. For string s and t we denote by |s| the length of the string s = s1, .., s|s|, and by st the string obtained by concatenating the string s and t. The string s[i : j] is the substring si, .., sj of s. We say that u is a subsequence of s, if there exist indices ~I = (i1, ..., i|u|), with 1  i1 &lt; ... &lt; i|u|  |s|, such that uj = si</p>
    <p>j</p>
    <p>, for j = 1, ..., |u|, or u = s[~I] for short. The length l(~I) of the subsequence in s is i|u|  ii + 1. We denote by  the set of all string</p>
    <p>= 1[</p>
    <p>n=0</p>
    <p>n</p>
    <p>We now define the feature space, F = {u1, u2..} = , i.e. the space of all possible substrings. We map a string s in R1 space as follows:</p>
    <p>u(s) = X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I) (2.25)</p>
  </div>
  <div class="page">
    <p>Kernel between Bank and Rank</p>
    <p>for some   1. These features measure the number of occurrences of subsequences in the string s weighting them according to their lengths. Hence, the inner product of the feature vectors for two strings s and t give a sum over all common subsequences weighted according to their frequency of occurrences and lengths, i.e.</p>
    <p>K(s, t) = X</p>
    <p>u2 u(s)  u(t) =</p>
    <p>X</p>
    <p>u2</p>
    <p>X</p>
    <p>~I:u=s[~I]</p>
    <p>l( ~I)</p>
    <p>X</p>
    <p>~J:u=t[ ~J]</p>
    <p>l( ~J) =</p>
    <p>= X</p>
    <p>u2</p>
    <p>X</p>
    <p>~I:u=s[~I]</p>
    <p>X</p>
    <p>~J:u=t[ ~J]</p>
    <p>l( ~I)+l( ~J) (2.26)</p>
    <p>The above equation defines a class of similarity functions known as string kernels or sequence kernels. These functions are interesting for text categorization as it allows the learning algorithm to quantify the matching between two different words, phrases, sentences or whole documents. For example, given two strings, Bank and Rank:</p>
    <p>B, a, n, k, Ba, Ban, Bank, an, ank, nk, Bn, Bnk, Bk and ak are the substrings of Bank.</p>
    <p>R, a, n, k, Ra, Ran, Rank, an, ank, nk, Rn, Rnk, Rk and ak are the substrings of Rank.</p>
    <p>Such substrings are the features in the  that have non-null weights. These are evaluated by means of Eq. 2.25, e.g. B(Bank) = (i1i1+1) = (11+1) = , k(Bank) = (i1i1+1) = (44+1) = , an(Bank) = (i2i1+1) = (32+1) = 2 and Bk(Bank) = (i2i1+1) = (41+1) = 4.</p>
    <p>Since Eq. 2.26 requires that the substrings in Bank and Rank match, we need to evaluate Eq. 2.25 only for the common substrings, i.e.:</p>
    <p>- a(Bank) = a(Rank) = (i1i1+1) = (22+1) = ,</p>
    <p>- n(Bank) = n(Rank) = (i1i1+1) = (33+1) = ,</p>
    <p>- k(Bank) = k(Rank) = (i1i1+1) = (44+1) = ,</p>
    <p>- an(Bank) = an(Rank) = (i1i2+1) = (32+1) = 2,</p>
    <p>- ank(Bank) = ank(Rank) = (i1i3+1) = (42+1) = 3,</p>
  </div>
  <div class="page">
    <p>An example of string kernel computation</p>
  </div>
  <div class="page">
    <p>Efficient Evaluation: Intuition</p>
    <p>! Dynamic Programming technique</p>
    <p>! Evaluate the spectrum string kernels</p>
    <p>! Substrings of size p</p>
    <p>! Sum the contribution of the different spectra</p>
  </div>
  <div class="page">
    <p>Efficient Evaluation</p>
  </div>
  <div class="page">
    <p>Evaluating DP2</p>
    <p>! Evaluate the weight of the string of size p in case</p>
    <p>a character will be matched</p>
    <p>! This is done by multiplying the double summation</p>
    <p>by the number of substrings of size p-1</p>
  </div>
  <div class="page">
    <p>Tree kernels</p>
    <p>! Syntactic Tree Kernel, Partial Tree kernel (PTK),</p>
    <p>Semantic Syntactic Tree Kernel, Smoothed PTK</p>
    <p>! Efficient computation</p>
  </div>
  <div class="page">
    <p>Example of a parse tree</p>
    <p>! John delivers a talk in Rome</p>
    <p>S  N VP</p>
    <p>VP  V NP PP</p>
    <p>PP  IN N</p>
    <p>N  Rome N</p>
    <p>Rome</p>
    <p>S</p>
    <p>N</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V John</p>
    <p>in</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>PP</p>
    <p>IN</p>
  </div>
  <div class="page">
    <p>The Syntactic Tree Kernel (STK) [Collins and Duffy, 2002]</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>delivers</p>
    <p>a</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>delivers</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V NP</p>
    <p>VP</p>
    <p>V</p>
  </div>
  <div class="page">
    <p>The overall fragment set</p>
    <p>NP</p>
    <p>D</p>
    <p>VP</p>
    <p>a</p>
    <p>Children are not divided</p>
  </div>
  <div class="page">
    <p>Explicit kernel space</p>
    <p>zx</p>
    <p>(Tx) =  x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0)</p>
    <p>! counts the number of common substructures</p>
    <p>(Tz) =  z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0)</p>
  </div>
  <div class="page">
    <p>Efficient evaluation of the scalar product</p>
    <p>x   z = (Tx ) (Tz ) = K(Tx ,Tz ) =</p>
    <p>= n x Tx</p>
    <p>(nx , nz ) n z Tz</p>
  </div>
  <div class="page">
    <p>Efficient evaluation of the scalar product</p>
    <p>! [Collins and Duffy, ACL 2002] evaluate  in O(n2):</p>
    <p>(nx,nz ) = 0, if the productions are different else (nx,nz ) = 1, if pre - terminals else</p>
    <p>(nx,nz ) = (1 + (ch(nx, j),ch(nz, j))) j=1</p>
    <p>nc(nx )</p>
    <p>x   z = (Tx ) (Tz ) = K(Tx ,Tz ) =</p>
    <p>= n x Tx</p>
    <p>(nx , nz ) n z Tz</p>
  </div>
  <div class="page">
    <p>Other Adjustments</p>
    <p>! Normalization</p>
    <p>(nx,nz ) = , if pre - terminals else</p>
    <p>(nx,nz ) =  (1 + (ch(nx, j),ch(nz, j))) j=1</p>
    <p>nc(nx )</p>
    <p>&quot; K (Tx ,Tz ) = K(Tx ,Tz )</p>
    <p>K (Tx ,Tx )  K (Tz ,Tz )</p>
    <p>! Decay factor</p>
  </div>
  <div class="page">
    <p>Observations</p>
    <p>! We order the production rules used in Tx and Tz,</p>
    <p>at loading time</p>
    <p>! At learning time we can evaluate NP in</p>
    <p>|Tx|+|Tz | running time [Moschitti, EACL 2006]</p>
    <p>! If Tx and Tz are generated by only one production rule  O(|Tx||Tz | )Very Unlikely!!!!</p>
  </div>
  <div class="page">
    <p>Labeled Ordered Tree Kernel</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>a talk</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>a talk</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>a</p>
    <p>NP</p>
    <p>D</p>
    <p>VP</p>
    <p>a</p>
    <p>NP</p>
    <p>D</p>
    <p>VP</p>
    <p>NP</p>
    <p>N</p>
    <p>VP</p>
    <p>NP</p>
    <p>N</p>
    <p>NP NP</p>
    <p>D N D</p>
    <p>NP</p>
    <p>VP</p>
    <p>! STK satisfies the constraint remove 0 or all</p>
    <p>children at a time.</p>
    <p>! If we relax such constraint we get more general</p>
    <p>substructures [Kashima and Koyanagi, 2002]</p>
  </div>
  <div class="page">
    <p>Weighting Problems</p>
    <p>! Both matched pairs give the</p>
    <p>same contribution</p>
    <p>! Gap based weighting is</p>
    <p>needed</p>
    <p>! A novel efficient evaluation</p>
    <p>has to be defined</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>a talk</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>gives</p>
    <p>JJ</p>
    <p>good</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>JJ</p>
    <p>bad</p>
  </div>
  <div class="page">
    <p>Partial Tree Kernel (PTK) [Moschitti, ECML 2006]</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>brought</p>
    <p>a cat</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>a cat</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>a cat</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>a</p>
    <p>NP</p>
    <p>D</p>
    <p>VP</p>
    <p>a</p>
    <p>NP</p>
    <p>D</p>
    <p>VP</p>
    <p>NP</p>
    <p>N</p>
    <p>VP</p>
    <p>NP</p>
    <p>N</p>
    <p>NP NP</p>
    <p>D N D</p>
    <p>NP</p>
    <p>VP</p>
    <p>! STK + String Kernel with weighted gaps on</p>
    <p>nodes children</p>
  </div>
  <div class="page">
    <p>Partial Tree Kernel - Definition</p>
    <p>! By adding two decay factors we obtain:</p>
  </div>
  <div class="page">
    <p>Efficient Evaluation (1)</p>
    <p>! In [Taylor and Cristianini, 2004 book], sequence kernels with weighted gaps are factorized with respect to different subsequence sizes.</p>
    <p>! We treat children as sequences and apply the same theory</p>
    <p>Dp</p>
    <p>those defined in [7, 2, 3, 5, 13]. Additionally, we add two decay factors:  for the height of the tree and  for the length of the child sequences. It follows that</p>
    <p>(n1, n2) =</p>
    <p>X</p>
    <p>J1,J2,l(J1)=l(J2)</p>
    <p>d(J1)+d(J2)</p>
    <p>l(J 1)Y</p>
    <p>i=1</p>
    <p>(cn1 [J 1i], cn2 [J 2i])</p>
    <p>(3)</p>
    <p>where d(J 1) = J 1l(J1)  J 11 and d(J 2) = J 2l(J2)  J 21. In this way, we penalize both larger trees and subtrees built on child subsequences that contain gaps. Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K0(T1, T2) = K(T1,T2)p</p>
    <p>K(T1,T1)K(T2,T2) .</p>
    <p>Clearly, the naive approach to evaluate Eq. 3 requires exponential time. We can eciently compute it by considering that the summation in Eq. 3 can be distributed with respect to dierent types of sequences, e.g. those composed by p children; it follows that (n1, n2) =</p>
    <p>2 +</p>
    <p>P lm</p>
    <p>p=1 p(cn1 , cn2 )  , (4)</p>
    <p>where  p</p>
    <p>evaluates the number of common subtrees rooted in subsequences of exactly p children (of n1 and n2) and lm = min{l(cn1), l(cn2)}. Note also that if we consider only the contribution of the longest child sequence from node pairs that have the same children, we implement the SST kernel. For the STs computation we need also to remove the 2 term from Eq. 4.</p>
    <p>Given the two child sequences s1a = cn1 and s2b = cn2 (a and b are the last children),</p>
    <p>p</p>
    <p>(s1a, s2b) = (a, b)  |s1|X</p>
    <p>i=1</p>
    <p>|s2|X</p>
    <p>r=1</p>
    <p>|s1|i+|s2|r   p1(s1[1 : i], s2[1 : r]),</p>
    <p>where s1[1 : i] and s2[1 : r] are the child subsequences from 1 to i and from 1 to r of s1 and s2. If we name the double summation term as Dp, we can rewrite the relation as:</p>
    <p>p</p>
    <p>(s1a, s2b) =</p>
    <p>( (a, b)D</p>
    <p>p</p>
    <p>(|s1|, |s2|) if a = b; 0 otherwise.</p>
    <p>Note that D p</p>
    <p>satisfies the recursive relation: D p</p>
    <p>(k, l) =</p>
    <p>p1(s1[1 : k], s2[1 : l]) + Dp(k, l  1) + Dp(k  1, l) + 2Dp(k  1, l  1) (5) By means of the above relation, we can compute the child subsequences of two</p>
    <p>sequences s1 and s2 in O(p|s1||s2|). This means that the worst case complexity of the PT kernel is O(p2|N</p>
    <p>T1 ||NT2|), where  is the maximum branching factor of the two trees. Note that the average  in natural language parse trees is very small and the overall complexity can be reduced by avoiding the computation of node pairs with dierent labels. The next section shows our fast algorithm to find non-null node pairs. 3.3 Fast non-null node pair computation</p>
    <p>To compute the tree kernels, we sum the  function for each pair hn1, n2i2 N</p>
    <p>T1  NT2 (Eq. 1). When the labels associated with n1 and n2 are dierent, we can avoid evaluating (n1, n2) since it is 0. Thus, we look for a node pair</p>
  </div>
  <div class="page">
    <p>Efficient Evaluation (2)</p>
    <p>! The complexity of finding the subsequences is</p>
    <p>! Therefore the overall complexity is</p>
    <p>where  is the maximum branching factor (p = )</p>
  </div>
  <div class="page">
    <p>Running Time of Tree Kernel Functions</p>
    <p>FSTK STK</p>
    <p>FPTK</p>
    <p>! STK vs. Fast STK (FSTK) and Fast PTK (FPTK)</p>
  </div>
  <div class="page">
    <p>Syntactic/Semantic Tree Kernels (SSTK) [Bloehdorn &amp; Moschitti, ECIR 2007 &amp; CIKM 2007]</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>N</p>
    <p>good</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>N</p>
    <p>solid</p>
    <p>! Similarity between the fragment leaves ! Tree kernels + Lexical Similarity Kernel</p>
  </div>
  <div class="page">
    <p>Equations of SSTK</p>
    <p>Definition 4 (Tree Fragment Similarity Kernel). For two tree fragments</p>
    <p>f1, f2 2 F, we define the Tree Fragment Similarity Kernel as6:</p>
    <p>F (f1, f2) = comp(f1, f2) nt(f1)Y</p>
    <p>t=1</p>
    <p>S (f1(t), f2(t))</p>
    <p>where comp(f1, f2) (compatible) is 1 if f1 diers from f2 only in the terminal nodes and is 0 otherwise, nt(fi) is the number of terminal nodes and fi(t) is the t-th terminal symbol of fi (numbered from left to right).</p>
    <p>Conceptually, this means that the similarity of two tree fragments is above zero only if the tree fragments have an identical structure. The fragment similarity is evaluated as the product of all semantic similarities of corresponding terminal nodes (i.e. sitting at identical positions). It is maximal if all pairs have a similarity score of 1. We now define the overall tree kernel as the sum over the evaluations of F over all pairs of tree fragments in the argument trees. Technically, this means changing the summation in the second formula of definition 3 as in the following definition.</p>
    <p>Definition 5 (Semantic Syntactic Tree Kernel). Given two trees T1 and</p>
    <p>T2 we define the Semantic Syntactic Tree Kernel as:</p>
    <p>T (T1, T2) = X</p>
    <p>n12NT1</p>
    <p>X</p>
    <p>n22NT2</p>
    <p>(n1, n2)</p>
    <p>where (n1, n2) = P|F|</p>
    <p>i=1</p>
    <p>P|F| j=1 Ii(n1)Ij (n2)F (fi, fj ).</p>
    <p>Obviously, the naive evaluation of this kernel would require even more computation and memory than for the naive computation of the standard kernel as also all compatible pairs of tree fragments would need to be considered in the summation. Luckily, this enhanced kernel can be evaluated in the same way as the standard tree kernel by adding the following step</p>
    <p>),</p>
    <p>as the first condition of the  function definition (Section 4), where label(ni) is the label of node ni and S is a term similarity kernel, e.g. based on the superconcept kernel defined in section 3.2. Note that: (a) since n1 and n2 are pre-terminals of a parse tree they can have only one child (i.e. ch1n1 and ch</p>
    <p>) and such children are words and (b) Step 2 is no longer necessary.</p>
    <p>Beside the novelty of taking into account tree fragments that are not identical it should be noted that the lexical semantic similarity is constrained in syntactic structures, which limit errors/noise due to incorrect (or, as in our case, not provided) word sense disambiguation. 6 Note that, as the tree fragments need to be compatible, they have the same number</p>
    <p>of terminal symbols at compatible positions.</p>
    <p>Definition 4 (Tree Fragment Similarity Kernel). For two tree fragments</p>
    <p>f1, f2 2 F, we define the Tree Fragment Similarity Kernel as6:</p>
    <p>F (f1, f2) = comp(f1, f2) nt(f1)Y</p>
    <p>t=1</p>
    <p>S (f1(t), f2(t))</p>
    <p>where comp(f1, f2) (compatible) is 1 if f1 diers from f2 only in the terminal nodes and is 0 otherwise, nt(fi) is the number of terminal nodes and fi(t) is the t-th terminal symbol of fi (numbered from left to right).</p>
    <p>Conceptually, this means that the similarity of two tree fragments is above zero only if the tree fragments have an identical structure. The fragment similarity is evaluated as the product of all semantic similarities of corresponding terminal nodes (i.e. sitting at identical positions). It is maximal if all pairs have a similarity score of 1. We now define the overall tree kernel as the sum over the evaluations of F over all pairs of tree fragments in the argument trees. Technically, this means changing the summation in the second formula of definition 3 as in the following definition.</p>
    <p>Definition 5 (Semantic Syntactic Tree Kernel). Given two trees T1 and</p>
    <p>T2 we define the Semantic Syntactic Tree Kernel as:</p>
    <p>T (T1, T2) = X</p>
    <p>n12NT1</p>
    <p>X</p>
    <p>n22NT2</p>
    <p>(n1, n2)</p>
    <p>where (n1, n2) = P|F|</p>
    <p>i=1</p>
    <p>P|F| j=1 Ii(n1)Ij (n2)F (fi, fj ).</p>
    <p>Obviously, the naive evaluation of this kernel would require even more computation and memory than for the naive computation of the standard kernel as also all compatible pairs of tree fragments would need to be considered in the summation. Luckily, this enhanced kernel can be evaluated in the same way as the standard tree kernel by adding the following step</p>
    <p>),</p>
    <p>as the first condition of the  function definition (Section 4), where label(ni) is the label of node ni and S is a term similarity kernel, e.g. based on the superconcept kernel defined in section 3.2. Note that: (a) since n1 and n2 are pre-terminals of a parse tree they can have only one child (i.e. ch1n1 and ch</p>
    <p>) and such children are words and (b) Step 2 is no longer necessary.</p>
    <p>Beside the novelty of taking into account tree fragments that are not identical it should be noted that the lexical semantic similarity is constrained in syntactic structures, which limit errors/noise due to incorrect (or, as in our case, not provided) word sense disambiguation. 6 Note that, as the tree fragments need to be compatible, they have the same number</p>
    <p>of terminal symbols at compatible positions.</p>
  </div>
  <div class="page">
    <p>Example of an SSTK evaluation</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>N</p>
    <p>good</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>gives</p>
    <p>a talk</p>
    <p>N</p>
    <p>solid</p>
    <p>KS(gives,gives)*KS(a,a)*</p>
    <p>KS(good,solid)*KS(talk,talk)</p>
    <p>= 1 * 1 * 0.5 * 1 = 0.5</p>
    <p>Definition 4 (Tree Fragment Similarity Kernel). For two tree fragments</p>
    <p>f1, f2 2 F, we define the Tree Fragment Similarity Kernel as6:</p>
    <p>F (f1, f2) = comp(f1, f2) nt(f1)Y</p>
    <p>t=1</p>
    <p>S (f1(t), f2(t))</p>
    <p>where comp(f1, f2) (compatible) is 1 if f1 diers from f2 only in the terminal nodes and is 0 otherwise, nt(fi) is the number of terminal nodes and fi(t) is the t-th terminal symbol of fi (numbered from left to right).</p>
    <p>Conceptually, this means that the similarity of two tree fragments is above zero only if the tree fragments have an identical structure. The fragment similarity is evaluated as the product of all semantic similarities of corresponding terminal nodes (i.e. sitting at identical positions). It is maximal if all pairs have a similarity score of 1. We now define the overall tree kernel as the sum over the evaluations of F over all pairs of tree fragments in the argument trees. Technically, this means changing the summation in the second formula of definition 3 as in the following definition.</p>
    <p>Definition 5 (Semantic Syntactic Tree Kernel). Given two trees T1 and</p>
    <p>T2 we define the Semantic Syntactic Tree Kernel as:</p>
    <p>T (T1, T2) = X</p>
    <p>n12NT1</p>
    <p>X</p>
    <p>n22NT2</p>
    <p>(n1, n2)</p>
    <p>where (n1, n2) = P|F|</p>
    <p>i=1</p>
    <p>P|F| j=1 Ii(n1)Ij (n2)F (fi, fj ).</p>
    <p>Obviously, the naive evaluation of this kernel would require even more computation and memory than for the naive computation of the standard kernel as also all compatible pairs of tree fragments would need to be considered in the summation. Luckily, this enhanced kernel can be evaluated in the same way as the standard tree kernel by adding the following step</p>
    <p>),</p>
    <p>as the first condition of the  function definition (Section 4), where label(ni) is the label of node ni and S is a term similarity kernel, e.g. based on the superconcept kernel defined in section 3.2. Note that: (a) since n1 and n2 are pre-terminals of a parse tree they can have only one child (i.e. ch1n1 and ch</p>
    <p>) and such children are words and (b) Step 2 is no longer necessary.</p>
    <p>Beside the novelty of taking into account tree fragments that are not identical it should be noted that the lexical semantic similarity is constrained in syntactic structures, which limit errors/noise due to incorrect (or, as in our case, not provided) word sense disambiguation. 6 Note that, as the tree fragments need to be compatible, they have the same number</p>
    <p>of terminal symbols at compatible positions.</p>
  </div>
  <div class="page">
    <p>Delta Evaluation is very simple</p>
    <p>Definition 4 (Tree Fragment Similarity Kernel). For two tree fragments</p>
    <p>f1, f2 2 F, we define the Tree Fragment Similarity Kernel as6:</p>
    <p>F (f1, f2) = comp(f1, f2) nt(f1)Y</p>
    <p>t=1</p>
    <p>S (f1(t), f2(t))</p>
    <p>where comp(f1, f2) (compatible) is 1 if f1 diers from f2 only in the terminal nodes and is 0 otherwise, nt(fi) is the number of terminal nodes and fi(t) is the t-th terminal symbol of fi (numbered from left to right).</p>
    <p>Conceptually, this means that the similarity of two tree fragments is above zero only if the tree fragments have an identical structure. The fragment similarity is evaluated as the product of all semantic similarities of corresponding terminal nodes (i.e. sitting at identical positions). It is maximal if all pairs have a similarity score of 1. We now define the overall tree kernel as the sum over the evaluations of F over all pairs of tree fragments in the argument trees. Technically, this means changing the summation in the second formula of definition 3 as in the following definition.</p>
    <p>Definition 5 (Semantic Syntactic Tree Kernel). Given two trees T1 and</p>
    <p>T2 we define the Semantic Syntactic Tree Kernel as:</p>
    <p>T (T1, T2) = X</p>
    <p>n12NT1</p>
    <p>X</p>
    <p>n22NT2</p>
    <p>(n1, n2)</p>
    <p>where (n1, n2) = P|F|</p>
    <p>i=1</p>
    <p>P|F| j=1 Ii(n1)Ij (n2)F (fi, fj ).</p>
    <p>Obviously, the naive evaluation of this kernel would require even more computation and memory than for the naive computation of the standard kernel as also all compatible pairs of tree fragments would need to be considered in the summation. Luckily, this enhanced kernel can be evaluated in the same way as the standard tree kernel by adding the following step</p>
    <p>),</p>
    <p>as the first condition of the  function definition (Section 4), where label(ni) is the label of node ni and S is a term similarity kernel, e.g. based on the superconcept kernel defined in section 3.2. Note that: (a) since n1 and n2 are pre-terminals of a parse tree they can have only one child (i.e. ch1n1 and ch</p>
    <p>) and such children are words and (b) Step 2 is no longer necessary.</p>
    <p>Beside the novelty of taking into account tree fragments that are not identical it should be noted that the lexical semantic similarity is constrained in syntactic structures, which limit errors/noise due to incorrect (or, as in our case, not provided) word sense disambiguation. 6 Note that, as the tree fragments need to be compatible, they have the same number</p>
    <p>of terminal symbols at compatible positions.</p>
    <p>where (n1, n2) = P|F|</p>
    <p>i=1 Ii(n1)Ii(n2), and where Ii(n) is an indicator function which determines whether fragment fi is rooted in node n.</p>
    <p>is equal to the number of common fragments rooted at nodes n1 and n2. We can compute it more eciently as follows:</p>
    <p>leaf children (i.e. the argument nodes are pre-terminals symbols) then (n1, n2) = 1;</p>
    <p>(n1, n2) = nc(n1)Y</p>
    <p>j=1</p>
    <p>(1 + (chjn1 , ch j n2</p>
    <p>)).</p>
    <p>where nc(n1) is the number of children of n1 and chjn is the j-th child of node n. Note that, since the productions are the same, nc(n1) = nc(n2). Of course, the kernel can again be normalized using the cosine normalization modifier. Additionally, a decay factor  can be added by modifying steps (2) and (3) as follows:</p>
    <p>Qnc(n1) j=1 (1 + (ch</p>
    <p>j n1</p>
    <p>, ch</p>
    <p>j n2</p>
    <p>)).</p>
    <p>As an example, Figure 1 shows a parse tree of the sentence (fragment) bought a cat with some of the substructures that the tree kernel uses to represent it5.</p>
    <p>The Tree Kernel introduced in the previous section relies on the intuition of counting all common substructures of two trees. However, if two trees have similar structures but employ dierent though related terminology at the leaves, they will not be matched. From a semantic point of view, this is an evident drawback as brought a cat should be more related to brought a tomcat than to brought a note.</p>
    <p>In analogy with the semantic smoothing kernels for the bag-of-words kernel as described in section 3.2, we are now interested in also counting partial matches between tree fragments. A partial match occurs when two fragments dier only by their terminal symbols, e.g. [N [cat]] and [N [tomcat]]. In this case the match should give a contribution smaller than 1, depending on the semantic similarity of the respective terminal nodes. For this purpose, we first define the similarity of two such tree fragments. 5 The number of such fragments can be obtained by evaluating the kernel function</p>
    <p>between the tree with itself.</p>
    <p>where (n1, n2) = P|F|</p>
    <p>i=1 Ii(n1)Ii(n2), and where Ii(n) is an indicator function which determines whether fragment fi is rooted in node n.</p>
    <p>is equal to the number of common fragments rooted at nodes n1 and n2. We can compute it more eciently as follows:</p>
    <p>leaf children (i.e. the argument nodes are pre-terminals symbols) then (n1, n2) = 1;</p>
    <p>(n1, n2) = nc(n1)Y</p>
    <p>j=1</p>
    <p>(1 + (chjn1 , ch j n2</p>
    <p>)).</p>
    <p>where nc(n1) is the number of children of n1 and chjn is the j-th child of node n. Note that, since the productions are the same, nc(n1) = nc(n2). Of course, the kernel can again be normalized using the cosine normalization modifier. Additionally, a decay factor  can be added by modifying steps (2) and (3) as follows:</p>
    <p>Qnc(n1) j=1 (1 + (ch</p>
    <p>j n1</p>
    <p>, ch</p>
    <p>j n2</p>
    <p>)).</p>
    <p>As an example, Figure 1 shows a parse tree of the sentence (fragment) bought a cat with some of the substructures that the tree kernel uses to represent it5.</p>
    <p>The Tree Kernel introduced in the previous section relies on the intuition of counting all common substructures of two trees. However, if two trees have similar structures but employ dierent though related terminology at the leaves, they will not be matched. From a semantic point of view, this is an evident drawback as brought a cat should be more related to brought a tomcat than to brought a note.</p>
    <p>In analogy with the semantic smoothing kernels for the bag-of-words kernel as described in section 3.2, we are now interested in also counting partial matches between tree fragments. A partial match occurs when two fragments dier only by their terminal symbols, e.g. [N [cat]] and [N [tomcat]]. In this case the match should give a contribution smaller than 1, depending on the semantic similarity of the respective terminal nodes. For this purpose, we first define the similarity of two such tree fragments. 5 The number of such fragments can be obtained by evaluating the kernel function</p>
    <p>between the tree with itself.</p>
  </div>
  <div class="page">
    <p>Smoothed Partial Tree Kernels [Moschitti, EACL 2009; Croce et al., 2011]</p>
    <p>! Same idea of Syntactic Semantic Tree Kernel but</p>
    <p>the similarity is extended to any node of the tree</p>
    <p>! The tree fragments are those generated by PTK</p>
    <p>! Basically it extends PTK with similarities</p>
  </div>
  <div class="page">
    <p>Examples of Dependency Trees</p>
    <p>! What is the width of a football field?</p>
    <p>! What is the length of the biggest tennis court</p>
  </div>
  <div class="page">
    <p>Equation of SPTK</p>
    <p>If n1 and n2 are leaves then</p>
    <p>else</p>
    <p>PTK Lexical Similarity</p>
  </div>
  <div class="page">
    <p>Different versions of Computational Dependency Trees for PTK/SPTK</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
    <p>LOCT</p>
    <p>LCT</p>
    <p>LPST</p>
  </div>
  <div class="page">
    <p>Tree Kernel Efficiency</p>
    <p>y = 0.068x1.213</p>
    <p>y = 0.081x1.705</p>
    <p>y = 0,0513x2.005</p>
    <p>m ic</p>
    <p>ro se</p>
    <p>co n</p>
    <p>d s</p>
    <p>Number of Nodes</p>
    <p>LCT-PTK</p>
    <p>LCT-SPTK</p>
    <p>LPST-PTK</p>
    <p>LPST-SPTK</p>
  </div>
  <div class="page">
    <p>SVM-light-TK Software</p>
    <p>! Encodes STK, PTK and combination kernels</p>
    <p>in SVM-light [Joachims, 1999]</p>
    <p>! Available at http://disi.unitn.it/moschitti</p>
    <p>! Tree forests, vector sets</p>
  </div>
  <div class="page">
    <p>Data Format</p>
    <p>! What does S.O.S. stand for?</p>
    <p>! 1 |BT| (SBARQ (WHNP (WP What))(SQ (AUX does)(NP (NNP S.O.S.))(VP (VB stand)(PP (IN for))))(. ?))</p>
    <p>|BT| (BOW (What *)(does *)(S.O.S. *)(stand *)(for *)(? *))</p>
    <p>|BT| (BOP (WP *)(AUX *)(NNP *)(VB *)(IN *)(. *))</p>
    <p>|BT| (PAS (ARG0 (R-A1 (What *)))(ARG1 (A1 (S.O.S. NNP)))(ARG2 (rel stand)))</p>
    <p>|ET| 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 66:1 152:1 274:1 333:1</p>
    <p>|BV| 2:1 21:1.4421347148614654E-4 23:1 31:1 36:1 39:1 41:1 46:1 49:1 52:1 66:1 152:1 246:1 333:1 392:1 |EV|</p>
  </div>
  <div class="page">
    <p>Kernel Combinations an example</p>
    <p>! Kernel Combinations:</p>
    <p>,</p>
    <p>,</p>
    <p>pTree</p>
    <p>pTree PTree</p>
    <p>p</p>
    <p>p</p>
    <p>Tree</p>
    <p>Tree PTree</p>
    <p>pTreePTreepTreePTree</p>
    <p>KK KK</p>
    <p>K K K</p>
    <p>K K</p>
    <p>K</p>
    <p>KKKKKK</p>
    <p>=+=</p>
    <p>=+=</p>
    <p>+</p>
    <p>+</p>
    <p>kernel Tree</p>
    <p>featuresflat of kernel polynomial 3</p>
    <p>Tree</p>
    <p>p</p>
    <p>K</p>
    <p>K</p>
  </div>
  <div class="page">
    <p>Basic Commands</p>
    <p>! Training and classification ! ./svm_learn -t 5 -C T train.dat model</p>
    <p>! ./svm_classify test.dat model</p>
    <p>! Learning with a vector sequence ! ./svm_learn -t 5 -C V train.dat model</p>
    <p>! Learning with the sum of vector and kernel</p>
    <p>sequences ! ./svm_learn -t 5 -C + train.dat model</p>
  </div>
  <div class="page">
    <p>Applications with Simple Kernels</p>
  </div>
  <div class="page">
    <p>A QA Pipeline: Watson Overview</p>
    <p>Question Classification</p>
  </div>
  <div class="page">
    <p>Question Classification</p>
    <p>! Definition: What does HTML stand for?</p>
    <p>! Description: What's the final line in the Edgar Allan Poe poem &quot;The Raven&quot;?</p>
    <p>! Entity: What foods can cause allergic reaction in people?</p>
    <p>! Human: Who won the Nobel Peace Prize in 1992?</p>
    <p>! Location: Where is the Statue of Liberty?</p>
    <p>! Manner: How did Bob Marley die?</p>
    <p>! Numeric: When was Martin Luther King Jr. born?</p>
    <p>! Organization: What company makes Bentley cars?</p>
  </div>
  <div class="page">
    <p>Question Classifier based on Tree Kernels</p>
    <p>! Question dataset (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/)</p>
    <p>[Lin and Roth, 2005]) ! Distributed on 6 categories: Abbreviations, Descriptions, Entity,</p>
    <p>Human, Location, and Numeric.</p>
    <p>! Fixed split 5500 training and 500 test questions</p>
    <p>! Using the whole question parse trees ! Constituent parsing</p>
    <p>! Example</p>
    <p>What is an offer of direct stock purchase plan ?</p>
  </div>
  <div class="page">
    <p>Syntactic Parse Trees (PT)</p>
  </div>
  <div class="page">
    <p>Some fragments</p>
  </div>
  <div class="page">
    <p>Explicit kernel space</p>
    <p>zx</p>
    <p>(Tx) =  x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0)</p>
    <p>! counts the number of common substructures</p>
    <p>(Tz) =  z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0)</p>
  </div>
  <div class="page">
    <p>Question Classification with SSTK [Blohedorn&amp;Moschitti, CIKM2007]</p>
  </div>
  <div class="page">
    <p>Same Task with PTK, SPTK and Dependency Trees</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
    <p>LOCT</p>
    <p>LCT</p>
    <p>LPST</p>
  </div>
  <div class="page">
    <p>State-of-the-art Results [Croce et al., EMNLP 2011]</p>
    <p>be::v</p>
    <p>?::.width::n</p>
    <p>of::i</p>
    <p>field::n</p>
    <p>football::na::d</p>
    <p>the::d</p>
    <p>what::w</p>
    <p>Figure 2: Lexical Only Centered Tree (LOCT).</p>
    <p>be::v</p>
    <p>VBZROOT?::.</p>
    <p>.P</p>
    <p>width::n</p>
    <p>NNPRDof::i</p>
    <p>INNMODfield::n</p>
    <p>the::d</p>
    <p>DTNMOD</p>
    <p>what::w</p>
    <p>WPSBJ</p>
    <p>field::n</p>
    <p>NNPMODfootball::n</p>
    <p>NNNMOD</p>
    <p>a::d</p>
    <p>DTNMOD</p>
    <p>Figure 3: Lexical Centered Tree (LCT).</p>
    <p>TOP</p>
    <p>.</p>
    <p>?::.</p>
    <p>NN</p>
    <p>field::n</p>
    <p>NN</p>
    <p>football::n</p>
    <p>DT</p>
    <p>a::d</p>
    <p>IN</p>
    <p>of::i</p>
    <p>NN</p>
    <p>width::n</p>
    <p>DT</p>
    <p>the::d</p>
    <p>VBZ</p>
    <p>be::v</p>
    <p>WP</p>
    <p>what::w</p>
    <p>Figure 4: Lexical and PoS-Tag Sequences Tree (LPST).</p>
    <p>presents the experimental evaluation for QC and Section 4 derives the conclusions.</p>
    <p>Thanks to structural kernel similarity, a question classification (QC) task can be easily modeled by representing questions, i.e., the classification objects, with their parse trees. Several syntactic representations exist, we report the most interesting and effective structures that we proposed in [7]. Given the following sentence:</p>
    <p>(s1) What is the width of a football field?</p>
    <p>the representation tree according to a phrase structure paradigm, i.e. constituency tree (CT), is in Figure 1. We apply lemmatization to the lexicals to improve generalization and, at the same time, we add a generalized PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. This is useful to measure similarity between lexicals belonging to the same grammatical category. Our conversion of dependency structures in dependency trees is done in two steps:</p>
    <p>we generate the tree that includes only lexicals, where the edges encode their dependencies. We call it the Lexical Only Centered Tree (LOCT), e.g. see Figure 2.</p>
    <p>To each lexical node, we add two leftmost children, which encode the grammatical function and POS-Tag, i.e. node features. We call this structure the Lexical Centered Tree (LCT), e.g. see Figure 3.</p>
    <p>Additionally, for comparative purposes, we define a flat structure, the Lexical and PoS-tag Sequences Tree (LPST), e.g. see Figure 4, which ignores the syntactic structure of the sentence being a</p>
    <p>STK PTK SPTK(LSA) CT 91.20% 90.80% 91.00% LOCT - 89.20% 93.20% LCT - 90.80% 94.80% LPST - 89.40% 89.60% BOW 88.80%</p>
    <p>Table 1: Accuracy of structural kernels applied to different structures on QC</p>
    <p>simple sequence of PoS-tag nodes, where lexicals are simply added as children.</p>
    <p>larity embedded in syntactic structures. For this purpose, we present results on QC and the related error analysis.</p>
    <p>by a training set of 5,452 questions and a test set of 500 questions1. The latter are organized in six coarse-grained classes, i.e., ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMBER.</p>
    <p>For learning our models, we extended the SVM-LightTK software2 [14, 15] (which includes structural kernels, i.e., STK and PTK in SVMLight [8]) with the smooth match between tree nodes, i.e. the SPTK defined in [7].</p>
    <p>For generating constituency trees, we used Charniaks parser [5] whereas we applied LTH syntactic parser (described in [12]) to generate dependency trees.</p>
    <p>The lexical similarity was designed with LSA applied to ukWak [1], which is a large scale document collection made by 2 billion tokens (see [7] for more details). We implemented multiclassification using one-vs-all scheme and selecting the category associated with the maximum SVM margin.</p>
    <p>SPTK applied to the several structures for QC is reported in Table 1. The first column shows the different structures described in Section 2. The first row lists the tree kernel models. The last row reports the accuracy of bag-of-words (BOW), which is a linear kernel applied to lexical vectors.</p>
    <p>It is worth nothing that:</p>
    <p>BOW produces high accuracy, i.e. 88.8% but it is improved by STK, current state-of-the-art3 in QC [18, 17];</p>
    <p>PTK applied to the same tree of STK (i.e. CT) produces a slightly lower value (non-statistically significant difference); and</p>
    <p>PTK applied to LCT, which contains structures but also grammatical functions and PoS-tags, achieves higher accuracy than when applied to LOCT (no grammatical/syntactic features) or to LPST (no structure).</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Classification in Definition vs not Definition in Jeopardy</p>
    <p>! Definition: Usually, to do this is to lose a game</p>
    <p>without playing it</p>
    <p>(solution: forfeit)</p>
    <p>! Non Definition: When hit by electrons, a</p>
    <p>phosphor gives off electromagnetic energy in this</p>
    <p>form</p>
    <p>! Complex linguistic problem: let us learn it from</p>
    <p>training examples using a syntactic similarity</p>
  </div>
  <div class="page">
    <p>Automatic Learning of a Question Classifier</p>
    <p>! Similarity between definition vs non definition</p>
    <p>questions</p>
    <p>! Instead of using features-based similarity we use</p>
    <p>kernels</p>
    <p>! Combining several linguistic structures with</p>
    <p>several kernels for representing a question q:</p>
    <p>! K1(q1,q2)+K2(q1,q2)++Kn(q1,q2)</p>
    <p>! Tree kernels measure similarity between trees</p>
  </div>
  <div class="page">
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>hit</p>
    <p>a phosphor</p>
    <p>Syntactic Tree Kernel (STK) (Collins and Duffy 2002)</p>
  </div>
  <div class="page">
    <p>Syntactic Tree Kernel (STK) (Collins and Duffy 2002)</p>
  </div>
  <div class="page">
    <p>The resulting explicit kernel space</p>
    <p>zx  ! counts the number of common substructures</p>
    <p>hit</p>
    <p>phosphor phosphor</p>
    <p>phosphor phosphor</p>
    <p>(T x ) =  x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0)</p>
    <p>(T z ) =  z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0)</p>
    <p>hit</p>
    <p>phosphor phosphor</p>
    <p>phosphor</p>
  </div>
  <div class="page">
    <p>Experimental setup</p>
    <p>! Corpus: a random sample from 33 Jeopardy!</p>
    <p>Games</p>
    <p>! 306 definition and 4,964 non-definition clues</p>
    <p>! Tools: ! SVMLight-TK</p>
    <p>! Charniaks constituency parser</p>
    <p>! Syntactic/Semantic parser by Johansson and Nugues (2008)</p>
    <p>! Measures derived with leave-on-out</p>
  </div>
  <div class="page">
    <p>Constituency Tree (CT)</p>
    <p>ROOT</p>
    <p>SBARQ</p>
    <p>WHADVP</p>
    <p>WRB</p>
    <p>When</p>
    <p>S</p>
    <p>VP</p>
    <p>VBN</p>
    <p>hit</p>
    <p>PP</p>
    <p>IN</p>
    <p>by</p>
    <p>NP</p>
    <p>NNS</p>
    <p>electrons</p>
    <p>,</p>
    <p>,</p>
    <p>NP</p>
    <p>DT</p>
    <p>a</p>
    <p>NN</p>
    <p>phosphor</p>
    <p>VP</p>
    <p>VBZ</p>
    <p>gives</p>
    <p>PRP</p>
    <p>RP</p>
    <p>off</p>
    <p>NP</p>
    <p>NP</p>
    <p>JJ</p>
    <p>electromagnetic</p>
    <p>NN</p>
    <p>energy</p>
    <p>PP</p>
    <p>IN</p>
    <p>in</p>
    <p>NP</p>
    <p>DT</p>
    <p>this</p>
    <p>NN</p>
    <p>form</p>
    <p>ROOT</p>
    <p>SBARQ</p>
    <p>WHADVP</p>
    <p>WRB</p>
    <p>When</p>
    <p>S</p>
    <p>VP</p>
    <p>VBN</p>
    <p>hit</p>
    <p>PP</p>
    <p>IN</p>
    <p>by</p>
    <p>NP</p>
    <p>NNS</p>
    <p>electrons</p>
    <p>,</p>
    <p>,</p>
    <p>NP</p>
    <p>DT</p>
    <p>a</p>
    <p>NN</p>
    <p>phosphor</p>
    <p>VP</p>
    <p>VBZ</p>
    <p>gives</p>
    <p>PRP</p>
    <p>RP</p>
    <p>off</p>
    <p>NP</p>
    <p>NP</p>
    <p>JJ</p>
    <p>electromagnetic</p>
    <p>NN</p>
    <p>energy</p>
    <p>PP</p>
    <p>IN</p>
    <p>in</p>
    <p>NP</p>
    <p>DT</p>
    <p>this</p>
    <p>NN</p>
    <p>form</p>
    <p>PAS</p>
    <p>A0</p>
    <p>electrons</p>
    <p>predicate</p>
    <p>hit</p>
    <p>AM</p>
    <p>When</p>
    <p>PAS</p>
    <p>A0</p>
    <p>a phosphor</p>
    <p>predicate</p>
    <p>give off</p>
    <p>A1</p>
    <p>energy</p>
    <p>AM</p>
    <p>in this form</p>
    <p>PAS</p>
    <p>A0</p>
    <p>electrons</p>
    <p>predicate</p>
    <p>hit</p>
    <p>A1</p>
    <p>it</p>
    <p>AM</p>
    <p>if</p>
    <p>PAS</p>
    <p>A0</p>
    <p>a phosphor</p>
    <p>predicate</p>
    <p>give off</p>
    <p>A1</p>
    <p>photons</p>
  </div>
  <div class="page">
    <p>Dependency Tree (DT)</p>
    <p>PASS</p>
    <p>P</p>
    <p>A1</p>
    <p>phosphor</p>
    <p>A0</p>
    <p>electron</p>
    <p>PR</p>
    <p>hit</p>
    <p>P</p>
    <p>PR</p>
    <p>energy</p>
    <p>A2</p>
    <p>electromag.</p>
    <p>A1</p>
    <p>phosphor</p>
    <p>P</p>
    <p>A1</p>
    <p>energy</p>
    <p>PR</p>
    <p>give</p>
    <p>AM-TMP</p>
    <p>hit</p>
    <p>A0</p>
    <p>phosphor</p>
    <p>ROOT</p>
    <p>VBZ</p>
    <p>OBJ</p>
    <p>NN</p>
    <p>NMOD</p>
    <p>IN</p>
    <p>PMOD</p>
    <p>NN</p>
    <p>formNMOD</p>
    <p>DT</p>
    <p>this</p>
    <p>in</p>
    <p>energyNMOD</p>
    <p>JJ</p>
    <p>electromag.</p>
    <p>PRT</p>
    <p>RP</p>
    <p>o!</p>
    <p>givesSBJ</p>
    <p>NN</p>
    <p>phosphorNMOD</p>
    <p>DT</p>
    <p>a</p>
    <p>P</p>
    <p>,</p>
    <p>TMP</p>
    <p>VBN</p>
    <p>LGS</p>
    <p>IN</p>
    <p>PMOD</p>
    <p>NNS</p>
    <p>electrons</p>
    <p>by</p>
    <p>hitTMP</p>
    <p>WRB</p>
    <p>when</p>
  </div>
  <div class="page">
    <p>Predicate Argument Structure Set (PASS)</p>
    <p>negative mistake STK, ok PTK</p>
    <p>NP</p>
    <p>ADJP</p>
    <p>JJ</p>
    <p>conceited</p>
    <p>CC</p>
    <p>or</p>
    <p>JJ</p>
    <p>arrogant</p>
    <p>NP</p>
    <p>NN</p>
    <p>meaning</p>
    <p>NN</p>
    <p>adjective</p>
    <p>NN</p>
    <p>NN</p>
    <p>fowl</p>
    <p>positive mistake STK, ok PTK</p>
    <p>NP</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>field</p>
    <p>VBG</p>
    <p>playing</p>
    <p>DT</p>
    <p>a</p>
    <p>IN</p>
    <p>on</p>
    <p>VBN</p>
    <p>used</p>
    <p>NP</p>
    <p>NN</p>
    <p>grass</p>
    <p>JJ</p>
    <p>green</p>
    <p>JJ</p>
    <p>artificial</p>
    <p>NP</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>canal</p>
    <p>DT</p>
    <p>a</p>
    <p>IN</p>
    <p>on</p>
    <p>VBN</p>
    <p>used</p>
    <p>NP</p>
    <p>NN</p>
    <p>boat</p>
    <p>JJ</p>
    <p>flat-bottomed</p>
    <p>DT</p>
    <p>a</p>
    <p>PASS</p>
    <p>P</p>
    <p>A1</p>
    <p>phosphor</p>
    <p>A0</p>
    <p>electron</p>
    <p>PR</p>
    <p>hit</p>
    <p>P</p>
    <p>PR</p>
    <p>energy</p>
    <p>AM-MNR</p>
    <p>electromag.</p>
    <p>A1</p>
    <p>phosphor</p>
    <p>P</p>
    <p>A1</p>
    <p>energy</p>
    <p>PR</p>
    <p>give</p>
    <p>AM-TMP</p>
    <p>hit</p>
    <p>A0</p>
    <p>phosphor</p>
  </div>
  <div class="page">
    <p>Sequence Kernels</p>
    <p>CSK: [general][science] (category sequence kernel)</p>
    <p>WSK:</p>
    <p>PSK:</p>
    <p>CSK:</p>
  </div>
  <div class="page">
    <p>Individual models</p>
  </div>
  <div class="page">
    <p>Model Combinations</p>
  </div>
  <div class="page">
    <p>Impact of QC in Watson</p>
    <p>! Specific evaluation on definition questions ! 1,000 unseen games (60,000 questions)</p>
    <p>! Two test sets of 1,606 and 1,875 questions derived with:</p>
    <p>! Statistical model (StatDef) ! RBC (RuleDef)</p>
    <p>! Direct comparison only with NoDef</p>
    <p>! All questions evaluation ! Selected 66 unseen Jeopardy! games</p>
    <p>! 3,546 questions</p>
  </div>
  <div class="page">
    <p>Watsons Accuracy, Precision and Earnings</p>
    <p>! Comparison between use or not QC</p>
    <p>! Different set of questions</p>
  </div>
  <div class="page">
    <p>Error Analysis</p>
    <p>Test Example  PTK ok  STK not ok</p>
    <p>Training Example</p>
    <p>PTK similarity</p>
    <p>STK similarity</p>
  </div>
  <div class="page">
    <p>Answer/Passage Reranking</p>
    <p>Answer/Passage Reranking</p>
  </div>
  <div class="page">
    <p>TASK: Question/Answer Classification [Moschitti, CIKM 2008]</p>
    <p>! The classifier detects if a pair (question and answer) is</p>
    <p>correct or not</p>
    <p>! A representation for the pair is needed</p>
    <p>! The classifier can be used to re-rank the output of a basic</p>
    <p>QA system</p>
  </div>
  <div class="page">
    <p>Bags of words (BOW) and POS-tags (POS)</p>
    <p>! To save time, apply tree kernels to these trees:</p>
    <p>BOX</p>
    <p>is What an offer of</p>
    <p>* * * * *</p>
    <p>BOX</p>
    <p>VBZ WHNP DT NN IN</p>
    <p>* * * * *</p>
  </div>
  <div class="page">
    <p>Word and POS Sequences</p>
    <p>! What is an offer of? (word sequence, WSK)</p>
    <p>What_is_offer</p>
    <p>What_is</p>
    <p>! WHNP VBZ DT NN IN(POS sequence, POSSK)</p>
    <p>WHNP_VBZ_NN</p>
    <p>WHNP_NN_IN</p>
  </div>
  <div class="page">
    <p>Predicate Argument Structures for describing answers (PASPTK)</p>
    <p>! [ARG1 Antigens] were [AMTMP originally] [rel defined] [ARG2 as nonself molecules].</p>
    <p>! [ARG0 Researchers] [rel describe] [ARG1 antigens][ARG2 as foreign</p>
    <p>molecules] [ARGMLOC in the body]</p>
  </div>
  <div class="page">
    <p>Dataset 2: TREC data</p>
    <p>! 138 TREC 2001 test questions labeled as description</p>
    <p>! 2,256 sentences, extracted from the best ranked</p>
    <p>paragraphs (using a basic QA system based on Lucene</p>
    <p>search engine on TREC dataset)</p>
    <p>! 216 of which labeled as correct by one annotator</p>
  </div>
  <div class="page">
    <p>Kernels and Combinations</p>
    <p>! Exploiting the property: k(x,z) = k1(x,z)+k2(x,z)</p>
    <p>! Given: BOW, POS, WSK, POSSK, PT, PASPTK</p>
    <p>BOW+POS, BOW+PT, PT+POS,</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
    <p>BOW  24 POSSK+STK+PAS_PTK 39 62 % of improvement</p>
  </div>
  <div class="page">
    <p>Semantic Role Labeling</p>
    <p>! In an event: ! target words describe relation among different entities</p>
    <p>! the participants are often seen as predicate's arguments.</p>
    <p>! Example: Paul gives a talk in Rome</p>
  </div>
  <div class="page">
    <p>Example on Predicate Argument Classification</p>
    <p>! In an event: ! target words describe relation among different entities</p>
    <p>! the participants are often seen as predicate's arguments.</p>
    <p>! Example: [ Arg0 Paul] [ predicate gives ] [ Arg1 a talk] [ ArgM in Rome]</p>
  </div>
  <div class="page">
    <p>Predicate-Argument Feature Representation</p>
    <p>Given a sentence, a predicate p:</p>
    <p>F</p>
    <p>b. If Nx exactly covers the Arg-i, F is one of its positive examples</p>
    <p>c. F is a negative example otherwise</p>
  </div>
  <div class="page">
    <p>Vector Representation for the linear kernel</p>
    <p>Predicate</p>
    <p>S</p>
    <p>N</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V Paul</p>
    <p>in</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>PP</p>
    <p>IN N</p>
    <p>Rome</p>
    <p>Arg. 1</p>
    <p>Phrase Type</p>
    <p>Predicate Word</p>
    <p>Head Word</p>
    <p>Parse Tree Path</p>
    <p>Voice Active</p>
    <p>Position Right</p>
  </div>
  <div class="page">
    <p>PAT Kernel [Moschitti, ACL 2004]</p>
    <p>S</p>
    <p>N</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V Paul</p>
    <p>in</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>PP</p>
    <p>IN NP</p>
    <p>jj</p>
    <p>Fv,arg.0</p>
    <p>formal</p>
    <p>N</p>
    <p>style</p>
    <p>Arg. 0</p>
    <p>a) S</p>
    <p>N</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V Paul</p>
    <p>in</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>PP</p>
    <p>IN NP</p>
    <p>jj</p>
    <p>formal</p>
    <p>N</p>
    <p>style</p>
    <p>Fv,arg.1 b) S</p>
    <p>N</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V Paul</p>
    <p>in</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>PP</p>
    <p>IN NP</p>
    <p>jj</p>
    <p>formal</p>
    <p>N</p>
    <p>style Arg. 1</p>
    <p>Fv,arg.M</p>
    <p>c)</p>
    <p>Arg.M</p>
    <p>! These are Semantic Structures</p>
    <p>! Given the sentence:</p>
    <p>[ Arg0 Paul] [ predicate delivers] [ Arg1 a talk] [ ArgM in formal Style]</p>
  </div>
  <div class="page">
    <p>In other words we consider</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>S</p>
    <p>N</p>
    <p>Paul</p>
    <p>in</p>
    <p>PP</p>
    <p>IN NP</p>
    <p>jj</p>
    <p>formal</p>
    <p>N</p>
    <p>style Arg. 1</p>
  </div>
  <div class="page">
    <p>Sub-Categorization Kernel (SCF) [Moschitti, ACL 2004]</p>
    <p>S</p>
    <p>N</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V Paul</p>
    <p>in</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>PP</p>
    <p>IN NP</p>
    <p>jj</p>
    <p>formal</p>
    <p>N</p>
    <p>style</p>
    <p>Arg. 1</p>
    <p>Arg. M</p>
    <p>Arg. 0</p>
    <p>Predicate</p>
  </div>
  <div class="page">
    <p>Experiments on Gold Standard Trees</p>
    <p>! PropBank and PennTree bank ! about 53,700 sentences</p>
    <p>! Sections from 2 to 21 train., 23 test., 1 and 22 dev.</p>
    <p>! Arguments from Arg0 to Arg5, ArgA and ArgM for</p>
    <p>a total of 122,774 and 7,359</p>
    <p>! FrameNet and Collins automatic trees ! 24,558 sentences from the 40 frames of Senseval 3</p>
    <p>! 18 roles (same names are mapped together)</p>
    <p>! Only verbs</p>
    <p>! 70% for training and 30% for testing</p>
  </div>
  <div class="page">
    <p>Argument Classification with Poly Kernel</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y d</p>
    <p>FrameNet PropBank</p>
  </div>
  <div class="page">
    <p>PropBank Results</p>
    <p>Args P3 PAT PAT+P PATP SCF+P SCFP Arg0 90.8 88.3 92.6 90.5 94.6 94.7 Arg1 91.1 87.4 91.9 91.2 92.9 94.1 Arg2 80.0 68.5 77.5 74.7 77.4 82.0 Arg3 57.9 56.5 55.6 49.7 56.2 56.4 Arg4 70.5 68.7 71.2 62.7 69.6 71.1 ArgM 95.4 94.1 96.2 96.2 96.1 96.3 Global Accuracy</p>
  </div>
  <div class="page">
    <p>Argument Classification on PAT using different Tree Fragment Extractor</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>% Training Data</p>
    <p>ST SST</p>
    <p>Linear PT</p>
  </div>
  <div class="page">
    <p>Boundary Detection</p>
    <p>NP</p>
    <p>D N</p>
    <p>VP</p>
    <p>V</p>
    <p>delivers</p>
    <p>a talk</p>
    <p>S</p>
    <p>N</p>
    <p>Paul</p>
    <p>in</p>
    <p>PP</p>
    <p>IN NP</p>
    <p>jj</p>
    <p>formal</p>
    <p>N</p>
    <p>style Arg. 1</p>
  </div>
  <div class="page">
    <p>Improvement by Marking Boundary nodes</p>
  </div>
  <div class="page">
    <p>Node Marking Effect</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>! PropBank and PennTree bank ! about 53,700 sentences</p>
    <p>! Charniak trees from CoNLL 2005</p>
    <p>! Boundary detection: ! Section 2 training</p>
    <p>! Section 24 testing</p>
    <p>! PAF and MPAF</p>
  </div>
  <div class="page">
    <p>Number of examples/nodes of Section 2</p>
  </div>
  <div class="page">
    <p>Predicate Argument Feature (PAF) vs. Marked PAF (MPAF) [Moschitti et al, CLJ 2008]</p>
  </div>
  <div class="page">
    <p>Results on FrameNet SRL [Coppola and Moschitti, LREC 2010] ! 135,293 annotated and parsed sentences.</p>
    <p>! 782 different frames (including split per pos-tag)</p>
    <p>! 90% of training data for BD and BC 121,798 sentences</p>
    <p>! 10% of testing data (1,345 sentences)</p>
  </div>
  <div class="page">
    <p>Experiments on Luna Corpus [Coppola at al, SLT 2008]</p>
    <p>Evaluation Stage Precision Recall F1</p>
    <p>Boundary Detection 0.905 0.873 0.889</p>
    <p>Boundary Detection</p>
    <p>+ Role Classification</p>
    <p>! BD and RC over 50 Human-Human dialogs ! 1,677 target words spanning 162 different frames</p>
    <p>! manually-corrected syntactic trees</p>
    <p>! Training 90% data and testing on remaining 10%</p>
    <p>! Automatic SRL viable for Spoken Dialog Data</p>
  </div>
  <div class="page">
    <p>The Relation Extraction Problem</p>
    <p>EMPLOYMENT CEO  Google</p>
    <p>LOCATED</p>
    <p>research center  Beijing</p>
    <p>Given a text with some available entities, how to recognize relations ?</p>
    <p>Last Wednesday, Eric Schmidt, the CEO of Google, defended the s e a r c h e n g i n e ' s c o o p e r a t i o n w i t h Chinese censorship as h e a n n o u n c e d t h e creation of a research center in Beijing.</p>
  </div>
  <div class="page">
    <p>Relation Extraction: The task</p>
    <p>! Task definition: to label the semantic relation between</p>
    <p>pairs of entities in a sentence ! The governor from Connecticut</p>
    <p>! Is there a relation between M1 and M2? If, so what kind of relation?</p>
    <p>M1 type: PER</p>
    <p>M2 type: LOC</p>
    <p>M := Entity Mention</p>
  </div>
  <div class="page">
    <p>Relation Extraction defined in ACE</p>
    <p>! Major relation types (from ACE 2004)</p>
    <p>! Entity types: PER, ORG, LOC, GPE, FAC, VEH, WEA</p>
  </div>
  <div class="page">
    <p>System Description (Nguyen et al, 2009)</p>
    <p>Tree Kernelbased SVMs</p>
    <p>Multi-class Classification</p>
    <p>RELATIONS</p>
    <p>Stanford Parser</p>
    <p>Parse Trees with Entities Raw texts</p>
    <p>ACE documents</p>
    <p>Entities and Relations</p>
  </div>
  <div class="page">
    <p>Relation Representation (Moschitti 2004;Zhang et al. 2006)</p>
    <p>corporation in established Iowa the by Pylant Andrew</p>
    <p>NNP VBN IN NNP</p>
    <p>NP</p>
    <p>T1-ORG</p>
    <p>NP</p>
    <p>DT</p>
    <p>T2-LOC</p>
    <p>PP</p>
    <p>VP</p>
    <p>NP</p>
    <p>IN NNP NNP</p>
    <p>NP</p>
    <p>PP</p>
    <p>PER</p>
    <p>! The Path-enclosed tree captures the PHYSICAL.LOCATED relation between corporation and Iowa</p>
  </div>
  <div class="page">
    <p>Comparison</p>
    <p>Method Data P (%) R (%) F1 (%)</p>
    <p>Zhang et al. (2006)</p>
    <p>Composite Kernel (linear) with ContextFree Parse Tree</p>
    <p>ACE 2004 73.5 67.0 70.1</p>
    <p>Ours Composite Kernel (linear) with ContextFree Parse Tree</p>
    <p>ACE 2004 69.6 68.2 69.2</p>
    <p>Both use the Path-Enclosed Tree for Relation Representation</p>
  </div>
  <div class="page">
    <p>Several Combination Kernels [Vien et al, EMNLP 2009]</p>
  </div>
  <div class="page">
    <p>Results on ACE 2004</p>
  </div>
  <div class="page">
    <p>Coreference Resolution</p>
    <p>! Subtree that covers both anaphor and antecedent candidate</p>
    <p>syntactic relations between anaphor &amp; candidate (subject, object, c-commanding, predicate structure)</p>
    <p>! Include the nodes in path between anaphor and candidate, as well as their first_level children</p>
    <p>the man in the room saw him  inst(the man, him)</p>
  </div>
  <div class="page">
    <p>Context Sequence Feature</p>
    <p>! A word sequence representing the mention expression and its context ! Create a sequence for a mention</p>
    <p>Even so, Bill Gates says that he just doesnt understand our infatuation with thin client versions of Word</p>
    <p>(so)(,) (Bill)(Gates)(says)(that)</p>
  </div>
  <div class="page">
    <p>Composite Kernel</p>
    <p>! Different kernels for different features ! Poly Kernel for baseline flat features</p>
    <p>! Tree Kernel for syntax trees</p>
    <p>! Sequence Kernel for word sequences</p>
    <p>! A composite kernel for all kinds of features</p>
    <p>! Composite Kernel = TK*PolyK+PolyK+SK</p>
  </div>
  <div class="page">
    <p>Results for pronoun resolution [Vesley et al, Coling 2008]</p>
    <p>MUC-6 ACE-02-BNews</p>
    <p>R P F R P F</p>
    <p>All attribute</p>
    <p>value features 64.3 63.1 63.7 58.9 68.1 63.1</p>
    <p>+ Syntactic Tree</p>
    <p>+ Word</p>
    <p>Sequence</p>
  </div>
  <div class="page">
    <p>Results on the overall Coreference Resolution using SVMs</p>
    <p>MUC-6 ACE02-BNews</p>
    <p>R P F R P F BaseFeature SVMs 61.5 67.2 64.2 54.8 66.1 59.9 BaseFeature +</p>
    <p>Syntax Tree 63.4 67.5 65.4 56.6 66.0 60.9</p>
    <p>BaseFeature</p>
    <p>+SyntaxTree + Word</p>
    <p>Sequences</p>
    <p>All Sources of</p>
    <p>Knowledge 60.1 76.2 67.2 60.0 65.4 63.0</p>
  </div>
  <div class="page">
    <p>Kernels for Reranking</p>
  </div>
  <div class="page">
    <p>Reranking framework</p>
    <p>Local Model</p>
  </div>
  <div class="page">
    <p>More formally</p>
    <p>! Build a set of hypotheses: Q and A pairs</p>
    <p>! These are used to build pairs of pairs,</p>
    <p>! positive instances if Hi is correct and Hj is not correct</p>
    <p>! A binary classifier decides if Hi is more probable than Hj</p>
    <p>! Each candidate annotation Hi is described by a structural representation</p>
    <p>! This way kernels can exploit all dependencies between features and labels</p>
    <p>Hi, Hj</p>
  </div>
  <div class="page">
    <p>Preference Kernel</p>
    <p>where K is a kernels on the text, e.g., in case of question and answer:</p>
    <p>K x 1 , y</p>
    <p>(x 1 )(x</p>
  </div>
  <div class="page">
    <p>Syntactic Parsing Reranking</p>
    <p>! Pairs of parse trees (Collins and Duffy, 2002)</p>
    <p>! N-best parse generated by the Collins parser</p>
    <p>! Re-ranking using STK in a perceptron algorithm</p>
  </div>
  <div class="page">
    <p>Concept Segmentation and Classification of speech</p>
    <p>! Given a transcription, i.e., a sequence of words, chunk</p>
    <p>and label subsequences with concepts</p>
    <p>! Air Travel Information System (ATIS) ! Dialog systems answering user questions</p>
    <p>! Conceptually annotated dataset</p>
    <p>! Frames</p>
  </div>
  <div class="page">
    <p>An example of concept annotation in ATIS</p>
    <p>! User request: list TWA flights from Boston to</p>
    <p>Philadelphia</p>
    <p>! The concepts are used to build rules for the dialog manager</p>
    <p>(e.g. actions for using the DB) ! from location ! to location</p>
    <p>! airline code</p>
  </div>
  <div class="page">
    <p>Our Approach [Dinarelli et al., TASL 2012]</p>
    <p>! Use of Finite State Transducer (or CRF) to generate word</p>
    <p>sequences and concepts</p>
    <p>! Probability of each annotation</p>
    <p>m best hypothesis can be generated</p>
    <p>! Idea: use a discriminative model to choose the best one ! Re-ranking and selecting the top one</p>
  </div>
  <div class="page">
    <p>Reranking for SLU</p>
    <p>FST</p>
    <p>Input Utterance</p>
    <p>ASR</p>
  </div>
  <div class="page">
    <p>Reranking concept labeling</p>
    <p>! I have a problem with my monitor</p>
    <p>Hi: I NULL have NULL a PROBLEM-B problem PROBLEM-I with NULL my HW-B monitor HW-I</p>
    <p>Hj: I NULL have NULL a NULL problem HW-B with NULL my NULL monitor</p>
  </div>
  <div class="page">
    <p>Luna Corpus</p>
    <p>! Wizard of OZ, helpdesk scenario</p>
  </div>
  <div class="page">
    <p>Media Corpus</p>
  </div>
  <div class="page">
    <p>Flat tree representation</p>
    <p>have a problem with my</p>
    <p>NULL</p>
    <p>I</p>
    <p>NULL</p>
  </div>
  <div class="page">
    <p>Cross-language approach: Italian version</p>
  </div>
  <div class="page">
    <p>Multilevel Tree</p>
  </div>
  <div class="page">
    <p>Enriched Multilevel Tree</p>
  </div>
  <div class="page">
    <p>Results on LUNA</p>
  </div>
  <div class="page">
    <p>Results on Media</p>
  </div>
  <div class="page">
    <p>Reranking for Named-Entity Recognition [Vien et al, 2010]</p>
    <p>! CRF F1 from 84.86 to 88.16</p>
    <p>! Best Italian system F1 82, improved to 84.33</p>
  </div>
  <div class="page">
    <p>Today a car a ravine</p>
    <p>pushed</p>
    <p>Reranking Predicate Argument Structures [Moschitti et al, CoNLL 2006]</p>
    <p>! SVMs F1 from 75.89</p>
    <p>to 77.25</p>
    <p>! Today, a car was pushed into a ravine.</p>
  </div>
  <div class="page">
    <p>Relational Kernels</p>
  </div>
  <div class="page">
    <p>Recognizing Textual Entailment</p>
    <p>T1 H1</p>
    <p>At the end of the year, all solid companies pay dividends.</p>
    <p>At the end of the year, all solid insurance companies pay dividends.</p>
    <p>T1  H1</p>
    <p>the textual entailment recognition task: determine whether or not a text T implies a hypothesis H</p>
    <p>Traditional machine learning approaches:</p>
    <p>similarity-based methods  distance in feature spaces</p>
    <p>learning textual entailment recognition rules from annotated examples</p>
  </div>
  <div class="page">
    <p>Determine Intra-pair links</p>
  </div>
  <div class="page">
    <p>Determine cross pair links</p>
  </div>
  <div class="page">
    <p>Our Model (Zanzotto and Moschitti, ACL2006)</p>
    <p>Defining a similarity between pairs based on:</p>
    <p>Kent((T,H),(T,H))=KI((T,H),(T,H))+KS((T,H),(T,H))</p>
    <p>! Intra-pair similarity</p>
    <p>KI((T,H),(T,H))=s(T,H)s(T,H)</p>
    <p>! Cross-pair similarity</p>
    <p>KS((T,H),(T,H)) KT(T,T)+ KT(H,H)</p>
  </div>
  <div class="page">
    <p>The final kernel</p>
    <p>where: ! c is an assignment of placeholders</p>
    <p>! t transforms the trees according to the assigned placeholders</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>BOW+LS + TK + Kent System Avg.</p>
    <p>RTE1 0.5888 0.6213 0.6300 0.54</p>
    <p>RTE2 0.6038 0.6238 0.6388 0.59</p>
    <p>! RTE1 (1st Recognising Textual Entailment Challenge) [Dagan et al., 2005] ! 567 training and 800 test examples</p>
    <p>! RTE2, [Bar Haim et al., 2006] ! 800 training and 800 test examples</p>
  </div>
  <div class="page">
    <p>RTE-2 results</p>
    <p>! Most systems use ML</p>
    <p>! Best systems use a lot of knowledge</p>
    <p>! Average accuracy still low: 0.59</p>
  </div>
  <div class="page">
    <p>Relational Kernels for Answer Reranking</p>
  </div>
  <div class="page">
    <p>Results on TREC Data (5 folds cross validation)</p>
    <p>F 1</p>
    <p>-m e</p>
    <p>a s</p>
    <p>u re</p>
    <p>Kernel Type</p>
    <p>BOW  24 POSSK+STK+PAS_PTK 39 62 % of improvement</p>
  </div>
  <div class="page">
    <p>An example of Jeopardy! Question</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Baseline Model</p>
    <p>Question</p>
    <p>Answer</p>
    <p>Methodology:</p>
  </div>
  <div class="page">
    <p>!&quot;#$%&amp;'(</p>
    <p>)'$*#+(</p>
  </div>
  <div class="page">
    <p>Best Model</p>
    <p>Methodology:</p>
    <p>Question</p>
    <p>Answer</p>
  </div>
  <div class="page">
    <p>!&quot;#$%&amp;'(</p>
  </div>
  <div class="page">
    <p>Representation Issues</p>
    <p>! Very large sentences</p>
    <p>! The Jeopardy! cues can be constituted by more</p>
    <p>than one sentence</p>
    <p>! The answer is typically composed by several</p>
    <p>sentences</p>
    <p>! Too large structures cause inaccuracies in the</p>
    <p>similarity and the learning algorithm looses some</p>
    <p>of its power</p>
  </div>
  <div class="page">
    <p>Running example (randomly picked Q/A pair from Answerbag )</p>
    <p>Question: Is movie theater popcorn vegan?</p>
    <p>Answer:</p>
    <p>(01) Any movie theater popcorn that includes butter -- and therefore dairy products -- is not vegan.</p>
    <p>(02) However, the popcorn kernels alone can be considered vegan if popped using canola, coconut or other plant oils which some theaters offer as an alternative to standard popcorn.</p>
  </div>
  <div class="page">
    <p>Shallow models for Reranking: [Sveryn&amp;Moschitti, SIGIR2012]</p>
    <p>SQ</p>
    <p>VBZ</p>
    <p>is</p>
    <p>NN</p>
    <p>movie</p>
    <p>NN</p>
    <p>theater</p>
    <p>JJ</p>
    <p>popcorn</p>
    <p>NN</p>
    <p>vegan</p>
    <p>bag of pos tags</p>
    <p>bag of words</p>
    <p>and their combina3on</p>
    <p>S</p>
    <p>DT</p>
    <p>any</p>
    <p>NN</p>
    <p>movie</p>
    <p>NN</p>
    <p>theater</p>
    <p>NN</p>
    <p>popcorn</p>
    <p>WDT</p>
    <p>that</p>
    <p>VBZ</p>
    <p>includes</p>
    <p>NN</p>
    <p>butter</p>
    <p>CC</p>
    <p>and</p>
    <p>RB</p>
    <p>therefore</p>
    <p>JJ</p>
    <p>dairy</p>
    <p>NNS</p>
    <p>products</p>
    <p>VBZ</p>
    <p>is</p>
    <p>RB</p>
    <p>not</p>
    <p>NN</p>
    <p>vegan</p>
    <p>Ques%on</p>
    <p>Answer</p>
    <p>(is) (movie) (theater) (popcorn) (vegan)</p>
    <p>(any) (movie) (theater) (popcorn) (that) (includes) (bu:er) (and) (therefore) (dairy) (products) (is) (not) (vegan)</p>
    <p>(DT) (NN) (NN) (NN) (WDT) (VBZ) (NN) (CC) (RB) (JJ) (NNS) (VBZ) (RB) (NN)</p>
    <p>(VBZ) (NN) (NN) (JJ) (NN)</p>
  </div>
  <div class="page">
    <p>Linking question with the answer 01</p>
    <p>S</p>
    <p>DT</p>
    <p>any</p>
    <p>NN</p>
    <p>movie</p>
    <p>NN</p>
    <p>theater</p>
    <p>NN</p>
    <p>popcorn</p>
    <p>WDT</p>
    <p>that</p>
    <p>VBZ</p>
    <p>includes</p>
    <p>NN</p>
    <p>butter</p>
    <p>CC</p>
    <p>and</p>
    <p>RB</p>
    <p>therefore</p>
    <p>JJ</p>
    <p>dairy</p>
    <p>NNS</p>
    <p>products</p>
    <p>VBZ</p>
    <p>is</p>
    <p>RB</p>
    <p>not</p>
    <p>NN</p>
    <p>vegan</p>
    <p>SQ</p>
    <p>VBZ</p>
    <p>is</p>
    <p>NN</p>
    <p>movie</p>
    <p>NN</p>
    <p>theater</p>
    <p>JJ</p>
    <p>popcorn</p>
    <p>NN</p>
    <p>vegan</p>
    <p>Lexical matching is on word lemmas (using WordNet lemma3zer)</p>
    <p>S</p>
    <p>RB</p>
    <p>however</p>
    <p>DT</p>
    <p>the</p>
    <p>JJ</p>
    <p>popcorn</p>
    <p>NNS</p>
    <p>kernels</p>
    <p>RB</p>
    <p>alone</p>
    <p>MD</p>
    <p>can</p>
    <p>VB</p>
    <p>be</p>
    <p>VBN</p>
    <p>considered</p>
    <p>NN</p>
    <p>vegan</p>
    <p>IN</p>
    <p>if</p>
    <p>VBN</p>
    <p>popped</p>
    <p>VBG</p>
    <p>using</p>
    <p>NN</p>
    <p>canola</p>
    <p>NN</p>
    <p>coconut</p>
    <p>CC</p>
    <p>or</p>
    <p>JJ</p>
    <p>other</p>
    <p>NN</p>
    <p>plant</p>
    <p>NNS</p>
    <p>oils</p>
    <p>WDT</p>
    <p>which</p>
    <p>DT</p>
    <p>some</p>
    <p>NNS</p>
    <p>theaters</p>
    <p>VBP</p>
    <p>offer</p>
    <p>IN</p>
    <p>as</p>
    <p>DT</p>
    <p>an</p>
    <p>NN</p>
    <p>alternative</p>
    <p>TO</p>
    <p>to</p>
    <p>JJ</p>
    <p>standard</p>
    <p>NN</p>
    <p>popcorn</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>RB</p>
    <p>however</p>
    <p>DT</p>
    <p>the</p>
    <p>JJ</p>
    <p>popcorn</p>
    <p>NNS</p>
    <p>kernels</p>
    <p>RB</p>
    <p>alone</p>
    <p>MD</p>
    <p>can</p>
    <p>VB</p>
    <p>be</p>
    <p>VBN</p>
    <p>considered</p>
    <p>NN</p>
    <p>vegan</p>
    <p>IN</p>
    <p>if</p>
    <p>VBN</p>
    <p>popped</p>
    <p>VBG</p>
    <p>using</p>
    <p>NN</p>
    <p>canola</p>
    <p>NN</p>
    <p>coconut</p>
    <p>CC</p>
    <p>or</p>
    <p>JJ</p>
    <p>other</p>
    <p>NN</p>
    <p>plant</p>
    <p>NNS</p>
    <p>oils</p>
    <p>WDT</p>
    <p>which</p>
    <p>DT</p>
    <p>some</p>
    <p>NNS</p>
    <p>theaters</p>
    <p>VBP</p>
    <p>offer</p>
    <p>IN</p>
    <p>as</p>
    <p>DT</p>
    <p>an</p>
    <p>NN</p>
    <p>alternative</p>
    <p>TO</p>
    <p>to</p>
    <p>JJ</p>
    <p>standard</p>
    <p>NN</p>
    <p>popcorn</p>
    <p>Linking question with the answer 02</p>
    <p>S</p>
    <p>DT</p>
    <p>any</p>
    <p>NN</p>
    <p>movie</p>
    <p>NN</p>
    <p>theater</p>
    <p>NN</p>
    <p>popcorn</p>
    <p>WDT</p>
    <p>that</p>
    <p>VBZ</p>
    <p>includes</p>
    <p>NN</p>
    <p>butter</p>
    <p>CC</p>
    <p>and</p>
    <p>RB</p>
    <p>therefore</p>
    <p>JJ</p>
    <p>dairy</p>
    <p>NNS</p>
    <p>products</p>
    <p>VBZ</p>
    <p>is</p>
    <p>RB</p>
    <p>not</p>
    <p>NN</p>
    <p>vegan</p>
    <p>SQ</p>
    <p>VBZ</p>
    <p>is</p>
    <p>NN</p>
    <p>movie</p>
    <p>NN</p>
    <p>theater</p>
    <p>JJ</p>
    <p>popcorn</p>
    <p>NN</p>
    <p>vegan</p>
    <p>Ques3on sentence</p>
    <p>Lexical matching is on word lemmas (using WordNet lemma3zer)</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>DT</p>
    <p>any</p>
    <p>REL-NN</p>
    <p>movie</p>
    <p>REL-NN</p>
    <p>theater</p>
    <p>REL-NN</p>
    <p>popcorn</p>
    <p>WDT</p>
    <p>that</p>
    <p>VBZ</p>
    <p>includes</p>
    <p>NN</p>
    <p>butter</p>
    <p>CC</p>
    <p>and</p>
    <p>RB</p>
    <p>therefore</p>
    <p>JJ</p>
    <p>dairy</p>
    <p>NNS</p>
    <p>products</p>
    <p>REL-VBZ</p>
    <p>is</p>
    <p>RB</p>
    <p>not</p>
    <p>REL-NN</p>
    <p>vegan</p>
    <p>SQ</p>
    <p>REL-VBZ</p>
    <p>is</p>
    <p>REL-NN</p>
    <p>movie</p>
    <p>REL-NN</p>
    <p>theater</p>
    <p>REL-JJ</p>
    <p>popcorn</p>
    <p>REL-NN</p>
    <p>vegan</p>
    <p>Linking question with the answer: relational tag</p>
    <p>Marking pos tags of the aligned words by a rela3onal tag: REL</p>
  </div>
  <div class="page">
    <p>Answerbag data</p>
    <p>! www.answerbag.com: professional question</p>
    <p>answer interactions</p>
    <p>! Divided in 30 categories, Art, education, culture,</p>
    <p>! 180,000 question-answer pairs</p>
  </div>
  <div class="page">
    <p>Learning Curve-Answerbag</p>
    <p>M R</p>
    <p>R</p>
    <p>training size (in thousands)</p>
    <p>PTK</p>
    <p>STK</p>
    <p>baseline</p>
    <p>Figure 11: Learning curve for MRR using PTK and STK. Labels along the curves correspond to the training time in hours.</p>
    <p>R E</p>
    <p>C 1 @</p>
    <p>training size (in thousands)</p>
    <p>PTK</p>
    <p>STK</p>
    <p>baseline</p>
    <p>Figure 12: Learning curve for REC1@1 using PTK and STK.</p>
    <p>on relatively small datasets. Hence we studied the learning curves for these models on 50k and 100k training sets and found that CH+REL with pruning at ray=1 provided the steepest learning curve, while maintaining the optimal tradeo between the training runtime and accuracy. We also prefer a simpler CH+REL model, which only requires to perform POS-tagging and chunking, over more refined models with NER and WNSS tags, which require additional preprocessing. Thus, we build learning curves for the CH+REL models using STK and PTK reporting MRR (Fig. 11) and REC1@1 (Fig. 12). The plots demonstrate nice scaling behavior when training CH+REL re-ranker model on larger data. For example, the PTK-based rerankers improve BM25 by about 6 absolute points in MRR, i.e., 71.6 vs. 77.8, and about 7 points in R1@1, i.e., 59.1 vs. 66.5, for a relative error reduction of about 18-20% in R1@1.</p>
    <p>building a meaningful learning curve we report the plot of R1@x, which measures the percentage of questions with at least one correct answer in the first x positions. We experimented with PTK applied to CH+REL structures also encoding NER and WNSS. Figure 13 shows that for any rank position, the simplest model outperforms semantic models. Most importantly, the Primary Search of Watson is improved up to 5 points for an error reduction of 20%.</p>
    <p>QA systems is the use of relationships between the ques</p>
    <p>R E</p>
    <p>C 1</p>
    <p>threshold</p>
    <p>baseline</p>
    <p>CH+REL</p>
    <p>CH+REL+NER</p>
    <p>CH+REL+NER+WNSS</p>
    <p>Figure 13: Recall of 1 at dierent rank position for the Jeopardy! dataset.</p>
    <p>tion and the supporting passages of its answer candidates. Supervised methods can generalize the properties found in dierent question/answer pairs and use them to evaluate the validity of new candidates. In this perspective, the most dicult aspect is the design of relational features that can enable the learning algorithm to learn the properties above. In this paper, we propose robust and simple models to learn such properties from large datasets. On one hand, we use shallow syntactic and semantic (at lexical level) representations, which can be eciently and automatically derived with high accuracy. On the other hand, we exploit the power of structural kernels for automatic engineering of a huge number of structural features. Applied to large training sets (hundreds of thousands) our models allow for ecient learning of complex question/answer relationships. Our experiments with Support Vector Machines (SVMs)</p>
    <p>and various shallow models on two datasets: Answerbag and Jeopardy! show that: (i) bag-of-features of question and answer passages, ranging from words to POS-tags or translation probabilities are not eective; (ii) relational features, i.e., encoding pair properties, become eective only when used in structures, e.g, using SK; and (iii) the best compromise between eciency and accuracy is given by the pure shallow syntactic tree structures as NER or WNSS may introduce noise. Additionally, large scale experiments show significant improvement - about 18-20% of reduction in Recall error, on two strong baselines for passage re-ranking, i.e., BM25 and IBM Watson primary search.</p>
    <p>Structured retrieval for question answering. In Proceedings of ACM SIGIR, 2007.</p>
    <p>[2] M. W. Bilotti, J. L. Elsas, J. Carbonell, and E. Nyberg. Rank learning for factoid question answering with linguistic and semantic constraints. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010), 2010.</p>
    <p>[3] M. W. Bilotti and E. Nyberg. Improving text retrieval precision and answer accuracy in question answering systems. In Proceedings of the Second Information Retrieval for Question Answering (IR4QA) Workshop at COLING 2008, 2008.</p>
    <p>[4] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer. Answering definitional questions: A hybrid approach. In M. Maybury, editor, Proceedings of AAAI 2004. AAAI Press, 2004.</p>
  </div>
  <div class="page">
    <p>Jeopardy! data (T9)</p>
    <p>! Total number of questions: 517</p>
    <p>! 50+ candidate answer passages per question</p>
    <p>! Questions with at least one correct answer: 375</p>
    <p>! Use only questions with at least one correct answer</p>
    <p>! Each relevant passage is paired with each irrelevant</p>
    <p>! Split the data: ! train 70% (259 questions): 63361 examples for re-ranker</p>
    <p>! test 30% (116 question): 5706 examples for re-ranker</p>
  </div>
  <div class="page">
    <p>Jeopardy! data</p>
    <p>M R</p>
    <p>R</p>
    <p>training size (in thousands)</p>
    <p>PTK</p>
    <p>STK</p>
    <p>baseline</p>
    <p>Figure 11: Learning curve for MRR using PTK and STK. Labels along the curves correspond to the training time in hours.</p>
    <p>R E</p>
    <p>C 1</p>
    <p>@ 1</p>
    <p>training size (in thousands)</p>
    <p>PTK</p>
    <p>STK</p>
    <p>baseline</p>
    <p>Figure 12: Learning curve for REC1@1 using PTK and STK.</p>
    <p>on relatively small datasets. Hence we studied the learning curves for these models on 50k and 100k training sets and found that CH+REL with pruning at ray=1 provided the steepest learning curve, while maintaining the optimal tradeo between the training runtime and accuracy. We also prefer a simpler CH+REL model, which only requires to perform POS-tagging and chunking, over more refined models with NER and WNSS tags, which require additional preprocessing. Thus, we build learning curves for the CH+REL models using STK and PTK reporting MRR (Fig. 11) and REC1@1 (Fig. 12). The plots demonstrate nice scaling behavior when training CH+REL re-ranker model on larger data. For example, the PTK-based rerankers improve BM25 by about 6 absolute points in MRR, i.e., 71.6 vs. 77.8, and about 7 points in R1@1, i.e., 59.1 vs. 66.5, for a relative error reduction of about 18-20% in R1@1.</p>
    <p>building a meaningful learning curve we report the plot of R1@x, which measures the percentage of questions with at least one correct answer in the first x positions. We experimented with PTK applied to CH+REL structures also encoding NER and WNSS. Figure 13 shows that for any rank position, the simplest model outperforms semantic models. Most importantly, the Primary Search of Watson is improved up to 5 points for an error reduction of 20%.</p>
    <p>QA systems is the use of relationships between the ques</p>
    <p>R E</p>
    <p>C 1</p>
    <p>threshold</p>
    <p>baseline</p>
    <p>CH+REL</p>
    <p>CH+REL+NER</p>
    <p>CH+REL+NER+WNSS</p>
    <p>Figure 13: Recall of 1 at dierent rank position for the Jeopardy! dataset.</p>
    <p>tion and the supporting passages of its answer candidates. Supervised methods can generalize the properties found in dierent question/answer pairs and use them to evaluate the validity of new candidates. In this perspective, the most dicult aspect is the design of relational features that can enable the learning algorithm to learn the properties above. In this paper, we propose robust and simple models to learn such properties from large datasets. On one hand, we use shallow syntactic and semantic (at lexical level) representations, which can be eciently and automatically derived with high accuracy. On the other hand, we exploit the power of structural kernels for automatic engineering of a huge number of structural features. Applied to large training sets (hundreds of thousands) our models allow for ecient learning of complex question/answer relationships. Our experiments with Support Vector Machines (SVMs)</p>
    <p>and various shallow models on two datasets: Answerbag and Jeopardy! show that: (i) bag-of-features of question and answer passages, ranging from words to POS-tags or translation probabilities are not eective; (ii) relational features, i.e., encoding pair properties, become eective only when used in structures, e.g, using SK; and (iii) the best compromise between eciency and accuracy is given by the pure shallow syntactic tree structures as NER or WNSS may introduce noise. Additionally, large scale experiments show significant improvement - about 18-20% of reduction in Recall error, on two strong baselines for passage re-ranking, i.e., BM25 and IBM Watson primary search.</p>
    <p>Structured retrieval for question answering. In Proceedings of ACM SIGIR, 2007.</p>
    <p>[2] M. W. Bilotti, J. L. Elsas, J. Carbonell, and E. Nyberg. Rank learning for factoid question answering with linguistic and semantic constraints. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010), 2010.</p>
    <p>[3] M. W. Bilotti and E. Nyberg. Improving text retrieval precision and answer accuracy in question answering systems. In Proceedings of the Second Information Retrieval for Question Answering (IR4QA) Workshop at COLING 2008, 2008.</p>
    <p>[4] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer. Answering definitional questions: A hybrid approach. In M. Maybury, editor, Proceedings of AAAI 2004. AAAI Press, 2004.</p>
  </div>
  <div class="page">
    <p>Part II: Advanced Topics</p>
  </div>
  <div class="page">
    <p>Efficiency Issue</p>
    <p>! Working in dual space with SVMs implies</p>
    <p>quadratic complexity</p>
    <p>! Our solutions: ! cutting-plane algorithm with sampling uSVMs [Yu &amp; Joachims, 2009] [Severyn&amp;Moschitti, ECML PKDD 2010]</p>
    <p>! Compacting SVM models with DAGs [Severyn&amp;Moschitti, ECML PKDD 2011]</p>
    <p>! Compacting SVM models with DAGs in on line models [Aiolli et al, CIDM 2007]</p>
  </div>
  <div class="page">
    <p>CPA in a nutshell</p>
    <p>Original SVM Problem ! Exponential constraints</p>
    <p>! Most are dominated by a small set of</p>
    <p>important constraints</p>
    <p>CPA SVM Approach ! Repeatedly finds the next most</p>
    <p>violated constraint</p>
    <p>! until set of constraints is a good</p>
    <p>approximation.</p>
  </div>
  <div class="page">
    <p>CPA in a nutshell</p>
    <p>Original SVM Problem ! Exponential constraints</p>
    <p>! Most are dominated by a small set of</p>
    <p>important constraints</p>
    <p>CPA SVM Approach ! Repeatedly finds the next most</p>
    <p>violated constraint</p>
    <p>! until set of constraints is a good</p>
    <p>approximation.</p>
  </div>
  <div class="page">
    <p>CPA in a nutshell</p>
    <p>Original SVM Problem ! Exponential constraints</p>
    <p>! Most are dominated by a small set of</p>
    <p>important constraints</p>
    <p>CPA SVM Approach ! Repeatedly finds the next most</p>
    <p>violated constraint</p>
    <p>! until set of constraints is a good</p>
    <p>approximation.</p>
  </div>
  <div class="page">
    <p>CPA in a nutshell</p>
    <p>Original SVM Problem ! Exponential constraints</p>
    <p>! Most are dominated by a small set of</p>
    <p>important constraints</p>
    <p>CPA SVM Approach ! Repeatedly finds the next most</p>
    <p>violated constraint</p>
    <p>! until set of constraints is a good</p>
    <p>approximation.</p>
  </div>
  <div class="page">
    <p>Computing most violated constraint (MVC)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j~g (j)  (~xi)</p>
  </div>
  <div class="page">
    <p>Computing most violated constraint (MVC)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j~g (j)  (~xi)</p>
    <p>~</p>
    <p>g</p>
    <p>(j) = 1</p>
    <p>n</p>
    <p>nX</p>
    <p>k=1</p>
    <p>c</p>
    <p>(j) k yk(~xk)</p>
  </div>
  <div class="page">
    <p>Computing most violated constraint (MVC)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j~g (j)  (~xi)</p>
    <p>~</p>
    <p>g</p>
    <p>(j) = 1</p>
    <p>n</p>
    <p>nX</p>
    <p>k=1</p>
    <p>c</p>
    <p>(j) k yk(~xk)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j</p>
    <p>nX</p>
    <p>k=1</p>
    <p>1 n</p>
    <p>c</p>
    <p>(j) k yk</p>
    <p>K(~xi, ~xk)</p>
  </div>
  <div class="page">
    <p>Approximate CPA (Yu &amp; Joachims, 2009)</p>
    <p>! Main bottleneck to apply kernels comes from the</p>
    <p>inner product:</p>
    <p>! Use sampling to approximate exact cutting plane</p>
    <p>models</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j</p>
    <p>nX</p>
    <p>k=1</p>
    <p>1 n</p>
    <p>c</p>
    <p>(j) k yk</p>
    <p>K(~xi, ~xk)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j</p>
    <p>rX</p>
    <p>k=1</p>
    <p>1 r</p>
    <p>c</p>
    <p>(j) k yk</p>
    <p>K(~xi, ~xk)</p>
  </div>
  <div class="page">
    <p>Three syntac%c trees and the resul%ng DAG</p>
    <p>VP,1VP,1</p>
    <p>NP,2V,2 NP,1</p>
    <p>D,3 JJ,1 N,3buy,2</p>
    <p>a,3 red,1 car,3</p>
    <p>VP</p>
    <p>NPV</p>
    <p>D JJ Nbuy</p>
    <p>a red car</p>
    <p>NP</p>
    <p>D N</p>
    <p>a car</p>
    <p>VP</p>
    <p>V</p>
    <p>buy</p>
    <p>NP</p>
    <p>D N</p>
    <p>a car</p>
  </div>
  <div class="page">
    <p>Three syntac%c trees and the resul%ng DAG</p>
    <p>VP,1VP,1</p>
    <p>NP,2V,2 NP,1</p>
    <p>D,3 JJ,1 N,3buy,2</p>
    <p>a,3 red,1 car,3</p>
    <p>VP</p>
    <p>NPV</p>
    <p>D JJ Nbuy</p>
    <p>a red car</p>
    <p>NP</p>
    <p>D N</p>
    <p>a car</p>
    <p>VP</p>
    <p>V</p>
    <p>buy</p>
    <p>NP</p>
    <p>D N</p>
    <p>a car</p>
  </div>
  <div class="page">
    <p>SDAG</p>
    <p>! Compacts each CPA model into a single DAG</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>jKdag( ~dag(j), ~xi)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j</p>
    <p>rX</p>
    <p>k=1</p>
    <p>1 r</p>
    <p>c</p>
    <p>(j) k yk</p>
    <p>K(~xi, ~xk)</p>
  </div>
  <div class="page">
    <p>SDAG+</p>
    <p>! Compacts all CPA models in the working set into</p>
    <p>a single DAG</p>
    <p>~w  (~xi) = Kdag( d ~</p>
    <p>dag(t), ~xi)</p>
    <p>~w  (~xi) = tX</p>
    <p>j=1</p>
    <p>j</p>
    <p>rX</p>
    <p>k=1</p>
    <p>1 r</p>
    <p>c</p>
    <p>(j) k yk</p>
    <p>K(~xi, ~xk)</p>
  </div>
  <div class="page">
    <p>Reverse Kernel Engineering</p>
    <p>! Input: an SVM model, i.e.,</p>
    <p>! Output: a ranked list of tree fragments</p>
    <p>! Intuitively the more a fragment is important the</p>
    <p>higher is its weight</p>
    <p>! Mine tree structures with higher weight first ! Start from the smallest structures</p>
    <p>! Add nodes to them</p>
    <p>! Stop when reached the max size of the list</p>
    <p>! More in detail</p>
    <p>w</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Mining the weight of a fragment</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Reverse Engineering Framework</p>
  </div>
  <div class="page">
    <p>Semantic Role Labeling</p>
  </div>
  <div class="page">
    <p>Setting</p>
  </div>
  <div class="page">
    <p>Results About 10 time faster -Training (and testing) Parallelizable!</p>
  </div>
  <div class="page">
    <p>Question Classification</p>
  </div>
  <div class="page">
    <p>Question Classification</p>
    <p>! Definition: What does HTML stand for?</p>
    <p>! Description: What's the final line in the Edgar Allan Poe poem &quot;The Raven&quot;?</p>
    <p>! Entity: What foods can cause allergic reaction in people?</p>
    <p>! Human: Who won the Nobel Peace Prize in 1992?</p>
    <p>! Location: Where is the Statue of Liberty?</p>
    <p>! Manner: How did Bob Marley die?</p>
    <p>! Numeric: When was Martin Luther King Jr. born?</p>
    <p>! Organization: What company makes Bentley cars?</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>! Tr+, Te+: number of positive/negative training instances</p>
    <p>! SSTl : linearized tree kernel</p>
  </div>
  <div class="page">
    <p>Interpretation (Abbreviation Class)</p>
    <p>(NN(abbreviation))</p>
    <p>(NP(DT)(NN(abbreviation))) (NP(DT(the))(NN(abbreviation)))</p>
    <p>(IN(for))</p>
    <p>(VB(stand))</p>
    <p>(VBZ(does))</p>
    <p>(PP(IN)) (VP(VB(stand))(PP))</p>
    <p>(NP(NP(DT)(NN(abbreviation)))(PP))</p>
    <p>(SQ(VBZ)(NP)(VP(VB(stand))(PP)))</p>
    <p>(SBARQ(WHNP)(SQ(VBZ)(NP)(VP(VB(stand))(PP)))(.))</p>
    <p>(SQ(VBZ(does))(NP)(VP(VB(stand))(PP))) (VP(VBZ)(NP(NP(DT)(NN(abbreviation)))(PP)))</p>
  </div>
  <div class="page">
    <p>Interpretation (Numeric Class)</p>
    <p>(WRB(How))</p>
    <p>(WHADVP(WRB(When))) (WRB(When))</p>
    <p>(JJ(many))</p>
    <p>(NN(year))</p>
    <p>(WHADJP(WRB)(JJ))</p>
    <p>(NP(NN(year))) (WHADJP(WRB(How))(JJ))</p>
    <p>(NN(date))</p>
    <p>(SBARQ(WHADVP(WRB(When)))(SQ)(.(?)))</p>
    <p>(SBARQ(WHADVP(WRB(When)))(SQ)(.))</p>
    <p>(NN(day))</p>
  </div>
  <div class="page">
    <p>Interpretation (Description Class)</p>
    <p>(WRB(Why))</p>
    <p>(WHADVP(WRB(Why))) (WHADVP(WRB(How)))</p>
    <p>(WHADVP(WRB))</p>
    <p>(VB(mean))</p>
    <p>(VBZ(causes))</p>
    <p>(VB(do)) (SBARQ(WHADVP(WRB(How)))(SQ))</p>
    <p>(WRB(How))</p>
    <p>(SBARQ(WHADVP(WRB(How)))(SQ)(.))</p>
    <p>(SBARQ(WHADVP(WRB(How)))(SQ)(.(?)))</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>! We used powerful ML algorithms ! e.g., Support Vector Machines</p>
    <p>! Robust to noise</p>
    <p>! Abstract representations of examples ! Similarity functions (Kernel Methods)</p>
    <p>! Structural syntactic/semantic similarity</p>
    <p>! Modeling NLP tasks with: advanced syntactic and shallow</p>
    <p>semantic structures and relational marker</p>
    <p>! Experiments demonstrate the benefit of such approach</p>
  </div>
  <div class="page">
    <p>Conclusions (contd)</p>
    <p>! Kernel methods and SVMs are useful tools to design language applications</p>
    <p>! Basic general kernel functions can be used to engineer new kernels</p>
    <p>! Little effort in selecting and marking/tailoring/decorating/ designing trees or designing sequences</p>
    <p>! Easy modeling produces state-of-the-art accuracy in many tasks, SRL, RE, CR, QA, NER, SLU, RTE</p>
    <p>! Fast prototyping and model adaptation</p>
  </div>
  <div class="page">
    <p>Future (on going work)</p>
    <p>! Deeper modeling of paragraphs: shallow semantics and</p>
    <p>discourse structures</p>
    <p>! The objective is to design more compact and accurate</p>
    <p>models applicable to whole paragraphs.</p>
    <p>! Use of reverse kernel engineering to study linguistic</p>
    <p>phenomena: ! [Pighin&amp;Moschitti, CoNLL2009, EMNLP2009, CoNLL2010]</p>
    <p>! To mine the most relevant fragments according to SVMs gradient</p>
    <p>! To use the linear space</p>
    <p>! Experimenting with combined uSVMs and linearized</p>
    <p>models: learning on large-scale data</p>
  </div>
  <div class="page">
    <p>Thank you</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>This research has been partially supported by the European project EternalS #247758:</p>
    <p>Trustworthy Eternal Systems via ! Evolving Software, Data and ! Knowledge</p>
    <p>Many Thanks to the IBM Watson team and all the</p>
    <p>other co-authors and contributors of the iKernels</p>
    <p>group</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>! I wish to thank Thorsten Joachims, Fabio</p>
    <p>Massimo Zanzotto, Daniele Pighin, Aliaksei</p>
    <p>Severyn for using some of their slides</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! M. Dinarelli, A. Moschitti, and G. Riccardi. Discriminative Reranking for Spoken Language Understanding. IEEE Transaction on Audio, Speech and Language</p>
    <p>Processing, 2011.</p>
    <p>! Alessandro Moschitti and Silvia Quarteroni, Linguistic Kernels for Answer Re-ranking in</p>
    <p>Question Answering Systems, Information and Processing Management: an International journal, ELSEVIER, 2011</p>
    <p>! Danilo Croce, Alessandro Moschitti, and Roberto Basili. Structured lexical similarity via</p>
    <p>convolution kernels on dependency trees. In Proceedings of EMNLP, Edinburgh,</p>
    <p>Scotland, UK., July 2011. Association for Computational Linguistics.</p>
    <p>! Aliaksei Severyn and Alessandro Moschitti. Fast support vector machines for structural</p>
    <p>kernels. In ECML-PKDD, 2011, Greece, 2011. Best Machine Learning Student Paper</p>
    <p>Award</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Truc Vien T. Nguyen and Alessandro Moschitti. Joint distant and direct supervision for relation extraction. In Proceedings of the The 5th International Joint Conference on</p>
    <p>Natural Language Processing, Chiang Mai, Thailand, November 2011, Association for</p>
    <p>Computational Linguistics.</p>
    <p>! Alessandro Moschitti, Jennifer Chu-carroll, Siddharth Patwardhan, James Fan, and</p>
    <p>Giuseppe Riccardi. Using syntactic and semantic structural kernels for classifying</p>
    <p>definition questions in Jeopardy! In Proceedings of EMNLP, pages 712724,</p>
    <p>Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics.</p>
    <p>! Truc Vien T. Nguyen and Alessandro Moschitti. End-to-end relation extraction using</p>
    <p>distant supervision from external semantic repositories. In Proceedings of HLT-ACL,</p>
    <p>Portland, Oregon, USA, June 2011. Association for Computational Linguistics.</p>
    <p>! Large-Scale Support Vector Learning with Structural Kernels, In Proceedings of the</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Alessandro Moschitti handouts http://disi.unitn.eu/~moschitt/teaching.html</p>
    <p>! Alessandro Moschitti and Silvia Quarteroni, Linguistic Kernels for Answer Re-ranking in</p>
    <p>Question Answering Systems, Information and Processing Management, ELSEVIER,</p>
    <p>! Yashar Mehdad, Alessandro Moschitti and Fabio Massimo Zanzotto. Syntactic/</p>
    <p>Semantic Structures for Textual Entailment Recognition. Human Language Technology</p>
    <p>- North American chapter of the Association for Computational Linguistics (HLT</p>
    <p>NAACL), 2010, Los Angeles, Calfornia.</p>
    <p>! Daniele Pighin and Alessandro Moschitti. On Reverse Feature Engineering of Syntactic</p>
    <p>Tree Kernels. In Proceedings of the 2010 Conference on Natural Language Learning,</p>
    <p>Upsala, Sweden, July 2010. Association for Computational Linguistics.</p>
    <p>! Thi Truc Vien Nguyen, Alessandro Moschitti and Giuseppe Riccardi. Kernel-based</p>
    <p>Reranking for Entity Extraction. In proceedings of the 23rd International Conference on</p>
    <p>Computational Linguistics (COLING), August 2010, Beijing, China.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Alessandro Moschitti. Syntactic and semantic kernels for short text pair categorization. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</p>
    <p>! Truc-Vien Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. Convolution kernels</p>
    <p>on constituent, dependency and sequential structures for relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</p>
    <p>pages 13781387, Singapore, August 2009.</p>
    <p>! Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. Re-ranking models</p>
    <p>based-on small training data for spoken language understanding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 10761085,</p>
    <p>Singapore, August 2009.</p>
    <p>! Alessandra Giordani and Alessandro Moschitti. Syntactic Structural Kernels for Natural</p>
    <p>Language Interfaces to Databases. In ECML/PKDD, pages 391406, Bled, Slovenia, 2009.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Alessandro Moschitti, Daniele Pighin and Roberto Basili. Tree Kernels for Semantic Role Labeling, Special Issue on Semantic Role Labeling, Computational Linguistics</p>
    <p>Journal. March 2008.</p>
    <p>! Fabio Massimo Zanzotto, Marco Pennacchiotti and Alessandro Moschitti, A Machine</p>
    <p>Learning Approach to Textual Entailment Recognition, Special Issue on Textual Entailment Recognition, Natural Language Engineering, Cambridge University Press.,</p>
    <p>! Mona Diab, Alessandro Moschitti, Daniele Pighin, Semantic Role Labeling Systems for</p>
    <p>Arabic Language using Kernel Methods. In proceedings of the 46th Conference of the Association for Computational Linguistics (ACL'08). Main Paper Section. Columbus,</p>
    <p>OH, USA, June 2008.</p>
    <p>! Alessandro Moschitti, Silvia Quarteroni, Kernels on Linguistic Structures for Answer</p>
    <p>Extraction. In proceedings of the 46th Conference of the Association for Computational Linguistics (ACL'08). Short Paper Section. Columbus, OH, USA, June 2008.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Yannick Versley, Simone Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang and Alessandro Moschitti, BART: A Modular Toolkit for</p>
    <p>Coreference Resolution, In Proceedings of the Conference on Language Resources</p>
    <p>and Evaluation, Marrakech, Marocco, 2008.</p>
    <p>! Alessandro Moschitti, Kernel Methods, Syntax and Semantics for Relational Text</p>
    <p>Categorization. In proceeding of ACM 17th Conference on Information and Knowledge</p>
    <p>Management (CIKM). Napa Valley, California, 2008.</p>
    <p>! Bonaventura Coppola, Alessandro Moschitti, and Giuseppe Riccardi. Shallow semantic</p>
    <p>parsing for spoken language understanding. In Proceedings of HLT-NAACL Short Papers, pages 8588, Boulder, Colorado, June 2009. Association for Computational</p>
    <p>Linguistics.</p>
    <p>! Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for</p>
    <p>Relational Learning from Texts, Proceedings of The 24th Annual International Conference on Machine Learning (ICML 2007).</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, Proceedings of the 45th Conference of the Association for Computational Linguistics (ACL), Prague, June 2007.</p>
    <p>! Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for Relational Learning from Texts, Proceedings of The 24th Annual International Conference on Machine Learning (ICML 2007), Corvallis, OR, USA.</p>
    <p>! Daniele Pighin, Alessandro Moschitti and Roberto Basili, RTV: Tree Kernels for Thematic Role Classification, Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-4), English Semantic Labeling, Prague, June 2007.</p>
    <p>! Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semanitc Kernels for Text Classification, to appear in the 29th European Conference on Information Retrieval (ECIR), April 2007, Rome, Italy.</p>
    <p>! Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification,</p>
    <p>Proceedings of the 45th Conference of the Association for Computational Linguistics</p>
    <p>(ACL), Prague, June 2007.</p>
    <p>! Alessandro Moschitti, Giuseppe Riccardi, Christian Raymond, Spoken Language</p>
    <p>Understanding with Kernels for Syntactic/Semantic Structures, Proceedings of IEEE</p>
    <p>Automatic Speech Recognition and Understanding Workshop (ASRU2007), Kyoto,</p>
    <p>Japan, December 2007</p>
    <p>! Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semantic</p>
    <p>Kernels for Text Classification, to appear in the 29th European Conference on</p>
    <p>Information Retrieval (ECIR), April 2007, Rome, Italy.</p>
    <p>! Stephan Bloehdorn, Alessandro Moschitti: Structure and semantics for expressive text</p>
    <p>kernels. In proceeding of ACM 16th Conference on Information and Knowledge Management (CIKM-short paper) 2007: 861-864, Portugal.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on</p>
    <p>Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.</p>
    <p>! Alessandro Moschitti, Efficient Convolution Kernels for Dependency and Constituent</p>
    <p>Syntactic Trees. In Proceedings of the 17th European Conference on Machine Learning, Berlin, Germany, 2006.</p>
    <p>! Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,</p>
    <p>Fast On-line Kernel Learning for Trees, International Conference on Data Mining</p>
    <p>(ICDM) 2006 (short paper).</p>
    <p>! Stephan Bloehdorn, Roberto Basili, Marco Cammisa, Alessandro Moschitti, Semantic</p>
    <p>Kernels for Text Classification based on Topological Measures of Feature Similarity. In</p>
    <p>Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), Hong</p>
    <p>Kong, 18-22 December 2006. (short paper).</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Roberto Basili, Marco Cammisa and Alessandro Moschitti, A Semantic Kernel to classify texts with very few training examples, in Informatica, an international journal of</p>
    <p>Computing and Informatics, 2006.</p>
    <p>! Fabio Massimo Zanzotto and Alessandro Moschitti, Automatic learning of textual</p>
    <p>entailments with cross-pair similarities. In Proceedings of COLING-ACL, Sydney, Australia, 2006.</p>
    <p>! Ana-Maria Giuglea and Alessandro Moschitti, Semantic Role Labeling via FrameNet,</p>
    <p>VerbNet and PropBank. In Proceedings of COLING-ACL, Sydney, Australia, 2006.</p>
    <p>! Alessandro Moschitti, Making tree kernels practical for natural language learning. In</p>
    <p>Proceedings of the Eleventh International Conference on European Association for</p>
    <p>Computational Linguistics, Trento, Italy, 2006.</p>
    <p>! Alessandro Moschitti, Daniele Pighin and Roberto Basili. Semantic Role Labeling via</p>
    <p>Tree Kernel joint inference. In Proceedings of the 10th Conference on Computational</p>
    <p>Natural Language Learning, New York, USA, 2006.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>! Roberto Basili, Marco Cammisa and Alessandro Moschitti, Effective use of Wordnet semantics via kernel-based learning. In Proceedings of the 9th Conference on</p>
    <p>Computational Natural Language Learning (CoNLL 2005), Ann Arbor (MI), USA, 2005</p>
    <p>! Alessandro Moschitti, A study on Convolution Kernel for Shallow Semantic Parsing. In</p>
    <p>proceedings of the 42-th Conference on Association for Computational Linguistic (ACL-2004), Barcelona, Spain, 2004.</p>
    <p>! Alessandro Moschitti and Cosmin Adrian Bejan, A Semantic Kernel for Predicate</p>
    <p>Argument Classification. In proceedings of the Eighth Conference on Computational</p>
    <p>Natural Language Learning (CoNLL-2004), Boston, MA, USA, 2004.</p>
  </div>
  <div class="page">
    <p>An introductory book on SVMs, Kernel methods and Text Categorization</p>
  </div>
  <div class="page">
    <p>Non-exhaustive reference list from other authors</p>
    <p>! V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.</p>
    <p>! P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel Methods Support Vector Learning, chapter Generalization Performance of Support Vector Machines and other Pattern Classifiers. MIT Press.</p>
    <p>! David Haussler. 1999. Convolution kernels on discrete structures. Technical report, Dept. of Computer Science, University of California at Santa Cruz.</p>
    <p>! Lodhi, Huma, Craig Saunders, John Shawe Taylor, Nello Cristianini, and Chris Watkins. Text classification using string kernels. JMLR,2000</p>
    <p>! Schlkopf, Bernhard and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA.</p>
  </div>
  <div class="page">
    <p>Non-exhaustive reference list from other authors</p>
    <p>! N. Cristianini and J. Shawe-Taylor, An introduction to support vector machines (and other kernel-based learning methods) Cambridge University Press, 2002</p>
    <p>! M. Collins and N. Duffy, New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL02, 2002.</p>
    <p>! Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for semistructured data. In Proceedings of ICML02.</p>
    <p>! S.V.N. Vishwanathan and A.J. Smola. Fast kernels on strings and trees. In Proceedings of NIPS, 2002.</p>
    <p>! Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:10591082. D. Zelenko, C. Aone, and A. Richardella. Kernel methods for relation extraction. JMLR, 3:10831106, 2003.</p>
  </div>
  <div class="page">
    <p>Non-exhaustive reference list from other authors ! Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based</p>
    <p>text analysis. In Proceedings of ACL03.</p>
    <p>! Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of SIGIR03, pages 2632.</p>
    <p>! Libin Shen, Anoop Sarkar, and Aravind k. Joshi. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP03, 2003</p>
    <p>! C. Cumby and D. Roth. Kernel Methods for Relational Learning. In Proceedings of ICML 2003, pages 107114, Washington, DC, USA, 2003.</p>
    <p>! J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
    <p>! A. Culotta and J. Sorensen. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting on ACL, Barcelona, Spain, 2004.</p>
  </div>
  <div class="page">
    <p>Non-exhaustive reference list from other authors ! Kristina Toutanova, Penka Markova, and Christopher Manning. The</p>
    <p>Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</p>
    <p>! Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree Kernels with Statistical Feature Mining. In Proceedings of NIPS05.</p>
    <p>! Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting based parse reranking with subtree features. In Proceedings of ACL05.</p>
    <p>! R. C. Bunescu and R. J. Mooney. Subsequence kernels for relation extraction. In Proceedings of NIPS, 2005.</p>
    <p>! R. C. Bunescu and R. J. Mooney. A shortest path dependency kernel for relation extraction. In Proceedings of EMNLP, pages 724731, 2005.</p>
    <p>! S. Zhao and R. Grishman. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Meeting of the ACL, pages 419426, Ann Arbor, Michigan, USA, 2005.</p>
  </div>
  <div class="page">
    <p>Non-exhaustive reference list from other authors</p>
    <p>! J. Kazama and K. Torisawa. Speeding up Training with Tree Kernels for Node Relation Labeling. In Proceedings of EMNLP 2005, pages 137 144, Toronto, Canada, 2005.</p>
    <p>! M. Zhang, J. Zhang, J. Su, , and G. Zhou. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of COLING-ACL 2006, pages 825832, 2006.</p>
    <p>! M. Zhang, G. Zhou, and A. Aw. Exploring syntactic structured features over parse trees for relation extraction using kernel methods. Information Processing and Management, 44(2):825832, 2006.</p>
    <p>! G. Zhou, M. Zhang, D. Ji, and Q. Zhu. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of EMNLP-CoNLL 2007, pages 728736, 2007.</p>
  </div>
  <div class="page">
    <p>Non-exhaustive reference list from other authors</p>
    <p>! Ivan Titov and James Henderson. Porting statistical parsers with datadefined kernels. In Proceedings of CoNLL-X, 2006</p>
    <p>! Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL.</p>
    <p>! M. Wang. A re-examination of dependency path kernels for relation extraction. In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP, 2008.</p>
  </div>
</Presentation>
