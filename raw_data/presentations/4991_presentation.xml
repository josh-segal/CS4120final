<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning to Rank with Partially-Labeled Data</p>
    <p>Learning to Rank with Partially-Labeled Data</p>
    <p>Kevin Duh University of Washington</p>
    <p>(Joint work with Katrin Kirchhoff)</p>
  </div>
  <div class="page">
    <p>MotivationMotivation</p>
    <p>Machine learning can be an effective solution for ranking problems in IR  But success depends on quality and size of training data</p>
    <p>Labeled Data Unlabeled</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>Problem StatementProblem Statement</p>
    <p>Labeled Data</p>
    <p>Supervised Learning Algorithm</p>
    <p>Ranking function f(x)</p>
    <p>Labeled Data</p>
    <p>Unlabeled Data</p>
    <p>Semi-supervised Learning Algorithm</p>
    <p>Ranking function f(x)</p>
    <p>Can we build a better ranker by adding cheap, unlabeled data?</p>
  </div>
  <div class="page">
    <p>OutlineOutline</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Query: SIGIR</p>
    <p>Query: Hotels in Singapore</p>
    <p>Ranking as Supervised Learning ProblemRanking as Supervised Learning Problem</p>
    <p>) 3 (1 [ , ,...]x tfidf pagerank=</p>
    <p>) 2 (1 [ , ,...]x tfidf pagerank=</p>
    <p>) 2 ( 2 [ , ,...]x tfidf pagerank=</p>
    <p>( 2) 1 [ , ,...]x tfidf pagerank=</p>
    <p>(1) 1 [ , ,...]x tfidf pagerank=</p>
    <p>Labels</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Query: SIGIR</p>
    <p>Query: Hotels in Singapore</p>
    <p>Ranking as Supervised Learning ProblemRanking as Supervised Learning Problem</p>
    <p>) 3 (1 [ , ,...]x tfidf pagerank=</p>
    <p>) 2 (1 [ , ,...]x tfidf pagerank=</p>
    <p>) 2 ( 2 [ , ,...]x tfidf pagerank=</p>
    <p>( 2) 1 [ , ,...]x tfidf pagerank=</p>
    <p>(1) 1 [ , ,...]x tfidf pagerank=</p>
    <p>(1) (1) (1) 1</p>
    <p>(</p>
    <p>( )</p>
    <p>) 1</p>
    <p>Train ( ) such that:</p>
    <p>( ) ( ) ( )</p>
    <p>( ) ( )</p>
    <p>f x</p>
    <p>f x f x f x</p>
    <p>f x f x</p>
    <p>&gt; &gt;</p>
    <p>&gt;</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>Test Query: Singapore Airport</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Two kinds of Partially-Labeled DataTwo kinds of Partially-Labeled Data</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query3</p>
    <p>Doc1 ? Doc2 ? Doc3 ?</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 ?</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 ?</p>
    <p>Query3</p>
    <p>Doc1 Label Doc2 Label Doc3 ?</p>
    <p>This paper Truong+, ICMIST06</p>
    <p>Some references: Amini+, SIGIR08 Agarwal, ICML06 Wang+, MSRA TechRep05 Zhou+, NIPS04 He+, ACM Multimedia 04</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Focus of this work: Transductive Learning</p>
    <p>Focus of this work: Transductive Learning</p>
    <p>Unlabeled data = Test data</p>
    <p>Transductive Learning</p>
    <p>Main question: How can knowledge of the test list help our learning algorithm?</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Test Query</p>
    <p>Doc1 ? Doc2 ? Doc3 ?</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Why transductive learning?Why transductive learning?</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Test Query</p>
    <p>Doc1 ? Doc2 ? Doc3 ?</p>
    <p>Transductive learning: Test data is fixed and observed during learning; Arguably, transduction is easier than induction</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query3</p>
    <p>Doc1 ? Doc2 ? Doc3 ?</p>
    <p>Test Query</p>
    <p>Doc1 ? Doc2 ? Doc3 ?</p>
    <p>Inductive (semi-supervised) learning: Need to generalize to new data</p>
    <p>f(x)</p>
    <p>Inductive learning = closed-book exam</p>
    <p>Transductive learning = open-note exam</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>OutlineOutline</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Thought Experiment: What information does unlabeled data provide?</p>
    <p>Thought Experiment: What information does unlabeled data provide?</p>
    <p>Query 1 &amp; Documents</p>
    <p>HITS</p>
    <p>B M</p>
    <p>HITS</p>
    <p>Query 2 &amp; Documents</p>
    <p>Observation: Direction of variance differs according to query</p>
    <p>Implication: Different feature representations are optimal for different queries</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Good results can be achieved by: Ranking Query 1 by BM25 only Ranking Query 2 by HITS only</p>
    <p>Good results can be achieved by: Ranking Query 1 by BM25 only Ranking Query 2 by HITS only</p>
    <p>Query 1 &amp; Documents</p>
    <p>Relevant webpages (high rank)</p>
    <p>Irrelevant webpages (low rank)</p>
    <p>HITS</p>
    <p>B M</p>
    <p>HITS</p>
    <p>Query 2 &amp; Documents</p>
  </div>
  <div class="page">
    <p>Proposed Method: Main IdeasProposed Method: Main Ideas</p>
    <p>Main Assumptions: 1. Different queries are best modeled by different features 2. Unlabeled data can help us discover this representation</p>
    <p>Requires: - DISCOVER(): unsupervised method for finding useful features - LEARN(): supervised method for learning to rank</p>
    <p>For each Test List: - Run DISCOVER() - Augment Feature Representation - Run LEARN() and Predict</p>
    <p>Two-Step Algorithm:</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Proposed Method: IllustrationProposed Method: Illustration</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Test Query1</p>
    <p>Doc1 ? Doc2 ? Doc3 ?</p>
    <p>x: initial feature representation Unsupervised learning outputs projection matrix A</p>
    <p>z=Ax: new feature representation</p>
    <p>Query1</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Query2</p>
    <p>Doc1 Label Doc2 Label Doc3 Label</p>
    <p>Supervised learning of ranking function</p>
    <p>predict</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>DISCOVER( ) ComponentDISCOVER( ) Component</p>
    <p>Goal of DISCOVER( ): Find useful patterns on the test list</p>
    <p>Principal Components Analysis (PCA)  Discovers direction of maximum variance  View low variance directions as noise</p>
    <p>Kernel PCA [Scholkopf+, Neural Computation 98]  Non-linear extension to PCA via the Kernel Trick 1. Maps inputs non-linearly to high-dimensional space. 2. Performs PCA in that space</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Kernels for Kernel PCAKernels for Kernel PCA</p>
    <p>)( ,,K x x x x =&lt;  &gt; ) exp( || ||)( ,K xx x x =</p>
    <p>) (1( , ), dK xx x x = + &lt;  &gt;</p>
    <p>Linear</p>
    <p>Polynomial</p>
    <p>Gaussian</p>
    <p>Diffusion Random walk between x, x on graph</p>
    <p>( , )K x x =</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>LEARN( ) ComponentLEARN( ) Component</p>
    <p>Goal of LEARN( ):  Optimize some ranking metric on labeled data</p>
    <p>RankBoost [Freund+, JMLR 2003]  Inherent Feature Selection  Few parameters to tune</p>
    <p>Other supervised ranking methods are possible:  RankNet, Rank SVM, ListNet, FRank, SoftRank, etc.</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Summary of Proposed MethodSummary of Proposed Method</p>
    <p>Relies on unlabeled test data to learn good feature representation</p>
    <p>Adapts the supervised learning process to each test list</p>
    <p>Caveats:  DISCOVER() may not always find features that are</p>
    <p>helpful for LEARN()  Run LEARN() at query time  Computational speedup is</p>
    <p>needed in practical application</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>OutlineOutline</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Experiment Setup (1/2)Experiment Setup (1/2)</p>
    <p>LETOR Dataset [Liu+, LR4IR 2007]:</p>
    <p>Additional features generated by Kernel PCA:  5 kernels: Linear, Polynomial, Gaussian, Diffusion 1, Diffusion 2  Extract 5 principal components for each</p>
    <p>OHSUMEDTREC04TREC03</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Experiment Setup (2/2)Experiment Setup (2/2)</p>
    <p>Comparison of 3 systems:  Baseline: Supervised RankBoost  Transductive: Proposed method:</p>
    <p>Kernel PCA + Supervised RankBoost  Combined: Average of Baseline, Transductive outputs</p>
    <p>Evaluation:  Mean Averaged Precision (MAP)  Normalized Discount Cumulative Gain (NDCG)  see the paper</p>
    <p>( ) ( ) ( )) { ( ( )}( )i i ibaseline n transductive nf x sort f x f x= +</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Overall Results (MAP)Overall Results (MAP)</p>
    <p>T R E C 0 3</p>
    <p>T R E C 0 4</p>
    <p>O H S U M E D</p>
    <p>Baseline</p>
    <p>Transductive</p>
    <p>Combined</p>
    <p>(2 datasets)  The rankers make complementary mistakes</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
    <p>b a se</p>
    <p>lin e</p>
    <p>tr a n sd</p>
    <p>u ct</p>
    <p>iv e</p>
    <p>co m</p>
    <p>b in</p>
    <p>e d</p>
  </div>
  <div class="page">
    <p>Did improvements come from Kernel PCA per se, or its transductive use?</p>
    <p>Did improvements come from Kernel PCA per se, or its transductive use?</p>
    <p>T R E C 0 3</p>
    <p>T R E C 0 4</p>
    <p>O H S U M E D</p>
    <p>Baseline</p>
    <p>Transductive</p>
    <p>KernelPCA on</p>
    <p>Train</p>
    <p>Answer: Transductive use - Running KPCA on the training set</p>
    <p>(traditional feature extraction) gives little gains - Gains are a result of test-specific rankers</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
    <p>b a se</p>
    <p>lin e</p>
    <p>tr a n sd</p>
    <p>u ct</p>
    <p>iv e</p>
    <p>K P</p>
    <p>C A</p>
    <p>o n t</p>
    <p>ra inMAP</p>
  </div>
  <div class="page">
    <p>Do results vary by query?Do results vary by query?</p>
    <p>Answer: - Yes. For some queries, it is better not to use the transductive method</p>
    <p>TREC 2003. MAP by query</p>
    <p>Transductive</p>
    <p>B a</p>
    <p>se lin</p>
    <p>e</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>What kernels are most useful?What kernels are most useful?</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
    <p>Original+Diffusion Original+Polynomial+Linear Original+Polynomial+Diffusion Original+Polynomial</p>
    <p>Original+Linear Original+Gaussian+Diffusion Original+Diffusion+Linear Original only</p>
    <p>Answer: There is a diversity of kernels that lead to good performance. Different test list have different structure</p>
  </div>
  <div class="page">
    <p>ConclusionConclusion</p>
    <p>Unlabeled data can be useful for ranking problems</p>
    <p>Two-step transductive algorithm:  Adapts the supervised component using a feature</p>
    <p>representation that better models the test list</p>
    <p>Overall results are positive  but results vary at the query-level</p>
    <p>Future work:  Computational speed-up  Different LEARN() and DISCOVER() components  Other ways to exploit unlabeled data</p>
    <p>Problem Definition | Proposed Method | Result and Analysis</p>
  </div>
  <div class="page">
    <p>Thanks for your attention!Thanks for your attention!</p>
    <p>Acknowledgments:  U.S. National Science Foundation Graduate Fellowship  Travel Grant supported by:</p>
    <p>SIGIR  Dr. Amit Singhal (made in honor of Donald B. Crouch)  Microsoft Research (in honor of Karen Spark Jones)</p>
  </div>
  <div class="page">
    <p>The time is ripe for Semi-supervised Ranking!The time is ripe for Semi-supervised Ranking!</p>
    <p>Both Semi-supervised Classification and Learning to Rank have become well-established sub-fields with many techniques</p>
    <p>Semisupervised Ranking</p>
    <p>Paper Count in SIGIR, CIKM, ICML, NIPS</p>
  </div>
  <div class="page">
    <p>Computation Time (OHSUMED)Computation Time (OHSUMED)</p>
    <p>On Intel x86-32 (3GHz CPU)  Kernel PCA (Matlab/C-Mex): 4.3sec/query  Rankboost (C++): 0.7sec/iteration  Total time (Assuming 150 iterations): 109sec/query</p>
    <p>(233sec/query for TREC)</p>
    <p>Kernel PCA: O(n^3) for n documents  Sparse KPCA: O(n)</p>
  </div>
</Presentation>
