<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Enabling Fast, Noncontiguous GPU Data Movement in Hybrid MPI+GPU Environments</p>
    <p>John Jenkins, Nagiza Samatova</p>
    <p>James Dinan, Pavan Balaji, Rajeev Thakur  Argonne National Laboratory</p>
    <p>North Carolina State University</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>MPI-ACC</p>
    <p>Contributions</p>
    <p>GPU, MPI, datatypes background</p>
    <p>Datatype processing algorithm</p>
    <p>Experimental evaluation</p>
  </div>
  <div class="page">
    <p>MPI-ACC Accelerator-aware</p>
    <p>Data Movement Within MPI</p>
    <p>Accelerator-aware Data Movement</p>
    <p>Within MPI</p>
    <p>Inter-node1Inter-node 1</p>
    <p>Intra-node2,3Intra-node 2,3</p>
    <p>ContiguousContiguous Noncontiguous4Noncontiguous 4</p>
    <p>Multiple accel. (OpenCL and CUDA)</p>
    <p>Portably leverage systemand vendor-specific optimizations</p>
    <p>-I/O Hub Aware, -IPC handle reuse, -Low overhead</p>
    <p>-Attribute-driven -NUMA-aff. aware -PCIe-aff. aware -OpenCL handle caching</p>
  </div>
  <div class="page">
    <p>MPI-ACC  Features</p>
    <p>Communication  Dynamic Pipelining</p>
    <p>NUMA-affinity aware  PCIe-affinity aware</p>
    <p>Topology-agnostic  Extended large-message protocol,</p>
    <p>handle passing</p>
    <p>API  Explicit Interfaces</p>
    <p>MPI_CUDA_Send, MPI_OpenCL_Recv, etc.</p>
    <p>Datatype attributes  Elegantly handles different accelerator</p>
    <p>views of memory  E.g. CUDA void* vs. OpenCL context,</p>
    <p>memory handles.</p>
    <p>GPU 1 GPU 0</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
    <p>GPU 2</p>
    <p>Intel (with QPI)</p>
    <p>AMD (with HT)</p>
    <p>MPI Datatype</p>
    <p>BUFTYPE=OCL</p>
    <p>CL_Context</p>
    <p>CL_Mem</p>
    <p>CL_Device_ID</p>
    <p>CL_Cmd_queue</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>MPI datatype processing algorithm:  generalized to any datatype  fine-grained parallel, GPU-optimized</p>
    <p>Investigate resource contention scenarios for packing on the GPU, for both DMA-based and kernel-based packing.</p>
  </div>
  <div class="page">
    <p>GPU Memory Spaces / Interconnect</p>
    <p>Global Memory (GDDR_)</p>
    <p>SM SM SM SM SM</p>
    <p>L2 Cache</p>
    <p>shared cache shared cache shared cache shared cache shared cache</p>
    <p>PCI Bus - High Bandwidth - High Latency</p>
    <p>RAM</p>
    <p>PCI Controller NIC</p>
    <p>GPU/CPU Memory Distinct!</p>
    <p>CPU</p>
  </div>
  <div class="page">
    <p>Derived Datatypes: Vector Example</p>
    <p>Communication, I/O on noncontiguous data  MPI_Type_vector(count, blocklength, stride, oldtype, newtype)</p>
    <p>Parameters:  count  number of blocks  blocklength  number of contiguous oldtype per block  stride  distance between elements (wrt oldtype or bytes)</p>
    <p>extent</p>
    <p>Pack</p>
    <p>size Recorded by Dataloop</p>
    <p>MPI_Type_vector( 3, 1, 2, DOUBLE, v0)</p>
    <p>MPI_Type_vector( 4, 1, 3, v0, v1)</p>
    <p>PCIe, network, disk utilization!</p>
  </div>
  <div class="page">
    <p>GPU Datatype Processing Goals</p>
    <p>Characteristic Reason CPU processing comparison</p>
    <p>Pack non-contiguous data</p>
    <p>Efficient PCIe, network, disk usage</p>
    <p>Same</p>
    <p>Fine-grain parallel GPU arch. Serial</p>
    <p>No inter-thread dependencies</p>
    <p>Avoid parallel computation stalls</p>
    <p>Stack-based packing state</p>
    <p>Computation/transfer overlap</p>
    <p>PCIe, network best practice</p>
    <p>Same, but no PCIe</p>
    <p>Compact datatype representation</p>
    <p>Multiprocessor caching, memory optimization</p>
    <p>Tree-based</p>
  </div>
  <div class="page">
    <p>Traversal Algorithm</p>
    <p>Key Insight  We can determine where each element (double, int, etc.) is using only datatype encoding + number of primitive elements per datatype!</p>
    <p>input: element ID output: read_offset, write_offset</p>
    <p>Load child type read_offset=0 write_offset=0</p>
    <p>read_offset += f(ID, type, type.extent) write_offset += g(ID, type, type.size)</p>
    <p>Is type leaf?</p>
    <p>No</p>
    <p>End Yes</p>
    <p>How to compute f, g?</p>
  </div>
  <div class="page">
    <p>Example Offset Computation: Vector</p>
    <p>child_offset = ID / type.child_elements block_offset = child_offset / type.blocklength offset_in += block_offset * type.stride + (child_offset%type.blocklength) * type.extent offset_out += child_offset * type.size ID = ID % type.child_elements</p>
    <p>child_offset</p>
    <p>block_offset</p>
    <p>offset_out</p>
    <p>offset_in</p>
    <p>input buffer</p>
    <p>output buffer</p>
    <p>child need not know about parent!</p>
  </div>
  <div class="page">
    <p>Variable length handling: Blockwise binary search (indexed, struct)</p>
    <p>prefix sum</p>
    <p>blocklength 2 1 3 1</p>
    <p>(7 threads)</p>
  </div>
  <div class="page">
    <p>Variable length handling: Blockwise binary search (indexed, struct)</p>
    <p>prefix sum</p>
    <p>blocklength 2 1 3 1</p>
    <p>Iteration 1</p>
    <p>Thread ID</p>
  </div>
  <div class="page">
    <p>Variable length handling: Blockwise binary search (indexed, struct)</p>
    <p>prefix sum</p>
    <p>blocklength 2 1 3 1</p>
    <p>Iteration 2</p>
  </div>
  <div class="page">
    <p>Variable length handling: Blockwise binary search (indexed, struct)</p>
    <p>prefix sum</p>
    <p>blocklength 2 1 3 1</p>
    <p>Iteration 3</p>
  </div>
  <div class="page">
    <p>Type Fixed Variable</p>
    <p>Common count, size, extent, #primitives</p>
    <p>Contig</p>
    <p>Vector stride, blocklength</p>
    <p>Indexed lookaside offset blocklengths, displacements</p>
    <p>Blockindexed blocklength, lookaside offset</p>
    <p>displacements</p>
    <p>Struct lookaside offset blocklengths, displacements, types, child type IDs</p>
    <p>Subarray dimensions, lookaside offset</p>
    <p>start offsets, sizes, subsizes</p>
    <p>Inorder Buffer Lookaside Buffer</p>
    <p>cache in shared memory cache if enough space</p>
    <p>GPU Datatypes Representation</p>
  </div>
  <div class="page">
    <p>Algorithm Characteristics - Summary</p>
    <p>Cache  Fixed-size parameter caching  Variable-length parameter caching if size permits</p>
    <p>Coalescence  Type representation load  fully coalesced  High on output  contiguous data  Implicit on input (adjacent threads, adjacent items)</p>
    <p>Sensitive to data distribution</p>
    <p>Thread Branch Divergence  Only on indexed, struct types  Clustering due to locality creates similar search paths</p>
    <p>Bus Efficiency  Single, contiguous writes zero-copy!</p>
  </div>
  <div class="page">
    <p>Benchmarking - Test Datatypes</p>
    <p>Datatype CUDA Impl. Notes</p>
    <p>contiguous cudaMemcpy Kernel overhead.</p>
    <p>Common. Cannot be represented by vector type.</p>
    <p>indexed cudaMemcpy per-block Irregular, does not map well to CUDA DMA.</p>
    <p>blockindexed cudaMemcpy per-block No binary search, compare against indexed</p>
    <p>C-style struct cudaMemcpy per-extent Branch divergence on read/write.</p>
    <p>* Aligned on CUDA-optimized byte boundaries</p>
  </div>
  <div class="page">
    <p>Comparison  CUDA DMA</p>
    <p>Vector  depends on parameterization (later)</p>
    <p>Subarray  latency aggregated in CUDA calls.</p>
    <p>Irregular types  huge speedup (no reasonable CUDA equivalent)</p>
    <p>Overhead  ~ a few s</p>
  </div>
  <div class="page">
    <p>Comparison  Vector Parameterizations</p>
    <p>CUDA performs best on multiples of 64 bytes.</p>
    <p>CUDA performs poorly otherwise.</p>
    <p>Same goes for vector stride.</p>
  </div>
  <div class="page">
    <p>Comparison  Vector Communication (Ping Pong)</p>
    <p>CUDA DMA best for small buffers, large blocks.  NOTE: data laid out to be CUDA</p>
    <p>optimal</p>
    <p>Packing kernel best for larger buffers, small blocks.</p>
    <p>MVAPICH  serialized packing, PCIe (ours fully pipelined through zerocopy).  Performance doesn't carry over to</p>
    <p>other datatypes  tied to CUDA DMA.</p>
  </div>
  <div class="page">
    <p>Resource Contention</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>PCI-e (full duplex) SMs</p>
    <p>Contention Point!</p>
    <p>Contention Point!</p>
    <p>Contention Point???</p>
  </div>
  <div class="page">
    <p>Resource Contention</p>
    <p>SM CPU  GPU</p>
    <p>GPU  CPU</p>
    <p>Packing (kernel)</p>
    <p>yes somewhat* yes**</p>
    <p>CUDA no somewhat* yes</p>
    <p>* shouldn't happen (scheduler artifact?)</p>
    <p>** PCIe transactions driven by SMs (zero copy) treated more favorably</p>
  </div>
  <div class="page">
    <p>Results Summary</p>
    <p>Use packing for  irregular types (Order of Magnitude Speedup)  large sparse transfers (Order of Magnitude Speedup)  types which do not adhere to CUDA-optimized memory layouts</p>
    <p>Use CUDA DMA for  small transfers,  2D, 3D arrays with large, contiguous chunks.</p>
    <p>Use hand-coded packing kernels for small sized, simple types.</p>
    <p>Datatypes implementations can control for these cases!  Packing is complementary rather than competing</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Acknowledgements:</p>
    <p>- U.S. Department of Energy under contract DE-AC02-06CH11357 - National Science Foundation under Grant No. 0958311.</p>
    <p>Thanks!</p>
  </div>
  <div class="page">
    <p>Extra Slides</p>
  </div>
  <div class="page">
    <p>Future Work / CUDA Wish List</p>
    <p>Decide which packing method best at runtime (CUDA DMA, kernel, etc.)</p>
    <p>We don't have control over what's going on in the GPU  Greedy scheduler  Priority mechanisms?</p>
    <p>CUDA, OpenCL can't query GPU utilization!  Nvidia-smi  coarse grain measurement  Need queue information!</p>
    <p>Other ideas  persistent kernel (allocate single SM for</p>
    <p>entirety of MPI application)</p>
    <p>SM SM SM SM</p>
    <p>UserMPI-ACC</p>
  </div>
  <div class="page">
    <p>MPI+GPU Support in Other Libraries</p>
    <p>MVAPICH (Wang et al., ISC '11, Cluster '11)  Allows communication of contiguous CUDA GPU buffers, vector</p>
    <p>datatypes using 2D DMA, derived datatypes using DMA per item.  Enabled through CUDA Unified Virtual Addressing (UVA).</p>
    <p>OpenMPI  Enabled through CUDA UVA, derived datatypes using DMA per item.</p>
    <p>DCGN (Stuart &amp; Owens, IPDPS '09)  GPU-sourced communication, using polling CPU thread.</p>
  </div>
  <div class="page">
    <p>Comparison  CUDA DMA</p>
  </div>
  <div class="page">
    <p>Comparison  Type-specific Packing Kernels</p>
  </div>
  <div class="page">
    <p>Comparison  Packing Component Costs</p>
  </div>
  <div class="page">
    <p>Comparison  Vector Communication</p>
  </div>
</Presentation>
