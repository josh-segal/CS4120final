<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>M a t r i x F a c t o r i z a t i o n w i t h K n o w l e d g e G r a p h P r o p a g a t i o n f o r U n s u p e r v i s e d S p o k e n L a n g u a g e U n d e r s t a n d i n g</p>
    <p>Yun-Nung (Vivian) Chen William Yang Wang Anatole Gershman</p>
    <p>Alexander I. Rudnicky</p>
    <p>Email: yvchen@cs.cmu.edu Website: http://vivianchen.idv.tw</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>A POPULAR ROBOT - BAYMAX</p>
    <p>Big Hero 6 -- Video content owned and licensed by Disney Entertainment, Marvel Entertainment, LLC, etc 4</p>
    <p>Baymax is capable of maintaining a good spoken dialogue system and learning new knowledge for better understanding and interacting with people.</p>
  </div>
  <div class="page">
    <p>SPOKEN DIALOGUE SYSTEM (SDS)</p>
    <p>Spoken dialogue systems are the intelligent agents that are able to help users finish tasks more efficiently via speech interactions.</p>
    <p>Spoken dialogue systems are being incorporated into various devices (smart-phones, smart TVs, in-car navigating system, etc).</p>
    <p>Apple</p>
    <p>s Siri</p>
    <p>Microsofts</p>
    <p>Cortana</p>
    <p>Amazon</p>
    <p>s Echo Samsungs SMART TV</p>
    <p>Google Now</p>
    <p>https://www.apple.com/ios/siri/</p>
    <p>http://www.windowsphone.com/en-us/how-to/wp8/cortana/meet-cortana</p>
    <p>http://www.xbox.com/en-US/</p>
    <p>http://www.amazon.com/oc/echo/</p>
    <p>http://www.samsung.com/us/experience/smart-tv/</p>
    <p>https://www.google.com/landing/now/</p>
    <p>Microsofts</p>
    <p>XBOX Kinect</p>
  </div>
  <div class="page">
    <p>CHALLENGES FOR SDS</p>
    <p>An SDS in a new domain requires 1) A hand-crafted domain ontology</p>
    <p>With increasing spoken interactions, building domain ontologies and annotating utterances cost a lot so that the data does not scale up.</p>
    <p>The goal is to enable an SDS to automatically learn this knowledge so that open domain requests can be handled.</p>
  </div>
  <div class="page">
    <p>INTERACTION EXAMPLE</p>
    <p>find an inexpensive eating place for taiwanese food User</p>
    <p>Intelligent Agent Q: How does a dialogue system process this request?</p>
    <p>Inexpensive Taiwanese eating places include Din Tai Fung, etc. What do you want to choose?</p>
  </div>
  <div class="page">
    <p>SDS PROCESS  AVAILABLE DOMAIN ONTOLO GY</p>
    <p>target</p>
    <p>food price AMOD</p>
    <p>NN</p>
    <p>seeking PREP_FOR</p>
    <p>Organized Domain Knowledge</p>
    <p>find an inexpensive eating place for taiwanese food</p>
    <p>Intelligent Agent</p>
    <p>User</p>
  </div>
  <div class="page">
    <p>SDS PROCESS  AVAILABLE DOMAIN ONTOLO GY</p>
    <p>target</p>
    <p>food price AMOD</p>
    <p>NN</p>
    <p>seeking PREP_FOR</p>
    <p>Organized Domain Knowledge</p>
    <p>find an inexpensive eating place for taiwanese food</p>
    <p>Intelligent Agent</p>
    <p>Ontology Induction (semantic slot)</p>
    <p>User</p>
  </div>
  <div class="page">
    <p>SDS PROCESS  AVAILABLE DOMAIN ONTOLO GY</p>
    <p>target</p>
    <p>food price AMOD</p>
    <p>NN</p>
    <p>seeking PREP_FOR</p>
    <p>Organized Domain Knowledge</p>
    <p>find an inexpensive eating place for taiwanese food User</p>
    <p>Intelligent Agent</p>
    <p>Ontology Induction (semantic slot)</p>
    <p>Structure Learning (inter-slot relation)</p>
  </div>
  <div class="page">
    <p>SDS PROCESS  SPOKEN LANGUAGE UNDERSTA N DI N G ( SLU)</p>
    <p>target</p>
    <p>food price AMOD</p>
    <p>NN</p>
    <p>seeking PREP_FOR</p>
    <p>Organized Domain Knowledge</p>
    <p>find an inexpensive eating place for taiwanese food</p>
    <p>Intelligent Agent</p>
    <p>seeking=find target=eating place price=inexpensive food=taiwanese food</p>
    <p>Spoken Language Understanding</p>
    <p>User</p>
  </div>
  <div class="page">
    <p>find an inexpensive eating place for taiwanese food</p>
    <p>SELECT restaurant { restaurant.price=inexpensive restaurant.food=taiwanese food }</p>
    <p>Din Tai Fung Boiling Point</p>
    <p>: :</p>
    <p>SDS PROCESS  DIALOGU E MANAGEMENT (DM)</p>
    <p>Intelligent Agent</p>
    <p>User</p>
    <p>Inexpensive Taiwanese eating places include Din Tai Fung, Boiling Point, etc. What do you want to choose?</p>
  </div>
  <div class="page">
    <p>GOALS</p>
    <p>find an inexpensive eating place for taiwanese food User</p>
    <p>target</p>
    <p>food price AMOD</p>
    <p>NN</p>
    <p>seeking PREP_FOR</p>
    <p>SELECT restaurant { restaurant.price=inexpensive restaurant.food=taiwanese food }</p>
    <p>Ontology Induction (semantic slot)</p>
    <p>Structure Learning (inter-slot relation)</p>
    <p>Spoken Language Understanding</p>
  </div>
  <div class="page">
    <p>GOALS</p>
    <p>Ontology Induction</p>
    <p>Structure Learning</p>
    <p>Spoken Language Understanding</p>
    <p>Knowledge Acquisition SLU Modeling</p>
    <p>find an inexpensive eating place for taiwanese food User</p>
  </div>
  <div class="page">
    <p>SPOKEN LANGUAGE UNDERSTANDING</p>
    <p>Input: user utterances</p>
    <p>Output: the domain-specific semantic concepts included in each utterance</p>
    <p>SLU Model</p>
    <p>target=restaurant price=cheap</p>
    <p>can I have a cheap restaurant Ontology Induction</p>
    <p>Unlabeled Collection</p>
    <p>Semantic KG</p>
    <p>Frame-Semantic Parsing Fw Fs</p>
    <p>Feature Model</p>
    <p>Rw</p>
    <p>Rs</p>
    <p>Knowledge Graph Propagation Model</p>
    <p>Word Relation Model</p>
    <p>Lexical KG</p>
    <p>Slot Relation Model</p>
    <p>Structure Learning</p>
    <p>.</p>
    <p>Semantic KG</p>
    <p>SLU Modeling by Matrix Factorization</p>
    <p>Semantic Representation</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>PROBABILISTIC FRAME-SEMANTIC PARSING</p>
    <p>FrameNet [Baker et al., 1998]  a linguistically semantic resource, based on the frame-semantics</p>
    <p>theory</p>
    <p>words/phrases can be represented as frames</p>
    <p>low fat milk  milk evokes the food frame;</p>
    <p>low fat fills the descriptor frame element</p>
    <p>SEMAFOR [Das et al., 2014]  a state-of-the-art frame-semantics parser, trained on manually</p>
    <p>annotated FrameNet sentences</p>
    <p>Baker et al., &quot;The berkeley framenet project,&quot; in Proc. of International Conference on Computational linguistics, 1998.</p>
    <p>Das et al., &quot; Frame-semantic parsing,&quot; in Proc. of Computational Linguistics, 2014. 17</p>
  </div>
  <div class="page">
    <p>FRAME-SEMANTIC PARSING FOR UTTERANCES</p>
    <p>can i have a cheap restaurant</p>
    <p>Frame: capability FT LU: can FE LU: i</p>
    <p>Frame: expensiveness FT LU: cheap</p>
    <p>Frame: locale by use FT/FE LU: restaurant</p>
    <p>Good!</p>
    <p>Good! ?</p>
    <p>FT: Frame Target; FE: Frame Element; LU: Lexical Unit</p>
  </div>
  <div class="page">
    <p>SPOKEN LANGUAGE UNDERSTANDING</p>
    <p>Input: user utterances</p>
    <p>Output: the domain-specific semantic concepts included in each utterance</p>
    <p>SLU Model</p>
    <p>target=restaurant price=cheap</p>
    <p>can I have a cheap restaurant Ontology Induction</p>
    <p>Unlabeled Collection</p>
    <p>Semantic KG</p>
    <p>Frame-Semantic Parsing Fw Fs</p>
    <p>Feature Model</p>
    <p>Rw</p>
    <p>Rs</p>
    <p>Knowledge Graph Propagation Model</p>
    <p>Word Relation Model</p>
    <p>Lexical KG</p>
    <p>Slot Relation Model</p>
    <p>Structure Learning</p>
    <p>.</p>
    <p>Semantic KG</p>
    <p>SLU Modeling by Matrix Factorization</p>
    <p>Semantic Representation</p>
    <p>Y.-N. Chen et al., &quot;Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding,&quot; in Proc. of ACL-IJCNLP, 2015.</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation (for 1st issue)</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>KNOWLEDGE GRAPH PROPAGATION MODEL</p>
    <p>Assumption: The domain-specific words/slots have more dependency to each other.</p>
    <p>Word Relation Model Slot Relation Model</p>
    <p>word relation matrix</p>
    <p>slot relation matrix</p>
    <p>Word Observation Slot Candidate</p>
    <p>T ra</p>
    <p>in</p>
    <p>cheap restaurant food expensiveness</p>
    <p>locale_by_use</p>
    <p>food</p>
    <p>st</p>
    <p>Slot Induction</p>
    <p>Relation matrices allow each node to propagate scores to its neighbors in the knowledge graph, so that domain-specific words/slots have higher scores after matrix multiplication.</p>
    <p>i like</p>
    <p>capability</p>
    <p>locale_by_use</p>
    <p>food expensiveness</p>
    <p>seeking</p>
    <p>relational_quantity desiring</p>
    <p>Utterance 1 i would like a cheap restaurant</p>
    <p>find a restaurant with chinese food Utterance 2</p>
    <p>show me a list of cheap restaurants Test Utterance</p>
  </div>
  <div class="page">
    <p>KNOWLEDGE GRAPH CONSTRUCTION</p>
    <p>ccomp</p>
    <p>amod dobj nsubj det</p>
    <p>Syntactic dependency parsing on utterances</p>
    <p>can i have a cheap restaurant capability expensiveness locale_by_use</p>
    <p>Word-based lexical knowledge graph</p>
    <p>Slot-based semantic knowledge graph</p>
    <p>restaurant</p>
    <p>can</p>
    <p>have</p>
    <p>i</p>
    <p>a</p>
    <p>cheap</p>
    <p>w</p>
    <p>w</p>
    <p>capability locale_by_use expensiveness</p>
    <p>s</p>
  </div>
  <div class="page">
    <p>KNOWLEDGE GRAPH CONSTRUCTION</p>
    <p>Word-based lexical knowledge graph</p>
    <p>Slot-based semantic knowledge graph</p>
    <p>restaurant</p>
    <p>can</p>
    <p>have</p>
    <p>i</p>
    <p>a</p>
    <p>cheap</p>
    <p>w</p>
    <p>w</p>
    <p>capability locale_by_use expensiveness</p>
    <p>s</p>
    <p>The edge between a node pair is weighted as relation importance to propagate the scores via a relation matrix</p>
    <p>How to decide the weights to represent relation importance?</p>
  </div>
  <div class="page">
    <p>WEIGHT MEASUREMENT BY EMBEDDINGS</p>
    <p>Levy and Goldberg, &quot; Dependency-Based Word Embeddings,&quot; in Proc. of ACL, 2014. 24</p>
    <p>Dependency-based word embeddings</p>
    <p>Dependency-based slot embeddings</p>
    <p>can = [0.8  0.24] have = [0.3  0.21]</p>
    <p>:</p>
    <p>:</p>
    <p>expensiveness = [0.12  0.7]</p>
    <p>capability = [0.3  0.6]</p>
    <p>:</p>
    <p>:</p>
    <p>can i have a cheap restaurant</p>
    <p>ccomp</p>
    <p>amod dobj nsubj det</p>
    <p>have a capability expensiveness locale_by_use</p>
    <p>ccomp</p>
    <p>amod dobj nsubj det</p>
  </div>
  <div class="page">
    <p>WEIGHT MEASUREMENT BY EMBEDDINGS</p>
    <p>Compute edge weights to represent relation importance</p>
    <p>Slot-to-slot semantic relation  : similarity between slot embeddings</p>
    <p>Slot-to-slot dependency relation  : dependency score between slot embeddings</p>
    <p>Word-to-word semantic relation   : similarity between word embeddings</p>
    <p>Word-to-word dependency relation  : dependency score between word</p>
    <p>embeddings</p>
    <p>=</p>
    <p>+</p>
    <p>=</p>
    <p>+</p>
    <p>w1</p>
    <p>w2</p>
    <p>w3</p>
    <p>w4</p>
    <p>w5</p>
    <p>w6</p>
    <p>w7</p>
    <p>s2 s1 s3</p>
    <p>Y.-N. Chen et al., Jointly Modeling Inter-Slot Relations by Random Walk on Knowledge Graphs for Unsupervised Spoken</p>
    <p>Language Understanding,&quot; in Proc. of NAACL, 2015.</p>
  </div>
  <div class="page">
    <p>KNOWLEDGE GRAPH PROPAGATION MODEL</p>
    <p>Word Relation Model Slot Relation Model</p>
    <p>word relation matrix</p>
    <p>slot relation matrix</p>
    <p>Word Observation Slot Candidate</p>
    <p>T ra</p>
    <p>in</p>
    <p>cheap restaurant food expensiveness</p>
    <p>locale_by_use</p>
    <p>food</p>
    <p>T e st</p>
    <p>Slot Induction</p>
  </div>
  <div class="page">
    <p>FEATURE MODEL</p>
    <p>Ontology Induction</p>
    <p>SLU Fw Fs</p>
    <p>Structure Learning</p>
    <p>.</p>
    <p>Utterance 1</p>
    <p>i would like a cheap restaurant</p>
    <p>Word Observation Slot Candidate</p>
    <p>T ra</p>
    <p>in</p>
    <p>cheap restaurant food expensiveness</p>
    <p>locale_by_use</p>
    <p>find a restaurant with chinese food</p>
    <p>Utterance 2 1 1</p>
    <p>food</p>
    <p>Slot Induction</p>
    <p>show me a list of cheap restaurants</p>
    <p>Test Utterance hidden semantics</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization (for 2nd issue)</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>MATRIX FACTORIZATION (MF)</p>
    <p>Reasoning with Matrix Factorization</p>
    <p>Word Relation Model Slot Relation Model</p>
    <p>word relation matrix</p>
    <p>slot relation matrix</p>
    <p>Word Observation Slot Candidate</p>
    <p>T ra</p>
    <p>in</p>
    <p>cheap restaurant food expensiveness</p>
    <p>locale_by_use</p>
    <p>food</p>
    <p>e st</p>
    <p>Slot Induction</p>
    <p>MF method completes a partially-missing matrix based on a low-rank latent semantics assumption.</p>
  </div>
  <div class="page">
    <p>MATRIX FACTORIZATION (MF)</p>
    <p>The decomposed matrices represent low-rank latent semantics for utterances and words/slots respectively</p>
    <p>The product of two matrices fills the probability of hidden semantics</p>
    <p>Word Observation Slot Candidate T</p>
    <p>ra in</p>
    <p>cheap restaurant food expensiveness</p>
    <p>locale_by_use</p>
    <p>food</p>
    <p>T e st</p>
    <p>+</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>BAYESIAN PERSONALIZED RANKING FOR MF</p>
    <p>Model implicit feedback  not treat unobserved facts as negative samples (true or false)</p>
    <p>give observed facts higher scores than unobserved facts</p>
    <p>Objective:</p>
    <p>+</p>
    <p>The objective is to learn a set of well-ranked semantic slots per utterance.</p>
  </div>
  <div class="page">
    <p>MATRIX FACTORIZATION (MF)</p>
    <p>Reasoning with Matrix Factorization</p>
    <p>Word Relation Model Slot Relation Model</p>
    <p>word relation matrix</p>
    <p>slot relation matrix</p>
    <p>Word Observation Slot Candidate</p>
    <p>T ra</p>
    <p>in</p>
    <p>cheap restaurant food expensiveness</p>
    <p>locale_by_use</p>
    <p>food</p>
    <p>T e st</p>
    <p>Slot Induction</p>
    <p>MF method completes a partially-missing matrix based on a low-rank latent semantics assumption.</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>EXPERIMENTAL SETUP</p>
    <p>Dataset  Cambridge University SLU corpus [Henderson, 2012]</p>
    <p>Restaurant recommendation in an in-car setting in Cambridge</p>
    <p>WER = 37%</p>
    <p>vocabulary size = 1868</p>
    <p>2,166 dialogues</p>
    <p>15,453 utterances</p>
    <p>dialogue slot: addr, area, food, name,</p>
    <p>phone, postcode, price range,</p>
    <p>task, type</p>
    <p>The mapping table between induced and reference slots</p>
    <p>Henderson et al., &quot;Discriminative spoken language understanding using word confusion networks,&quot; in Proc. of SLT, 2012. 34</p>
  </div>
  <div class="page">
    <p>EXPERIMENT 1: QUALITY OF SEMANTICS ESTIMATION</p>
    <p>Metric: Mean Average Precision (MAP) of all estimated slot probabilities for each utterance</p>
    <p>Approach ASR Manual</p>
    <p>w/o w/ Explicit w/o w/ Explicit</p>
    <p>Explicit Support Vector Machine 32.5 36.6</p>
    <p>Multinomial Logistic Regression 34.0 38.8</p>
  </div>
  <div class="page">
    <p>EXPERIMENT 1: QUALITY OF SEMANTICS ESTIMATION</p>
    <p>Metric: Mean Average Precision (MAP) of all estimated slot probabilities for each utterance</p>
    <p>Approach ASR Manual</p>
    <p>w/o w/ Explicit w/o w/ Explicit</p>
    <p>Explicit Support Vector Machine 32.5 36.6</p>
    <p>Multinomial Logistic Regression 34.0 38.8</p>
    <p>Implicit</p>
    <p>Baseline Random</p>
    <p>Majority</p>
    <p>MF</p>
    <p>Feature Model</p>
    <p>Feature Model + Knowledge Graph Propagation</p>
    <p>Modeling Implicit</p>
    <p>Semantics</p>
  </div>
  <div class="page">
    <p>EXPERIMENT 1: QUALITY OF SEMANTICS ESTIMATION</p>
    <p>Metric: Mean Average Precision (MAP) of all estimated slot probabilities for each utterance</p>
    <p>Approach ASR Manual</p>
    <p>w/o w/ Explicit w/o w/ Explicit</p>
    <p>Explicit Support Vector Machine 32.5 36.6</p>
    <p>Multinomial Logistic Regression 34.0 38.8</p>
    <p>Implicit</p>
    <p>Baseline Random 3.4 2.6</p>
    <p>Majority 15.4 16.4</p>
    <p>MF</p>
    <p>Feature Model 24.2 22.6</p>
    <p>Feature Model + Knowledge Graph Propagation</p>
    <p>(+19.1%) 52.1*</p>
    <p>(+34.3%)</p>
    <p>Modeling Implicit</p>
    <p>Semantics</p>
  </div>
  <div class="page">
    <p>Approach ASR Manual</p>
    <p>w/o w/ Explicit w/o w/ Explicit</p>
    <p>Explicit Support Vector Machine 32.5 36.6</p>
    <p>Multinomial Logistic Regression 34.0 38.8</p>
    <p>Implicit</p>
    <p>Baseline Random 3.4 22.5 2.6 25.1</p>
    <p>Majority 15.4 32.9 16.4 38.4</p>
    <p>MF</p>
    <p>Feature Model 24.2 37.6* 22.6 45.3*</p>
    <p>Feature Model + Knowledge Graph Propagation</p>
    <p>(+19.1%) 43.5*</p>
    <p>(+27.9%) 52.1*</p>
    <p>(+34.3%) 53.4*</p>
    <p>(+37.6%)</p>
    <p>Modeling Implicit</p>
    <p>Semantics</p>
    <p>The MF approach effectively models hidden semantics to improve SLU.</p>
    <p>Adding a knowledge graph propagation model further improves performance.</p>
    <p>EXPERIMENT 1: QUALITY OF SEMANTICS ESTIMATION</p>
    <p>Metric: Mean Average Precision (MAP) of all estimated slot probabilities for each utterance</p>
  </div>
  <div class="page">
    <p>All types of relations are useful to infer hidden semantics.</p>
    <p>Approach ASR Manual</p>
    <p>Feature Model 37.6 45.3</p>
    <p>Feature + Knowledge Graph</p>
    <p>Propagation</p>
    <p>Semantic   0</p>
    <p>Dependency   0</p>
    <p>Word   0 0 0</p>
    <p>Slot 0 0 0</p>
    <p>42.1* 49.9*</p>
    <p>Both w  0</p>
    <p>EXPERIMENT 2: EFFECTIVENESS OF RELATIONS</p>
  </div>
  <div class="page">
    <p>Approach ASR Manual</p>
    <p>Feature Model 37.6 45.3</p>
    <p>Feature + Knowledge Graph</p>
    <p>Propagation</p>
    <p>Semantic   0</p>
    <p>Dependency   0</p>
    <p>Word   0 0 0</p>
    <p>Slot 0 0 0</p>
    <p>42.1* 49.9*</p>
    <p>Both w  0</p>
    <p>Combining different relations further improves the performance.</p>
    <p>EXPERIMENT 2: EFFECTIVENESS OF RELATIONS</p>
    <p>All types of relations are useful to infer hidden semantics.</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction</p>
    <p>Ontology Induction: Frame-Semantic Parsing</p>
    <p>Structure Learning: Knowledge Graph Propagation</p>
    <p>Spoken Language Understanding (SLU): Matrix Factorization</p>
    <p>Experiments</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>CONCLUSIONS</p>
    <p>Ontology induction and knowledge graph construction enable systems to automatically acquire open domain knowledge.</p>
    <p>MF for SLU provides a principle model that is able to  unify the automatically acquired knowledge</p>
    <p>adapt to a domain-specific setting</p>
    <p>and then allows systems to consider implicit semantics for better understanding.</p>
    <p>The work shows the feasibility and the potential of improving generalization, maintenance, efficiency, and scalability of SDSs.</p>
    <p>The proposed unsupervised SLU achieves 43% of MAP on ASR-transcribed conversations.</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
    <p>Thanks for your attentions!!</p>
  </div>
</Presentation>
