<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SeongJae Park, Yunjae Lee, Moonsub Kim, and Heon Y. Yeom</p>
    <p>Seoul National University</p>
    <p>USENIX HotStorage19 Jul.2019</p>
    <p>Automating Context-Based Access Pattern Hint Injection for System Performance and</p>
    <p>Swap Storage Durability</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Memory Pressure is Inevitable  Size of working sets is rapidly increasing</p>
    <p>Common characteristic of modern workloads (e.g., cloud, big data, ML)</p>
    <p>Growth of DRAM space in a single machine is obviously slower than that  Main memory alone will not be able to accommodate all of the working sets</p>
    <p>Memory to CPU ratio for AWS instances (left) and physical servers (right) Nitu, Vlad, et al. &quot;Welcome to zombieland: Practical and energy-efficient memory disaggregation</p>
    <p>in a datacenter.&quot; Proceedings of the Thirteenth EuroSys Conference. ACM, 2018.</p>
  </div>
  <div class="page">
    <p>Modern Storage Devices as Secondary Memory  New type of storage devices have rapidly evolved for last decade  Much denser than DRAM, super-faster than traditional storages  Systems utilizing fast storage devices as secondary memory are proposed  Among those, we focus on swap system</p>
    <p>Most stable and easy way to construct such a heterogeneous memory system  Our main idea is not for only swap but general heterogeneous memory systems, though</p>
    <p>https://www.theverge.com/circuitbreaker/2017/10/31/16582018/intel-optane-p900-ssd-fast-dramnand-flash-memory-desktop-computer https://www.theverge.com/circuitbreaker/2018/2/20/17031256/worlds-largest-ssd-drive-samsung -30-terabyte-pm1643</p>
  </div>
  <div class="page">
    <p>The Challenge: Optimal Data Objects Placement  Modern storage devices cannot simply substitute DRAM</p>
    <p>Obviously slower than DRAM  Number of writes to those is limited</p>
    <p>Hot data object should be placed in DRAM to minimize access to swap device  Non-optimal data placement can degrade performance and/or durability</p>
    <p>Complex and dynamic data access patterns make it hard to be optimal</p>
    <p>Time</p>
    <p>Obj A</p>
    <p>Obj B</p>
    <p>Obj C</p>
    <p>Obj D</p>
    <p>Hot</p>
    <p>Cold</p>
  </div>
  <div class="page">
    <p>Existing Soln #1: Data Access Pattern Estimations  Underlying system cannot know the access pattern of the application  Thus, most systems estimate data access pattern using LRU-like schemes</p>
    <p>Linux kernel swap system also uses pseudo-LRU page reclamation scheme</p>
    <p>Estimation-based schemes cannot make the optimal decision</p>
    <p>Non-optimal: Evicted pages are reused soon</p>
  </div>
  <div class="page">
    <p>Existing Soln #2: Self Access Pattern Notification  Systems provide special system calls which can be used for the pattern hint  Applications can voluntarily hint their data access pattern via the system calls</p>
    <p>E.g., mlock() forces specific data objects to be locked in main memory  Proper use of such system calls can significantly improve performance and durability</p>
    <p>That said, proper use of these hinting system calls is challenging</p>
    <p>Operating System</p>
    <p>DRAM Swap Space</p>
    <p>Application</p>
    <p>A C B</p>
    <p>Phase 1: A is hot, B is cold, C is cool Phase 2: A is cold, B is hot, C is cool Phase 3: ...</p>
    <p>(systemcall): Here is my data access pattern for this phase Item A should be in DRAM.</p>
    <p>Item B and C need not to be in DRAM.</p>
  </div>
  <div class="page">
    <p>Challenges for Hint-based Optimizations  Data access pattern analysis</p>
    <p>Code review and multiple experimental evaluations are essential  Natively time consuming and complex</p>
    <p>Hint code injection (program modification)  Program is constructed with multiple phases that each having unique access pattern  How to know which phase we are currently executing?  Injecting hint code for every phase including short ones can incur high overhead</p>
    <p>(Hinting system call itself has overhead)</p>
    <p>More efforts are required as the size and the complexity of program grows</p>
  </div>
  <div class="page">
    <p>Contribution of This Paper  Automate the exhaustive access pattern analysis and optimizations</p>
    <p>Use program context as the minimal phase of execution  Analyze the data access pattern using a combination of static/dynamic analysis  Inject the hinting system calls by prioritizing execution phases and objects  We call the prototype as DAPHICX: Data Access Pattern Hint Injecting Compiler Extension</p>
    <p>Evaluate the overhead and improvement with 8 realistic workloads  Shows consistent and significant improvement for performance and durability</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>The Workflow of DAPHICX  Receive the source code of the target</p>
    <p>workload as an input  Extract dynamic data access pattern  Split given program into multiple</p>
    <p>execution phases that each has unique and meaningful access pattern</p>
    <p>Inject hints that notifying important objects for each phase</p>
    <p>Data Access Pattern Profiler</p>
    <p>Access Pattern Hint Injector</p>
    <p>Source C ode</p>
    <p>Swap System</p>
    <p>H int&lt;Kernel Space&gt;</p>
    <p>DAPHICX H int N</p>
    <p>otifying E</p>
    <p>xecution File</p>
    <p>DRAM Swap Device</p>
    <p>&lt;Device Space&gt;</p>
    <p>&lt;User Space&gt;</p>
  </div>
  <div class="page">
    <p>Static Analysis of Program Contexts and Objects  Program context: Function or loop containing cumulated callsite information</p>
    <p>Each context would have unique data access pattern  We use contexts as the minimal program execution phase</p>
    <p>Extract context and data objects information via static analysis of the code  Analyze function/loop entrance/exit for the context analysis  Analyze malloc()/realloc()/calloc()/free() invocations for the object analysis</p>
    <p>main()</p>
    <p>foo()</p>
    <p>for()</p>
    <p>bar()</p>
    <p>bar()</p>
  </div>
  <div class="page">
    <p>Dynamic Analysis of Data Access Pattern  Build profiling program by injecting dynamic analysis code</p>
    <p>Injected code counts number of accesses to each object from each program context  Hook every function/loop entrance/exit and every memory access</p>
    <p>Data access pattern of the workload is profiled by running the program  That is, the pattern represents the number of accesses to each object from each context</p>
    <p>Context</p>
    <p>Obj A</p>
    <p>Obj B</p>
    <p>Obj C</p>
    <p>Obj D</p>
    <p>Hot</p>
    <p>Cold</p>
  </div>
  <div class="page">
    <p>Verbose Contexts to Meaningful Phases  Program context based access patterns are too verbose to be used as is</p>
    <p>If a context has very short execution time, giving hint for it incurs only overhead  If adjacent contexts have similar access pattern, hint should be given only once</p>
    <p>Merge small/unnecessary program contexts into meaningful phase  If execution time of a contexts is smaller than given threshold, merge it with adjacent one  If two adjacent contexts has similar data access pattern, merge them  Contexts remaining after this merging step is called phases</p>
    <p>DAPHICX injects access pattern hints for each phase, not for each context</p>
    <p>Contexts Phases</p>
  </div>
  <div class="page">
    <p>Prioritizing Data Objects  If we lock a data object in memory,</p>
    <p>Number of DRAM hits increases as the number of accesses to the object grows  Every accesses to the object will incur DRAM hit</p>
    <p>Number of DRAM misses increases as the size of the object grows  Other objects which might incur DRAM hits would be evicted to the secondary memory</p>
    <p>for the data object pinning</p>
    <p>Based on this finding, we reason a simple object priority calculation model  Two constants (alpha and beta) control the growth rate of each metric</p>
    <p>Priority(object) = size_of(object)</p>
    <p>Number_of_accesses(object)</p>
  </div>
  <div class="page">
    <p>Selecting Objects to be Locked in DRAM  Select objects to be placed in DRAM so that total priority of those be biggest  Limited DRAM, different size and priority of objects: a Knap-sack problem  DAPHICX utilizes a simple solution</p>
    <p>Key difference with other knap-sack solution: The objects can be splitted  For each phase, select highest priority objects as many as possible  If next highest priority object cannot be locked in to remaining DRAM space, split the object in</p>
    <p>half, re-calculate priority, and repeat selection until DRAM is full</p>
    <p>DRAM</p>
    <p>Objects sorted by their priority</p>
  </div>
  <div class="page">
    <p>Hint Injection  DAPHICX generates optimized program by injecting hint notification code  Injected hint code does</p>
    <p>Check current phase by hooking phase-related function/loop entrance/exit  Phase checking overhead is minimized by hooking only phase-related ones</p>
    <p>For entrance to each phase, invoke mlock() calls for selected data objects of current phase</p>
    <p>Phases</p>
    <p>mlock(B,C);</p>
    <p>Obj A Obj B Obj C Obj D</p>
    <p>mlock(A,B); mlock(A,D); mlock(B,C);</p>
  </div>
  <div class="page">
    <p>Implementation  The prototype implementation is based on LLVM  5,000 lines of code for data access pattern analysis  200 lines of code for hinting object selection  1,300 lines of code for the hint injection</p>
    <p>https://upload.wikimedia.org/wikipedia/en/thumb/4/4c/LLVM_Logo.svg/1200px-LLVM_Logo.svg.png</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Evaluation Setup  Intel Xeon E7-8837 @ 2.67 GHz  Intel Optane SSD (DC4800) as a swap device  Workloads: memory-intensive workloads from SPEC CPU 2006  Memory pressure: synthetically induced via cgroups</p>
    <p>Shrink available memory down to 70% of working sets (30% shortage)  30% shortage would be reasonable for 1.5:1 memory overcommit environment</p>
    <p>(OpenStack encourages this ratio)</p>
    <p>https://images-na.ssl-images-amazon.com/images/I/51Q1zcv%2Bk%2BL._SY355_.jpg https://www.storagereview.com/images/StorageReview-Intel-Optane-SSD_4800.jpg</p>
  </div>
  <div class="page">
    <p>Performance  Runtime speedup = runtime(original) / runtime(optimized)  Injected hint code induces no overhead due to the merge of contexts  For 30% memory shortage, 1.4x to 2.65x performance with 5 workloads</p>
  </div>
  <div class="page">
    <p>Durability  Lifetime improvement = nr_swpout(original) / nr_swpout(optimized)  Up to 2.98x durability improvement measured in best case  Durability degrades with 482.sphinx3 due to its plethora of data objects</p>
    <p>The size of hint information becomes too huge so that it can increase number of swap events</p>
  </div>
  <div class="page">
    <p>Summary  Systems utilizing fast swap storages will widespread</p>
    <p>Size of working sets is rapidly increasing while DRAM space is not  Modern storage devices have continuously evolved so that those could be secondary memory</p>
    <p>Optimization for swap systems is required  Modern storage devices are slower than DRAM and restricts total number of writes  Underlying swap system cannot know accurate data access pattern of running workloads  Manually analyzing and notifying the pattern via system calls are exhaustive to human</p>
    <p>We introduce an automated data access pattern hint injection compiler  Extracts program contexts and data access patterns via static/dynamic analysis  Injects access pattern hint code in the execution binary  Hint injected version achieves up to 2.65x performance and 2.98x durability</p>
  </div>
  <div class="page">
    <p>E-mail is also always welcome: SeongJae Park &lt;sj38.park@gmail.com&gt;</p>
    <p>Any question, please!</p>
  </div>
</Presentation>
