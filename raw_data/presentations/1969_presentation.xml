<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Study of Hardware Assisted IP over InfiniBand and its Impact on</p>
    <p>Enterprise Data Center Performance</p>
    <p>Ryan E. Grant1, Pavan Balaji2, Ahmad Afsahi1</p>
    <p>Queens University Kingston, ON, Canada K7L 3N6</p>
    <p>Argonne National Laboratory</p>
    <p>Argonne, IL, USA</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation  Background Information  Experimental Framework  Experimental Results</p>
    <p>Baseline Performance  Offloading Performance  Data Center Performance Results  Performance Bottleneck Investigation  Validation</p>
    <p>Conclusions  Future Work</p>
    <p>Questions</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Sockets-based protocols are used extensively in enterprise data centers</p>
    <p>IPoIB provides a high performance socket interface that does not rely on upper layer protocol support (such as TCP)</p>
    <p>Future convergence of networking fabrics, will use elements of InfiniBand and Ethernet</p>
    <p>Behaviour and performance of such systems is important to study in order to guide the development and installation of advanced networking technologies</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Why is UD/RC offloading important?</p>
    <p>Its new to IPoIB (UD offloading)</p>
    <p>It narrows the performance gap between IPoIBUD and Sockets Direct Protocol (SDP)</p>
    <p>UD offloading allows us to effectively use software that does not utilize TCP (TCP is required for SDP)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Background Information  Experimental Framework  Experimental Results</p>
    <p>Baseline Performance  Offloading Performance  Data Center Performance Results  Performance Bottleneck Investigation  Validation</p>
    <p>Conclusions  Future Work</p>
    <p>Questions</p>
  </div>
  <div class="page">
    <p>Background Information</p>
    <p>InfiniBand  Queue Pair Operation, supporting RDMA and</p>
    <p>send/receive modes</p>
    <p>Mellanox ConnectX host channel adapters support 4x InfiniBand operation  Bandwidth of 40 Gigabit/s (32 Gigabits/s data)</p>
    <p>The OpenFabrics Enterprise Software Distribution (OFED-1.4) (InfiniBand drivers) supports both SDP and IPoIB protocols</p>
  </div>
  <div class="page">
    <p>Background Information</p>
    <p>Socket-based protocols provide IP functionality, using IP over IB (IPoIB)  IPoIB provides Large receive offload (LRO) and Large</p>
    <p>send offload (LSO)  Large receive offload aggregates incoming packets  Large send offload segments large messages into</p>
    <p>appropriate packet sizes in hardware  Sockets Direct Protocol (SDP) provides RDMA</p>
    <p>capabilities  Bypasses the operating systems TCP/IP stack  Utilizes hardware flow control, offloaded network and</p>
    <p>transport stack in addition to RDMA  Operates in buffered-copy and zero-copy modes</p>
  </div>
  <div class="page">
    <p>Background Information</p>
  </div>
  <div class="page">
    <p>Background Information</p>
    <p>InfiniBand operates in several modes:  Reliable Connection (RC): Keeps traditional</p>
    <p>connections while offering low level reliability and in order delivery</p>
    <p>Unreliable Datagram (UD): non-connection based datagram transmission, with no reliability</p>
    <p>IB is capable of RDMA, which is utilized for socket interfaces through the SDP API which replaces TCP programming semantics</p>
    <p>Additional IB modes: unreliable connection and reliable datagram, do not currently have hardware offloading implementations</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Background Information  Experimental Framework  Experimental Results</p>
    <p>Baseline Performance  Offloading Performance  Data Center Performance Results  Performance Bottleneck Investigation  Validation</p>
    <p>Conclusions  Future Work</p>
    <p>Questions</p>
  </div>
  <div class="page">
    <p>Experimental Framework</p>
    <p>OS Processors InfiniBand HCA Switch OFED version</p>
    <p>Fedora Kernel 2.6.27</p>
    <p>Opteron</p>
    <p>ConnectX 4X DDR HCA Firmware:</p>
    <p>Mellanox 24-port MT47396</p>
    <p>Infiniscale-III</p>
    <p>Network performance data was collected using Netperf2.4.4</p>
    <p>Performance was validated using iperf-2.0.4</p>
  </div>
  <div class="page">
    <p>Baseline Performance Results</p>
    <p>As expected Verbs RDMA and SDP (RDMA) show major latency advantages for small messages over IPoIB</p>
    <p>Single Connection Latency</p>
    <p>L a</p>
    <p>te n</p>
    <p>c y</p>
    <p>(</p>
    <p>s )</p>
    <p>IB Verbs RDMA IPoIB-RC IPoIB-LRO-LSO</p>
    <p>SDP-BC SDP-ZC</p>
  </div>
  <div class="page">
    <p>Baseline Multi-Stream Performance</p>
    <p>The use of multiple streams for IPoIB shows good performance for IPoIBRC/UD while IPoIB RC/UD and SDP Buffered copy are comparable</p>
    <p>Multi-Stream Bandwidth</p>
    <p>B a n</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b p</p>
    <p>s )</p>
    <p>IB Verbs RDMA IPoIB-RC IPoIB-LRO-LSO</p>
    <p>SDP-BC SDP-ZC</p>
  </div>
  <div class="page">
    <p>Offloading Performance</p>
    <p>IPoIB-UD with offloading provides similar latency to that of IPoIB-RC</p>
    <p>IPoIB Offloading Latency</p>
    <p>L a</p>
    <p>te n</p>
    <p>c y</p>
    <p>(</p>
    <p>s )</p>
    <p>IPoIB-UD-LRO-LSO IPoIB-UD-LRO-noLSO IPoIB-UD-noLRO-LSO IPoIB-UD-noLRO-noLSO IPoIB-RC</p>
  </div>
  <div class="page">
    <p>Offloading Performance</p>
    <p>Although IPoIB-UD offloading outperforms non-offloaded IPoIB-UD, the single stream is still outperformed by IPoIB-RC</p>
    <p>IPoIB Offloading Single-Stream Bandwidth</p>
    <p>B a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th (</p>
    <p>M b</p>
    <p>p s</p>
    <p>)</p>
    <p>IPoIB-UD-LRO-LSO IPoIB-UD-LRO-noLSO IPoIB-UD-noLRO-LSO IPoIB-UD-noLRO-noLSO IPoIB-RC</p>
  </div>
  <div class="page">
    <p>Offloading Performance</p>
    <p>With multiple streams, IPoIB-UD-LRO-LSO outperforms IPoIB-RC, and greatly outperforms non offloaded IPoIB-UD</p>
    <p>IPoIB Offloading Multi-Stream Bandwidth</p>
    <p>B a n</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b p</p>
    <p>s )</p>
    <p>IPoIB-UD-LRO-LSO IPoIB-UD-LRO-noLSO IPoIB-UD-noLRO-LSO IPoIB-UD-noLRO-noLSO IPoIB-RC</p>
  </div>
  <div class="page">
    <p>Offloading Performance</p>
    <p>Offloaded IPoIB-UD provides a 85.1% improvement in bandwidth over nonoffloaded IPoIB-UD</p>
    <p>Offloaded IPoIB-UD outperforms Multistream IPoIB-RC by 7.1%, and provides similar latency</p>
    <p>Offloaded IPoIB-UD provides bandwidth only 6.5% less than that of SDP</p>
  </div>
  <div class="page">
    <p>Data Center Performance</p>
  </div>
  <div class="page">
    <p>Data Center Performance</p>
    <p>Data center throughput shows IPoIB-UD-LRO-LSO to maintain the highest level of throughput, while SDP is unexpectedly the worst performer of the group</p>
    <p>Work Interactions Per Second</p>
    <p>Time (s)</p>
    <p>W IP</p>
    <p>S</p>
    <p>IPoIB-LRO-LSO IPoIB-noLRO-noLSO IPoIB-RC SDP</p>
  </div>
  <div class="page">
    <p>Data Center Performance IPoIB-UD-LRO-LSO IPoIB-RC</p>
    <p>IPoIB-UD-noLRO-noLSO SDP</p>
  </div>
  <div class="page">
    <p>Data Center Performance</p>
    <p>IPoIB-UD-LRO-LSO provides the highest sustained bandwidth of all of the protocols, beating non-offloaded IPoIB-UD by 15.4% and IPoIB-RC by 5.8% and SDP by 29.1%</p>
    <p>IPoIB-UD-LRO-LSO provides similar response time to its nearest competitor, IPoIB-RC</p>
    <p>All of the IPoIB configurations provide higher bandwidth than SDP</p>
  </div>
  <div class="page">
    <p>Performance Bottleneck Investigation</p>
    <p>SDP shows poor throughput and latency, much worse than would be initially expected</p>
    <p>Given the excellent performance of SDP at the micro-benchmarks level, several tests were conducted to determine the cause of SDPs poor performance in the data center test</p>
    <p>It was determined that the large number of simultaneous connections were causing poor performance with SDP</p>
    <p>The number of connections used by the SDP data center were reduced while increasing the activity level of each connection to confirm this analysis</p>
  </div>
  <div class="page">
    <p>Performance Investigation</p>
    <p>The resulting performance of SDP (with 50 connections) is increased greatly to a level that is more inline with expectations</p>
  </div>
  <div class="page">
    <p>Performance Investigation</p>
    <p>SDP delay - with many connectionsSDP delay - with fewer connections</p>
  </div>
  <div class="page">
    <p>Performance Validation</p>
    <p>IPoIB and SDP show similar performance results on SPECWeb as were seen using the TPC-W benchmarks</p>
  </div>
  <div class="page">
    <p>Performance Validation</p>
    <p>SPECWeb response time results show IPoIB-UD-LROLSO to have the overall lowest response times</p>
    <p>Maximum Response Time</p>
    <p>Time (s)</p>
    <p>T im</p>
    <p>e (</p>
    <p>m s</p>
    <p>)</p>
    <p>LRO-LSO noLRO-noLSO SDP</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Background Information  Experimental Framework  Experimental Results</p>
    <p>Baseline Performance  Offloading Performance  Data Center Performance Results  Performance Bottleneck Investigation  Validation</p>
    <p>Conclusions  Future Work</p>
    <p>Questions</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Micro-benchmarks have shown a 85.1% improvement in bandwidth of offloaded IB-UD over the non-offloaded case and a 26.2% maximum reduction in latency</p>
    <p>Offloaded IPoIB-UD shows a 15.4% improvement in throughput over non offloaded IPoIB-UD</p>
    <p>IPoIB-UD-LRO-LSO has a 29.1% higher throughput than SDP in our data center testing</p>
    <p>The benefits of using IPoIB-RC are minimal over those of IPoIB-UD when utilizing offloading capabilities</p>
    <p>Therefore, for future networks such as CEE the inclusion of a reliable connection mode is most likely unnecessary</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Resolving the issues holding back SDP performance when using large numbers of connections</p>
    <p>Utilizing Quality of Service to further enhance enterprise data center performance</p>
    <p>Combining IPoIB-UD, QoS and Virtual Protocol Interconnect to improve overall data center performance</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Questions?</p>
    <p>Questions?</p>
  </div>
</Presentation>
