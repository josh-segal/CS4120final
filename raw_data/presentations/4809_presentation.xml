<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>(C) 2003 Milo Martin</p>
    <p>Token Coherence: Decoupling Performance and Correctness</p>
    <p>Milo Martin, Mark Hill, and David Wood</p>
    <p>Wisconsin Multifacet Project http://www.cs.wisc.edu/multifacet/ University of WisconsinMadison</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 2</p>
    <p>We See Two Problems in Cache Coherence</p>
    <p>Virtual bus interconnect (snooping protocols)</p>
    <p>Indirection (directory protocols)</p>
    <p>Why? A distributed &amp; concurrent system</p>
    <p>Often enhancements too complicated to implement (predictive/adaptive/hybrid protocols)</p>
    <p>Performance and correctness tightly intertwined</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 3</p>
    <p>Rethinking Cache-Coherence Protocols</p>
    <p>Goal of invalidation-based coherence  Invariant: many readers -or- single writer</p>
    <p>Enforced by globally coordinated actions</p>
    <p>Enforce this invariant directly using tokens  Fixed number of tokens per block</p>
    <p>One token to read, all tokens to write</p>
    <p>Guarantees safety in all cases  Global invariant enforced with only local rules</p>
    <p>Independent of races, request ordering, etc.</p>
    <p>Key innovation</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 4</p>
    <p>Token Coherence: A New Framework for Cache Coherence</p>
    <p>Goal: Decouple performance and correctness  Fast in the common case</p>
    <p>Correct in all cases</p>
    <p>To remove ordering bottlenecks  Ignore races (fast common case)  Tokens enforce safety (all cases)</p>
    <p>To reduce complexity  Performance enhancements (fast common case)  Without affecting correctness (all cases)</p>
    <p>(without increased complexity)</p>
    <p>Focus of this talk</p>
    <p>Briefly described</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 5</p>
    <p>Outline</p>
    <p>Overview  Problem: ordering bottlenecks</p>
    <p>Solution: Token Coherence (TokenB)</p>
    <p>Evaluation</p>
    <p>Further exploiting decoupling</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 6</p>
    <p>Technology Trends</p>
    <p>High-speed point-to-point links  No (multi-drop) busses</p>
    <p>Desire: low-latency interconnect  Avoid virtual bus ordering</p>
    <p>Enabled by directory protocols</p>
    <p>Technology trends  unordered interconnects</p>
    <p>Increasing design integration  Glueless multiprocessors</p>
    <p>Improve cost &amp; latency</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 7</p>
    <p>Workload Trends</p>
    <p>P P P M</p>
    <p>P P P M</p>
    <p>Directory Protocol</p>
    <p>Workload trends  avoid indirection, broadcast ok</p>
    <p>Commercial workloads  Many cache-to-cache misses</p>
    <p>Clusters of small multiprocessors</p>
    <p>Goals:  Direct cache-to-cache misses</p>
    <p>(2 hops, not 3 hops)</p>
    <p>Moderate scalability</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 8</p>
    <p>Basic Approach</p>
    <p>Low-latency protocol  Broadcast with direct responses</p>
    <p>As in snooping protocols</p>
    <p>P P P M</p>
    <p>Fast &amp; works fine with no races but what happens in the case of a race?</p>
    <p>Low-latency interconnect  Use unordered interconnect</p>
    <p>As in directory protocols</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 9</p>
    <p>P0 issues a request to write (delayed to P2)</p>
    <p>Request to write</p>
    <p>Basic approach but not yet correct</p>
    <p>P2</p>
    <p>Read/Write</p>
    <p>P1</p>
    <p>No Copy</p>
    <p>P0</p>
    <p>No Copy</p>
    <p>Delayed in interconnect</p>
    <p>P1 issues a request to read</p>
    <p>Request to read</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 10</p>
    <p>P2</p>
    <p>Read/Write</p>
    <p>P1</p>
    <p>No Copy</p>
    <p>P0</p>
    <p>No Copy</p>
    <p>Basic approach but not yet correct</p>
    <p>Read-only Read-only</p>
    <p>P2 responds with data to P1</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 11</p>
    <p>Basic approach but not yet correct</p>
    <p>P2</p>
    <p>Read/Write</p>
    <p>P1</p>
    <p>No Copy</p>
    <p>P0</p>
    <p>No Copy 1 2</p>
    <p>Read-only Read-only</p>
    <p>P0s delayed request arrives at P2</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 12</p>
    <p>Basic approach but not yet correct</p>
    <p>P2</p>
    <p>Read/Write</p>
    <p>P1</p>
    <p>No Copy</p>
    <p>P0</p>
    <p>Read/Write 1 2</p>
    <p>Read-only Read-only No Copy</p>
    <p>P2 responds to P0</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 13</p>
    <p>Basic approach but not yet correct</p>
    <p>P2</p>
    <p>Read/Write</p>
    <p>P1</p>
    <p>No Copy</p>
    <p>P0</p>
    <p>Read/Write 1 2</p>
    <p>Read-only Read-only No Copy</p>
    <p>Problem: P0 and P1 are in inconsistent states</p>
    <p>Locally correct operation, globally inconsistent</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 14</p>
    <p>Contribution #1: Token Counting</p>
    <p>Tokens control reading &amp; writing of data  At all times, all blocks have T tokens</p>
    <p>E.g., one token per processor  One or more to read  All tokens to write</p>
    <p>Tokens: in caches, memory, or in transit  Components exchange tokens &amp; data</p>
    <p>Provides safety in all cases</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 15</p>
    <p>Basic Approach (Revisited)</p>
    <p>As before:  Broadcast with direct responses (like snooping)</p>
    <p>Use unordered interconnect (like directory)</p>
    <p>Track tokens for safety</p>
    <p>More refinement in a moment</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 16</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=16 (R/W)</p>
    <p>P1</p>
    <p>T=0</p>
    <p>P0</p>
    <p>T=0</p>
    <p>Delayed</p>
    <p>P0 issues a request to write (delayed to P2)</p>
    <p>Request to write</p>
    <p>P1 issues a request to read</p>
    <p>Delayed Request to read</p>
    <p>Max Tokens</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 17</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=16 (R/W)</p>
    <p>P1</p>
    <p>T=0</p>
    <p>P0</p>
    <p>T=0 1 2</p>
    <p>T=1(R) T=15(R)</p>
    <p>P2 responds with data to P1</p>
    <p>T=1</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 18</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=16 (R/W)</p>
    <p>P1</p>
    <p>T=0</p>
    <p>P0</p>
    <p>T=0 1 2</p>
    <p>T=1(R) T=15(R)</p>
    <p>P0s delayed request arrives at P2</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 19</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=16 (R/W)</p>
    <p>P1</p>
    <p>T=0</p>
    <p>P0</p>
    <p>T=15(R) 1 2</p>
    <p>T=1(R) T=15(R) T=0</p>
    <p>P2 responds to P0</p>
    <p>T=15</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 20</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=16 (R/W)</p>
    <p>P1</p>
    <p>T=0</p>
    <p>P0</p>
    <p>T=15(R) 1 2</p>
    <p>T=1(R) T=15(R) T=0</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 21</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=0</p>
    <p>P1</p>
    <p>T=1(R)</p>
    <p>P0</p>
    <p>T=15(R)</p>
    <p>Now what? (P0 wants all tokens)</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 22</p>
    <p>Basic Approach (Re-Revisited)</p>
    <p>As before:  Broadcast with direct responses (like snooping)</p>
    <p>Use unordered interconnect (like directory)</p>
    <p>Track tokens for safety</p>
    <p>Reissue requests as needed  Needed due to racing requests (uncommon)</p>
    <p>Timeout to detect failed completion</p>
    <p>Wait twice average miss latency</p>
    <p>Small hardware overhead</p>
    <p>All races handled in this uniform fashion</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 23</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=0</p>
    <p>P1</p>
    <p>T=1(R)</p>
    <p>P0</p>
    <p>T=15(R)</p>
    <p>P0 reissues request</p>
    <p>P1 responds with a token</p>
    <p>T=19 Timeout!</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 24</p>
    <p>Token Coherence Example</p>
    <p>P2</p>
    <p>T=0</p>
    <p>P0</p>
    <p>T=16 (R/W)</p>
    <p>P1</p>
    <p>T=0</p>
    <p>P0s request completed</p>
    <p>One final issue: What about starvation?</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 25</p>
    <p>Contribution #2: Guaranteeing Starvation-Freedom</p>
    <p>Handle pathological cases  Infrequently invoked  Can be slow, inefficient, and simple</p>
    <p>When normal requests fail to succeed (4x)  Longer timeout and issue a persistent request  Request persists until satisfied</p>
    <p>Table at each processor  Deactivate upon completion</p>
    <p>Implementation  Arbiter at memory orders persistent requests</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 26</p>
    <p>Outline</p>
    <p>Overview  Problem: ordering bottlenecks</p>
    <p>Solution: Token Coherence (TokenB)</p>
    <p>Evaluation</p>
    <p>Further exploiting decoupling</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 27</p>
    <p>Evaluation Goal: Four Questions</p>
    <p>Quantitative evidence for qualitative behavior</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 28</p>
    <p>Workloads and Simulation Methods</p>
    <p>Workloads  OLTP - On-line transaction processing  SPECjbb - Java middleware workload  Apache - Static web serving workload  All workloads use Solaris 8 for SPARC</p>
    <p>Simulation methods  16 processors  Simics full-system simulator  Out-of-order processor model  Detailed memory system model  Many assumptions and parameters (see paper)</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 29</p>
    <p>Q1: Reissued Requests (percent of all L2 misses)</p>
    <p>OLTP SPECjbb Apache</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 30</p>
    <p>Q1: Reissued Requests (percent of all L2 misses)</p>
    <p>Outcome OLTP SPECjbb Apache</p>
    <p>Not Reissued 98% 98% 96%</p>
    <p>Reissued Once</p>
    <p>Reissued &gt; 1 0.4% 0.3% 0.7%</p>
    <p>Persistent Requests (Reissued &gt; 4)</p>
    <p>Yes; reissued requests are rare (these workloads, 16p)</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 31</p>
    <p>Q2: Runtime: Snooping vs. Token Coherence Hierarchical Switch Interconnect</p>
    <p>Similar performance on same interconnect</p>
    <p>Tree interconnect</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 32</p>
    <p>Q2: Runtime: Snooping vs. Token Coherence Direct Interconnect</p>
    <p>Snooping not applicable</p>
    <p>Torus interconnect</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 33</p>
    <p>Q2: Runtime: Snooping vs. Token Coherence</p>
    <p>Yes; Token Coherence can outperform</p>
    <p>snooping</p>
    <p>(15-28% faster)</p>
    <p>Why? Lower-latency interconnect</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 34</p>
    <p>Q3: Runtime: Directory vs. Token Coherence</p>
    <p>Yes; Token Coherence can outperform</p>
    <p>directories</p>
    <p>(17-54% faster with slow directory)</p>
    <p>Why? Direct 2-hop cache-to-cache misses</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 35</p>
    <p>Q4: Traffic per Miss: Directory vs. Token</p>
    <p>Yes; broadcast overheads reasonable</p>
    <p>for 16 processors</p>
    <p>(directory uses 21-25% less bandwidth)</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 36</p>
    <p>Q4: Traffic per Miss: Directory vs. Token</p>
    <p>Yes; broadcast overheads reasonable</p>
    <p>for 16 processors</p>
    <p>(directory uses 21-25% less bandwidth)</p>
    <p>Why? Requests are smaller than data</p>
    <p>(8B v. 64B)</p>
    <p>Requests &amp; forwards</p>
    <p>Responses</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 37</p>
    <p>Outline</p>
    <p>Overview  Problem: ordering bottlenecks</p>
    <p>Solution: Token Coherence (TokenB)</p>
    <p>Evaluation</p>
    <p>Further exploiting decoupling</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 38</p>
    <p>Contribution #3: Decoupled Coherence</p>
    <p>Cache Coherence Protocol</p>
    <p>Correctness Substrate (all cases)</p>
    <p>Performance Protocol (common cases)</p>
    <p>Safety (token counting)</p>
    <p>Starvation Freedom (persistent requests)</p>
    <p>Many Implementation choices</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 39</p>
    <p>Example Opportunities of Decoupling</p>
    <p>Predict a destination-set [ISCA 03]  Based on past history  Need not be correct (rely on persistent requests)  Enables larger or more cost-effective systems</p>
    <p>Example#2: predictive push</p>
    <p>P2P2P2P2PnP1</p>
    <p>T=16</p>
    <p>P0 T=16</p>
    <p>Example#1: Broadcast is not required</p>
    <p>Requires no changes to correctness substrate</p>
    <p>Predictive push</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 40</p>
    <p>Conclusions</p>
    <p>Token Coherence (broadcast version)  Low cache-to-cache miss latency (no indirection)</p>
    <p>Avoids virtual bus interconnects</p>
    <p>Faster and/or cheaper</p>
    <p>Token Coherence (in general)  Correctness substrate</p>
    <p>Tokens for safety</p>
    <p>Persistent requests for starvation freedom</p>
    <p>Performance protocol for performance</p>
    <p>Decouple correctness from performance</p>
    <p>Enables further protocol innovation</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 41</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 42</p>
    <p>Cache-Coherence Protocols</p>
    <p>Goal: provide a consistent view of memory</p>
    <p>Permissions in each cache per block  One read/write -or</p>
    <p>Many readers</p>
    <p>Cache coherence protocols  Distributed &amp; complex</p>
    <p>Correctness critical</p>
    <p>Performance critical</p>
    <p>Races: the main source of complexity  Requests for the same block at the same time</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 43</p>
    <p>Evaluation Parameters</p>
    <p>Processors  SPARC ISA  2 GHz, 11 pipe stages  4-wide fetch/execute  Dynamically scheduled  128 entry ROB  64 entry scheduler</p>
    <p>Memory system  64 byte cache lines  128KB L1 Instruction and</p>
    <p>Data, 4-way SA, 2 ns (4 cycles)</p>
    <p>4MB L2, 4-way SA, 6 ns (12 cycles)</p>
    <p>2GB main memory, 80 ns (160 cycles)</p>
    <p>Interconnect  15ns link latency  Switched tree (4 link</p>
    <p>latencies) - 240 cycles 2-hop round trip</p>
    <p>2D torus (2 link latencies on average) - 120 cycles 2-hop round trip</p>
    <p>Link bandwidth: 3.2 Gbyte/second</p>
    <p>Coherence Protocols  Aggressive snooping  Alpha 21364-like directory</p>
    <p>72 byte data messages</p>
    <p>8 byte request messages</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 44</p>
    <p>Q3: Runtime: Directory vs. Token Coherence</p>
    <p>Yes; Token Coherence can outperform</p>
    <p>directories</p>
    <p>(17-54% faster with slow directory)</p>
    <p>Why? Direct 2-hop cache-to-cache misses</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 45</p>
    <p>More Information in Paper</p>
    <p>Traffic optimization  Transfer tokens without data</p>
    <p>Add an owner token</p>
    <p>Note: no silent read-only replacements  Worst case: 10% interconnect traffic overhead</p>
    <p>Comparison to AMDs Hammer protocol</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 46</p>
    <p>Verifiability &amp; Complexity</p>
    <p>Divide and conquer complexity  Formal verification is work in progress  Difficult to quantify, but promising  All races handled uniformly (reissuing)</p>
    <p>Local invariants  Safety is response-centric; independent of requests  Locally enforced with tokens  No reliance on global ordering properties</p>
    <p>Explicit starvation avoidance  Simple mechanism</p>
    <p>Further innovation  no correctness worries</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 47</p>
    <p>Traditional v. Token Coherence  Traditional protocols</p>
    <p>Sensitive to request ordering  Interconnect or directory</p>
    <p>Monolithic  Complicated  Intertwine correctness and performance</p>
    <p>Token Coherence  Track tokens (safety)  Persistent requests (starvation avoidance)  Request are only hints</p>
    <p>Separate Correctness and Performance</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 48</p>
    <p>Conceptual Interface</p>
    <p>Performance Protocol</p>
    <p>Correctness Substrate</p>
    <p>Data, Tokens, &amp; Persistent Requests</p>
    <p>Hint requests</p>
    <p>MP0</p>
    <p>Controller</p>
    <p>MP1</p>
    <p>Controller</p>
    <p>Controller Controller</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 49</p>
    <p>Conceptual Interface</p>
    <p>Performance Protocol</p>
    <p>Correctness Substrate</p>
    <p>Im looking for block B</p>
    <p>Please send Block B to P1</p>
    <p>Here is block B &amp; one token</p>
    <p>(Or, no response)</p>
    <p>MP0</p>
    <p>Controller</p>
    <p>MP1</p>
    <p>Controller</p>
    <p>Controller Controller</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 50</p>
    <p>Snooping multiprocessors  Uses broadcast  Virtual bus interconnect + Directly locate data (2 hops)</p>
    <p>Directory-based multiprocessors  Directory tracks writer or readers + Avoids broadcast + Avoids virtual bus interconnect  Indirection for cache-to-cache (3 hops)</p>
    <p>Examine workload and technology trends</p>
    <p>Snooping v. Directories: Which is Better?</p>
    <p>P P P M</p>
    <p>P P P M</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 51</p>
    <p>Workload Trends</p>
    <p>Commercial workloads  Many cache-to-cache misses or sharing misses</p>
    <p>Cluster small- or moderate-scale multiprocessors</p>
    <p>Goals:  Low cache-to-cache miss latency (2 hops)</p>
    <p>Moderate scalability</p>
    <p>Workload trends  snooping protocols</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 52</p>
    <p>Technology Trends</p>
    <p>High-speed point-to-point links  No (multi-drop) busses</p>
    <p>Desire: unordered interconnect  No virtual bus ordering</p>
    <p>Decouple interconnect &amp; protocol</p>
    <p>Technology trends  directory protocols</p>
    <p>Increasing design integration  Glueless multiprocessors</p>
    <p>Improve cost &amp; latency</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 53</p>
    <p>Multiprocessor Taxonomy</p>
    <p>Directories</p>
    <p>Interconnect</p>
    <p>Virtual Bus</p>
    <p>Unordered</p>
    <p>Snooping</p>
    <p>Cache-to-cache latency High LowTechnology</p>
    <p>Trends</p>
    <p>Workload Trends</p>
    <p>Our Goal</p>
  </div>
  <div class="page">
    <p>Token Coherence  Milo Martin slide 54</p>
    <p>Overview</p>
    <p>Two approaches to cache-coherence</p>
    <p>The Problem  Workload trends  snooping protocols</p>
    <p>Technology trends  directory protocols</p>
    <p>Want a protocol that fits both trends</p>
    <p>A Solution  Unordered broadcast &amp; direct response</p>
    <p>Track tokens, reissue requests, prevent starvation</p>
    <p>Generalization: a new coherence framework  Decouple correctness from performance policy</p>
    <p>Enables further opportunities</p>
  </div>
</Presentation>
