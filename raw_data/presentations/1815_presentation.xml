<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Justin Thaler, Harvard University Mike Roberts, Harvard University</p>
    <p>Michael Mitzenmacher, Harvard University Hanspeter Pfister, Harvard University</p>
    <p>Verifiable Computation with Massively Parallel Interactive Proofs</p>
  </div>
  <div class="page">
    <p>Outsourcing  Many applications require outsourcing computation to</p>
    <p>untrusted service providers.  Main motivation: Commercial cloud computing services.  Also, weak peripheral devices; fast but faulty co-processors.  Volunteer Computing (SETI@home,World Community</p>
    <p>Grid, etc.)</p>
    <p>User requires a guarantee that the cloud performed the computation correctly.</p>
    <p>One solution: require cloud to prove correctness of answer.</p>
  </div>
  <div class="page">
    <p>Goals of Verifiable Computation  Provide user with a correctness guarantee, without requiring</p>
    <p>her to perform the requested computations herself.  Ideally user will not even maintain a local copy of the data.  User may have resorted to the cloud in the first place because</p>
    <p>she has more data than she can store.</p>
    <p>Minimize the amount of extra bookkeeping the cloud has to do to prove the integrity of the computation.</p>
    <p>Ideally our protocols will be secure against arbitrarily malicious clouds, but sufficiently lightweight for use in more benign settings.</p>
  </div>
  <div class="page">
    <p>Interactive Proofs  Two Parties: Prover P and Verifier V.</p>
    <p>Think of P and powerful, V as weak. P solves a problem, tells V the answer.  Then P and V have a conversation.  Ps goal: convince V the answer is correct.</p>
    <p>Requirements:  1. Completeness: An honest P can convince V</p>
    <p>shes telling the truth.  2. Soundness: V will catch a lying P with high</p>
    <p>probability no matter what P says to try to convince V (Secure even if P is computationally unbounded).</p>
  </div>
  <div class="page">
    <p>Interactive Proofs  IPs have revolutionized Complexity Theory in the last 25</p>
    <p>years.  IP=PSPACE [Shamir 90].  PCP Theorem e.g. [AS 98]. Hardness of approximation.  Zero Knowledge Proofs.</p>
    <p>But IPs have had very little impact in real delegation scenarios.  Why?  Not due to lack of applications!</p>
  </div>
  <div class="page">
    <p>Interactive Proofs  Old Answer: Most results on IPs dealt with hard</p>
    <p>problems, needed P to be too powerful.  But recent constructions focus on easy problems</p>
    <p>(e.g. Interactive Proofs for Muggles [GKR 08]).  Allows V to run very quickly, so outsourcing is</p>
    <p>useful even though problems are easy.  P does not need much more time to prove</p>
    <p>correctness than she does to solve the problem in the first place!</p>
  </div>
  <div class="page">
    <p>Interactive Proofs  Why does GKR not yield a practical protocol out</p>
    <p>of the box?  P has to do a lot of extra bookkeeping (cubic</p>
    <p>blowup in runtime).  Naively, V has to retain the full input.  Substantial overhead due to finite field arithmetic</p>
    <p>and other technical issues.</p>
  </div>
  <div class="page">
    <p>Engineering Practical IPs [CMT12, TRMP12]</p>
  </div>
  <div class="page">
    <p>A Two-Pronged Approach  The present paper is part of a recent line of work aiming to</p>
    <p>develop practical IPs [CCMT12, CMT10, CTY12, CMT12]  Ideal: General purpose implementation allowing to verify</p>
    <p>arbitrary computation.  Based on general-purpose Interactive Proofs for Muggles</p>
    <p>construction [GKR 08].  Also develop highly optimized protocols for specific important</p>
    <p>problems.  Reporting queries (what value is stored in memory location x of my</p>
    <p>database?)  Matrix multiplication.  Graph problems like perfect matching.  Certain kinds of linear programs.  Etc.</p>
  </div>
  <div class="page">
    <p>Main Results: Part 1  Can save V substantial amounts of space essentially for free.</p>
    <p>Reason: GKR protocol (and several others) only requires V to store a fingerprint of the data.</p>
    <p>This fingerprint can be computed in a single, light-weight pass over the input.</p>
    <p>Fingerprint serves as a sort of &quot;secret&quot; that V can use to catch the cloud in a lie.</p>
    <p>Fits cloud computing well: pass by V can occur while uploading data to cloud.</p>
    <p>V never needs to store entirety of data!  The fingerprint is a few KBs in size, even if the input contains</p>
    <p>terabytes of data.</p>
  </div>
  <div class="page">
    <p>Main Results: Part 2  Can save V substantial amounts of time.  E.g. when multiplying two 512x512 matrices, V requires .12s</p>
    <p>to process the input, while naive matrix multiplication takes about .70 seconds.</p>
    <p>Savings for V will be much larger on at larger input sizes, when applying our implementation to more time-intensive computations than matrix multiplication (because Vs runtime grows quasi-linearly with input size; she just needs to compute a fingerprint of the input).</p>
  </div>
  <div class="page">
    <p>Main Results: Part 3  We've come a long way in making P more efficient.  In [CMT12], we brought the runtime of P down from cubic in</p>
    <p>the size of a circuit computing the function of interest, to quasilinear in the size of the circuit.</p>
    <p>Lots of additional engineering in the implementation (helps make V fast too).  Choosing the right finite field to work over.  Using the right circuits.  Etc.</p>
    <p>Practically speaking, this is still not good enough on its own.  256 x 256 matrix multiplication takes P about 27 minutes for our</p>
    <p>previous single-threaded implementation.</p>
  </div>
  <div class="page">
    <p>Main Results: Part 4 (Focus of [TRMP12])  Our implementation is extremely amenable to parallelization.  Holds for both P and V (although V runs quickly even without</p>
    <p>parallelization, see Insight 2).</p>
    <p>If V also has a GPU, we get close to 100-fold speedups for V relative to single-threaded implementation. !</p>
    <p>Problem! P time (single</p>
    <p>threaded)!</p>
    <p>P time (GPU)!</p>
    <p>V time! Rounds! Communi cation!</p>
    <p>F2! (n=2^20)!</p>
    <p>MatMult! (256 x 256)!</p>
  </div>
  <div class="page">
    <p>Main Results: Part 4 (Focus of [TRMP12])  Main challenge to parallelizing and scaling to large inputs was</p>
    <p>the memory-intensive nature of Ps computation in the GKR protocol.  Nave n x n matrix multiplication only requires O(n2) space.  P has to store a circuit of size O(n3) (we use 40 bytes per gate).  Even 256 x 256 matrix multiplication over 1.5 GBs of space.  Took steps to mitigate this issue despite limited device memory.</p>
  </div>
  <div class="page">
    <p>Related Work  Setty, McPherson, Blumberg, and Walfish [NDSS 12]</p>
    <p>implement an argument system original due to Ishai, Kushilevitz, and Ostrovsky [CCC 07].  Bring the runtime of the cloud down by a factor of 10^20</p>
    <p>relative to a naive implementation.  Advantages of our implementation: save V time even when</p>
    <p>outsourcing a single computation, secure against computationally unbounded clouds.</p>
    <p>Canetti, Riva, and Rothblum [CCS 12] give highly practical protocols which are secure when there are two clouds, at least one of whom is honest.</p>
    <p>Ben-Sasson, Chiesa, Genkin, and Tromer working toward practical PCPs.</p>
  </div>
  <div class="page">
    <p>Conclusions  Interactive Proofs and other protocols for verifiable</p>
    <p>computation represent some of the most celebrated results in complexity theory.</p>
    <p>They have the potential to mitigate trust issues in cloud computing, but were wildly impractical until recently.</p>
    <p>We can already save the user a lot of time and space.  The main remaining bottleneck is the extra bookkeeping the</p>
    <p>cloud must do to provide integrity guarantees.  Parallelization helps mitigate this issue, but there is still much</p>
    <p>work to be done.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>Sample Variance of Data Stream  The (scaled) sample variance of a data stream is defined as</p>
    <p>follows:  Let X be the frequency vector of the stream (Xi is number of occurrences of i in the stream)  F2(X)=i Xi</p>
    <p>[CCM 09/CCMT 12] give a one-message protocol for F2</p>
    <p>requiring O(n) communication and O(n) space for V.  This is optimal.</p>
  </div>
  <div class="page">
    <p>Sample-Variance Protocol  Recall: F2(X)=i Xi</p>
    <p>View universe [n] as [n] x [n].</p>
    <p>Frequency Vector X</p>
  </div>
  <div class="page">
    <p>First idea: Have P send the answer in pieces:  F2(row 1). F2(row 2). And so on. Requires n</p>
    <p>communication.</p>
    <p>V exactly tracks a row at random (denoted in yellow) so if P lies about any piece, V has a chance of catching her. Requires space n.</p>
    <p>Frequency Square X</p>
    <p>P sends</p>
    <p>Slide derived from [McGregor 10]</p>
  </div>
  <div class="page">
    <p>Problem: If P lies in only one place, V has small chance of catching her.</p>
    <p>We would like the following to hold: if P lies about even one piece, she will have to lie about many.</p>
    <p>Solution: Have P commit (succinctly) to second frequency moment</p>
    <p>of rows of an error-corrected encoding of the input.</p>
    <p>Need V to evaluate any row of the encoding in a streaming fashion. Can do this for low-degree extension code. Note: this code is systematic, meaning the first n symbols are just the input itself.</p>
  </div>
  <div class="page">
    <p>Error-corrected Encoding of Frequency Square X</p>
    <p>H sends</p>
    <p>Input is embedded in</p>
    <p>encoding (low-degree extension)</p>
    <p>These values will all lie on low-degree polynomial s(X)</p>
  </div>
</Presentation>
