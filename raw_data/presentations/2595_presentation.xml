<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>High Throughput Data Center Topology Design</p>
    <p>Ankit Singla, P. Brighten Godfrey, Alexandra Kolla</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>How long must we wait until our pigeon system</p>
    <p>rivals those of the Continental Powers?</p>
    <p>- The Nineteenth Century, 1899</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>The need for throughput</p>
    <p>March 2011</p>
    <p>May 2012[Facebook, via Wired]</p>
  </div>
  <div class="page">
    <p>Many topology options</p>
  </div>
  <div class="page">
    <p>How do we design throughput optimal network topologies?</p>
  </div>
  <div class="page">
    <p>How do we design throughput optimal network</p>
    <p>topologies?</p>
  </div>
  <div class="page">
    <p>How close can we get to optimal network capacity?</p>
  </div>
  <div class="page">
    <p>How close can we get to optimal network capacity?</p>
    <p>How do we handle heterogeneity?2</p>
  </div>
  <div class="page">
    <p>Jellyfish: Networking Data Centers Randomly</p>
    <p>[NSDI 2012: Singla, Hong, Popa, Godfrey]</p>
  </div>
  <div class="page">
    <p>[NSDI 2012: Singla, Hong, Popa, Godfrey]</p>
    <p>Jellyfish: Networking Data Centers Randomly</p>
  </div>
  <div class="page">
    <p>[NSDI 2012: Singla, Hong, Popa, Godfrey]</p>
    <p>High capacity  Beat fat-trees by 25%+</p>
    <p>Jellyfish: Networking Data Centers Randomly</p>
  </div>
  <div class="page">
    <p>[NSDI 2012: Singla, Hong, Popa, Godfrey]</p>
    <p>High capacity  Beat fat-trees by 25%+</p>
    <p>Easier to expand  60% cheaper expansion</p>
    <p>Jellyfish: Networking Data Centers Randomly</p>
  </div>
  <div class="page">
    <p>[NSDI 2012: Singla, Hong, Popa, Godfrey]</p>
    <p>High capacity  Beat fat-trees by 25%+</p>
    <p>Easier to expand  60% cheaper expansion</p>
    <p>Routing and cabling are solvable problems</p>
    <p>Jellyfish: Networking Data Centers Randomly</p>
  </div>
  <div class="page">
    <p>How close can we get to optimal network capacity?</p>
  </div>
  <div class="page">
    <p>How close can we get to optimal network capacity?</p>
    <p>How do we handle heterogeneity?2</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
    <p>Maximize the minimum flow</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
    <p>Maximize the minimum flow</p>
    <p>under random permutation traffic</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
    <p>Maximize the minimum flow</p>
    <p>under random permutation traffic</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
    <p>Bisection bandwidth  throughput</p>
  </div>
  <div class="page">
    <p>How do we measure throughput?</p>
    <p>Bisection bandwidth  throughput  Near-worst case traffic patterns</p>
  </div>
  <div class="page">
    <p>How close can we get to optimal network capacity?</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p># flows</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p>capacity used per flow# flows</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p>capacity used per flow# flows</p>
    <p>total capacity</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p>capacity used per flow# flows</p>
    <p>total capacity</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p># flows</p>
    <p>total capacity</p>
    <p>throughput per flow  mean path length</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p>total capacity</p>
    <p># flows  mean path length throughput per flow</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p>links capacity(link) # flows  mean path length</p>
    <p>throughput per flow</p>
  </div>
  <div class="page">
    <p>A simple upper bound</p>
    <p>links capacity(link) # flows  mean path length</p>
    <p>throughput per flow</p>
    <p>Lower bound this!</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
    <p>[Cerf et al., A lower bound on the average shortest path length in regular graphs, 1974]</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
    <p>Distance # Nodes</p>
    <p>[Cerf et al., A lower bound on the average shortest path length in regular graphs, 1974]</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
    <p>Distance # Nodes</p>
    <p>[Cerf et al., A lower bound on the average shortest path length in regular graphs, 1974]</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
    <p>Distance # Nodes</p>
    <p>[Cerf et al., A lower bound on the average shortest path length in regular graphs, 1974]</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
    <p>Distance # Nodes</p>
    <p>(Ugliness omitted)</p>
    <p>[Cerf et al., A lower bound on the average shortest path length in regular graphs, 1974]</p>
  </div>
  <div class="page">
    <p>Lower bound on mean path length</p>
    <p>Distance # Nodes</p>
    <p>(Ugliness omitted)</p>
    <p>[Cerf et al., A lower bound on the average shortest path length in regular graphs, 1974]</p>
  </div>
  <div class="page">
    <p>Random graphs vs. bound</p>
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
  </div>
  <div class="page">
    <p>Random graphs vs. bound</p>
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
  </div>
  <div class="page">
    <p>Random graphs vs. bound</p>
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
    <p>all-to-all</p>
  </div>
  <div class="page">
    <p>Random graphs vs. bound</p>
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
    <p>all-to-all</p>
    <p>Random graphs within a few percent of optimal!</p>
  </div>
  <div class="page">
    <p>Random graphs vs. bound</p>
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
    <p>all-to-all</p>
    <p>Random graphs within a few percent of optimal!</p>
    <p>Random graphs exceed throughput of other topologies</p>
  </div>
  <div class="page">
    <p>How close can we get to optimal network capacity?</p>
    <p>Very close!!</p>
  </div>
  <div class="page">
    <p>How do we handle heterogeneity?</p>
    <p>Image credit: Legolizer (www.drububu.com)</p>
  </div>
  <div class="page">
    <p>Heterogeneity</p>
  </div>
  <div class="page">
    <p>Heterogeneity</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Random graphs as a building block</p>
  </div>
  <div class="page">
    <p>Random graphs as a building block</p>
    <p>Low-degree switches</p>
    <p>High-degree switches</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>? ?</p>
    <p>Random graphs as a building block</p>
    <p>Low-degree switches</p>
    <p>High-degree switches</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>? ?</p>
    <p>?</p>
    <p>Random graphs as a building block</p>
    <p>Low-degree switches</p>
    <p>High-degree switches</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>? ?</p>
    <p>?</p>
    <p>Random graphs as a building block</p>
    <p>Low-degree switches</p>
    <p>High-degree switches</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
    <p>Distributing servers in proportion to switch port-counts</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
    <p>Distributing servers in proportion to switch port-counts</p>
  </div>
  <div class="page">
    <p>Distributing servers</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
    <p>Distributing servers in proportion to switch port-counts</p>
    <p>Networks arent built like this today!</p>
  </div>
  <div class="page">
    <p>Random graphs as a building block</p>
    <p>Low-degree switches</p>
    <p>High-degree switches</p>
    <p>Servers</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Interconnecting switches</p>
  </div>
  <div class="page">
    <p>Interconnecting switches</p>
  </div>
  <div class="page">
    <p>Interconnecting switches</p>
  </div>
  <div class="page">
    <p>Interconnecting switches</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Interconnecting switches</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches</p>
  </div>
  <div class="page">
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Interconnecting switches Vanilla random interconnect</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
  </div>
  <div class="page">
    <p>Intuition</p>
    <p>Still need one crossing!</p>
  </div>
  <div class="page">
    <p>Intuition</p>
    <p>Still need one crossing!</p>
    <p>1</p>
    <p>APL</p>
  </div>
  <div class="page">
    <p>Intuition</p>
    <p>Still need one crossing!</p>
    <p>1</p>
    <p>APL</p>
    <p>Throughput should drop when less than</p>
    <p>of total capacity crosses the cut!</p>
  </div>
  <div class="page">
    <p>Explaining throughput</p>
    <p>Upper bound</p>
    <p>And constant-factor matching lower bounds in special case</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>Empirical value</p>
  </div>
  <div class="page">
    <p>Two regimes of throughput</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
  </div>
  <div class="page">
    <p>Two regimes of throughput</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>sparsest cut</p>
  </div>
  <div class="page">
    <p>Two regimes of throughput</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
    <p>sparsest cut plateau:</p>
    <p>(total cap) / APL</p>
  </div>
  <div class="page">
    <p>Implications</p>
    <p>A wide range of connectivity options</p>
  </div>
  <div class="page">
    <p>Implications</p>
    <p>A wide range of connectivity options</p>
  </div>
  <div class="page">
    <p>Implications</p>
    <p>A wide range of connectivity options</p>
    <p>Bisection bandwidth  throughput</p>
  </div>
  <div class="page">
    <p>Implications</p>
    <p>A wide range of connectivity options</p>
    <p>Bisection bandwidth  throughput</p>
    <p>Greater freedom in cabling</p>
  </div>
  <div class="page">
    <p>Quick recap!</p>
  </div>
  <div class="page">
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
  </div>
  <div class="page">
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
  </div>
  <div class="page">
    <p>T hr</p>
    <p>o ug</p>
    <p>hp ut</p>
    <p>(R at</p>
    <p>io t</p>
    <p>o U</p>
    <p>pp er</p>
    <p>-b o un</p>
    <p>d)</p>
    <p>Network Size</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Number of Servers at Large Switches (Ratio to Expected Under Random Distribution)</p>
    <p>N o rm</p>
    <p>al iz</p>
    <p>ed T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>Cross-cluster Links (Ratio to Expected Under Random Connection)</p>
  </div>
  <div class="page">
    <p>Improving a REAL heterogeneous topology</p>
  </div>
  <div class="page">
    <p>The VL2 topology</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>!&quot;#</p>
    <p>$%&amp;</p>
    <p>. . .</p>
    <p>. . . .</p>
    <p>'(()</p>
    <p>DA/2 x 10G</p>
    <p>DA/2 x 10G</p>
    <p>DI x10G</p>
    <p>DI x Aggregate Switches</p>
    <p>InternetLink-state network carrying only LAs</p>
    <p>(e.g., 10/8) DA/2 x Intermediate Switches</p>
    <p>Fungible pool of servers owning AAs</p>
    <p>(e.g., 20/8)</p>
    <p>Figure : An example Clos network between Aggregation and Intermediate switches provides a richly-connected backbone wellsuited for VLB. The network is built with two separate address families  topologically significant Locator Addresses (LAs) and flat Application Addresses (AAs).</p>
    <p>dundancy to improve reliability at higher layers of the hierarchical tree. Despite these techniques, we find that in . of failures all redundant components in a network device group became unavailable (e.g., the pair of switches that comprise each node in the conventional network (Figure ) or both the uplinks from a switch). In one incident, the failure of a core switch (due to a faulty supervisor card) affected ten million users for about four hours. We found the main causes of these downtimes are network misconfigurations, firmware bugs, and faulty components (e.g., ports). With no obvious way to eliminate all failures from the top of the hierarchy, VLs approach is to broaden the topmost levels of the network so that the impact of failures is muted and performance degrades gracefully, moving from : redundancy to n:m redundancy.</p>
    <p>ciples and preview how they will be used in the VL design. Randomizing to Cope with Volatility: VL copes with</p>
    <p>the high divergence and unpredictability of data-center traffic matrices by using Valiant Load Balancing to do destinationindependent (e.g., random) traffic spreading across multiple intermediate nodes. We introduce our network topology suited for VLB in ., and the corresponding flow spreading mechanism in ..</p>
    <p>VLB, in theory, ensures a non-interfering packet switched network [], the counterpart of a non-blocking circuit switched network, as long as (a) traffic spreading ratios are uniform, and (b) the offered traffic patterns do not violate edge constraints (i.e., line card speeds). To meet the latter condition, we rely on TCPs end-to-end congestion control mechanism. While our mechanisms to realize VLB do not perfectly meet either of these conditions, we show in . that our schemes performance is close to the optimum.</p>
    <p>Building on proven networking technology: VL is based on IP routing and forwarding technologies that are already available in commodity switches: link-state routing, equal-cost multi-path (ECMP) forwarding, IP anycasting, and IP multicasting. VL uses a link-state routing protocol to maintain the switch-level topology, but not to disseminateend hosts information. This strategyprotects switches from needing to learn voluminous, frequently-changing host information. Furthermore, the routing design uses ECMP forwarding along with anycast addresses to enable VLB with minimal control plane messaging or churn.</p>
    <p>Separating names from locators: The data center network must support agility, which means, in particular, support for hosting any service on any server, for rapid growing and shrinking of server pools, and for rapid virtual machine migration. In turn, this calls for separating names from locations. VLs addressing scheme separates server names, termed application-specific addresses (AAs), from their locations, termed location-specific addresses (LAs). VL uses a scalable, reliable directory system to maintain the mappings between names and locators. A shim layer running in the network stack on every server, called the VL agent, invokes the directory systems resolution service. We evaluate the performance of the directory system in ..</p>
    <p>Embracing End Systems: The rich and homogeneous programmability available at data-center hosts provides a mechanism to rapidly realize new functionality. For example, the VL agent enables fine-grained path control by adjusting the randomization used in VLB. The agent also replaces Ethernets ARP functionality with queries to the VL directory system. The directory system itself is also realized on servers, rather than switches, and thus offers flexibility, such as fine-grained, context-aware server access control and dynamic service re-provisioning.</p>
    <p>We next describe each aspect of the VL system and how they work together to implement a virtual layer- network. These aspects include the network topology, the addressing and routing design, and the directory that manages name-locator mappings.</p>
    <p>topologies have poor bisection bandwidth and are also susceptible to major disruptions due to device failures at the highest levels. Rather than scale up individual network devices with more capacity and features, we scale out the devices  build a broad network offering huge aggregate capacity using a large number of simple, inexpensive devices, as shown in Figure . This is an example of a folded Clos network [] where the links between the Intermediate switches and the Aggregation switches form a complete bipartite graph. As in the conventional topology, ToRs connect to two Aggregation switches, but the large number of paths between every two Aggregation switches means that if there are n Intermediate switches, the failure of any one of them reduces the bisection bandwidth by only 1/na desirable graceful degradation of bandwidth that we evaluate in .. Further, it is easy and less expensive to build a Clos network for which there is no over-subscription (further discussion on cost is in ). For example, in Figure , we use DA-port Aggregation and DI -port Intermediate switches, and connect these switches such that the capacity between each layer is DI DA/2 times the link capacity.</p>
    <p>The Clos topology is exceptionally well suited for VLB in that by indirectly forwarding traffic through an Intermediate switch at the top tier or spine of the network, the network can provide bandwidth guarantees for any traffic matrices subject to the hose model. Meanwhile, routing is extremely simple and resilient on this topology  take a random path up to a random intermediate switch and a random path down to a destination ToR switch.</p>
    <p>VL leverages the fact that at every generation of technology, switch-to-switch links are typically faster than server-to-switch links, and trends suggest that this gap will remain. Our current design uses G server links and G switch links, and the next design point will probably be G server links with G switch links. By leveraging this gap, we reduce the number of cables required to implement the Clos (as compared with a fat-tree []), and we simplify the task of spreading load over the links (.).</p>
    <p>[Greenburg, Hamilton, Jain, Kandula, Kim, Lahiri, Maltz, Patel, Sengupta, SIGCOMM09]</p>
  </div>
  <div class="page">
    <p>The VL2 topology</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>!&quot;#</p>
    <p>$%&amp;</p>
    <p>. . .</p>
    <p>. . . .</p>
    <p>'(()</p>
    <p>DA/2 x 10G</p>
    <p>DA/2 x 10G</p>
    <p>DI x10G</p>
    <p>DI x Aggregate Switches</p>
    <p>InternetLink-state network carrying only LAs</p>
    <p>(e.g., 10/8) DA/2 x Intermediate Switches</p>
    <p>Fungible pool of servers owning AAs</p>
    <p>(e.g., 20/8)</p>
    <p>Figure : An example Clos network between Aggregation and Intermediate switches provides a richly-connected backbone wellsuited for VLB. The network is built with two separate address families  topologically significant Locator Addresses (LAs) and flat Application Addresses (AAs).</p>
    <p>dundancy to improve reliability at higher layers of the hierarchical tree. Despite these techniques, we find that in . of failures all redundant components in a network device group became unavailable (e.g., the pair of switches that comprise each node in the conventional network (Figure ) or both the uplinks from a switch). In one incident, the failure of a core switch (due to a faulty supervisor card) affected ten million users for about four hours. We found the main causes of these downtimes are network misconfigurations, firmware bugs, and faulty components (e.g., ports). With no obvious way to eliminate all failures from the top of the hierarchy, VLs approach is to broaden the topmost levels of the network so that the impact of failures is muted and performance degrades gracefully, moving from : redundancy to n:m redundancy.</p>
    <p>ciples and preview how they will be used in the VL design. Randomizing to Cope with Volatility: VL copes with</p>
    <p>the high divergence and unpredictability of data-center traffic matrices by using Valiant Load Balancing to do destinationindependent (e.g., random) traffic spreading across multiple intermediate nodes. We introduce our network topology suited for VLB in ., and the corresponding flow spreading mechanism in ..</p>
    <p>VLB, in theory, ensures a non-interfering packet switched network [], the counterpart of a non-blocking circuit switched network, as long as (a) traffic spreading ratios are uniform, and (b) the offered traffic patterns do not violate edge constraints (i.e., line card speeds). To meet the latter condition, we rely on TCPs end-to-end congestion control mechanism. While our mechanisms to realize VLB do not perfectly meet either of these conditions, we show in . that our schemes performance is close to the optimum.</p>
    <p>Building on proven networking technology: VL is based on IP routing and forwarding technologies that are already available in commodity switches: link-state routing, equal-cost multi-path (ECMP) forwarding, IP anycasting, and IP multicasting. VL uses a link-state routing protocol to maintain the switch-level topology, but not to disseminateend hosts information. This strategyprotects switches from needing to learn voluminous, frequently-changing host information. Furthermore, the routing design uses ECMP forwarding along with anycast addresses to enable VLB with minimal control plane messaging or churn.</p>
    <p>Separating names from locators: The data center network must support agility, which means, in particular, support for hosting any service on any server, for rapid growing and shrinking of server pools, and for rapid virtual machine migration. In turn, this calls for separating names from locations. VLs addressing scheme separates server names, termed application-specific addresses (AAs), from their locations, termed location-specific addresses (LAs). VL uses a scalable, reliable directory system to maintain the mappings between names and locators. A shim layer running in the network stack on every server, called the VL agent, invokes the directory systems resolution service. We evaluate the performance of the directory system in ..</p>
    <p>Embracing End Systems: The rich and homogeneous programmability available at data-center hosts provides a mechanism to rapidly realize new functionality. For example, the VL agent enables fine-grained path control by adjusting the randomization used in VLB. The agent also replaces Ethernets ARP functionality with queries to the VL directory system. The directory system itself is also realized on servers, rather than switches, and thus offers flexibility, such as fine-grained, context-aware server access control and dynamic service re-provisioning.</p>
    <p>We next describe each aspect of the VL system and how they work together to implement a virtual layer- network. These aspects include the network topology, the addressing and routing design, and the directory that manages name-locator mappings.</p>
    <p>topologies have poor bisection bandwidth and are also susceptible to major disruptions due to device failures at the highest levels. Rather than scale up individual network devices with more capacity and features, we scale out the devices  build a broad network offering huge aggregate capacity using a large number of simple, inexpensive devices, as shown in Figure . This is an example of a folded Clos network [] where the links between the Intermediate switches and the Aggregation switches form a complete bipartite graph. As in the conventional topology, ToRs connect to two Aggregation switches, but the large number of paths between every two Aggregation switches means that if there are n Intermediate switches, the failure of any one of them reduces the bisection bandwidth by only 1/na desirable graceful degradation of bandwidth that we evaluate in .. Further, it is easy and less expensive to build a Clos network for which there is no over-subscription (further discussion on cost is in ). For example, in Figure , we use DA-port Aggregation and DI -port Intermediate switches, and connect these switches such that the capacity between each layer is DI DA/2 times the link capacity.</p>
    <p>The Clos topology is exceptionally well suited for VLB in that by indirectly forwarding traffic through an Intermediate switch at the top tier or spine of the network, the network can provide bandwidth guarantees for any traffic matrices subject to the hose model. Meanwhile, routing is extremely simple and resilient on this topology  take a random path up to a random intermediate switch and a random path down to a destination ToR switch.</p>
    <p>VL leverages the fact that at every generation of technology, switch-to-switch links are typically faster than server-to-switch links, and trends suggest that this gap will remain. Our current design uses G server links and G switch links, and the next design point will probably be G server links with G switch links. By leveraging this gap, we reduce the number of cables required to implement the Clos (as compared with a fat-tree []), and we simplify the task of spreading load over the links (.).</p>
    <p>High-degree switches</p>
  </div>
  <div class="page">
    <p>The VL2 topology</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>!&quot;#</p>
    <p>$%&amp;</p>
    <p>. . .</p>
    <p>. . . .</p>
    <p>'(()</p>
    <p>DA/2 x 10G</p>
    <p>DA/2 x 10G</p>
    <p>DI x10G</p>
    <p>DI x Aggregate Switches</p>
    <p>InternetLink-state network carrying only LAs</p>
    <p>(e.g., 10/8) DA/2 x Intermediate Switches</p>
    <p>Fungible pool of servers owning AAs</p>
    <p>(e.g., 20/8)</p>
    <p>Figure : An example Clos network between Aggregation and Intermediate switches provides a richly-connected backbone wellsuited for VLB. The network is built with two separate address families  topologically significant Locator Addresses (LAs) and flat Application Addresses (AAs).</p>
    <p>dundancy to improve reliability at higher layers of the hierarchical tree. Despite these techniques, we find that in . of failures all redundant components in a network device group became unavailable (e.g., the pair of switches that comprise each node in the conventional network (Figure ) or both the uplinks from a switch). In one incident, the failure of a core switch (due to a faulty supervisor card) affected ten million users for about four hours. We found the main causes of these downtimes are network misconfigurations, firmware bugs, and faulty components (e.g., ports). With no obvious way to eliminate all failures from the top of the hierarchy, VLs approach is to broaden the topmost levels of the network so that the impact of failures is muted and performance degrades gracefully, moving from : redundancy to n:m redundancy.</p>
    <p>ciples and preview how they will be used in the VL design. Randomizing to Cope with Volatility: VL copes with</p>
    <p>the high divergence and unpredictability of data-center traffic matrices by using Valiant Load Balancing to do destinationindependent (e.g., random) traffic spreading across multiple intermediate nodes. We introduce our network topology suited for VLB in ., and the corresponding flow spreading mechanism in ..</p>
    <p>VLB, in theory, ensures a non-interfering packet switched network [], the counterpart of a non-blocking circuit switched network, as long as (a) traffic spreading ratios are uniform, and (b) the offered traffic patterns do not violate edge constraints (i.e., line card speeds). To meet the latter condition, we rely on TCPs end-to-end congestion control mechanism. While our mechanisms to realize VLB do not perfectly meet either of these conditions, we show in . that our schemes performance is close to the optimum.</p>
    <p>Building on proven networking technology: VL is based on IP routing and forwarding technologies that are already available in commodity switches: link-state routing, equal-cost multi-path (ECMP) forwarding, IP anycasting, and IP multicasting. VL uses a link-state routing protocol to maintain the switch-level topology, but not to disseminateend hosts information. This strategyprotects switches from needing to learn voluminous, frequently-changing host information. Furthermore, the routing design uses ECMP forwarding along with anycast addresses to enable VLB with minimal control plane messaging or churn.</p>
    <p>Separating names from locators: The data center network must support agility, which means, in particular, support for hosting any service on any server, for rapid growing and shrinking of server pools, and for rapid virtual machine migration. In turn, this calls for separating names from locations. VLs addressing scheme separates server names, termed application-specific addresses (AAs), from their locations, termed location-specific addresses (LAs). VL uses a scalable, reliable directory system to maintain the mappings between names and locators. A shim layer running in the network stack on every server, called the VL agent, invokes the directory systems resolution service. We evaluate the performance of the directory system in ..</p>
    <p>Embracing End Systems: The rich and homogeneous programmability available at data-center hosts provides a mechanism to rapidly realize new functionality. For example, the VL agent enables fine-grained path control by adjusting the randomization used in VLB. The agent also replaces Ethernets ARP functionality with queries to the VL directory system. The directory system itself is also realized on servers, rather than switches, and thus offers flexibility, such as fine-grained, context-aware server access control and dynamic service re-provisioning.</p>
    <p>We next describe each aspect of the VL system and how they work together to implement a virtual layer- network. These aspects include the network topology, the addressing and routing design, and the directory that manages name-locator mappings.</p>
    <p>topologies have poor bisection bandwidth and are also susceptible to major disruptions due to device failures at the highest levels. Rather than scale up individual network devices with more capacity and features, we scale out the devices  build a broad network offering huge aggregate capacity using a large number of simple, inexpensive devices, as shown in Figure . This is an example of a folded Clos network [] where the links between the Intermediate switches and the Aggregation switches form a complete bipartite graph. As in the conventional topology, ToRs connect to two Aggregation switches, but the large number of paths between every two Aggregation switches means that if there are n Intermediate switches, the failure of any one of them reduces the bisection bandwidth by only 1/na desirable graceful degradation of bandwidth that we evaluate in .. Further, it is easy and less expensive to build a Clos network for which there is no over-subscription (further discussion on cost is in ). For example, in Figure , we use DA-port Aggregation and DI -port Intermediate switches, and connect these switches such that the capacity between each layer is DI DA/2 times the link capacity.</p>
    <p>The Clos topology is exceptionally well suited for VLB in that by indirectly forwarding traffic through an Intermediate switch at the top tier or spine of the network, the network can provide bandwidth guarantees for any traffic matrices subject to the hose model. Meanwhile, routing is extremely simple and resilient on this topology  take a random path up to a random intermediate switch and a random path down to a destination ToR switch.</p>
    <p>VL leverages the fact that at every generation of technology, switch-to-switch links are typically faster than server-to-switch links, and trends suggest that this gap will remain. Our current design uses G server links and G switch links, and the next design point will probably be G server links with G switch links. By leveraging this gap, we reduce the number of cables required to implement the Clos (as compared with a fat-tree []), and we simplify the task of spreading load over the links (.).</p>
    <p>High-degree switches</p>
    <p>Low-degree switches</p>
  </div>
  <div class="page">
    <p>The VL2 topology</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>!&quot;#</p>
    <p>$%&amp;</p>
    <p>. . .</p>
    <p>. . . .</p>
    <p>'(()</p>
    <p>DA/2 x 10G</p>
    <p>DA/2 x 10G</p>
    <p>DI x10G</p>
    <p>DI x Aggregate Switches</p>
    <p>InternetLink-state network carrying only LAs</p>
    <p>(e.g., 10/8) DA/2 x Intermediate Switches</p>
    <p>Fungible pool of servers owning AAs</p>
    <p>(e.g., 20/8)</p>
    <p>Figure : An example Clos network between Aggregation and Intermediate switches provides a richly-connected backbone wellsuited for VLB. The network is built with two separate address families  topologically significant Locator Addresses (LAs) and flat Application Addresses (AAs).</p>
    <p>dundancy to improve reliability at higher layers of the hierarchical tree. Despite these techniques, we find that in . of failures all redundant components in a network device group became unavailable (e.g., the pair of switches that comprise each node in the conventional network (Figure ) or both the uplinks from a switch). In one incident, the failure of a core switch (due to a faulty supervisor card) affected ten million users for about four hours. We found the main causes of these downtimes are network misconfigurations, firmware bugs, and faulty components (e.g., ports). With no obvious way to eliminate all failures from the top of the hierarchy, VLs approach is to broaden the topmost levels of the network so that the impact of failures is muted and performance degrades gracefully, moving from : redundancy to n:m redundancy.</p>
    <p>ciples and preview how they will be used in the VL design. Randomizing to Cope with Volatility: VL copes with</p>
    <p>the high divergence and unpredictability of data-center traffic matrices by using Valiant Load Balancing to do destinationindependent (e.g., random) traffic spreading across multiple intermediate nodes. We introduce our network topology suited for VLB in ., and the corresponding flow spreading mechanism in ..</p>
    <p>VLB, in theory, ensures a non-interfering packet switched network [], the counterpart of a non-blocking circuit switched network, as long as (a) traffic spreading ratios are uniform, and (b) the offered traffic patterns do not violate edge constraints (i.e., line card speeds). To meet the latter condition, we rely on TCPs end-to-end congestion control mechanism. While our mechanisms to realize VLB do not perfectly meet either of these conditions, we show in . that our schemes performance is close to the optimum.</p>
    <p>Building on proven networking technology: VL is based on IP routing and forwarding technologies that are already available in commodity switches: link-state routing, equal-cost multi-path (ECMP) forwarding, IP anycasting, and IP multicasting. VL uses a link-state routing protocol to maintain the switch-level topology, but not to disseminateend hosts information. This strategyprotects switches from needing to learn voluminous, frequently-changing host information. Furthermore, the routing design uses ECMP forwarding along with anycast addresses to enable VLB with minimal control plane messaging or churn.</p>
    <p>Separating names from locators: The data center network must support agility, which means, in particular, support for hosting any service on any server, for rapid growing and shrinking of server pools, and for rapid virtual machine migration. In turn, this calls for separating names from locations. VLs addressing scheme separates server names, termed application-specific addresses (AAs), from their locations, termed location-specific addresses (LAs). VL uses a scalable, reliable directory system to maintain the mappings between names and locators. A shim layer running in the network stack on every server, called the VL agent, invokes the directory systems resolution service. We evaluate the performance of the directory system in ..</p>
    <p>Embracing End Systems: The rich and homogeneous programmability available at data-center hosts provides a mechanism to rapidly realize new functionality. For example, the VL agent enables fine-grained path control by adjusting the randomization used in VLB. The agent also replaces Ethernets ARP functionality with queries to the VL directory system. The directory system itself is also realized on servers, rather than switches, and thus offers flexibility, such as fine-grained, context-aware server access control and dynamic service re-provisioning.</p>
    <p>We next describe each aspect of the VL system and how they work together to implement a virtual layer- network. These aspects include the network topology, the addressing and routing design, and the directory that manages name-locator mappings.</p>
    <p>topologies have poor bisection bandwidth and are also susceptible to major disruptions due to device failures at the highest levels. Rather than scale up individual network devices with more capacity and features, we scale out the devices  build a broad network offering huge aggregate capacity using a large number of simple, inexpensive devices, as shown in Figure . This is an example of a folded Clos network [] where the links between the Intermediate switches and the Aggregation switches form a complete bipartite graph. As in the conventional topology, ToRs connect to two Aggregation switches, but the large number of paths between every two Aggregation switches means that if there are n Intermediate switches, the failure of any one of them reduces the bisection bandwidth by only 1/na desirable graceful degradation of bandwidth that we evaluate in .. Further, it is easy and less expensive to build a Clos network for which there is no over-subscription (further discussion on cost is in ). For example, in Figure , we use DA-port Aggregation and DI -port Intermediate switches, and connect these switches such that the capacity between each layer is DI DA/2 times the link capacity.</p>
    <p>The Clos topology is exceptionally well suited for VLB in that by indirectly forwarding traffic through an Intermediate switch at the top tier or spine of the network, the network can provide bandwidth guarantees for any traffic matrices subject to the hose model. Meanwhile, routing is extremely simple and resilient on this topology  take a random path up to a random intermediate switch and a random path down to a destination ToR switch.</p>
    <p>VL leverages the fact that at every generation of technology, switch-to-switch links are typically faster than server-to-switch links, and trends suggest that this gap will remain. Our current design uses G server links and G switch links, and the next design point will probably be G server links with G switch links. By leveraging this gap, we reduce the number of cables required to implement the Clos (as compared with a fat-tree []), and we simplify the task of spreading load over the links (.).</p>
    <p>High-degree switches</p>
    <p>Low-degree switches</p>
    <p>ToRs  Servers</p>
  </div>
  <div class="page">
    <p>Rewiring VL2</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>!&quot;#</p>
    <p>$%&amp;</p>
    <p>. . .</p>
    <p>. . . .</p>
    <p>'(()</p>
    <p>DA/2 x 10G</p>
    <p>DA/2 x 10G</p>
    <p>DI x10G</p>
    <p>DI x Aggregate Switches</p>
    <p>InternetLink-state network carrying only LAs</p>
    <p>(e.g., 10/8) DA/2 x Intermediate Switches</p>
    <p>Fungible pool of servers owning AAs</p>
    <p>(e.g., 20/8)</p>
    <p>Figure : An example Clos network between Aggregation and Intermediate switches provides a richly-connected backbone wellsuited for VLB. The network is built with two separate address families  topologically significant Locator Addresses (LAs) and flat Application Addresses (AAs).</p>
    <p>dundancy to improve reliability at higher layers of the hierarchical tree. Despite these techniques, we find that in . of failures all redundant components in a network device group became unavailable (e.g., the pair of switches that comprise each node in the conventional network (Figure ) or both the uplinks from a switch). In one incident, the failure of a core switch (due to a faulty supervisor card) affected ten million users for about four hours. We found the main causes of these downtimes are network misconfigurations, firmware bugs, and faulty components (e.g., ports). With no obvious way to eliminate all failures from the top of the hierarchy, VLs approach is to broaden the topmost levels of the network so that the impact of failures is muted and performance degrades gracefully, moving from : redundancy to n:m redundancy.</p>
    <p>ciples and preview how they will be used in the VL design. Randomizing to Cope with Volatility: VL copes with</p>
    <p>the high divergence and unpredictability of data-center traffic matrices by using Valiant Load Balancing to do destinationindependent (e.g., random) traffic spreading across multiple intermediate nodes. We introduce our network topology suited for VLB in ., and the corresponding flow spreading mechanism in ..</p>
    <p>VLB, in theory, ensures a non-interfering packet switched network [], the counterpart of a non-blocking circuit switched network, as long as (a) traffic spreading ratios are uniform, and (b) the offered traffic patterns do not violate edge constraints (i.e., line card speeds). To meet the latter condition, we rely on TCPs end-to-end congestion control mechanism. While our mechanisms to realize VLB do not perfectly meet either of these conditions, we show in . that our schemes performance is close to the optimum.</p>
    <p>Building on proven networking technology: VL is based on IP routing and forwarding technologies that are already available in commodity switches: link-state routing, equal-cost multi-path (ECMP) forwarding, IP anycasting, and IP multicasting. VL uses a link-state routing protocol to maintain the switch-level topology, but not to disseminateend hosts information. This strategyprotects switches from needing to learn voluminous, frequently-changing host information. Furthermore, the routing design uses ECMP forwarding along with anycast addresses to enable VLB with minimal control plane messaging or churn.</p>
    <p>Separating names from locators: The data center network must support agility, which means, in particular, support for hosting any service on any server, for rapid growing and shrinking of server pools, and for rapid virtual machine migration. In turn, this calls for separating names from locations. VLs addressing scheme separates server names, termed application-specific addresses (AAs), from their locations, termed location-specific addresses (LAs). VL uses a scalable, reliable directory system to maintain the mappings between names and locators. A shim layer running in the network stack on every server, called the VL agent, invokes the directory systems resolution service. We evaluate the performance of the directory system in ..</p>
    <p>Embracing End Systems: The rich and homogeneous programmability available at data-center hosts provides a mechanism to rapidly realize new functionality. For example, the VL agent enables fine-grained path control by adjusting the randomization used in VLB. The agent also replaces Ethernets ARP functionality with queries to the VL directory system. The directory system itself is also realized on servers, rather than switches, and thus offers flexibility, such as fine-grained, context-aware server access control and dynamic service re-provisioning.</p>
    <p>We next describe each aspect of the VL system and how they work together to implement a virtual layer- network. These aspects include the network topology, the addressing and routing design, and the directory that manages name-locator mappings.</p>
    <p>topologies have poor bisection bandwidth and are also susceptible to major disruptions due to device failures at the highest levels. Rather than scale up individual network devices with more capacity and features, we scale out the devices  build a broad network offering huge aggregate capacity using a large number of simple, inexpensive devices, as shown in Figure . This is an example of a folded Clos network [] where the links between the Intermediate switches and the Aggregation switches form a complete bipartite graph. As in the conventional topology, ToRs connect to two Aggregation switches, but the large number of paths between every two Aggregation switches means that if there are n Intermediate switches, the failure of any one of them reduces the bisection bandwidth by only 1/na desirable graceful degradation of bandwidth that we evaluate in .. Further, it is easy and less expensive to build a Clos network for which there is no over-subscription (further discussion on cost is in ). For example, in Figure , we use DA-port Aggregation and DI -port Intermediate switches, and connect these switches such that the capacity between each layer is DI DA/2 times the link capacity.</p>
    <p>The Clos topology is exceptionally well suited for VLB in that by indirectly forwarding traffic through an Intermediate switch at the top tier or spine of the network, the network can provide bandwidth guarantees for any traffic matrices subject to the hose model. Meanwhile, routing is extremely simple and resilient on this topology  take a random path up to a random intermediate switch and a random path down to a destination ToR switch.</p>
    <p>VL leverages the fact that at every generation of technology, switch-to-switch links are typically faster than server-to-switch links, and trends suggest that this gap will remain. Our current design uses G server links and G switch links, and the next design point will probably be G server links with G switch links. By leveraging this gap, we reduce the number of cables required to implement the Clos (as compared with a fat-tree []), and we simplify the task of spreading load over the links (.).</p>
    <p>Connect ToRs proportional to Intermediate/Agg degree</p>
  </div>
  <div class="page">
    <p>Rewiring VL2</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>!&quot;#</p>
    <p>$%&amp;</p>
    <p>. . .</p>
    <p>. . . .</p>
    <p>'(()</p>
    <p>DA/2 x 10G</p>
    <p>DA/2 x 10G</p>
    <p>DI x10G</p>
    <p>DI x Aggregate Switches</p>
    <p>InternetLink-state network carrying only LAs</p>
    <p>(e.g., 10/8) DA/2 x Intermediate Switches</p>
    <p>Fungible pool of servers owning AAs</p>
    <p>(e.g., 20/8)</p>
    <p>Figure : An example Clos network between Aggregation and Intermediate switches provides a richly-connected backbone wellsuited for VLB. The network is built with two separate address families  topologically significant Locator Addresses (LAs) and flat Application Addresses (AAs).</p>
    <p>dundancy to improve reliability at higher layers of the hierarchical tree. Despite these techniques, we find that in . of failures all redundant components in a network device group became unavailable (e.g., the pair of switches that comprise each node in the conventional network (Figure ) or both the uplinks from a switch). In one incident, the failure of a core switch (due to a faulty supervisor card) affected ten million users for about four hours. We found the main causes of these downtimes are network misconfigurations, firmware bugs, and faulty components (e.g., ports). With no obvious way to eliminate all failures from the top of the hierarchy, VLs approach is to broaden the topmost levels of the network so that the impact of failures is muted and performance degrades gracefully, moving from : redundancy to n:m redundancy.</p>
    <p>ciples and preview how they will be used in the VL design. Randomizing to Cope with Volatility: VL copes with</p>
    <p>the high divergence and unpredictability of data-center traffic matrices by using Valiant Load Balancing to do destinationindependent (e.g., random) traffic spreading across multiple intermediate nodes. We introduce our network topology suited for VLB in ., and the corresponding flow spreading mechanism in ..</p>
    <p>VLB, in theory, ensures a non-interfering packet switched network [], the counterpart of a non-blocking circuit switched network, as long as (a) traffic spreading ratios are uniform, and (b) the offered traffic patterns do not violate edge constraints (i.e., line card speeds). To meet the latter condition, we rely on TCPs end-to-end congestion control mechanism. While our mechanisms to realize VLB do not perfectly meet either of these conditions, we show in . that our schemes performance is close to the optimum.</p>
    <p>Building on proven networking technology: VL is based on IP routing and forwarding technologies that are already available in commodity switches: link-state routing, equal-cost multi-path (ECMP) forwarding, IP anycasting, and IP multicasting. VL uses a link-state routing protocol to maintain the switch-level topology, but not to disseminateend hosts information. This strategyprotects switches from needing to learn voluminous, frequently-changing host information. Furthermore, the routing design uses ECMP forwarding along with anycast addresses to enable VLB with minimal control plane messaging or churn.</p>
    <p>Separating names from locators: The data center network must support agility, which means, in particular, support for hosting any service on any server, for rapid growing and shrinking of server pools, and for rapid virtual machine migration. In turn, this calls for separating names from locations. VLs addressing scheme separates server names, termed application-specific addresses (AAs), from their locations, termed location-specific addresses (LAs). VL uses a scalable, reliable directory system to maintain the mappings between names and locators. A shim layer running in the network stack on every server, called the VL agent, invokes the directory systems resolution service. We evaluate the performance of the directory system in ..</p>
    <p>Embracing End Systems: The rich and homogeneous programmability available at data-center hosts provides a mechanism to rapidly realize new functionality. For example, the VL agent enables fine-grained path control by adjusting the randomization used in VLB. The agent also replaces Ethernets ARP functionality with queries to the VL directory system. The directory system itself is also realized on servers, rather than switches, and thus offers flexibility, such as fine-grained, context-aware server access control and dynamic service re-provisioning.</p>
    <p>We next describe each aspect of the VL system and how they work together to implement a virtual layer- network. These aspects include the network topology, the addressing and routing design, and the directory that manages name-locator mappings.</p>
    <p>topologies have poor bisection bandwidth and are also susceptible to major disruptions due to device failures at the highest levels. Rather than scale up individual network devices with more capacity and features, we scale out the devices  build a broad network offering huge aggregate capacity using a large number of simple, inexpensive devices, as shown in Figure . This is an example of a folded Clos network [] where the links between the Intermediate switches and the Aggregation switches form a complete bipartite graph. As in the conventional topology, ToRs connect to two Aggregation switches, but the large number of paths between every two Aggregation switches means that if there are n Intermediate switches, the failure of any one of them reduces the bisection bandwidth by only 1/na desirable graceful degradation of bandwidth that we evaluate in .. Further, it is easy and less expensive to build a Clos network for which there is no over-subscription (further discussion on cost is in ). For example, in Figure , we use DA-port Aggregation and DI -port Intermediate switches, and connect these switches such that the capacity between each layer is DI DA/2 times the link capacity.</p>
    <p>The Clos topology is exceptionally well suited for VLB in that by indirectly forwarding traffic through an Intermediate switch at the top tier or spine of the network, the network can provide bandwidth guarantees for any traffic matrices subject to the hose model. Meanwhile, routing is extremely simple and resilient on this topology  take a random path up to a random intermediate switch and a random path down to a destination ToR switch.</p>
    <p>VL leverages the fact that at every generation of technology, switch-to-switch links are typically faster than server-to-switch links, and trends suggest that this gap will remain. Our current design uses G server links and G switch links, and the next design point will probably be G server links with G switch links. By leveraging this gap, we reduce the number of cables required to implement the Clos (as compared with a fat-tree []), and we simplify the task of spreading load over the links (.).</p>
    <p>Uniform-random interconnection} Connect ToRs proportional to Intermediate/Agg degree</p>
  </div>
  <div class="page">
    <p>Rewiring VL2</p>
    <p>Se rv</p>
    <p>er s</p>
    <p>at F</p>
    <p>ul l T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>(R</p>
    <p>at io</p>
    <p>O ve</p>
    <p>r V</p>
    <p>L2 )</p>
    <p>Aggregation Switch Degree</p>
  </div>
  <div class="page">
    <p>Rewiring VL2</p>
    <p>Se rv</p>
    <p>er s</p>
    <p>at F</p>
    <p>ul l T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>(R</p>
    <p>at io</p>
    <p>O ve</p>
    <p>r V</p>
    <p>L2 )</p>
    <p>Aggregation Switch Degree</p>
    <p>rack-to-rack</p>
  </div>
  <div class="page">
    <p>Rewiring VL2</p>
    <p>Se rv</p>
    <p>er s</p>
    <p>at F</p>
    <p>ul l T</p>
    <p>hr o ug</p>
    <p>hp ut</p>
    <p>(R</p>
    <p>at io</p>
    <p>O ve</p>
    <p>r V</p>
    <p>L2 )</p>
    <p>Aggregation Switch Degree</p>
    <p>all-to-all</p>
    <p>rack-to-rack</p>
  </div>
  <div class="page">
    <p>https://github.com/ankitsingla/topobench</p>
    <p>How do we design throughput optimal network topologies?</p>
  </div>
</Presentation>
