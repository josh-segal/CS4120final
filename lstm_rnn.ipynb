{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuasegal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n",
      "File \u001B[0;32m~/Coding/Jupyter/NLP/final/utils.py:18\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stopwords\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TfidfVectorizer\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/__init__.py:21\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mDetailed documentation and user guides are available at\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m[keras.io](https://keras.io).\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/models/__init__.py:18\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Functional\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/functional.py:24\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdtensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layout_map \u001B[38;5;28;01mas\u001B[39;00m layout_map_lib\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Input, Concatenate, Dropout\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('text_entailment_dataset/train.csv')\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=42)  # Shuffle with fixed seed for reproducibility\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Write the training and validation DataFrames to separate CSV files\n",
    "train_df.to_csv('text_entailment_dataset/train_data.csv', index=False)\n",
    "validation_df.to_csv('text_entailment_dataset/validation_data.csv', index=False)\n",
    "test_df.to_csv('text_entailment_dataset/test_data.csv', index=False)\n",
    "\n",
    "\n",
    "train_dataset = df = pd.read_csv('text_entailment_dataset/train_data.csv')\n",
    "validation_dataset = df = pd.read_csv('text_entailment_dataset/validation_data.csv')\n",
    "test_dataset = df = pd.read_csv('text_entailment_dataset/test_data.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_dataset.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset[[\"premise\"]] = train_dataset[[\"premise\"]].astype(str)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.change_lower)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.clean_data)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"hypothesis\"]] = train_dataset[[\"hypothesis\"]].astype(str)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.remover)\n",
    "\n",
    "validation_dataset[[\"premise\"]] = validation_dataset[[\"premise\"]].astype(str)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.change_lower)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.clean_data)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "validation_dataset[[\"hypothesis\"]] = validation_dataset[[\"hypothesis\"]].astype(str)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"premise\"]] = test_dataset[[\"premise\"]].astype(str)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.change_lower)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.clean_data)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"hypothesis\"]] = test_dataset[[\"hypothesis\"]].astype(str)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.remover)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_train = train_dataset['premise'] + train_dataset['hypothesis']\n",
    "# X_Val = validation_dataset['premise'] + validation_dataset['hypothesis']\n",
    "# X_test = test_dataset['premise'] + test_dataset['hypothesis']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize text data\n",
    "sentences = train_dataset['premise'].tolist() + train_dataset['hypothesis'].tolist()\n",
    "sentences = [sentence.split() for sentence in sentences]  # Split sentences into words\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(word2vec_model.wv.key_to_index) + 1)\n",
    "tokenizer.fit_on_texts(train_dataset['premise'].tolist() + train_dataset['hypothesis'].tolist())\n",
    "\n",
    "train_premise_sequences = tokenizer.texts_to_sequences(train_dataset['premise'].tolist())\n",
    "train_hypothesis_sequences = tokenizer.texts_to_sequences(train_dataset['hypothesis'].tolist())\n",
    "\n",
    "val_premise_sequences = tokenizer.texts_to_sequences(validation_dataset['premise'].tolist())\n",
    "val_hypothesis_sequences = tokenizer.texts_to_sequences(validation_dataset['hypothesis'].tolist())\n",
    "\n",
    "test_premise_sequences = tokenizer.texts_to_sequences(test_dataset['premise'].tolist())\n",
    "test_hypothesis_sequences = tokenizer.texts_to_sequences(test_dataset['hypothesis'].tolist())\n",
    "\n",
    "\n",
    "max_premise_length = max(len(seq) for seq in train_premise_sequences)\n",
    "max_hypothesis_length = max(len(seq) for seq in train_hypothesis_sequences)\n",
    "max_sequence_length = max(max_premise_length, max_hypothesis_length)\n",
    "\n",
    "\n",
    "train_premise_sequences = pad_sequences(train_premise_sequences, maxlen=max_sequence_length, padding='post')\n",
    "train_hypothesis_sequences = pad_sequences(train_hypothesis_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "val_premise_sequences = pad_sequences(val_premise_sequences, maxlen=max_sequence_length, padding='post')\n",
    "val_hypothesis_sequences = pad_sequences(val_hypothesis_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "test_premise_sequences = pad_sequences(test_premise_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_hypothesis_sequences = pad_sequences(test_hypothesis_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "print(\"Size of Vocabulary:\", len(tokenizer.word_index))\n",
    "print(\"Size of Train Premise Sequences:\", len(train_premise_sequences))\n",
    "print(\"Size of Train Hypothesis Sequences:\", len(train_hypothesis_sequences))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(max_sequence_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer object\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_premise_sequences.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Generate word embeddings for each word in the sequences and pad sequences\n",
    "# train_vectorized_data = []\n",
    "# for seq in train_sequences:\n",
    "#     # Generate embeddings for individual words in the sequence\n",
    "#     word_embeddings = [word2vec_model.wv[word_index] for word_index in seq]\n",
    "#     # Pad the sequence of embeddings to the maximum length\n",
    "#     padded_embeddings = pad_sequences([word_embeddings], maxlen=max_sequence_length, padding='post', dtype='float32')[0]\n",
    "#\n",
    "#     # Append padded embeddings to vectorized_data\n",
    "#     train_vectorized_data.append(padded_embeddings)\n",
    "#\n",
    "# # Convert vectorized data to numpy array\n",
    "# train_vectorized_data = np.array(train_vectorized_data)\n",
    "#\n",
    "# print(\"Vectorized Train Data Shape:\", train_vectorized_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Generate embeddings for validation set\n",
    "# val_vectorized_data = []\n",
    "# for seq in val_sequences:\n",
    "#     word_embeddings = [word2vec_model.wv[word_index] if word_index in word2vec_model.wv else np.zeros(word2vec_model.vector_size) for word_index in seq]\n",
    "#\n",
    "#     padded_embeddings = pad_sequences([word_embeddings], maxlen=max_sequence_length, padding='post', dtype='float32')[0]\n",
    "#\n",
    "#     val_vectorized_data.append(padded_embeddings)\n",
    "#\n",
    "# val_vectorized_data = np.array(val_vectorized_data)\n",
    "#\n",
    "# print(\"Vectorized Validation Data Shape:\", val_vectorized_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Generate embeddings for test set\n",
    "# test_vectorized_data = []\n",
    "# for seq in test_sequences:\n",
    "#     word_embeddings = [word2vec_model.wv[word_index] if word_index in word2vec_model.wv else np.zeros(word2vec_model.vector_size) for word_index in seq]\n",
    "#\n",
    "#     padded_embeddings = pad_sequences([word_embeddings], maxlen=max_sequence_length, padding='post', dtype='float32')[0]\n",
    "#\n",
    "#     test_vectorized_data.append(padded_embeddings)\n",
    "#\n",
    "# test_vectorized_data = np.array(test_vectorized_data)\n",
    "#\n",
    "# print(\"Vectorized Test Data Shape:\", test_vectorized_data.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_labels = train_dataset[\"label\"]\n",
    "validation_labels = validation_dataset[\"label\"]\n",
    "\n",
    "# Convert to one-hot encoded format\n",
    "num_classes = len(set(train_labels))  # Calculate the number of classes\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "print(\"train label shape:\", train_labels.shape)\n",
    "print(\"val label shape:\", validation_labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_generator(premise_sequences, hypothesis_sequences, labels, batch_size):\n",
    "    '''\n",
    "    Returns a data generator to be used for training or validation.\n",
    "\n",
    "    Yields batches of separate premise and hypothesis sequences (tokenized) and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "      premise_sequences: List of tokenized premise sequences\n",
    "      hypothesis_sequences: List of tokenized hypothesis sequences\n",
    "      labels: List of corresponding labels\n",
    "      batch_size: Number of sequences per batch (premise and hypothesis pairs)\n",
    "\n",
    "    Returns:\n",
    "      Data generator\n",
    "    '''\n",
    "\n",
    "    num_samples = len(premise_sequences)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "    idx = 0  # Initialize index to keep track of where we are in the dataset\n",
    "\n",
    "    while True:  # Loop indefinitely to generate batches\n",
    "        batch_premise_sequences = premise_sequences[idx:idx+batch_size]\n",
    "        batch_hypothesis_sequences = hypothesis_sequences[idx:idx+batch_size]\n",
    "        batch_labels = labels[idx:idx+batch_size]\n",
    "\n",
    "        # Yield the batch (separate premise and hypothesis sequences)\n",
    "        yield ([batch_premise_sequences, batch_hypothesis_sequences], batch_labels)\n",
    "\n",
    "        # Move to the next batch\n",
    "        idx += batch_size\n",
    "\n",
    "        # If we reach the end of the dataset, start over\n",
    "        if idx + batch_size > num_samples:\n",
    "            idx = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(train_vectorized_data.shape[1], \",\", train_vectorized_data.shape[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_generator = data_generator(train_premise_sequences, train_hypothesis_sequences, train_labels, batch_size)\n",
    "val_generator = data_generator(val_premise_sequences, val_hypothesis_sequences, validation_labels, batch_size)\n",
    "\n",
    "steps_per_epoch = len(train_premise_sequences) // batch_size\n",
    "validation_steps = len(val_premise_sequences) // batch_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_batch_X, sample_batch_y = next(train_generator)\n",
    "print(sample_batch_X[0].shape)\n",
    "print(sample_batch_X[1].shape)\n",
    "print(sample_batch_y.shape)\n",
    "\n",
    "sample_batch_X, sample_batch_y = next(val_generator)\n",
    "print(sample_batch_X[0].shape)\n",
    "print(sample_batch_X[1].shape)\n",
    "print(sample_batch_y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MetricsCallback(Callback):\n",
    "    def __init__(self, val_premise_sequences, val_hypothesis_sequences, val_labels):\n",
    "        super(MetricsCallback, self).__init__()\n",
    "        self.val_premise_sequences = val_premise_sequences\n",
    "        self.val_hypothesis_sequences = val_hypothesis_sequences\n",
    "        self.val_labels = val_labels\n",
    "        self.metrics = {'epoch': [], 'precision': [], 'recall': [], 'f1': [], 'val_loss': [], 'val_accuracy': [], 'loss': [], 'accuracy': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss, val_accuracy = self.model.evaluate(\n",
    "            [self.val_premise_sequences, self.val_hypothesis_sequences], self.val_labels, verbose=0)\n",
    "        val_pred = self.model.predict([self.val_premise_sequences, self.val_hypothesis_sequences])\n",
    "        val_pred_classes = np.argmax(val_pred, axis=1)  # Convert probabilities to classes\n",
    "\n",
    "        precision = precision_score(np.argmax(self.val_labels, axis=1), val_pred_classes, average='weighted')\n",
    "        recall = recall_score(np.argmax(self.val_labels, axis=1), val_pred_classes, average='weighted')\n",
    "        f1 = f1_score(np.argmax(self.val_labels, axis=1), val_pred_classes, average='weighted')\n",
    "\n",
    "        self.metrics['epoch'].append(epoch + 1)\n",
    "        self.metrics['precision'].append(precision)\n",
    "        self.metrics['recall'].append(recall)\n",
    "        self.metrics['f1'].append(f1)\n",
    "        self.metrics['val_loss'].append(val_loss)\n",
    "        self.metrics['val_accuracy'].append(val_accuracy)\n",
    "        self.metrics['loss'].append(logs['loss'])\n",
    "        self.metrics['accuracy'].append(logs['accuracy'])\n",
    "\n",
    "        print(\"Epoch {}: Validation Precision = {:.4f}, Recall = {:.4f}, F1 Score = {:.4f}, Val Loss = {:.4f}, Val Accuracy = {:.4f}\".format(epoch + 1, precision, recall, f1, val_loss, val_accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_callback = MetricsCallback(val_premise_sequences, val_hypothesis_sequences, validation_labels)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Best Hyperparameters: {'learning_rate': 0.001000649183264211, 'lstm_units': 67, 'dropout_rate': 0.23348344445560876, 'activation': 'relu', 'dense_units': 89}\n",
    "# Best Hyperparameters: {'dropout_rate_lstm': 0.2783331011414365, 'dropout_rate_dense': 0.13998996632720118}\n",
    "\n",
    "# Define embedding layer based on embedding matrix (assuming pre-trained Word2Vec)\n",
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "# Input layers for premise and hypothesis sequences\n",
    "premise_input = Input(shape=(max_sequence_length,))\n",
    "hypothesis_input = Input(shape=(max_sequence_length,))\n",
    "\n",
    "# Embedding layers for premise and hypothesis inputs\n",
    "embedded_premise = embedding_layer(premise_input)\n",
    "embedded_hypothesis = embedding_layer(hypothesis_input)\n",
    "\n",
    "# LSTM layers for premise and hypothesis sequences\n",
    "premise_lstm = LSTM(units=67, dropout=0.2783331011414365)(embedded_premise)\n",
    "hypothesis_lstm = LSTM(units=67, dropout=0.2783331011414365)(embedded_hypothesis)\n",
    "\n",
    "# Concatenate LSTM outputs\n",
    "combined_lstm = Concatenate()([premise_lstm, hypothesis_lstm])\n",
    "\n",
    "# Final dense layers for prediction with 3 outputs (entailment, neutral, contradiction)\n",
    "dense1 = Dense(units=89, activation='relu')(combined_lstm)\n",
    "dense1_dropout = Dropout(0.13998996632720118)(dense1)  # Add dropout after dense layer\n",
    "output = Dense(units=3, activation='softmax')(dense1_dropout)\n",
    "\n",
    "# Define the model with premise and hypothesis sequence inputs\n",
    "model = tf.keras.Model(inputs=[premise_input, hypothesis_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001000649183264211),\n",
    "              loss='categorical_crossentropy',  # Use categorical cross-entropy for one-hot encoded labels\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_premise_sequences))\n",
    "print(len(val_premise_sequences))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # subset_train_data = (train_premise_sequences[:1000], train_hypothesis_sequences[:1000])\n",
    "# # subset_val_data = (val_premise_sequences[:500], val_hypothesis_sequences[:500])\n",
    "# subset_train_data = (train_premise_sequences[:50000], train_hypothesis_sequences[:50000])\n",
    "# subset_val_data = (val_premise_sequences[:5000], val_hypothesis_sequences[:5000])\n",
    "#\n",
    "# # subset_train_labels = train_labels[:1000]\n",
    "# # subset_validation_labels = validation_labels[:500]\n",
    "# subset_train_labels = train_labels[:50000]\n",
    "# subset_validation_labels = validation_labels[:5000]\n",
    "#\n",
    "# # Define the search space\n",
    "# space = [\n",
    "#     Real(0.1, 0.5, name='dropout_rate_lstm'),  # Dropout rate for LSTM layers\n",
    "#     Real(0.1, 0.5, name='dropout_rate_dense')  # Dropout rate for dense layer\n",
    "# ]\n",
    "#\n",
    "# # Define the objective function\n",
    "# @use_named_args([\n",
    "#     Real(0.1, 0.5, name='dropout_rate_lstm'),  # Dropout rate for LSTM layers\n",
    "#     Real(0.1, 0.5, name='dropout_rate_dense')  # Dropout rate for dense layer\n",
    "# ])\n",
    "#\n",
    "# #Best Hyperparameters: {'learning_rate': 0.001000649183264211, 'lstm_units': 67, 'dropout_rate': 0.23348344445560876, 'activation': 'relu', 'dense_units': 89}\n",
    "#\n",
    "# def objective(dropout_rate_lstm, dropout_rate_dense):\n",
    "#     # Define and compile the model with hyperparameters\n",
    "#     embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "#                                 output_dim=embedding_matrix.shape[1],\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 trainable=False)\n",
    "#\n",
    "#     premise_input = Input(shape=(max_sequence_length,))\n",
    "#     hypothesis_input = Input(shape=(max_sequence_length,))\n",
    "#\n",
    "#     embedded_premise = embedding_layer(premise_input)\n",
    "#     embedded_hypothesis = embedding_layer(hypothesis_input)\n",
    "#\n",
    "#     premise_lstm = LSTM(units=67, dropout=dropout_rate_lstm)(embedded_premise)\n",
    "#     hypothesis_lstm = LSTM(units=67, dropout=dropout_rate_lstm)(embedded_hypothesis)\n",
    "#\n",
    "#     combined_lstm = Concatenate()([premise_lstm, hypothesis_lstm])\n",
    "#\n",
    "#     dense1 = Dense(units=89, activation='relu')(combined_lstm)\n",
    "#     dense1_dropout = Dropout(dropout_rate_dense)(dense1)  # Add dropout after dense layer\n",
    "#     output = Dense(units=3, activation='softmax')(dense1_dropout)\n",
    "#\n",
    "#     model = tf.keras.Model(inputs=[premise_input, hypothesis_input], outputs=output)\n",
    "#\n",
    "#     #use tf.keras.optimizers.legacy.Adam\n",
    "#     model.compile(optimizer=tf.keras.optimizers.legacy.Adam  (learning_rate=0.001000649183264211),\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#\n",
    "#     # Train the model\n",
    "#     # use model.fit\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "#     history = model.fit(subset_train_data, subset_train_labels,\n",
    "#                         epochs=10,\n",
    "#                         validation_data=(subset_val_data, subset_validation_labels),\n",
    "#                         callbacks=[early_stopping, metrics_callback],\n",
    "#                         verbose=1)\n",
    "#\n",
    "#     # Return the validation loss\n",
    "#     return np.min(history.history['val_loss'])\n",
    "#\n",
    "# # Run Bayesian optimization\n",
    "# result = gp_minimize(objective, space, n_calls=10, random_state=42)\n",
    "#\n",
    "# # Get the best hyperparameters\n",
    "# best_hyperparameters = dict(zip(['dropout_rate_lstm', 'dropout_rate_dense'], result.x))\n",
    "# print(\"Best Hyperparameters:\", best_hyperparameters)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=10,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=validation_steps,\n",
    "                              callbacks=[metrics_callback, checkpoint])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = metrics_callback.metrics['epoch']\n",
    "precision = metrics_callback.metrics['precision']\n",
    "recall = metrics_callback.metrics['recall']\n",
    "f1 = metrics_callback.metrics['f1']\n",
    "accuracy = metrics_callback.metrics['accuracy']\n",
    "loss = metrics_callback.metrics['loss']\n",
    "val_accuracy = metrics_callback.metrics['val_accuracy']\n",
    "val_loss = metrics_callback.metrics['val_loss']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot all four metrics on one graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "\n",
    "plt.title('Metrics across Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.savefig(\"LSTM_RNN_val_loss_acc.pdf\")  # Save the plot before showing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot all four metrics on one graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, precision, label='Precision')\n",
    "plt.plot(epochs, recall, label='Recall')\n",
    "plt.plot(epochs, f1, label='F1 Score')\n",
    "plt.plot(epochs, accuracy, label='Accuracy')\n",
    "\n",
    "plt.title('Metrics across Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.savefig(\"LSTM_RNN_PRFA.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
